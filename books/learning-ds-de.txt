 PDF in Markdown Konverter
Debug-Ansicht
Ergebnis-Ansicht
Datenwissenschaft lernen
Mit Early Release ebooks erhalten Sie Bücher in ihrer frühesten

Form - der rohe und unbearbeitete Inhalt des Autors, während er schreibt -

damit Sie die Vorteile dieser Technologien nutzen können, lange bevor

die offizielle Freigabe dieser Titel.

Sam Lau, Deborah Nolan und Joseph Gonzalez
Datenwissenschaft lernen
Grundlagen der Programmierung und Statistik
Python verwenden
PekingPeking BostonBoston FarnhamFarnham SebastopolSebastopol TokioTokio

978-1-098-11293-
Datenwissenschaft lernen

von Sam Lau , Deborah Nolan , und Joseph Gonzalez

Copyright © 2023 O'Reilly Media. Alle Rechte vorbehalten.

Gedruckt in den Vereinigten Staaten von Amerika.

Herausgegeben von O'Reilly Media, Inc. , 1005 Gravenstein Highway North, Sebastopol, CA 95472.

O'Reilly-Bücher können für Bildungszwecke, geschäftliche Zwecke oder zur Verkaufsförderung erworben werden. Online-Ausgaben sind
sind für die meisten Titel ebenfalls erhältlich ( http://oreilly.com ). Weitere Informationen erhalten Sie von unserer Vertriebsabteilung für Unternehmen/Institute.
tionalen Vertriebsabteilung: 800-998-9938 oder corporate@oreilly.com.

Redakteure: Melissa Potter und Jessica Haberman
Produktions-Editor: Katherine Tozer
Innenarchitekt: David Futato

Umschlagdesigner: Karen Montgomery
Illustratorin: Kate Dullea
Mai 2023: Erste Ausgabe

Änderungshistorie für das Early Release

2022-02-09: Erste Veröffentlichung
2022-05-11: Zweite Veröffentlichung
2022-09-20: Dritte Veröffentlichung
2022-11-09: Vierte Veröffentlichung

Siehe http://oreilly.com/catalog/errata.csp?isbn=9781098113001 für Einzelheiten zur Veröffentlichung.

Das O'Reilly-Logo ist eine eingetragene Marke von O'Reilly Media, Inc. Learning Data Science, das Titelbild,
und die zugehörige Aufmachung sind Marken von O'Reilly Media, Inc.

Die in diesem Werk geäußerten Ansichten sind die der Autoren und stellen nicht die Ansichten des Herausgebers dar.
Der Herausgeber und die Autoren haben sich nach bestem Wissen und Gewissen bemüht, die Richtigkeit der Informationen und
der Herausgeber und die Autoren nach bestem Wissen und Gewissen bemüht sind, die Richtigkeit der in diesem Werk enthaltenen Informationen und Anweisungen zu gewährleisten, lehnen der Herausgeber und die Autoren jede Verantwortung für Fehler oder Auslassungen ab.
für Fehler oder Auslassungen, einschließlich und ohne Einschränkung der Verantwortung für Schäden, die durch die Verwendung von
oder dem Vertrauen auf dieses Werk. Die Nutzung der in diesem Werk enthaltenen Informationen und Anleitungen erfolgt auf eigene
Risiko. Wenn Codebeispiele oder andere Technologien, die in diesem Werk enthalten oder beschrieben sind, Open-Source
Open-Source-Lizenzen oder den Rechten am geistigen Eigentum anderer unterliegen, sind Sie dafür verantwortlich, dass Ihre Nutzung
mit diesen Lizenzen und/oder Rechten übereinstimmt.

Inhaltsübersicht
Fragen und Datenumfang.
Big Data und neue Möglichkeiten
Beispiel: Google Grippe-Trends
Zielpopulation, Zugangsrahmen, Stichprobe
Instrumente und Protokolle
Messung eines Naturphänomens
Genauigkeit
Arten von Verzerrungen
Arten der Variation
Zusammenfassung
Übungen
Simulation und Datenentwurf.
Das Urnenmodell
Stichprobenentwürfe
Stichprobenverteilung einer Statistik
Simulation der Stichprobenverteilung
Die Hypergeometrische
Beispiel: Simulation der Verzerrung und Varianz bei Wahlumfragen
Das Pennsylvania Urnenmodell
Ein Urnenmodell mit Verzerrungen
Durchführen größerer Umfragen
Beispiel: Simulation eines randomisierten Versuchs für einen Impfstoff
Umfang
Das Urnenmodell für zufällige Zuweisung
Beispiel: Messfehler bei der Luftqualität
Zusammenfassung
Übungen
Modellierung mit zusammenfassenden Statistiken.
Das Konstantenmodell
Verlustfunktionen
Mittlerer absoluter Fehler
Mittlerer quadratischer Fehler
Auswahl der Verlustfunktionen
Zusammenfassung
Übungen
Arbeiten mit Dataframes unter Verwendung von pandas.
Teilmenge
Über die Daten
DataFrames und Indizes
Schneiden von
Zeilen filtern
Beispiel: Seit wann ist Luna ein beliebter Name?
Mitbringsel
Aggregieren
Grundlegendes Gruppieren/Aggregieren
Gruppieren nach mehreren Spalten
Benutzerdefinierte Aggregationsfunktionen
Beispiel: Sind die Menschen bei Babynamen kreativer geworden?
Pivotieren
Mitnahmeeffekte
Verknüpfung von
Innere Verknüpfungen
Linke, rechte und äußere Joins
Beispiel: Beliebtheit von NYT-Namenskategorien
Mitbringsel
Umwandlung von
anwenden
Beispiel: Beliebtheit von "L"-Namen
Der Preis von Apply
Mitbringsel
Wie unterscheiden sich Datenrahmen von anderen Datendarstellungen?
Datenrahmen und Tabellenkalkulationen
Datenrahmen und Matrizen
Datenrahmen und Beziehungen
Zusammenfassung
Übungen
Arbeiten mit Beziehungen unter Verwendung von SQL.
Untergliederung
Über die Daten
Was ist eine Relation?
Zerschneiden von
Zeilen filtern
Beispiel: Seit wann ist Luna ein beliebter Name?
Mitbringsel
Aggregieren
Grundlegendes Gruppieren/Aggregieren
Gruppieren nach mehreren Spalten
Andere Aggregationsfunktionen
Beispiel: Sind die Menschen bei Babynamen kreativer geworden?
Mitbringsel
Verknüpfung von
Innere Joins
Linke, rechte und äußere Joins
Beispiel: Beliebtheit von NYT-Namenskategorien
Mitbringsel
Umwandlung von
SQL-Funktionen
Mehrstufige Abfragen mit einer WITH-Klausel
Beispiel: Beliebtheit von "L"-Namen
Mitbringsel
Wie unterscheiden sich Relationen von anderen Datendarstellungen?
Relationen und Tabellenkalkulationen
Beziehungen und Matrizen
Relationen und Datenrahmen
Schlussfolgerung
Übungen
Umgang mit Dateien.
Beispiele für Datenquellen
Umfrage des Warnnetzes für Drogenmissbrauch (DAWN)
Lebensmittelsicherheit in Restaurants in San Francisco
Dateiformate
Getrenntes Format
Format mit fester Breite
Hierarchische Formate
Locker strukturierte Formate
Dateikodierung
Dateigröße
Arbeiten mit großen Datensätzen
Die Shell und Befehlszeilen-Tools
Tabellenform und Granularität
Granularität von Restaurantinspektionen und Verstößen
Form und Granularität der DAWN-Umfrage
Zusammenfassung
KAPITEL 1

Fragen und Datenumfang

Ein Hinweis für die Leser von Early Release

Mit Early Release ebooks erhalten Sie die Bücher in ihrer frühesten Form, d. h. den unbearbeiteten und
unbearbeiteten Inhalt, so dass Sie die Vorteile dieser Technologien schon lange
vor der offiziellen Veröffentlichung dieser Titel.
Dies wird das 2. Kapitel des endgültigen Buches sein. Bitte beachten Sie, dass das GitHub-Repositorium
zu einem späteren Zeitpunkt aktiviert wird.
Wenn Sie Anmerkungen haben, wie wir den Inhalt und/oder die Beispiele in diesem Buch verbessern können
diesem Buch verbessern könnten, oder wenn Sie fehlendes Material in diesem Kapitel bemerken, wenden Sie sich bitte an den
Autor unter mpotter@oreilly.com.
Als Datenwissenschaftler verwenden wir Daten, um Fragen zu beantworten, und die Qualität der gesammelten Daten ist uns wichtig.

tionsprozess kann die Gültigkeit und Genauigkeit der Daten erheblich beeinflussen, die

die Stärke der Schlussfolgerungen, die wir aus einer Analyse ziehen, und die Entscheidungen, die wir treffen. Unter

In diesem Kapitel wird ein allgemeiner Ansatz zum Verständnis der Datenerhebung und

Bewertung der Nützlichkeit der Daten für die Beantwortung der interessierenden Frage. Idealerweise sollten wir

die Daten sollen repräsentativ für das untersuchte Phänomen sein, unabhängig davon, ob

Dieses Phänomen ist ein Merkmal der Bevölkerung, ein physikalisches Modell oder eine Art von

Sozialverhalten. Typischerweise enthalten unsere Daten keine vollständigen Informationen (der Umfang ist

in gewisser Weise eingeschränkt), aber wir wollen die Daten nutzen, um eine Population genau zu beschreiben.

eine wissenschaftliche Größe zu schätzen, die Form einer Beziehung zwischen Merkmalen abzuleiten,

oder zukünftige Ergebnisse vorhersagen. In all diesen Situationen, wenn unsere Daten nicht repräsentativ sind

des Gegenstands unserer Studie, dann können unsere Schlussfolgerungen begrenzt und möglicherweise irreführend sein,

oder sogar falsch.

7
1 Der Begriff "Geltungsbereich" wurde aus Hellersteins Kursnotizen über Geltungsbereich, Zeitlichkeit und Glaubwürdigkeit übernommen.
ness.
Um die Notwendigkeit zu verdeutlichen, über diese Themen nachzudenken, beginnen wir mit einem Beispiel aus dem

Macht von Big Data und was schiefgehen kann ("Big Data und neue Möglichkeiten" auf

Seite 8). Wir stellen dann einen Rahmen zur Verfügung, der Ihnen helfen kann, das Ziel Ihrer

Studie (Ihre Frage) mit dem Prozess der Datenerhebung. Wir bezeichnen dies als die Daten

Anwendungsbereich 1 und stellen eine Terminologie zur Beschreibung des Datenumfangs sowie Beispiele zur Verfügung

aus Erhebungen, Regierungsdaten, wissenschaftlichen Instrumenten und Online-Ressourcen. Später im

In diesem Kapitel gehen wir der Frage nach, was es bedeutet, dass Daten korrekt sind. Dort führen wir ein

verschiedene Formen von Verzerrungen und Abweichungen und beschreiben die Bedingungen, unter denen sie auftreten können.

Die Beispiele decken das gesamte Spektrum der Daten ab, die Sie möglicherweise benötigen.

als Datenwissenschaftler zu nutzen; diese Beispiele stammen aus der Wissenschaft, politischen Wahlen, öffentlichen

Gesundheit und Online-Gemeinschaften.

Big Data und neue Möglichkeiten
Die enorme Zunahme offen zugänglicher Daten hat neue Rollen und Möglichkeiten geschaffen.

nitäten in der Datenwissenschaft. Datenjournalisten suchen zum Beispiel nach interessanten Geschichten in Daten

ähnlich wie herkömmliche Berichterstatter auf der Suche nach Nachrichten. Der Lebenszyklus der Daten für

Der Datenjournalist beginnt mit der Suche nach vorhandenen Daten, die für ihn von Interesse sein könnten.

Geschichte zu erzählen, anstatt mit einer Forschungsfrage zu beginnen und zu suchen, wie man

neue Daten zu erheben oder vorhandene Daten zu nutzen, um die Frage zu beantworten.

Bürgerwissenschaftliche Projekte sind ein weiteres Beispiel. Sie engagieren viele Menschen (und Instru-

ments) bei der Datenerhebung. Gemeinsam werden diese Daten oft zur Verfügung gestellt

Forscher, die das Projekt organisieren, und oft werden sie in Repositories zur Verfügung gestellt

für die breite Öffentlichkeit zur weiteren Untersuchung.

Die Verfügbarkeit von administrativen/organisatorischen Daten bietet weitere Möglichkeiten.

Forscher können Daten aus wissenschaftlichen Studien beispielsweise mit medizinischen Daten verknüpfen, die

für Zwecke des Gesundheitswesens erhoben wurden, d. h. Verwaltungsdaten, die für die

die aus Gründen, die nicht direkt mit der Frage von Interesse zusammenhängen, erhoben werden, können nützlich sein

in anderen Bereichen. Solche Verknüpfungen können Datenwissenschaftlern helfen, die Möglichkeiten der

ihre Analysen und überprüfen die Qualität ihrer Daten. Darüber hinaus können die gefundenen Daten

digitale Spuren, wie z. B. Ihre Surfaktivitäten im Internet, Beiträge in sozialen Medien und

Online-Netzwerk von Freunden und Bekannten und kann recht komplex sein.

Wenn wir große Mengen an Verwaltungsdaten oder umfangreiche digitale Spuren haben, kann es

Es wäre verlockend, sie als endgültiger zu betrachten als Daten, die aus traditionellen

kleinere Forschungsstudien. Wir könnten diese großen Datensätze sogar als Ersatz betrachten.

für wissenschaftliche Studien oder im Wesentlichen eine Volkszählung. Diese Überschreitung wird als die

8 | Kapitel 1: Fragen und Datenumfang

2 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5754279/
"Big Data Hybris" (Lazer et al. 2014). Daten mit einem großen Umfang bedeuten nicht, dass wir

können wir grundlegende Fragen der Repräsentativität der Daten ignorieren, und wir können auch nicht ignorieren

Probleme mit der Messung, der Abhängigkeit und der Zuverlässigkeit. Ein bekanntes Beispiel ist die

Google Flu Trends Tracking-System.

Beispiel: Google Grippe-Trends
Die digitale Epidemiologie, ein neues Teilgebiet der Epidemiologie, nutzt Daten, die von

des öffentlichen Gesundheitssystems zur Untersuchung von Krankheitsmustern und Gesundheitsdynamik in der Bevölkerung.

ulationen^2 Das Google Flu Trends (GFT)-Verfolgungssystem war eines der ersten, das

Beispiele für digitale Epidemiologie. Im Jahr 2007 fanden Forscher heraus, dass die Zählung der

Suchanfragen nach grippebezogenen Begriffen konnte die Zahl der Grippefälle genau geschätzt werden.

Grippefälle. Das sorgte für Schlagzeilen und trug dazu bei, dass sich die Forscher für die Möglichkeit einer

von Big Data. GFT erfüllte jedoch nicht die Erwartungen und wurde im Jahr 2000 aufgegeben.

Was lief bei GFT schief? Immerhin wurden Millionen von digitalen Spuren aus dem Internet verwendet.

Abfragen nach Begriffen im Zusammenhang mit der Grippe, um die Grippeaktivität vorherzusagen. Trotz anfänglicher Erfolge, in

der Grippesaison 2011-2012 stellten die Datenwissenschaftler von Google fest, dass GFT kein Ersatz für

für mehr traditionell erhobene Daten von den Centers for Disease Control (CDC)

Überwachungsberichte, die von Laboratorien in den Vereinigten Staaten gesammelt wurden. Im Vergleich dazu

Sohn überschätzte GFT die CDC-Zahlen für 100 von 108 Wochen (siehe Abbildung 1-1).

Woche für Woche waren die GFT-Werte für die Grippefälle zu hoch, auch wenn sie

auf der Grundlage von Big Data.

Big Data und neue Möglichkeiten | 9
Abbildung 1-1. Wöchentliche Schätzungen von Google Flu Trend (GFT) für grippeähnliche Erkrankungen. Für 108

Wochen schätzte GFT (durchgezogene Linie) die tatsächlichen CDC-Berichte (gestrichelte Linie) um das 100-fache über.

Dargestellt sind auch die Vorhersagen eines Modells, das auf 3 Wochen alten CDC-Daten und saisonalen

Trends (gepunktete Linie).

Datenwissenschaftler stellten fest, dass die GFT kein Ersatz für traditionellere Sammlungen ist.

Daten von der CDC. Ein einfaches Modell, das aus früheren CDC-Berichten mit 3

Wochen alte CDC-Daten und saisonale Trends haben die Grippeprävalenz besser vorhergesagt

als die GFT. Das heißt, die GFT übersieht beträchtliche Informationen, die extrahiert werden könnten.

mit grundlegenden statistischen Methoden ausgewertet werden. Dies bedeutet nicht, dass große Daten, die von

Online-Aktivitäten nutzlos sind. In der Tat haben Forscher gezeigt, dass die Kombination von GFT

Daten mit CDC-Daten können sowohl die GFT-Vorhersagen als auch die

CDC-basiertes Modell (Lazer 2015). Es ist oft der Fall, dass die Kombination verschiedener

Ansätze führt zu Verbesserungen gegenüber einzelnen Methoden.

Das GFT-Beispiel zeigt uns, dass wir selbst dann, wenn wir über enorme Mengen an Informationen verfügen, die

die Zusammenhänge zwischen den Daten, dem Thema der Untersuchung und der Fragestellung

gefragt werden, sind von größter Bedeutung. Das Verständnis dieses Rahmens kann uns helfen, Antworten zu vermeiden.

die falsche Fragestellung, die Anwendung ungeeigneter Methoden auf die Daten und die Überbewertung der Daten

unsere Erkenntnisse.

Im Zeitalter von Big Data sind wir versucht, immer mehr Daten zu sammeln. Schließlich ist ein Zen-

sus gibt uns perfekte Informationen, sollten Big Data also nicht nahezu perfekt sein? Ein Schlüsselfaktor

10 | Kapitel 1: Fragen und Datenumfang

Zu beachten ist der Umfang der Daten. Welche Bevölkerungsgruppe wollen wir untersuchen? Wie

können wir auf Informationen über diese Bevölkerung zugreifen? Wen oder was untersuchen wir eigentlich?

tung? Die Antworten auf diese Fragen helfen uns, mögliche Lücken in unserem Ansatz zu erkennen. Dies ist

das Thema des nächsten Abschnitts.

Zielpopulation, Zugangsrahmen, Stichprobe
Ein wichtiger erster Schritt im Lebenszyklus der Daten ist die Formulierung der interessierenden Frage in

den Kontext des Fachgebiets und berücksichtigen den Zusammenhang zwischen der Frage und

die zur Beantwortung dieser Frage gesammelten Daten. Es ist eine gute Praxis, dies zu tun, bevor überhaupt

über die Analyse- oder Modellierungsschritte nachzudenken, weil dadurch möglicherweise eine Unstimmigkeit aufgedeckt wird

wenn die interessierende Frage mit diesen Daten nicht direkt beantwortet werden kann. Als Teil von

Herstellung einer Verbindung zwischen dem Datenerhebungsprozess und dem Thema der Untersuchung

identifizieren wir die Population, die Mittel für den Zugang zur Population, die Instrumente

der Messung und zusätzliche Protokolle, die bei der Erhebung verwendet werden. Diese Kon-

Daten helfen uns, den Umfang der Daten zu verstehen, und zwar unabhängig davon, ob wir Erkenntnisse gewinnen wollen

über eine Bevölkerung, eine wissenschaftliche Größe, ein physikalisches Modell, ein soziales Verhalten usw.

Die Zielpopulation besteht aus der Gesamtheit der Elemente, die Sie letztendlich beabsichtigen

zu beschreiben und Schlussfolgerungen zu ziehen. Mit Element meinen wir jene Individuen, die

die unsere Bevölkerung ausmachen. Das Element kann eine Person in einer Gruppe von Menschen sein, ein Wähler

bei einer Wahl, ein Tweet aus einer Sammlung von Tweets oder ein Bezirk in einem Bundesland. Wir haben einige...

Ein Element wird manchmal auch als Einheit oder als Atom bezeichnet.

Der Zugriffsrahmen ist die Sammlung der Elemente, auf die Sie für die Messung zugreifen können.

und Beobachtung. Dies sind die Einheiten, über die Sie Zugang zur Zielbevölkerung haben.

tion. Im Idealfall sind der Rahmen und die Population perfekt aufeinander abgestimmt, d. h. sie bestehen aus

die exakt gleichen Elemente. Die Einheiten in einem Zugangsrahmen können jedoch nur eine Teilmenge sein

der Zielpopulation; außerdem kann der Rahmen Einheiten enthalten, die nicht zur

auf die Bevölkerung. Zum Beispiel, um herauszufinden, wie ein Wähler bei einer Wahl abzustimmen gedenkt,

Sie könnten Leute per Telefon anrufen. Jemand, den Sie anrufen, ist vielleicht kein Wähler, also ist er in

in Ihrem Rahmen, aber nicht in der Bevölkerung. Andererseits kann ein Wähler, der nie antwortet

ein Anruf von einer unbekannten Nummer kann nicht erreicht werden, so dass sie in der Bevölkerung sind, aber

nicht in Ihrem Rahmen.

Die Stichprobe ist die Teilmenge der Einheiten, die aus dem Zugangsrahmen entnommen wird, um zu messen und zu beobachten,

und zu analysieren. Die Stichprobe liefert Ihnen die Daten, die Sie analysieren können, um Vorhersagen zu treffen oder allgemeine...

Erkenntnisse über die interessierende Population.

Der Inhalt des Zugangsrahmens im Vergleich zur Zielpopulation und die

die Methode zur Auswahl der in die Stichprobe aufzunehmenden Einheiten aus dem Rahmen sind wichtige Faktoren

bei der Entscheidung, ob die Daten als repräsentativ für die Zielgruppe angesehen werden können oder nicht

Bevölkerung. Wenn der Zugangsrahmen nicht repräsentativ für die Zielpopulation ist, dann muss die

Zielpopulation, Zugangsrahmen, Stichprobe | 11
sind die Daten aus der Stichprobe höchstwahrscheinlich auch nicht repräsentativ. Und, wenn die Einheiten

Wenn die Stichprobe unausgewogen ist, ergeben sich auch Probleme mit der Repräsentativität.

Bei der Festlegung des Datenumfangs müssen Sie auch Zeit und Ort berücksichtigen. Zum Beispiel kann die

Wirksamkeit eines Arzneimittels, das in einem Teil der Welt getestet wird, in dem eine Krankheit wütet

nicht so günstig mit einer Studie in einem anderen Teil der Welt vergleichen, wo

Die Hintergrundinfektionsraten sind niedriger (siehe Kapitel %s). Zusätzlich gesammelte Daten

zur Untersuchung von Veränderungen im Laufe der Zeit, wie bei den monatlichen Messungen

von CO2 in der Atmosphäre (siehe ???) und die wöchentliche Meldung von Google-Suchen nach

Die Vorhersage von Grippetrends hat eine zeitliche Struktur, die wir berücksichtigen müssen, wenn wir

die Daten untersuchen. In anderen Fällen können räumliche Muster in den Daten vorhanden sein. Für

So werden beispielsweise die weiter unten in diesem Abschnitt beschriebenen Daten zur Umweltgesundheit gemeldet.

für jeden Zählbezirk im Bundesstaat Kalifornien, und wir könnten z. B. Karten erstellen, die Folgendes zeigen

für räumliche Korrelationen.

Und wenn Sie die Daten nicht gesammelt haben, sollten Sie überlegen, wer sie gesammelt hat und zu welchem Zweck

Zweck. Dies ist jetzt besonders wichtig, da mehr Daten passiv gesammelt werden.

der mit einem bestimmten Ziel gesammelt wurden. Einen genauen Blick auf die gefundenen Daten werfen und fragen

selbst, ob und wie diese Daten zur Beantwortung Ihrer Frage verwendet werden können, kann

Sie verhindern, dass Sie eine fruchtlose Analyse durchführen oder unangemessene Schlussfolgerungen ziehen.

Für jedes der folgenden Beispiele beginnen wir mit einer allgemeinen Frage und grenzen sie ein auf

eine, die mit Daten beantwortet werden kann, und dabei identifizieren wir die Zielpopulation.

tion, Zugangsrahmen und Stichprobe. Diese Begriffe werden in einem Diagramm durch Kreise dargestellt

des Datenumfangs, und die Konfiguration ihrer Überschneidungen hilft, Schlüsselaspekte der

Umfang. Auch in jedem Beispiel beschreiben wir relevante zeitliche und räumliche Merkmale zum

Datenumfang.

Was macht die Mitglieder einer Online-Community aktiv? Der Inhalt von Wikipedia wird geschrieben

und wird von freiwilligen Mitgliedern der Wikipedia-Gemeinschaft bearbeitet. Diese Online-Kom-

Gemeinschaft ist für den Erfolg und die Lebendigkeit von Wikipedia entscheidend. Beim Versuch zu verstehen

wie Anreize für Mitglieder von Online-Gemeinschaften geschaffen werden können, führten Forscher eine

Experiment mit Wikipedia-Mitarbeitern als Probanden (Restivo und van de Rijt 2012). A

Eine eingeschränkte Version der allgemeinen Frage lautet: Erhöhen informelle Auszeichnungen die Aktivitätsrate?

lichkeit von Wikipedia-Beitragenden? Für dieses Experiment ist die Zielpopulation das Kollek-

die Zahl der aktiven Autoren, d. h. derjenigen, die mindestens einen Beitrag zu Wikipedia geleistet haben

in dem Monat vor Beginn der Studie. Außerdem wurde die Zielpopulation...

die auf die obersten 1 % der Beitragszahler beschränkt ist. Der Zugangsrahmen schloss jeden aus, der in

die Bevölkerung, die in diesem Monat einen informellen Anreiz erhalten hat. Der Zugangsrahmen

absichtlich einen Teil der Beitragszahler aus der Grundgesamtheit ausgeschlossen, weil die Forschungs

Diejenigen, die die Wirkung eines Anreizes messen wollen, und diejenigen, die bereits einen Anreiz erhalten haben

ein Anreiz könnte sich anders verhalten. (Siehe Abbildung 1-2).

12 | Kapitel 1: Fragen und Datenumfang

Abbildung 1-2. Der Zugriffsrahmen umfasst nicht die gesamte Population, da die Experi-

In die Berechnung flossen nur die Beitragszahler ein, die nicht bereits einen Anreiz erhalten hatten. Die

Die Stichprobe ist eine zufällig ausgewählte Teilmenge des Rahmens.

Bei der Stichprobe handelt es sich um eine nach dem Zufallsprinzip ausgewählte Gruppe von 200 Beitragszahlern aus der Grundgesamtheit. Die Stichprobe

der Beitragszahler wurden 90 Tage lang beobachtet und digitale Spuren ihrer Aktivitäten auf

Wikipedia gesammelt wurden. Beachten Sie, dass die Zahl der Mitwirkenden nicht statisch ist; es gibt

regelmäßiger Umsatz. Im Monat vor Beginn der Studie wurden mehr als 144.000 vol-

tern, die Inhalte für Wikipedia produzieren. Die Auswahl der Top-Beitragenden unter diesen

Gruppe schränkt die Verallgemeinerbarkeit der Ergebnisse ein, aber angesichts der Größe der Gruppe der Top

Beitragszahler, wenn sie durch eine informelle Belohnung beeinflusst werden können, um ihre Leistungen beizubehalten oder zu steigern

ihre Beiträge, was eine wertvolle Erkenntnis ist.

Bei vielen Experimenten und Studien haben wir nicht die Möglichkeit, alle Bevölkerungsgruppen einzubeziehen.

Einheiten in dem Rahmen. Häufig besteht der Zugangsrahmen aus Freiwilligen, die

bereit sind, an der Studie/dem Experiment teilzunehmen.

Wer wird die Wahl gewinnen? Der Ausgang der US-Präsidentschaftswahlen im Jahr 2016 nahm

Viele Menschen und viele Meinungsforscher waren überrascht. Die meisten Umfragen vor der Wahl sagten Clin-

ton würde Trump mit großem Vorsprung schlagen. Politische Umfragen sind eine Art der öffentlichen Meinung

Umfrage, die vor einer Wahl durchgeführt wird, um zu ermitteln, wen die Menschen wählen werden. Seit

Zielpopulation, Zugangsrahmen, Stichprobe | 13
3 https://projects.fivethirtyeight.com/pollster-ratings/
Meinungen im Laufe der Zeit ändern, reduziert sich der Fokus auf eine "Pferderennen"-Frage, bei der

Die Befragten werden gefragt, für wen sie bei einem Kopf-an-Kopf-Rennen stimmen würden, wenn die Wahl

tion waren morgen: Kandidat A oder Kandidat B.

Während des gesamten Präsidentschaftswahlkampfes werden regelmäßig Umfragen durchgeführt, und je näher die

Wahltag erwarten wir, dass die Umfragen das Ergebnis immer besser vorhersagen, da die Präferenzen

stabilisieren. Die Umfragen werden in der Regel auch landesweit durchgeführt und später kombiniert, um die

Vorhersagen für den Gesamtsieger. Aus diesen Gründen sind der Zeitpunkt und der Ort einer Umfrage

zählt. Auch der Meinungsforscher spielt eine Rolle; einige haben durchweg besser abgeschnitten als andere.

als andere^3.

Bei diesen Umfragen im Vorfeld der Wahlen besteht die Zielpopulation aus denjenigen, die an den Wahlen teilnehmen werden

die Wahl, die in diesem Beispiel die US-Präsidentschaftswahl 2016 war. Allerdings,

Die Meinungsforscher können nur raten, ob jemand bei der Wahl seine Stimme abgeben wird, so dass der Zugang

besteht aus Personen, die als wahrscheinliche Wähler angesehen werden (dies basiert in der Regel auf ihren bisherigen

Wahlbeteiligung, aber es können auch andere Faktoren herangezogen werden, um dies zu bestimmen), und da die Menschen

per Telefon kontaktiert werden, ist der Zugang auf Personen beschränkt, die einen Festnetzanschluss oder

Mobiltelefon. Die Stichprobe besteht aus den Personen im Frame, die ausgewählt werden

nach einem Zufallswahlverfahren. (Siehe Abbildung 1-3).

Abbildung 1-3. Diese Darstellung ist typisch für viele Umfragen. Der Zugangsrahmen enthält nicht

die gesamte Bevölkerung abdeckt und einige Personen einschließt, die nicht zur Bevölkerung gehören.

Später in Kapitel 2 werden wir die Auswirkungen auf die Wahlprognosen der Menschen

mangelnde Bereitschaft, ans Telefon zu gehen oder an der Umfrage teilzunehmen.

14 | Kapitel 1: Fragen und Datenumfang

Wie wirken sich Umweltgefahren auf die Gesundheit des Einzelnen aus? Um diese Frage zu beantworten,

die kalifornische Umweltschutzbehörde (CalEPA), das California Office of

Health Hazard Assessment (OEHHA), und die Öffentlichkeit entwickelten den CalEnviroScreen

Projekt. Das Projekt untersucht die Zusammenhänge zwischen der Gesundheit der Bevölkerung und der Um- welt.

Verschmutzung in kalifornischen Gemeinden anhand von Daten, die aus verschiedenen Quellen stammen.

enthält demografische Zusammenfassungen aus der US-Volkszählung, Gesundheitsstatistiken aus dem Cal-

ifornia Office of Statewide Health Planning and Development, und Verschmutzungsmessungen

Die Daten stammen von Luftüberwachungsstationen im ganzen Bundesstaat, die von der kalifornischen

Luft.

Idealerweise wollen wir die Menschen in Kalifornien untersuchen und die Auswirkungen dieser Umwelteinflüsse bewerten.

ronalen Gefahren für die Gesundheit des Einzelnen. In dieser Situation können die Daten jedoch

nur auf der Ebene eines Zählbezirks erhalten werden. Der Zugangsrahmen besteht aus Gruppen von

Einwohner, die in demselben Zählgebiet leben. Die Einheiten im Rahmen sind also Zählbezirke

und die Stichprobe ist eine Volkszählung - alle Gebiete -, da die Daten für alle Gebiete bereitgestellt werden

im Staat. (Siehe Abbildung 1-4).

Abbildung 1-4. Das Gitter im Zugangsrahmen stellt die Volkszählungsgebiete dar. Die Bevölkerung,

Rahmen und die Stichprobe decken alle Kalifornier ab, aber das Raster beschränkt die Messungen auf die Ebene der

Volkszählungsgebiet.

Leider können wir die Informationen in einem Trakt nicht deaggregieren, um zu untersuchen, was

mit einer einzelnen Person geschieht. Diese Aggregation hat Auswirkungen auf die Frage, die wir

und die Schlussfolgerungen, die wir daraus ziehen können.

Zielpopulation, Zugangsrahmen, Stichprobe | 15
Diese Beispiele haben einige der Konfigurationen eines Ziel- und Zugriffsrahmens gezeigt,

Die Übungen am Ende dieses Kapitels enthalten einige Beispiele für die

weitere Beispiele. Wenn ein Rahmen nicht alle erreicht, sollten wir überlegen, wie dieser

fehlende Informationen unsere Ergebnisse beeinflussen könnten. In ähnlicher Weise fragen wir, was passieren könnte

wenn ein Frame Personen umfasst, die nicht zur Grundgesamtheit gehören. Außerdem sind die Techniken für

Die Ziehung der Stichprobe kann sich darauf auswirken, wie repräsentativ die Stichprobe für die Grundgesamtheit ist.

Wenn Sie über die Verallgemeinerbarkeit der Ergebnisse Ihrer Daten nachdenken, sollten Sie auch

die Qualität der für die Datenerhebung verwendeten Instrumente und Verfahren berücksichtigen. Wenn

Ihre Stichprobe ist eine Volkszählung, die Ihrem Ziel entspricht, aber die Informationen sind schlecht gesammelt.

dann sind Ihre Ergebnisse von geringem Wert. Dies ist das Thema des nächsten Abschnitts.

Instrumente und Protokolle
Wenn wir den Umfang der Daten betrachten, berücksichtigen wir auch das verwendete Instrument

für die Durchführung der Messungen und das Verfahren zur Durchführung der Messungen, das wir als

das Protokoll. Bei einer Umfrage ist das Instrument in der Regel ein Fragebogen, den eine Einzelperson

ual in den Antworten der Stichprobe. Das Protokoll für eine Umfrage beinhaltet, wie die Stichprobe

die Auswahl, die Weiterverfolgung von Nichtbeantwortern, die Schulung der Interviewer, der Schutz von

Vertraulichkeit, usw.

Gute Instrumente und Protokolle sind für alle Arten der Datenerhebung wichtig. Wenn wir

Wenn wir ein Naturphänomen, wie z. B. die Lichtgeschwindigkeit, messen wollen, müssen wir es quan-

die Genauigkeit des Geräts zu überprüfen. Das Protokoll für die Kalibrierung des Geräts und

Die Durchführung von Messungen ist für die Erzielung genauer Messungen von entscheidender Bedeutung. Instrumente können

und Messungen können mit der Zeit abweichen, was zu schlechten, hochgradig

ungenaue Messungen (siehe ???).

Auch bei Experimenten sind Protokolle von entscheidender Bedeutung. Idealerweise sollte jeder Faktor, der das Ergebnis beeinflussen kann

das Ergebnis des Experiments kontrolliert wird. Zum Beispiel die Temperatur, die Tageszeit, der Kon-

Vertraulichkeit der medizinischen Aufzeichnungen, und sogar die Reihenfolge der Messungen muss

konsequent sein, um auszuschließen, dass sich diese Faktoren störend auswirken.

Bei digitalen Spuren sind die Algorithmen, die zur Unterstützung der Online-Aktivitäten verwendet werden, dynamisch und

ständig überarbeitet werden. Zum Beispiel werden die Suchalgorithmen von Google ständig

optimiert, um den Nutzerservice und die Werbeeinnahmen zu verbessern. Änderungen am Suchalgorithmus

rithmen können sich auf die bei der Suche erzeugten Daten auswirken, was wiederum Auswirkungen auf die Systeme hat.

tionen, die auf diesen Daten aufbauen, wie z. B. das Google Flu Trend Tracking System. Dieses

ein sich veränderndes Umfeld kann die Beibehaltung von Datenerhebungsprotokollen unhaltbar machen

und schwer zu wiederholende Ergebnisse.

Bei vielen Data-Science-Projekten werden Daten aus verschiedenen Quellen miteinander verknüpft. Jede

Quelle sollte anhand dieses Konstrukts des Datenumfangs geprüft werden, und jeder Unterschied

über die betrachteten Quellen hinweg. Außerdem werden Abgleichsalgorithmen verwendet, um Daten zu kombinieren

16 | Kapitel 1: Fragen und Datenumfang

aus verschiedenen Quellen müssen klar verstanden werden, damit Populationen und Rahmen

aus den Quellen verglichen werden können.

Messungen eines Instruments, das zur Untersuchung eines Naturphänomens eingesetzt wird, können auch

in das Scope-Diagramm eines Ziels, eines Zugriffsrahmens und einer Probe (siehe "Zielpopula-

tion, Access Frame, Sample" auf Seite 11 ). Dieser Ansatz ist hilfreich für das Verständnis

ihre Genauigkeit.

Messung von Naturphänomenen
Das für die Beobachtung einer Zielpopulation eingeführte Bereichsdiagramm kann erweitert werden auf

die Situation, in der wir eine Größe wie die Anzahl der Partikel in der Erde messen wollen

der Luft, das Alter eines Fossils, die Lichtgeschwindigkeit usw. In diesen Fällen betrachten wir die Quan-

eine unbekannte Größe, die wir messen wollen. (Dieser unbekannte Wert wird oft als

als Parameter bezeichnet). In unserem Diagramm schrumpfen wir das Ziel auf einen Punkt, der das Ziel darstellt.

gibt diese Unbekannte an. Die Genauigkeit des Geräts dient als Rahmen, und die Stichprobe kon-

besteht aus den Messungen, die das Messgerät innerhalb des Rahmens vornimmt. Sie könnten denken

des Rahmens als eine Dartscheibe, wobei das Instrument die Person ist, die die Darts wirft. Wenn

sie sind einigermaßen gut, die Darts landen innerhalb des Kreises, verstreut um die Bullen

Auge. Die Streuung der Pfeile entspricht den Messungen des Geräts.

Der Zielpunkt ist für den Werfer nicht sichtbar, fällt aber idealerweise mit dem Bullen zusammen.

Auge.

Zur Veranschaulichung des Konzepts des Messfehlers und der Verbindung zur Stichprobe

Fehler, untersuchen wir das Problem der Kalibrierung von Luftqualitätssensoren.

Wie genau ist mein Luftqualitätsmessgerät? In den gesamten USA werden Sensoren zur Messung der Luftverschmutzung...

von Einzelpersonen, kommunalen Gruppen sowie staatlichen und lokalen Luftüberwachungsstellen genutzt.

toring-Agenturen (Owyang 2020). Zum Beispiel an zwei Tagen im September 2020,

Etwa 600.000 Kalifornier und 500.000 Oregonier haben die Karte von PurpleAir angesehen.

als sich das Feuer in ihren Staaten ausbreitete und Evakuierungen geplant wurden. PurpleAir schafft

Luftqualitätskarten auf der Grundlage von Crowdsourced-Daten, die von ihren Sensoren einströmen.

Messung von Naturphänomenen | 17
Abbildung 1-5. Diese Darstellung ist typisch für viele Messverfahren. Der Zugang

Rahmen stellt den Messvorgang dar, der die Genauigkeit des Geräts widerspiegelt.

Wir können uns den Datenumfang folgendermaßen vorstellen: An jedem Ort und zu jedem Zeitpunkt gibt es eine

Die wahre Partikelzusammensetzung in der den Sensor umgebenden Luft ist unser Ziel. Unser

Das Gerät, der Sensor, nimmt viele Messungen vor, in manchen Fällen eine Messung pro Sekunde.

ond. Diese bilden eine Probe, die im Zugangsrahmen, dem Dartboard, enthalten ist. Wenn die Instru-

richtig funktioniert, die Messungen um das Bullseye zentriert sind und

das Ziel mit dem Bullseye übereinstimmt. Forscher haben herausgefunden, dass niedrige Luftfeuchtigkeit

die Messwerte so verzerren, dass sie zu hoch sind (Hug 2020). In diesem Artikel geht es darum, wie man

die Datenwissenschaft zur Kalibrierung dieser Instrumente nutzen, um ihre Genauigkeit zu verbessern.

Im nächsten Abschnitt wird die Analogie zur Dartscheibe weitergeführt, um die Konzepte der

Verzerrung und Variation, beschreiben Sie, wie es dazu kommen kann, dass eine Stichprobe nicht repräsentativ ist.

der Bevölkerung und stellen Verbindungen zwischen der Genauigkeit und dem Protokoll her.

Genauigkeit
Bei einer Volkszählung entspricht der Zugangsrahmen der Bevölkerung, und die Stichprobe erfasst die

gesamte Bevölkerung. Wenn wir in dieser Situation einen gut konzipierten Fragebogen ausfüllen,

dann haben wir vollständige und genaue Kenntnisse über die Bevölkerung, und der Umfang ist

vollständig. Ähnlich verhält es sich bei der Messung der Luftqualität, wenn unser Instrument eine perfekte Genauigkeit aufweist

und richtig eingesetzt wird, können wir den genauen Wert der Luftqualität messen. Diese

Situationen sind selten, wenn nicht gar unmöglich. In den meisten Fällen müssen wir die Genauigkeit der Daten quantifizieren.

rung unserer Messungen, um unsere Ergebnisse auf die unbeobachteten Personen zu verallgemeinern. Für

Zum Beispiel verwenden wir die Stichprobe oft, um einen Durchschnittswert für eine Grundgesamtheit zu schätzen, um Rückschlüsse

18 | Kapitel 1: Fragen und Datenumfang

den Wert einer wissenschaftlichen Unbekannten aus Messungen zu ermitteln oder das Verhalten einer

neue Person. In jeder dieser Situationen wollen wir auch ein quantifizierbares Maß an Akkuratesse.

rassig. Wir wollen wissen, wie nahe unsere Schätzungen, Schlussfolgerungen und Vorhersagen an der Realität sind.

Wahrheit.

Die bereits erwähnte Analogie von Dartpfeilen, die auf eine Dartscheibe geworfen werden, kann hilfreich sein

beim Verständnis der Genauigkeit. Wir unterteilen die Genauigkeit in zwei grundlegende Teile: Verzerrung und Varianz

(auch bekannt als Präzision). Unser Ziel ist es, dass die Darts das Bullseye auf der Dartscheibe treffen

und das Bullseye auf das unsichtbare Ziel ausrichten. Das Spritzen der Pfeile auf

die Tafel stellt die Abweichung in unseren Messungen dar, und der Abstand vom Bullseye

zu dem unbekannten Wert, den wir anvisieren, stellt die Verzerrung dar. Abbildung Abbildung 1-6

zeigt Kombinationen von geringer und hoher Verzerrung und Varianz.

Abbildung 1-6. In jedem dieser Diagramme stellen die Punkte die durchgeführten Messungen dar. Sie

bilden eine Streuung innerhalb des durch die Dartscheibe dargestellten Zugangsrahmens. Wenn die

Das Bullauge des Zugriffsrahmens befindet sich ungefähr in der Mitte des Zielwerts (obere Zeile), die

Die Messungen sind verstreut und die Verzerrung ist gering. Die größeren Dartscheiben (rechte Spalte)

umn) weisen auf eine größere Streuung der Messungen hin.

Repräsentative Daten befinden sich in der obersten Reihe des Diagramms, wo die Verzerrung gering ist,

Das bedeutet, dass das Bullseye und das unsichtbare Ziel in einer Linie liegen. Im Idealfall ist unser Instru-

und Protokollen befinden wir uns im oberen linken Teil des Diagramms, wo die Varianz

Die Genauigkeit | 19
ist ebenfalls niedrig. Das Muster der Punkte in der unteren Reihe verfehlt systematisch das Ziel

Wert. Die Entnahme größerer Stichproben wird diese Verzerrung nicht korrigieren.

Arten von Vorurteilen
Vorurteile gibt es in vielen Formen. Wir beschreiben hier einige klassische Arten und verbinden sie mit

unser Ziel-Zugangs-Proben-Rahmen.

Erfassungsfehler treten auf, wenn der Zugangsrahmen nicht alle Mitglieder der Grundgesamtheit umfasst.
Bevölkerung umfasst. Zum Beispiel kann eine Umfrage, die auf Mobiltelefonanrufen basiert
diejenigen, die nur einen Festnetzanschluss oder kein Telefon haben. In dieser Situation können sich diejenigen, die nicht
Diejenigen, die nicht erreicht werden können, können sich in wichtigen Punkten von denjenigen unterscheiden, die in der Zugangsgruppe enthalten sind.
Selektionsverzerrungen treten auf, wenn der Mechanismus zur Auswahl der Einheiten für die Stichprobe
dazu neigt, bestimmte Einheiten häufiger auszuwählen, als sie sollten. Zum Beispiel wählt eine Con-
Stichprobe die Einheiten aus, die am leichtesten verfügbar sind. Probleme können entstehen, wenn
die leicht zu erreichenden Einheiten sich in wichtigen Punkten von den schwerer zu erreichenden unterscheiden.
Ein weiteres Beispiel für Selektionsverzerrungen kann bei Beobachtungsstudien und
Experimenten. Diese Studien stützen sich häufig auf Freiwillige (Personen, die sich zur Teilnahme
(Personen, die sich zur Teilnahme entschließen), und diese Selbstselektion kann zu Verzerrungen führen, wenn sich die Freiwilligen
von der Zielpopulation in wichtigen Punkten unterscheiden.
Non-Response-Bias gibt es in zwei Formen: Unit und Item. Unit-Non-Response tritt auf
wenn jemand, der für eine Stichprobe ausgewählt wurde, nicht bereit ist, teilzunehmen, und
Item-Non-Response tritt auf, wenn sich beispielsweise jemand in der Stichprobe weigert, eine bestimmte Frage zu beantworten.
Frage verweigert. Non-Response kann zu Verzerrungen führen, wenn diejenigen, die sich
nicht teilnehmen oder eine bestimmte Frage nicht beantworten, sich systematisch
von denen unterscheiden, die antworten.
Messverzerrungen treten auf, wenn ein Instrument das Ziel systematisch in einer Richtung verfehlt.
in einer Richtung verfehlt. Zum Beispiel kann eine niedrige Luftfeuchtigkeit systematisch zu un
Luftverschmutzung systematisch zu hohen Messwerten führen. Darüber hinaus können Messgeräte
instabil werden und mit der Zeit abdriften, was zu systematischen Fehlern führt. Bei Umfragen
Befragungen können Messfehler entstehen, wenn die Fragen verwirrend formuliert oder
oder wenn es den Befragten unangenehm ist, ehrlich zu antworten.
Jede dieser Arten von Verzerrungen kann zu Situationen führen, in denen die Daten nicht auf das Wesentliche konzentriert sind.

den unbekannten Zielwert. Oft können wir die potenzielle Größenordnung des

Voreingenommenheit, da nur wenige oder gar keine Informationen über diejenigen verfügbar sind, die sich außerhalb des Zugangsbereichs befinden

Rahmen, die weniger wahrscheinlich für die Stichprobe ausgewählt werden, oder die nicht bereit sind, zu antworten. Protokolle

sind der Schlüssel zur Verringerung dieser Quellen von Verzerrungen. Zufallsmechanismen zur Auswahl einer Stichprobe aus

der Rahmen oder die Zuordnung der Einheiten zu den Versuchsbedingungen kann Auswahlverzerrungen ausschließen. A

Non-Response-Follow-up-Protokoll zur Förderung der Teilnahme kann die Non-Response

Antwortverzerrungen. Eine Piloterhebung kann die Formulierung der Fragen verbessern und so die Messwerte reduzieren.

20 | Kapitel 1: Fragen und Datenumfang

Verzerrungen zu vermeiden. Verfahren zur Kalibrierung von Instrumenten und Protokolle zur Durchführung von Messungen

in z. B. zufälliger Reihenfolge kann die Messverzerrung verringern.

Bei den US-Präsidentschaftswahlen 2016 waren Non-Response Bias und Measurement Bias

Schlüsselfaktoren für die ungenaue Vorhersage des Siegers. Fast alle Wählerumfragen führen

vor der Wahl hatten Clinton als Siegerin über Trump vorausgesagt. Clintons überraschender Sieg

war eine Überraschung. Nach der Wahl versuchten viele Meinungsforschungsexperten zu diagnostizieren

wo bei den Umfragen etwas schief gelaufen ist. Die amerikanische Vereinigung für öffentliche Meinung

Die Forschung (Kennedy et al. 2017) hat festgestellt, dass die Vorhersagen in zwei wichtigen Bereichen fehlerhaft sind.

Söhne:

Überrepräsentation von Wählern mit College-Abschluss. Wähler mit Hochschulbildung sind
nehmen eher an Umfragen teil als Wähler mit geringerer Bildung, und 2016
waren sie eher bereit, Clinton zu unterstützen. Die Nichtbeantwortung verzerrt die Stichprobe und
überschätzte die Unterstützung für Clinton (Pew Research Center 2012).
Die Wähler waren unentschlossen oder änderten ihre Präferenzen einige Tage vor der Wahl.
tion. Da eine Umfrage statisch ist und nur aktuelle Überzeugungen direkt messen kann, kann sie
eine Veränderung der Einstellungen widerspiegeln.
Es ist schwierig, herauszufinden, ob die Menschen ihre Präferenzen zurückhielten oder änderten.

Präferenz und wie groß die daraus resultierende Verzerrung ist. Die Exit Polls haben jedoch die Meinungsforschung unterstützt

Experten verstehen im Nachhinein, was passiert ist. Sie weisen darauf hin, dass auf dem Schlachtfeld

Staaten, wie Michigan, haben viele Wähler ihre Wahl in der letzten Woche des Wahlkampfes getroffen.

und diese Gruppe hat mit großer Mehrheit für Trump gestimmt.

Eine Verzerrung muss nicht unter allen Umständen vermieden werden. Wenn ein Instrument sehr

präzise ist (geringe Varianz) und eine geringe Verzerrung aufweist, dann könnte dieses Instrument vorzuziehen sein

zu einer anderen mit höherer Varianz und ohne Verzerrung. Ein Beispiel: Verzerrte Studien sind poten-

Pilotphase eines Erhebungsinstruments oder zur Erfassung nützlicher Informationen für die

das Design einer größeren Studie. In vielen Fällen können wir bestenfalls Freiwillige für eine Studie rekrutieren.

Angesichts dieser Einschränkung kann es dennoch sinnvoll sein, diese Probanden in die Studie einzubeziehen und

sie nach dem Zufallsprinzip in Behandlungsgruppen aufzuteilen. Das ist die Idee hinter

randomisierte kontrollierte Experimente.

Unabhängig davon, ob eine Verzerrung vorliegt oder nicht, weisen die Daten in der Regel auch eine Variation auf. Variation kann sein

durch die Verwendung eines Zufallsmechanismus zur Auswahl einer Stichprobe absichtlich eingeführt werden, und es kann

natürlich durch die Präzision eines Instruments entstehen. Im nächsten Abschnitt ermitteln wir

drei gemeinsame Quellen für Abweichungen.

Arten der Variation
Variation, die durch einen Zufallsmechanismus entsteht, hat den Vorteil, dass sie quantifizierbar ist.

ble.

Genauigkeit | 21
Stichprobenabweichungen ergeben sich aus der Verwendung des Zufalls bei der Stichprobenziehung. Wir können prinzipiell die
prinzipiell die Wahrscheinlichkeit berechnen, dass eine bestimmte Stichprobe ausgewählt wird.
Die unterschiedliche Zuweisung von Einheiten zu Behandlungsgruppen in einem kontrollierten Experiment
erzeugt Variation. Wenn wir die Einheiten unterschiedlich aufteilen, dann können wir unterschiedliche
Ergebnisse des Experiments erhalten. Diese Zufälligkeit ermöglicht es uns, die Wahrscheinlichkeit
einer bestimmten Gruppenzuordnung zu berechnen.
Messfehler bei Instrumenten ergeben sich aus dem Messprozess; wenn das
Instrument keine Drift und eine verlässliche Fehlerverteilung hat, dann werden wir bei
wenn wir mehrere Messungen an demselben Objekt vornehmen, erhalten wir Messabweichungen
die auf die Wahrheit zentriert sind.
Das Urnenmodell ist eine einfache Abstraktion, die für das Verständnis von Schwankungen hilfreich sein kann.

tion. Bei diesem Modell wird ein Behälter (eine Urne) mit identischen Murmeln untersucht, die

beschriftet wurden, und wir nutzen die einfache Handlung des Ziehens von Kugeln aus der Urne, um zu folgern

über Stichprobenverfahren, randomisierte kontrollierte Experimente und Messungen

Fehler. Für jede dieser Variationsarten hilft uns das Urnenmodell bei der Schätzung der Größe der

die Variation entweder mit Hilfe der Wahrscheinlichkeitsrechnung oder der Simulation (siehe Kapitel 2). Das Beispiel von

Die Auswahl von Wikipedia-Beitragenden, die eine informelle Auszeichnung erhalten sollen, liefert zwei Beispiele

des Urnenmodells.

Es sei daran erinnert, dass für das Wikipedia-Experiment eine Gruppe von 200 Personen ausgewählt wurde, die

nach dem Zufallsprinzip aus 1.440 Top-Beitragenden ausgewählt. Diese 200 Beitragszahler wurden dann aufgeteilt, wiederum nach

nach dem Zufallsprinzip in zwei Gruppen von je 100 Personen aufgeteilt. Eine Gruppe erhielt eine informelle Auszeichnung und die

andere nicht. Mit Hilfe des Urnenmodells können wir diesen Auswahlprozess folgendermaßen charakterisieren

und Spaltung:

Stellen Sie sich eine Urne mit 1.440 Murmeln vor, die in Form und Größe identisch sind, und
auf jeder Murmel steht einer der 1.440 Wikipedia-Benutzernamen. (Dies ist der
Zugangsrahmen.)
Mischen Sie die Murmeln in der Urne gut durch, wählen Sie eine Murmel aus und legen Sie sie beiseite.
Wiederholen Sie das Mischen und Auswählen der Murmeln, um 200 Murmeln zu erhalten.
Die gezogenen Murmeln bilden die Probe. Um zu bestimmen, welche der 200 Murmeln zur

tors die informelle Auszeichnung erhält, arbeiten wir mit einer anderen Urne.

In eine zweite Urne gibst du die 200 Murmeln aus der obigen Probe.
Mische diese Murmeln gut und wähle eine Murmel aus und lege sie beiseite.
Wiederholen Sie den Vorgang. Das heißt, du wählst 100 Murmeln aus, eine nach der anderen, mischst dazwischen und legst die
die ausgewählte Murmel beiseite.
22 | Kapitel 1: Fragen und Datenumfang

Die 100 gezogenen Murmeln werden der Behandlungsgruppe zugewiesen und entsprechen den

Beitragszahler, die den Preis erhalten. Die 100, die in der Urne bleiben, bilden die Kontrollgruppe

und erhalten keine Auszeichnung.

Sowohl bei der Auswahl der Stichprobe als auch bei der Auswahl der Preisträger wird ein Zufallsprinzip verwendet.

Mechanismus. Würden wir die erste Stichprobenaktion wiederholen und alle 1.440

Wenn wir die Murmeln in die Originalurne zurücklegen würden, würden wir höchstwahrscheinlich eine andere Probe erhalten. Diese

Variation ist die Quelle der Stichprobenvariation. Wenn wir die Stichprobe wiederholen würden.

erneuter Dom-Zuweisungsprozess (unter Beibehaltung der Stichprobe von 200 aus dem ersten Schritt)

unverändert), dann würden wir eine andere Behandlungsgruppe erhalten. Zuweisungsvariation ari-

ses aus diesem Prozess der zweiten Chance.

Das Wikipedia-Experiment ist ein Beispiel für Stichproben und Zuweisung

Variation. In beiden Fällen hat der Forscher den Daten einen Zufallsmechanismus aufgezwungen.

lektionsprozess. Messfehler können zuweilen auch als Zufallsprozess betrachtet werden

die einem Urnenmodell folgt. Wir charakterisieren den Messfehler in der Luftqualität

Sensoren auf diese Art und Weise im folgenden Beispiel.

Wenn wir eine genaue Analogie zwischen der Variation in den Daten und dem Urnenmodell ziehen können,

Das Urnenmodell liefert uns die Instrumente, um die Größe der Variation zu schätzen. (Siehe Kap.

ter 2). Dies ist sehr wünschenswert, denn so können wir konkrete Werte für die Varia-

tion in unseren Daten. Es ist jedoch wichtig zu bestätigen, dass das Urnenmodell ein vernünftiges Modell ist.

Darstellung der Quelle der Variation. Andernfalls können unsere Ansprüche an die Genauigkeit seriös sein.

äußerst mangelhaft. So viel wie möglich über den Umfang der Daten wissen, einschließlich der Instrumente

und Protokolle und Zufallsmechanismen, die bei der Datenerhebung verwendet werden, sind erforderlich, um die Urne

Modelle.

Zusammenfassung
Unabhängig von der Art der Daten, mit denen Sie arbeiten, sollten Sie, bevor Sie mit dem Bereinigen, Auswerten...

ration und Analyse sollten Sie sich einen Moment Zeit nehmen, um die Quelle der Daten zu prüfen. Wenn Sie die Daten nicht gesammelt haben

die Daten, fragen Sie sich:

Wer hat die Daten erhoben?
Warum wurden die Daten erhoben?
Die Antworten auf diese Fragen können helfen zu bestimmen, ob die gefundenen Daten verwendet werden können

um die Frage zu beantworten, die Sie interessiert.

Berücksichtigen Sie den Umfang der Daten. Fragen zu den zeitlichen und räumlichen Aspekten der

Die Datenerhebung kann wertvolle Erkenntnisse liefern:

Wann wurden die Daten erhoben?
Wo wurden die Daten erhoben?
Zusammenfassung | 23
Die Antworten auf diese Fragen helfen Ihnen festzustellen, ob Ihre Ergebnisse relevant sind für

die Situation, die Sie interessiert, oder ob Ihre Situation, die nicht vergleichbar ist

an diesen anderen Ort und in diese andere Zeit.

Im Mittelpunkt des Begriffs der Reichweite stehen Antworten auf die folgenden Fragen:

Was ist die Zielpopulation (oder der unbekannte Parameterwert)?
Wie wurde das Ziel erreicht?
Welche Methoden wurden für die Auswahl der Proben/Messungen verwendet?
Welche Instrumente wurden verwendet und wie wurden sie kalibriert?
Die Beantwortung möglichst vieler dieser Fragen kann Ihnen wertvolle Einblicke in folgende Bereiche geben

wie viel Vertrauen Sie in Ihre Ergebnisse setzen können und wie weit Sie sie verallgemeinern können

Ergebnisse.

Dieses Kapitel hat Ihnen eine Terminologie und einen Rahmen für Ihre Überlegungen zu folgenden Themen gegeben

und die Beantwortung dieser Fragen. Das Kapitel hat auch Wege aufgezeigt, wie man mögliche

Quellen von Verzerrungen und Abweichungen, die die Genauigkeit Ihrer Ergebnisse beeinträchtigen können. An

Um Ihnen das Nachdenken über Verzerrung und Varianz zu erleichtern, haben wir die folgenden Diagramme eingeführt

und Begriffen:

Umfangsdiagramm zur Darstellung der Überschneidung zwischen Zielpopulation, Zugangsrahmen und Stichprobe,
und Stichprobe;
Dartscheibe zur Beschreibung der Verzerrung und Varianz eines Instruments; und
Urnenmodell für Situationen, in denen ein Zufallsmechanismus verwendet wurde, um eine
Stichprobe aus einem Zugriffsrahmen auszuwählen, eine Gruppe in experimentelle Behandlungsgruppen zu unterteilen,
oder Messungen mit einem gut kalibrierten Instrument vorgenommen werden.
Mit diesen Diagrammen und Modellen wird versucht, die wichtigsten Konzepte zum Verständnis der folgenden Punkte zusammenzufassen

Einschränkungen zu erkennen und die Nützlichkeit Ihrer Daten für die Beantwortung Ihrer Fragen zu beurteilen.

tion. In Kapitel 2 wird die Entwicklung des Urnenmodells fortgesetzt, um eine formalere Quan-

Genauigkeit zu überprüfen und Simulationsstudien zu entwerfen.

Übungen
24 | Kapitel 1: Fragen und Datenumfang

KAPITEL 2

Simulation und Datenentwurf

Ein Hinweis für die Leser von Early Release

Mit Early Release ebooks erhalten Sie Bücher in ihrer frühesten Form - den unbearbeiteten und
unbearbeiteten Inhalt, so dass Sie die Vorteile dieser Technologien schon lange
vor der offiziellen Veröffentlichung dieser Titel.
Dies wird das 3. Kapitel des endgültigen Buches sein. Bitte beachten Sie, dass das GitHub-Repositorium
zu einem späteren Zeitpunkt aktiviert wird.
Wenn Sie Anmerkungen haben, wie wir den Inhalt und/oder die Beispiele in diesem Buch verbessern können
diesem Buch verbessern könnten, oder wenn Sie fehlendes Material in diesem Kapitel bemerken, wenden Sie sich bitte an den
Autor unter mpotter@oreilly.com.
In diesem Kapitel entwickeln wir die Theorie hinter den Zufallsprozessen, die in

Kapitel 1. Diese Theorie präzisiert die Konzepte der Verzerrung und der Variation. Wir

weiterhin die Genauigkeit unserer Daten durch die Abstraktion einer Urne zu motivieren

Modell, das in Kapitel 1 vorgestellt wurde, und wir verwenden Simulationsstudien, um uns zu helfen

zu verstehen und Entscheidungen auf der Grundlage der Daten zu treffen.

Das Urnenmodell bietet uns einen technischen Rahmen für die Entwicklung und Durchführung von Simulationsstudien

um größere und komplexere Situationen zu verstehen. Wir können zum Beispiel tiefer eintauchen

zu verstehen, wie die Meinungsforscher die Präsidentschaftswahlen 2016 beeinflusst haben könnten.

tionsvorhersagen falsch (Kapitel 1). Dazu verwenden wir die tatsächlich abgegebenen Stimmen in Penn-

sylvania und simulieren die Stichprobenvariation für eine Umfrage unter sechs Millionen Wählern. Diese

Die Simulation hilft uns aufzudecken, wie Antwortverzerrungen Umfragen verfälschen können, und überzeugt uns, dass

Die Erhebung von viel mehr Daten hätte die Situation nicht verbessert (ein weiteres Beispiel für

Big Data Hybris).

25
In einer zweiten Simulationsstudie untersuchen wir die Wirksamkeit eines COVID-19-Impfstoffs. A

Der für den Impfstoff konzipierte Versuch wurde an über 50.000 Freiwilligen durchgeführt.

Die Abstraktion des Experiments auf ein Urnenmodell gibt uns ein Instrument zur Untersuchung der Zuordnung

Variation in randomisierten kontrollierten Experimenten. Durch Simulation finden wir die

das erwartete Ergebnis der klinischen Prüfung. Unsere Simulation, zusammen mit einer sorgfältigen Prüfung

des Datenumfangs entkräftet die Behauptungen über die Unwirksamkeit von Impfstoffen.

Zusätzlich zur Stichproben- und Zuweisungsvariation haben wir auch Mess- und

Fehler in Form eines Urnenmodells zu ermitteln. Wir verwenden mehrere Messungen von verschiedenen

Tageszeiten, um die Genauigkeit eines Luftqualitätssensors abzuschätzen. Später in der zweiten Hälfte des Jahres pro-

eine umfassendere Behandlung von Messfehlern und Gerätekalibrierungen

für Luftqualitätssensoren.

Wir beginnen mit einem künstlichen Beispiel einer kleinen Bevölkerung, die so klein ist, dass wir sie auflisten können

alle möglichen Stichproben, die aus der Grundgesamtheit gezogen werden können.

Das Urnenmodell
Das Urnenmodell ist eine einfache Abstraktion eines Zufallsmechanismus. Wir können das Modell jedoch erweitern

Zufälligkeit bei der Auswahl von Murmeln aus einer Urne zu vielen Zufällen

Prozesse in realen Beispielen, und wir können dieses zufällige Verhalten simulieren und nutzen

unsere Ergebnisse, um die Genauigkeit unserer Daten besser zu verstehen. Um das Urnenmodell zu erklären,

verwenden wir ein kleines Beispiel mit sieben Murmeln. Die Urne ist so klein, dass wir auflisten können

alle möglichen Ergebnisse, die sich aus dem Ziehen von Murmeln aus der Urne ergeben könnten.

Wir verwenden die Prototypen des SpaceX-Raumschiffs als erfundenes Beispiel. Die Prototypen sind

mit der Bezeichnung SN 1 , SN 2 , ..., wobei SN für "Seriennummer" steht. In der ersten Hälfte des Jahres 2020,

Sieben dieser Prototypen wurden gebaut. Vor ihrem Einsatz wurden einige von ihnen unter Druck gesetzt

getestet. Angenommen, wir wollen drei der sieben Starship-Prototypen für den Drucktest auswählen

Testen. (Dieses Beispiel ist zwar erfunden, aber der Kontext basiert auf dem tatsächlichen SpaceX

Programm; Drucktests wurden an den Starship-Prototypen durchgeführt).

Wir können das Urnenmodell wie folgt aufbauen, um den Auswahlprozess darzustellen: Schreiben Sie a

beschriften Sie jede der sieben Murmeln, legen Sie alle Murmeln in die Urne, mischen Sie sie

gut, und ziehe drei, ohne zu schauen und ohne die Murmeln zwischen den Ziehungen zu ersetzen.

Die Urne ist so klein, dass wir alle möglichen Proben von drei Murmeln auflisten können, die

gezogen werden:

ABC ABD ABE ABF ABG ACD ACE ACF ACG ADE ADF ADG
AEF AEG AFG BCD BCE BCF BCG BDE BDF BDG BEF BEG
BFG CDE CDF CDG CEF CEG CFG DEF DEG DFG EFG
Wir verwenden die Bezeichnungen A, B usw. anstelle von SN 1 , SN 2 usw., weil sie kürzer sind und

leichter zu unterscheiden. Unsere Liste zeigt, dass wir mit jedem der ( 35 )

einzigartige Sätze von drei der sieben Murmeln.

26 | Kapitel 2: Simulation und Datenentwurf

Wir können eine Analogie zum Datenumfang aus Kapitel 1 ziehen: ein Satz von Murmeln, die aus

die Urne ist eine Stichprobe, und die Gesamtheit aller in die Urne gelegten Murmeln ist die Population.

tion. Dieses besondere Urnenmodell schreibt eine bestimmte Auswahlmethode vor, die als

Einfache Zufallsstichprobe (SRS). Wir beschreiben die SRS und andere Stichprobenverfahren

auf der Grundlage der SRS im nächsten Abschnitt.

Musterentwürfe
Das Urnenmodell für die SpaceX-Prototypen wurde auf einige wenige Grundlagen reduziert. Wir haben festgelegt:

die Anzahl der Murmeln in der Urne;
was auf jeder Murmel steht;
die Anzahl der Murmeln, die aus der Urne gezogen werden; und
ob die Kugeln zwischen den Ziehungen ersetzt werden oder nicht.
Dieses Verfahren entspricht einer einfachen Zufallsstichprobe. Unser Beispiel ist eine SRS von drei

aus einer Grundgesamtheit von sieben Personen gezogen wird, und jede der ( 35 ) Stichproben ist von vornherein

gleich wahrscheinlich gewählt werden, weil die Murmeln ununterscheidbar und gut gemischt sind.

Das bedeutet, dass die Wahrscheinlichkeit einer bestimmten Probe 1/35 betragen muss,

ℙABC =ℙABD =⋯=ℙEFG =

1
35
.
Wir verwenden das spezielle Symbol ℙ, um für "Wahrscheinlichkeit" oder "Zufall" zu stehen, und wir lesen die

die Aussage ℙABC als die "Wahrscheinlichkeit, dass die Probe die Murmeln mit der Bezeichnung A, B und C enthält

in beliebiger Reihenfolge."

Das Urnenmodell gibt uns eine formalere Definition von "repräsentativen Daten", die sehr gut ist

nützlich:

Bei einer *einfachen Zufallsstichprobe* hat jede Stichprobe die gleiche Chance, ausgewählt zu werden.
Viele Menschen denken fälschlicherweise, dass die definierende Eigenschaft einer SRS
ist, dass jede Einheit die gleiche Chance hat, in der Stichprobe zu sein. Wie-
Dies ist jedoch nicht der Fall. Eine SRS von n Einheiten aus einer Grundgesamtheit von N
bedeutet, dass jede mögliche Teilmenge von n Einheiten die gleiche Chance hat, ausgewählt
ausgewählt zu werden.
Wir können die Aufzählung aller möglichen Stichproben aus der Urne verwenden, um folgende Fragen zu beantworten

zusätzliche Fragen zu diesem Zufallsprozess. Zum Beispiel, um die Chance zu finden, dass

Marmor A in der Probe ist, können wir die Wahrscheinlichkeit aller Proben, die A enthalten, addieren.

Es gibt 15 von ihnen, also ist die Chance groß:

ℙA ist in der Stichprobe =

15
35
=
3
7
.
Das Urnenmodell | 27
Wenn es zu schwierig ist, alle möglichen Stichproben aufzulisten und zu zählen, können wir Simula-

um diesen zufälligen Prozess zu verstehen.

Das SRS (und die dazugehörige Urne) ist der wichtigste Baustein für komplexere

Erhebungsdesigns. Wir beschreiben kurz zwei der am häufigsten verwendeten Designs.

Geschichtete Stichprobenziehung Unterteilung der Grundgesamtheit in sich nicht überschneidende Gruppen, die
(eine Gruppe wird als Schicht bezeichnet, und mehrere Gruppen sind Schichten), und nehmen dann
eine einfache Zufallsstichprobe aus jeder Gruppe. Das ist so, als hätte man eine separate Urne für jede
Urne für jede Schicht zu haben und aus jeder Urne unabhängig voneinander Murmeln zu ziehen. Die Schichten müssen nicht
müssen nicht gleich groß sein, und wir müssen nicht die gleiche Anzahl von Einheiten aus
jeder Schicht.
Clusterstichprobe Unterteilen Sie die Grundgesamtheit in sich nicht überschneidende Untergruppen, nehmen Sie eine
einfache Zufallsstichprobe aus den Clustern und nehmen alle Einheiten eines Clusters in die
die Stichprobe. Man kann sich das wie eine SRS aus einer Urne vorstellen, die große Mar- ken enthält, die ihrerseits
Murmeln enthält, die ihrerseits Behälter für kleine Murmeln sind. Wird die Urne geöffnet, wird die Probe der
großen Murmeln in die Stichprobe der kleinen Murmeln um. (Cluster sind in der Regel kleiner
als Schichten.)
Oft sind wir an einer Zusammenfassung der Stichprobe interessiert, d. h. an einer Statistik. Für jede

Stichprobe können wir die Statistik berechnen, und das Urnenmodell hilft uns, die Verteilung zu finden

der möglichen Werte, die diese Statistik annehmen kann. Im nächsten Abschnitt untersuchen wir die Dis-

Verteilung einer Statistik für unser Beispiel.

Stichprobenverteilung einer Statistik
Nehmen wir an, es interessiert uns, ob die Prototypen einen Drucktest bestehen können oder nicht.

Die Durchführung der Druckprüfung ist teuer, daher testen wir zunächst nur eine Probe der Proto-

Typen. Wir können das Urnenmodell verwenden, um die Protoypen auszuwählen, die unter Druck getestet werden sollen, und

dann können wir unsere Testergebnisse z. B. nach dem Anteil der Prototypen, die nicht bestanden haben, zusammenfassen

den Test. Das Urnenmodell liefert uns das Wissen, dass jede der 35 möglichen Sam-

die gleiche Chance, ausgewählt zu werden, so dass die Ergebnisse der Druckprüfung als repräsentativ gelten.

der Bevölkerung.

Angenommen, die Prototypen A, B, D und F würden die Druckprüfung nicht bestehen, wenn

gewählt. Für jede Stichprobe von drei Murmeln kann man den Anteil der Fehlschläge ermitteln

je nachdem, wie viele dieser vier fehlerhaften Prototypen in der Stichprobe enthalten sind. Wir geben

ein paar Beispiele für diese Berechnung.

Probe ABC BCE BDF CEG
Anteil 2/3 1/3 1 0
28 | Kapitel 2: Simulation und Datenentwurf

Da wir drei Murmeln aus der Urne ziehen, ist die einzig mögliche Proportionalität

tionen sind ( 0 ), 1/3, 2/3 und ( 1 ), und für jedes Tripel können wir die Korrelationen berechnen.

sprechenden Anteil. Es gibt 4 Stichproben, die uns alle fehlgeschlagenen Tests liefern (eine Stichprobe

Anteil von 1): ABD , ABF, ADF, BDF, so dass die Wahrscheinlichkeit der Beobachtung einer Stichprobe pro-

Anteil von ( 1 ) ist 4/35. Wir können die Verteilung der Werte für die Stichprobe zusammenfassen

Verhältnis in eine Tabelle.

Anteil der fehlgeschlagenen Proben Anzahl der Proben Anteil der Proben
1 4 4/35
2/3 18 18/35
1/3 12 12/35
0 1 1/35
Gesamt 35 1
Obwohl diese Berechnungen relativ einfach sind, können wir sie näherungsweise durchführen

durch eine Simulationsstudie. Zu diesem Zweck nehmen wir drei Stichproben aus unserer Grundgesamtheit

wieder und wieder, sagen wir 10.000 Mal. Für jede Stichprobe berechnen wir den Anteil der Fehlschläge.

ures. Damit erhalten wir 10.000 simulierte Stichprobenanteile. Die Tabelle der simulierten

Proportionen sollten unserer Verteilungstabelle nahe kommen. Wir bestätigen dies mit einer Simulations-.

ulationsstudie.

Simulation der Stichprobenverteilung
Unsere ursprüngliche Urne hatte sieben Murmeln, die mit A bis G gekennzeichnet waren. Da wir jedoch

nur, ob der Prototyp den Test bestanden oder nicht bestanden hat, können wir jede Murmel als "nicht bestanden" kennzeichnen

oder 'passen'. Wir erstellen diese überarbeitete Urne als Array.

urn = ['fail', 'fail', 'fail', 'fail', 'pass', 'pass', 'pass']
Dann simulieren wir die Ziehung von drei Murmeln aus unserer Urne ohne Ersatz

zwischen den Ziehungen mit der Numpy-Methode "random.choice".

np.random.choice(urn, size=3, replace=False)
array(['pass', 'fail', 'pass'], dtype='<U4')
Nehmen wir noch ein paar Beispiele aus unserer Urne, um zu sehen, wie die Ergebnisse aussehen könnten.

[np.random.choice(urn, size = 3, replace = False) for i in range(10)]
[array(['fail', 'fail', 'pass'], dtype='<U4'),
array(['fail', 'fail', 'pass'], dtype='<U4'),
array(['fail', 'pass', 'pass'], dtype='<U4'), array(['fail', 'pass', 'pass'], dtype='<U4'),
array(['fail', 'pass', 'pass'], dtype='<U4'),
array(['fail', 'pass', 'fail'], dtype='<U4'),
array(['fail', 'fail', 'pass'], dtype='<U4'),
array(['fail', 'fail', 'fail'], dtype='<U4'),
array(['fail', 'fail', 'pass'], dtype='<U4'),
Das Urnenmodell | 29
array(['fail', 'fail', 'pass'], dtype='<U4'),
array(['fail', 'fail', 'pass'], dtype='<U4')]
Da wir lediglich die Anzahl der Fehlschläge in der Probe zählen wollen, ist es einfacher, wenn die

Die Murmeln sind mit 1 für "nicht bestanden" und 0 für "bestanden" gekennzeichnet. Auf diese Weise können wir die Ergebnisse der

drei Ziehungen, um die Anzahl der Fehlschläge in der Stichprobe zu ermitteln. Wir beschriften die Murmeln in der

und berechnen Sie den Anteil der Fehlschläge in einer Stichprobe.

urn = [1, 1, 1, 1, 0, 0, 0]
sum(np.random.choice(urn, size=3, replace=False))/3
0.6666666666666666
Wir haben den Prozess gestrafft und sind nun bereit, die Simulation auszuführen

Studie. Wiederholen wir den Vorgang 100.000 Mal.

simulations = [sum(np.random.choice(urn, size=3, replace=False)) / 3
for i in range(10000)]
Wir können diese 100.000 Stichprobenproportionen untersuchen und unsere Ergebnisse mit dem abgleichen, was

haben wir bereits anhand der vollständigen Auszählung aller 35 möglichen Proben berechnet. Wir

erwarten, dass die Simulationsergebnisse nahe an unseren früheren Berechnungen liegen, da wir

das Stichprobenverfahren viele, viele Male wiederholt. Das heißt, wir wollen die

Anteil der 100.000 Stichprobenmitglieder, die ( 0 ), 1/3, 2/3 und ( 1 ) sind, zu denen

in der Tabelle. Diese Bruchteile sollten ungefähr 1/35, 12/35, 18/35 und 4/35 betragen,

oder etwa 0. 03, 0. 34, 0. 51 und 0. 11.

unique_els, counts_els = np.unique(np.array(simulations), return_counts=True)
np.array((unique_els, counts_els/10000))
array([[0. , 0.33, 0.67, 1. ],
[0.03, 0.34, 0.52, 0.11]])
Die Simulationsergebnisse stimmen gut mit der Tabelle überein.

Diese Simulationsstudie beweist beispielsweise nicht, dass wir erwarten, dass 18/35 Proben zwei

scheitert, aber sie liefert uns ausgezeichnete Annäherungen an unsere früheren Berechnungen, und das ist

beruhigend. Noch wichtiger ist, dass wir in einem komplexeren Umfeld, in dem es möglicherweise

Da es schwierig ist, alle Möglichkeiten aufzuzählen, kann eine Simulationsstudie wertvolle Erkenntnisse liefern.

Bei einer Simulationsstudie wird ein Zufallsprozess viele Male wiederholt. A
Zusammenfassung der Muster, die sich aus der Simulation ergeben, kann
die theoretischen Eigenschaften des Zufallsprozesses annähern. Diese
Zusammenfassung ist nicht dasselbe wie der Nachweis der theoretischen Eigenschaften, aber
aber oft ist die Anleitung, die wir aus der Simulation erhalten, für unsere Zwecke ausreichend.
Zwecke.
Das Ziehen von Murmeln aus einer Urne mit 0en und 1en ist ein so beliebter Rahmen für die Untersu-

Zufall, dass dieser Zufallsprozess einen offiziellen Namen erhalten hat: die

30 | Kapitel 2: Simulation und Datenentwurf

hypergeometrisch. Und die meisten Programme bieten Funktionen zur schnellen Durchführung von Simu-

lationen der Hypergeomtrie. Wir können unsere Simulation mit der hypergeometrischen wiederholen.

Das hypergeometrische Modell
Die Version des Urnenmodells, bei der wir die Anzahl der Murmeln einer bestimmten Art zählen

(in unserem Fall "fehlgeschlagene" Murmeln) ist so häufig, dass es einen Zufallsprozess gibt

benannt: die hypergeometrische. Anstatt random.choice zu verwenden, können wir num-

py's random.hypergeometric, um das Ziehen von Murmeln aus der Urne und das Zählen zu simulieren

die Anzahl der Fehlschläge. Die random.hypergeometrische Methode ist optimiert für die 0-1

und ermöglicht es uns, 10.000 Simulationen in einem Aufruf anzufordern. Der Vollständigkeit halber sei gesagt, dass

unsere Simulationsstudie wiederholen und die empirischen Proportionen berechnen.

simulations_fast = np.random.hypergeometric(ngood=4, nbad=3, nsample=3, size=10000)
Hinweis: Wir glauben nicht, dass ein Durchlauf "schlecht" ist; es ist nur eine Namenskonvention, den Typ

Sie wollen "gut" und das andere "schlecht" zählen.

unique_els, counts_els = np.unique(np.array( simulations_fast ), return_counts=True)
np.array((unique_els, counts_els/10000))
array([[0. , 1. , 2. , 3. ],
[0.03, 0.34, 0.52, 0.11]])
Vielleicht haben Sie sich schon gefragt, warum die hypergeometrische Methode so beliebt ist.

nicht die genaue Verteilung der möglichen Werte angeben. In der Tat, diese sind verfügbar,

und wir zeigen im Folgenden, wie man sie berechnet.

aus scipy.stats importieren hypergeom
x = np.arange(0, 4)
hypergeom.pmf(x, 7, 4, 3)
array([0.03, 0.34, 0.51, 0.11])
Die beiden häufigsten Zufallsprozesse sind vielleicht diejenigen, die durch Zählen entstehen

die Anzahl der 1en, die aus einer 0-1 Urne gezogen werden: das Ziehen ohne Ersatz ist die Hyper-

Die geometrische und die ersatzweise Ziehung ist die binomische.

Wann immer es möglich ist, ist es eine gute Idee, die Funktionalität
Paket eines Drittanbieters für die Simulation aus einer benannten Verteilung
Distribution zu simulieren, anstatt eine eigene Funktion zu schreiben, wie z.B. die Zufalls
Zufallszahlengeneratoren, die in numpy angeboten werden. Es ist am besten, den Vorteil von
effizienter und genauer Code, den andere entwickelt haben.
Diese Simulation war zwar einfach, so einfach, dass wir hypergeom.pmf

um unsere Verteilung zu vervollständigen, wollten wir die Intuition demonstrieren, dass eine Simulati-

tionsstudie aufdecken kann. Der Ansatz, den wir in diesem Buch verfolgen, ist es, ein Verständnis

Das Urnenmodell | 31
1 https://www.aapor.org/Education-Resources/Reports/An-Evaluation-of-2016-Election-Polls-in-the-U-S.aspx
über Zufallsprozesse auf der Grundlage von Simulationsstudien. Wir formalisieren jedoch die

Begriff der Wahrscheinlichkeitsverteilung einer Statistik (z. B. der Anteil der Fehlschläge in einer Stichprobe)

plädiert) in...

Jetzt, da wir die Simulation als Werkzeug zum Verständnis der Genauigkeit haben, können wir die

Wahlbeispiel aus Kapitel 1 und führen Sie nach der Wahl eine Studie darüber durch, was man

bei den Wählerbefragungen schiefgegangen sind. Diese Simulationsstudie imitiert die Ziehung mehr

als tausend Murmeln (Wähler, die an der Umfrage teilnehmen) aus einer Urne mit sechs Millionen Murmeln.

Löwe. Wir können mögliche Quellen für Verzerrungen und die Schwankungen in den Umfrageergebnissen untersuchen,

und führen eine Was-wäre-wenn-Analyse durch, in der wir untersuchen, wie die Vorhersagen hätten ausfallen können.

weg, wenn eine noch größere Anzahl von Ziehungen aus der Urne vorgenommen wird.

Beispiel: Simulation der Verzerrung und Varianz von Wahlumfragen
Der Präsident der USA wird durch das Wahlmännerkollegium gewählt und nicht allein durch das Volk.

wählen. Jedem Staat wird eine bestimmte Anzahl von Stimmen zugeteilt, die er im Electoral College abgeben kann,

nach der Größe ihrer Bevölkerung. In der Regel gewinnt derjenige, der die Volksabstimmung

in einem Staat alle Stimmen des Wahlmännerkollegiums für diesen Staat erhält. Mit Hilfe von Umfragen

Vor der Wahl ermitteln Experten die Bundesstaaten, in denen die

Es wird erwartet, dass die Wahl knapp ausfallen wird, und die Stimmen des Wahlmännerkollegiums könnten den Wahlausgang beeinflussen.

tion.

Im Jahr 2016 waren fast alle Vorhersagen zum Wahlausgang falsch. Meinungsforscher

das Wahlergebnis in 46 der 50 Bundesstaaten richtig vorhergesagt. Für diese 46 Staaten,

Trump erhielt 231 und Clinton 232 Wahlmännerstimmen. Die restlichen 4

Staaten, Florida, Michigan, Pennsylvania und Wisconsin, wurden als Kampfgebiete identifiziert.

Bodenstaaten und entfielen insgesamt 75 Stimmen. Die Margen der Volksabstimmung

in diesen vier Staaten waren knapp. In Pennsylvania zum Beispiel erhielt Trump

48,18 % und Clinton erhielt 47,46 % der 6.165.478 abgegebenen Stimmen. Solche knappen Mar-

gins kann es angesichts der Stichprobengröße, die die Umfragen ergeben, schwierig sein, das Ergebnis vorherzusagen.

verwendet.

Viele Experten haben die Wahlergebnisse von 2016 untersucht, um herauszufinden, was bei den

falsch. Nach Angaben der American Association for Public Opinion Research^1 ist eine

Online-Umfrage mit Opt-in-Verfahren, bei der die Umfrageergebnisse an die Bildung der Befragten angepasst wurden

aber nur drei große Kategorien (High School oder weniger, etwas College, und College

Absolventen). Sie fanden heraus, dass, wenn sie die Personen mit fortgeschrittenen Abschlüssen herausgenommen hätten

von Personen mit Hochschulabschluss, dann hätten sie Clintons Vorsprung um

0,5 Prozentpunkte. Mit anderen Worten: Sie konnten im Nachhinein einen Bildungsweg identifizieren.

Wähler mit hohem Bildungsniveau waren eher bereit, sich an den Wahlen zu beteiligen.

32 | Kapitel 2: Simulation und Datenentwurf

2 https://blogs.lse.ac.uk/usappblog/2018/02/01/better-poll-sampling-would-have-cast-more-doubt-on-the-
potential-für-hillary-clinton-zu-gewinnen-die-2016-wahlen/
Umfragen. Diese Verzerrung ist von Bedeutung, weil diese Wähler auch dazu tendieren, Clinton zu bevorzugen

Trump.

Da wir nun wissen, wie die Menschen tatsächlich gewählt haben, können wir eine Simulationsstudie durchführen

das die Wahlumfrage in verschiedenen Szenarien imitiert, um ein Gespür für

Genauigkeit, Verzerrung und Varianz^2. Wir werden die Umfragen für Pennsylvania unter zwei Bedingungen simulieren

Szenarien:

Die Befragten haben ihre Meinung nicht geändert, haben nicht verheimlicht, wen sie gewählt haben, und
waren repräsentativ für diejenigen, die am Wahltag gewählt haben.
Personen mit höherer Bildung haben eher geantwortet, was zu einer Verzerrung
für Clinton führte.
Unser Ziel ist es, die Häufigkeit zu verstehen, mit der eine Umfrage das Wahlergebnis falsch angibt.

tion für Hillary Clinton, wenn die Stichprobe absolut unvoreingenommen erhoben wird und auch

wenn es einen geringen Non-Response-Bias gibt. Wir beginnen mit der Einrichtung der Urne

Modell für das erste Szenario.

Das Pennsylvania Urnenmodell
Unser Urnenmodell für die Durchführung einer Wählerbefragung in Pennsylvania ist eine nachträgliche Erhebung.

in der wir das Ergebnis der Wahl verwenden. In der Urne befinden sich 6.165.478 Murmeln,

eine für jeden Wähler. Wie bei unserer winzigen Bevölkerung, schreiben wir auf jede Murmel die Candi-

Datum, für das sie gestimmt haben, ziehen sie 1500 Murmeln aus der Urne (1500 ist etwa die typische

Größe der Umfragen), und zählen Sie die Stimmen für Trump, Clinton und alle anderen Kandidaten zusammen.

Daraus lässt sich der Vorsprung von Trump gegenüber Clinton errechnen.

Um unsere Simulation einzurichten, müssen wir herausfinden, wie die Urne aussieht. Wir müssen wissen, wie

Anzahl der für jeden der Kandidaten abgegebenen Stimmen. Da wir uns nur für Trumps

Vorsprung vor Clinton haben, können wir alle Stimmen für die anderen Kandidaten in einen Topf werfen. Diese

So hat jede Murmel eine von drei möglichen Stimmen: Trump, Clinton und Andere. Wir können nicht

Ignorieren Sie die Kategorie "Sonstige", da sie sich auf die Größe des Vorsprungs auswirkt.

Proportionen = np.array([0,4818, 0,4746, 1 - (0,4818 + 0,4746)])
n = 1_500
N = 6_165_478
Stimmen = np.trunc(N * Proportionen).astype(int)
Stimmen
array([2970527, 2926135, 268814])
Diese Version des Urnenmodells enthält drei Arten von Murmeln. Sie ist ein wenig komplexer

als die hypergeometrische, aber immer noch häufig genug, um eine eigene Verteilung zu haben: die

Beispiel: Simulation von Verzerrungen und Varianz bei Wahlumfragen | 33
multivariates hypergeometrisches Modell. In Python wird das Urnenmodell mit mehr als zwei Arten von

marbles ist als Methode scipy.stats.multivariate_hypergeom.rvs implementiert.

Die Funktion gibt die Anzahl der aus der Urne gezogenen Marmorarten zurück. Wir rufen

die Funktion wie folgt.

from scipy.stats import multivariate_hypergeom
multivariate_hypergeom.rvs(stimmen, n)
array([747, 681, 72])
Wie zuvor erhalten wir bei jedem Aufruf von multivariate_hypergeom.rvs eine andere Stichprobe

und zählt.

multivariate_hypergeom.rvs(votes, n)
array([749, 697, 54])
Wir müssen den Vorsprung von Trump für jede Stichprobe berechnen: nT-nC/n, wobei nT die

Anzahl der Trump-Stimmen in der Stichprobe und nC die Anzahl für Clinton. Beträgt der Vorsprung

positiv, dann zeigt die Stichprobe einen Sieg für Trump.

Wir wissen, dass der tatsächliche Vorsprung 0,4818 - 0,4746 = 0,0072 beträgt. Um ein Gefühl für die Abweichung zu bekommen

in der Umfrage können wir den Zufallsprozess der Ziehung aus der Urne immer wieder simulieren

und untersuchen die Werte, die wir als Antwort erhalten. Im Folgenden simulieren wir 100.000 Umfragen mit 1500

Wählern aus den in Pennsylvania abgegebenen Stimmen.

def trump_advantage(votes, n):
sample_votes = multivariate_hypergeom.rvs(votes, n)
return (stichprobe_stimme[0] - stichprobe_stimme[1]) / n
simulations = [trump_advantage(votes, n) for _ in range(100000)]
Im Durchschnitt zeigen die Umfrageergebnisse, dass Trump wie erwartet einen Vorsprung von fast 0,7 % hat

angesichts der Zusammensetzung der über sechs Millionen abgegebenen Stimmen.

np.mean(Simulationen)
0.007210106666666665
In vielen Fällen war der Vorsprung in der Stichprobe jedoch negativ, d. h. Clinton war die

Gewinner für diese Stichprobe von Wählern. Das nachstehende Histogramm zeigt die Stichprobenverteilung.

tion von Trumps Vorsprung in Pennsylvania für eine Stichprobe von 1500 Wählern. Die Vertikale

Die gestrichelte Linie bei 0 zeigt, dass Trump häufiger angerufen wird als nicht, aber es gibt viele

mal, wenn die Umfrage unter 1.500 Personen Clinton in Führung zeigt.

Text(0.5, 0, 'Trump-Blei in der Stichprobe')
34 | Kapitel 2: Simulation und Datendesign

3 https://blogs.lse.ac.uk/usappblog/2018/02/01/better-poll-sampling-would-have-cast-more-doubt-on-the-
potential-für-hillary-clinton-zu-gewinnen-die-2016-wahl/
In den 100.0000 simulierten Umfragen sehen wir Trump in etwa 60 % der Fälle als Sieger:

np.count_nonzero(np.array(simulationen) > 0) / 100000
0.60797
Mit anderen Worten: Eine Stichprobe sagt Trumps Sieg auch dann korrekt voraus, wenn die Stichprobe

die in etwa 60 % der Fälle völlig unvoreingenommen erhoben werden. Und diese unvoreingenommene Stichprobe wird

in etwa 40 % der Fälle falsch liegen.

Wir haben das Urnenmodell verwendet, um die Variation bei einer einfachen Umfrage zu untersuchen, und wir haben festgestellt

wie die Vorhersage einer Umfrage aussehen könnte, wenn es keine Verzerrungen in unserem Auswahlverfahren gäbe (die

Murmeln ununterscheidbar sind und jede mögliche Sammlung von 1500 Murmeln der

sechs und mehr Millionen Murmeln sind ebenso wahrscheinlich). Als Nächstes sehen wir, was passiert, wenn eine kleine

Voreingenommenheit ins Spiel kommt.

Ein Urnenmodell mit Verzerrungen
"In einer perfekten Welt würden Umfragen aus der Bevölkerung der Wähler stammen, die ihre

politische Präferenz ganz klar zu erkennen und dann entsprechend abzustimmen"^3 Das ist die Simulation

Beispiel: Simulation von Verzerrungen und Varianz bei Wahlumfragen | 35
Studie, die wir gerade durchgeführt haben. In der Realität ist es oft schwierig, jede Quelle zu kontrollieren

der Voreingenommenheit.

Wir untersuchen hier die Auswirkungen einer kleinen, bildungsbezogenen Verzerrung auf die Umfrageergebnisse. Spezi-

Wir untersuchen die Auswirkungen einer 0,5-prozentigen Verzerrung zugunsten von Clinton. Diese Verzerrung ist essentiell.

Das bedeutet, dass wir in unserer Umfrage ein verzerrtes Bild der Wählerpräferenzen sehen. Anstelle von

47,46 Prozent stimmen für Clinton, wir haben 47,96, und wir haben 48,18 - 0,5 = 47,68 Prozent

Cent für Trump. Wir passen die Anteile der Murmeln in der Urne an, um diese Verzerrung zu berücksichtigen.

proportions_bias = np.array([0,4818 - 0,005, 0,4747 + 0,005, 1 - (0,4818 + 0,4746) ])
anteile_verzerrt
array([0.48, 0.48, 0.04])
Stimmen_Voreingenommenheit = np.trunc(N * anteile_voreingenommenheit).astype(int)
Stimmen_Voreingenommenheit
array([2939699, 2957579, 268814])
Wenn wir die Simulationsstudie erneut durchführen, dieses Mal mit der verzerrten Urne, finden wir

ein anderes Ergebnis.

simulations_bias = [trump_advantage(votes_bias, n) for i in range(100000)]
Text(0.5, 0, 'Trump Vorsprung in der Stichprobe')
np.count_nonzero(np.array(simulations_bias) > 0) / 100000
0.44742
Nun hätte Trump in etwa 45 % der Umfragen einen positiven Vorsprung. Beachten Sie, dass die his-

togramme der beiden Simulationen haben eine ähnliche Form. Sie sind symmetrisch mit rea-

36 | Kapitel 2: Simulation und Datenentwurf

sonable length tails. Das heißt, sie scheinen in etwa der normalen Kurve zu folgen. Die

das zweite Histogramm ist leicht nach links verschoben, was den Non-Response-Bias widerspiegelt

die wir eingeführt haben. Hätte eine Vergrößerung der Stichprobe geholfen? Dies ist das Thema der

nächster Abschnitt.

Durchführen größerer Umfragen
Mit unserer Simulationsstudie können wir einen Einblick in die Auswirkungen einer größeren Umfrage auf die

Stichprobenvorsprung. Wir können zum Beispiel eine Stichprobengröße von 12.000 versuchen, achtmal so groß wie

die tatsächliche Umfrage, und führen Sie 100.000 Simulationen für beide Szenarien durch: die unvoreingenommene und die

voreingenommen.

simulations_big = [trump_advantage(votes, 12000) for i in range(100000)]
simulations_bias_big = [trump_advantage(votes_bias, 12000) for i in range(100000)]
scenario_no_bias = np.count_nonzero(np.array(simulations_big) > 0) / 100000
scenario_bias = np.count_nonzero(np.array(simulations_bias_big) > 0) / 100000
print(szenario_no_bias, szenario_bias)
0.78664 0.37131
Die Simulation zeigt, dass Trumps Vorsprung nur in etwa einem Drittel der Simulationen erkannt wird.

simulierten einseitigen Szenario. Die Streuung des Histogramms dieser Ergebnisse ist schmaler

als die Spanne, als nur 1.500 Wähler befragt wurden. Leider verengt sich der Abstand in

auf den falschen Wert. Wir haben die Verzerrung nicht überwunden; wir haben nur ein genaueres Bild.

tung der verzerrten Situation. Big Data ist nicht die Rettung. Außerdem sind größere

Umfragen haben andere Probleme. Sie sind oft schwieriger durchzuführen, weil die Meinungsforscher

die Arbeit mit begrenzten Ressourcen und die Anstrengungen zur Verbesserung des Datenumfangs sind

zur Erweiterung der Umfrage weitergeleitet werden.

Text(0.5, 0, 'Trump führt in der Stichprobe')
Beispiel: Simulation der Verzerrung und Varianz von Wahlumfragen | 37
Im Nachhinein, bei mehreren Umfragen für dieselbe Wahl, können wir Verzerrungen feststellen. In einem Post-

Wahlanalyse von über 4.000 Umfragen für 600 Bundesstaaten, Gouverneurs-, Senats- und Parlamentswahlen

Präsidentschaftswahlen (Shirani-Mehr et al. 2018), dass die Wahlumfragen im Durchschnitt

weisen eine Verzerrung von etwa 1,5 Prozentpunkten auf.

Wenn der Vorsprung des Siegers relativ gering ist, wie es 2016 der Fall war, ist ein größerer Stichprobenumfang

reduziert den Stichprobenfehler, aber leider sind die Vorhersagen, wenn es eine Verzerrung gibt, auch

nahe an der verzerrten Schätzung. Wenn die Verzerrung die Vorhersage von einem Kandidaten zu einem anderen verschiebt

(Trump) gegen eine andere (Clinton), dann kommt es zu einem "überraschenden" Umschwung. Meinungsforscher entwickeln Wähler

Auswahlverfahren, die versuchen, Verzerrungen zu verringern, wie die Trennung der Wählerpräferenzen

nach Bildungsniveau, aber, wie in diesem Fall, kann es schwierig oder sogar unmöglich sein, die

für neue, unerwartete Quellen der Verzerrung. Umfragen sind immer noch nützlich, aber wir müssen anerkennen

die Problematik der Voreingenommenheit zu erkennen und die Voreingenommenheit besser zu verringern.

In diesem Beispiel haben wir das Urnenmodell verwendet, um eine SRS bei einer Umfrage zu untersuchen. Eine andere gängige

Verwendung der Urne in randomisierten kontrollierten Experimenten.

Beispiel: Simulation eines randomisierten Versuchs für einen Impfstoff
Bei einer Arzneimittelstudie erhalten die Probanden entweder die neue Behandlung oder ein Placebo (ein

Fake-Behandlung), und die Forscher kontrollieren die Zuweisung der Freiwilligen zu den Gruppen. In einer

randomisierten kontrollierten Experiment verwenden sie einen Zufallsprozess, um die Zuweisung vorzunehmen.

ment. Die Wissenschaftler verwenden im Wesentlichen ein Urnenmodell, um die Probanden für die Behandlung auszuwählen.

und der Kontrollgruppe (die das Placebo erhielt). Wir können den Zufallsmechanismus simulieren

der Urne, um Schwankungen im Ergebnis eines Experiments besser zu verstehen und die

Bedeutung der Wirksamkeit in klinischen Studien.

38 | Kapitel 2: Simulation und Datenentwurf

Im März 2021 machte der Bürgermeister von Detroit, Mike Duggan, landesweit Schlagzeilen, als er sich

eine Lieferung von über 6.000 Impfstoffdosen von Johnson & Johnson mit der Begründung zurück, dass die Stadt

zen seiner Stadt sollten "das Beste bekommen". Der Bürgermeister bezog sich dabei auf die Wirksamkeitsrate der

des Impfstoffs, die Berichten zufolge bei etwa 66 % lag. Im Vergleich: Moderna und Pfizer

beide meldeten Wirksamkeitsraten von etwa 95 % für ihre Impfstoffe.

Oberflächlich betrachtet scheint Duggans Argumentation stichhaltig, aber der Umfang der drei klinischen Tri-

als nicht vergleichbar sind, d.h. ein direkter Vergleich der Versuchsergebnisse ist

problematisch (Irfan 2021). Außerdem betrachten die Centers for Disease Control (CDC) die

Mit einer Wirksamkeitsrate von 66 % ist es recht gut, weshalb es eine Notfallzulassung erhalten hat.

(Centers for Disease Control 2021).

Wir gehen auf diese Punkte nacheinander ein, zunächst auf den Anwendungsbereich und dann auf die Wirksamkeit.

Umfang
Erinnern Sie sich, dass wir bei der Bewertung des Umfangs der Daten das Wer, Wann und

wo die Studie durchgeführt wird. Für die klinische Studie von Johnson & Johnson, die Teilnehmer:

Die Studie umfasste Erwachsene ab 18 Jahren, von denen etwa 40 % Bedingungen aufwiesen, die als Komor-
Komorbiditäten, die mit einem erhöhten Risiko für eine schwere COVID-19-Erkrankung verbunden sind;
nahmen von Oktober bis November 2020 an der Studie teil; und
kamen aus 8 Ländern auf 3 Kontinenten, darunter die USA und Südafrika.
Die Teilnehmer an den Studien von Moderna und Pfizer stammten hauptsächlich aus den USA,

Etwa 40 % hatten Komorbiditäten für schwere COVID-19, und die Studie fand früher statt,

im Sommer 2020. Der Zeitpunkt und der Ort der Versuche erschweren ihre Durchführung.

pare. Die Zahl der COVID-19-Fälle war im Sommer in den USA auf einem Tiefstand, stieg aber

schnell im Spätherbst. Außerdem wurde eine Variante des Virus verbreitet, die ansteckender ist.

zum Zeitpunkt des J&J-Prozesses in Südafrika rasch zunahm.

Jede klinische Studie diente dazu, einen Impfstoff im Vergleich zu einer Situation ohne Impfstoff zu testen.

unter ähnlichen Bedingungen durch die zufällige Zuweisung von Probanden zur Behandlung

und Kontrollgruppen. Der Umfang der einzelnen Studien ist zwar sehr unterschiedlich, aber die

Durch die Randomisierung innerhalb einer Studie bleibt der Umfang der Behandlungs- und Kontrollgruppen erhalten.

in etwa gleich sind. Dies ermöglicht aussagekräftige Vergleiche zwischen Gruppen in der gleichen

Studie. Der Umfang war in den drei Impfstoffversuchen so unterschiedlich, dass eine direkte

Vergleiche zwischen den drei Studien problematisch.

Wie wurde die Studie für den Impfstoff von Johnson & Johnson durchgeführt? Zu Beginn wurden 43.738

Personen, die an der Studie teilnahmen (Janssen Biotech, Inc. 2021). Diese Teilnehmer wurden aufgeteilt

nach dem Zufallsprinzip in zwei Gruppen aufgeteilt. Die eine Hälfte erhielt den neuen Impfstoff, die andere Hälfte erhielt

ein Placebo, z. B. eine Kochsalzlösung. Dann wurden alle Teilnehmer 28 Tage lang beobachtet, um zu sehen

ob sie sich mit COVID-19 angesteckt haben.

Beispiel: Simulation eines randomisierten Versuchs für einen Impfstoff | 39
Zu jedem Patienten wurden zahlreiche Informationen erfasst, wie Alter, Rasse und Geschlecht,

und außerdem, ob sie an COVID erkrankt sind, einschließlich des Schweregrads der Krankheit. Unter

Nach 28 Tagen fanden sie 468 Fälle von COVID-19, davon 117 in der behandelten Gruppe.

Gruppe, und 351 in der Kontrollgruppe.

Die zufällige Zuteilung der Patienten zur Behandlung und zur Kontrolle gibt den Wissenschaftlern eine

Rahmen zur Bewertung der Wirksamkeit des Impfstoffs. Die typische Argumentation lautet wie folgt

Tiefpunkte:

Gehen Sie von der Annahme aus, dass der Impfstoff unwirksam ist.
Die 468 Personen, die sich mit COVID-19 angesteckt haben, hätten sich also auch ohne
den Impfstoff erhalten hätten.
Die übrigen 43 270 Studienteilnehmer, die nicht erkrankten, wären
gesund geblieben, ob sie nun geimpft wurden oder nicht.
Die Aufteilung von 117 Erkrankten in der Behandlungsgruppe und 351 in der Kontrollgruppe war ausschließlich auf den
Zufallsprozess bei der Zuteilung der Teilnehmer zur Behandlung oder zur Kontrolle.
Wir können ein Urnenmodell aufstellen, das dieses Szenario widerspiegelt, und dann mittels Simulation untersuchen,

das Verhalten der Versuchsergebnisse.

Das Urnenmodell für zufällige Zuweisung
Unsere Urne enthält 43.738 Murmeln, eine für jede Person in der klinischen Studie. Da es

468 Fälle von COVID-19 unter ihnen, kennzeichnen wir 468 Murmeln mit einer 1 und die verbleibenden -

43.270 mit 0. Wir ziehen die Hälfte der Murmeln (21.869) aus der Urne und erhalten die

Behandlung, die andere Hälfte erhält ein Placebo. Die Ergebnisse des Experiments

sind einfach die Anzahl der mit 1 gekennzeichneten Murmeln, die zufällig gezogen wurden

aus der Urne.

Wir können diesen Prozess simulieren, um ein Gefühl dafür zu bekommen, wie wahrscheinlich er unter diesen Bedingungen wäre.

Angenommen, es werden höchstens 117 Murmeln mit der Zahl 1 aus der Urne gezogen. Da wir die Hälfte ziehen

der Murmeln aus der Urne, würden wir erwarten, dass etwa die Hälfte der 468, also 234, aus der Urne stammen.

gezeichnet. Die Simulationsstudie gibt uns ein Gefühl für die Schwankungen, die sich ergeben könnten, wenn

den Prozess der zufälligen Zuweisung. Das heißt, die Simulation kann uns einen ungefähren

Die Wahrscheinlichkeit, dass bei den Versuchen so wenige Fälle des Virus in der Behandlungsgruppe auftraten, war sehr gering.

Dieses Urnenmodell geht von mehreren Grundannahmen aus,
zum Beispiel die Annahme, dass der Impfstoff unwirksam ist. Es ist wichtig
Es ist wichtig, die Abhängigkeit von diesen Annahmen im Auge zu behalten, da unsere Simulationsstudie
Simulationsstudie gibt uns eine Annäherung an die Seltenheit eines Ergebnisses
wie das, das nur unter diesen Schlüsselannahmen beobachtet wurde.
40 | Kapitel 2: Simulation und Datendesign

Wie zuvor können wir das Urnenmodell mit Hilfe der hypergeometrischen Wahrscheinlichkeitsrechnung simulieren.

Die Kommission muss den Zufallsprozess nicht von Grund auf neu programmieren.

simulations_fast = np.random.hypergeometric(ngood=468, nbad=43270, nsample=21869, size=500000)
Text(0.5, 0, 'Fälle in der Behandlungsgruppe')
In unserer Simulation wiederholten wir den Prozess der zufälligen Zuweisung zur Behandlungsgruppe

Gruppe 500.000 Mal. Tatsächlich haben wir festgestellt, dass bei keiner der 500 000 Simulationen 117 oder

weniger Fälle. Es wäre ein extrem seltenes Ereignis, wenn es so wenige Fälle von COVID-19 gäbe, wenn

Der Impfstoff war in der Tat nicht wirksam.

Nach den Problemen beim Vergleich von Arzneimittelstudien mit unterschiedlichem Anwendungsbereich und der Wirksamkeit

zur Vorbeugung schwerer COVID-19-Fälle erläutert wurde, hat der Bürgermeister von Detroit

zog seine ursprüngliche Erklärung zurück und sagte: "Ich habe volles Vertrauen, dass die Johnson &

Der Impfstoff von Johnson ist sowohl sicher als auch wirksam.

Dieses Beispiel hat gezeigt, dass:

Die Verwendung eines Zufallsprozesses bei der Zuordnung von Probanden zu Behandlungen in klinischen Tri-
als kann uns helfen, Was-wäre-wenn-Szenarien zu beantworten;
Die Berücksichtigung des Datenumfangs kann uns helfen zu bestimmen, ob es sinnvoll ist, Zahlen aus verschiedenen Datensätzen zu vergleichen.
Zahlen aus verschiedenen Datensätzen zu vergleichen.
Ein weiteres Beispiel für die Nützlichkeit des Urnenmodells ist der Messfehler.

Beispiel: Simulation eines randomisierten Versuchs für einen Impfstoff | 41
Beispiel: Messfehler bei der Luftqualität
Die Simulation des Ziehens von Murmeln aus einer Urne ist eine nützliche Abstraktion zur Untersuchung der

mögliche Ergebnisse aus Umfragestichproben und kontrollierten Experimenten. Die Simulation

funktioniert, weil sie den Zufallsmechanismus nachahmt, der bei der Auswahl einer Stichprobe oder bei der Zuordnung

Menschen zu einer Behandlung. In vielen Fällen folgt auch der Messfehler einer ähnlichen

Zufallsprozess. Wie in Kapitel 1 erwähnt, weisen Instrumente in der Regel einen Fehler auf.

und durch wiederholte Messungen an demselben Objekt können wir

die mit dem Instrument verbundene Variabilität zu quantifizieren.

Betrachten wir als Beispiel die Daten eines PurpleAir-Sensors, der die Luftqualität misst.

PurpleAir stellt ein Tool zum Herunterladen von Daten zur Verfügung, mit dem jeder auf Luftqualitätsmessungen zugreifen kann.

durch Interaktion mit ihrer Karte. Diese Daten sind in 2-Minuten-Intervallen verfügbar für

jeder Sensor, der auf ihrer Karte erscheint. Um ein Gefühl für das Ausmaß der Abweichungen in

Messungen für einen Sensor, haben wir die Daten für einen Sensor von einem 24-Stunden

Zeitraum und fünf über den Tag verteilte Stunden ausgewählt. Damit erhalten wir fünf Sätze von

dreißig aufeinanderfolgende Messungen für insgesamt 150 Messungen.

pm = pd.read_csv('data/purpleAir2minsample.csv')
pm
aq2.5 Zeit-Stunden-Diffs meds
0 6.14 2022-04-01 00:01:10 UTC^0 0.77 5.38
1 5,00 2022-04-01 00:03:10 UTC^0 -0,38 5,38
2 5,29 2022-04-01 00:05:10 UTC^0 -0,09 5,38
... ... ... ... ... ...
147 8,08 2022-04-01 19:57:20 UTC^19 -0,47 8,55
148 7,38 2022-04-01 19:59:20 UTC^19 -1,18 8,55
149 7,26 2022-04-01 20:01:20 UTC^19 -1,29 8,55
150 Zeilen × 5 Spalten

Das Merkmal aq2.5 in der Datentabelle bezieht sich auf die gemessene Menge an Feinstaub.

in der Luft, die einen Durchmesser von weniger als 2,5 Mikrometern haben (die Maßeinheit für die

Mikrogramm pro Kubikmeter: μg/m3). Diese Messungen sind 2-Minuten

Durchschnittswerte. Ein Streudiagramm kann uns einen Eindruck von den Schwankungen des Instruments vermitteln. Innerhalb einer

Stunde erwarten wir, dass die Messungen ungefähr gleich sind, so dass wir ein Gefühl für die

die Variabilität des Instruments.

42 | Kapitel 2: Simulation und Datenentwurf

Um 11 Uhr vormittags häufen sich die Messwerte um 7 μg/m3, und fünf

Stunden später häufen sie sich um 10 μg/m3 oder so. Während die Höhe der Feinstaubbelastung

sich im Laufe eines Tages ändert, sind die meisten Messungen innerhalb eines Stundenintervalls

innerhalb von etwa +/-1 μg/m3 des Medians. Um ein besseres Gefühl für diese Schwankungen zu bekommen, können wir

die Abweichungen jeder einzelnen Messung vom Median für die

Stunde.

Text(0.5, 0, 'Abweichung vom stündlichen Median')
Beispiel: Messfehler bei der Luftqualität | 43
Das Histogramm zeigt uns, dass die typischen Messfehler dieses Geräts folgende sind

etwa 1 μg/m3. Da die stündlichen Messungen zwischen 4 und 13 μg/m3 liegen, finden wir

das Instrument ist einigermaßen genau. Was wir leider nicht wissen, ist, ob

die Messungen nahe an der tatsächlichen Luftqualität liegen. Zur Erkennung von Verzerrungen im Instrument,

müssen wir Vergleiche mit einem genaueren Instrument anstellen oder Messungen vornehmen.

in einer geschützten Umgebung, in der die Luft eine bekannte Menge an PM2,5 enthält. A

eine umfassendere Analyse des Messfehlers des PurpleAir-Sensors, einschließlich

Voreingenommenheit, wird in ??? angesprochen.

Zusammenfassung
In diesem Kapitel haben wir die Analogie des Ziehens von Murmeln für eine Urne verwendet, um die zufällige

Stichproben aus Populationen, zufällige Zuteilung von Probanden zu Behandlungen in Experi-

und Messfehler. Dieser Rahmen ermöglicht es uns, Simulationsstudien durchzuführen

für hypothetische Erhebungen, Experimente oder andere Zufallsprozesse, um zu untersuchen

ihr Verhalten. Wir haben festgestellt, dass die Wahrscheinlichkeit, bestimmte Ergebnisse einer klinischen Studie zu beobachten

Studie unter der Annahme, dass die Behandlung nicht wirksam war, und wir untersuchten die

Unterstützung für Clinton und Trump mit Stichproben auf der Grundlage der tatsächlich abgegebenen Stimmen bei den Wahlen.

tion. Diese Simulationsstudien ermöglichten es uns, die typischen Abweichungen in der

Zufallsprozesses und zur Annäherung an die Verteilung der zusammenfassenden Statistiken, liegen Trumps

führen. Diese Simulationsstudien zeigten die Stichprobenverteilung einer Statistik auf, und

44 | Kapitel 2: Simulation und Datenentwurf

half uns bei der Beantwortung der Frage, wie wahrscheinlich es ist, dass Ergebnisse wie die unseren unter

das Urnenmodell.

Das Urnenmodell reduziert sich auf einige wenige Grundlagen: die Anzahl der Murmeln in der Urne; was ist

auf jeder Murmel steht; die Anzahl der Murmeln, die aus der Urne zu ziehen sind; und ob oder

nicht zwischen den Ziehungen ersetzt werden. Von dort aus können wir mehr und mehr simulieren

komplexe Datendesigns. Der Kernpunkt der Nützlichkeit der Urne ist jedoch die Abbildung von

der Datenentwurf an die Urne. Wenn die Stichproben nicht zufällig gezogen werden, werden die Probanden nicht überprüft.

Behandlungen zugewiesen werden, oder die Messungen werden nicht auf gut kalibrierten

Ausrüstung, dann hilft uns dieser Rahmen nicht, unsere Daten zu verstehen und

Entscheidungen zu treffen. Andererseits müssen wir auch bedenken, dass die Urne ein

Vereinfachung des eigentlichen Datenerhebungsprozesses. Wenn es in der Realität Verzerrungen in den Daten gibt

Sammlung, dann erfasst die Zufälligkeit, die wir in der Simulation beobachten, nicht die

vollständiges Bild. Allzu oft schieben Datenwissenschaftler diese Ärgernisse beiseite und gehen

nur die durch das Urnenmodell beschriebene Variabilität. Das war eines der Hauptprobleme

in den Umfragen zur Vorhersage des Ergebnisses der US-Präsidentschaftswahlen 2016.

In jedem dieser Beispiele wurde uns die zusammenfassende Statistik, die wir untersucht haben, vorgelegt

als Teil des Beispiels. Im nächsten Kapitel werden wir uns mit der Frage beschäftigen, wie man

eine zusammenfassende Statistik zur Darstellung Ihrer Daten.

Übungen
Bei Cluster-Stichproben wird die Grundgesamtheit in sich nicht überschneidende Untergruppen aufgeteilt,
die in der Regel kleiner als Schichten sind. Die Stichprobenmethode besteht in der Entnahme einer einfachen
Zufallsstichprobe aus den Clustern gezogen und alle Einheiten in den ausgewählten Clustern
in die Stichprobe ein. Verwenden Sie die Analogie der Urne, um die Cluster-Stichprobe darzustellen. Als einfaches
Beispiel: Nehmen wir an, unsere Grundgesamtheit von (7) Raumschiffprototypen wird in (4)
Clustern wie folgt unterteilt: A, B, C, D, E, F, G. Und stellen Sie sich vor, wir nehmen eine SRS aus (2)
Clustern.
-Listen Sie alle möglichen Sammlungen von Raumschiffen auf, die sich daraus ergeben könnten.

-Wie groß ist die Wahrscheinlichkeit, dass A in der Stichprobe enthalten ist?

-Wie groß ist die Wahrscheinlichkeit, dass A, C und E in der Stichprobe enthalten sind?

Cluster-Stichproben haben den Vorteil, dass sie die Probenahme erleichtern. Für
Es ist beispielsweise viel einfacher, 100 Haushalte mit jeweils 2-4 Personen zu befragen als 300
Einzelpersonen. Da sich die Personen in einem Cluster jedoch in der Regel sehr ähnlich sind, müssen wir
müssen wir das Stichprobenverfahren im Auge behalten, wenn wir von der Stichprobe auf die
Bevölkerung. Fahren Sie mit den Raumschiffclustern aus der vorherigen Übung fort und
nehmen Sie zusätzlich an, dass die Prototypen A, B, E und F defekt sind.
Übungen | 45
-Verwenden Sie die Liste aller möglichen Stichproben, die Sie in der vorherigen Übung gefunden haben.
Übung gefunden haben, berechnen Sie den Anteil der fehlerhaften Prototypen für jede Stichprobe.
ple.
-Erstellen Sie die Stichprobenverteilung des Anteils der fehlerhaften Prototypen. Stellen Sie
stellen Sie diese in einer Tabelle dar, die die möglichen Anteile und die Wahrscheinlichkeit der
jeden Bruchteil zu beobachten.
-Vergleich dieser Stichprobenverteilung mit derjenigen, die für eine SRS von drei Prototypen
Typen.
-Stellen Sie fest, dass die Cluster ähnliche Prototypen enthalten (in Bezug darauf, ob sie
defekt sind). Wie wirkt sich dies auf die Form der Stichprobenverteilung aus?
Die systematische Stichprobenziehung ist eine weitere beliebte Technik. Zu Beginn wird die Grundgesamtheit
geordnet, und die erste Einheit wird nach dem Zufallsprinzip aus den ersten k Elementen ausgewählt. Dann wird
wird jede k-te Einheit danach in die Stichprobe aufgenommen. Ein Beispiel: Nehmen wir an, unsere Population
Grundgesamtheit von (7) Prototypen ist alphabetisch geordnet, und wir wählen eine der
aus den ersten drei A, B, C nach dem Zufallsprinzip aus und danach jedes dritte Element.
-Listen Sie alle möglichen Stichproben auf, die sich daraus ergeben könnten.

-Wie groß ist die Wahrscheinlichkeit, dass A in der Stichprobe enthalten ist?

-Wie groß ist die Wahrscheinlichkeit, dass A und B in der Stichprobe enthalten sind? A und G?

Ein Beispiel für eine Online-Interview-Umfrage ist ein Popup-Fenster, in dem Sie aufgefordert werden
einen kurzen Fragebogen auszufüllen. Wenn jedes k
th
Besucher einer Website gebeten wird, einen Fragebogen auszufüllen
Besucher einer Website gebeten wird, einen Fragebogen auszufüllen, dann verhält sich diese Intercept-Umfrage wie eine systematische Stichprobe.
Beschreiben Sie die Grundgesamtheit, den Zugangsrahmen und die Stichprobe für eine Intercept-Umfrage.

-Betrachten Sie einen Strom von Besuchen auf einer Website an einem Tag, V 1 , V 2 , ... Angenommen, Sie
wählen Sie den 20. Besuch, V 20 , auf Ihrer Website als den ersten, der die Popup-Umfrage erhält.
Danach führen Sie die Popup-Umfrage bei jedem 20. Besuch durch. Wie groß ist die
ist die Wahrscheinlichkeit, dass der 17. und 27. Wie groß ist die Chance
dass der 17. und der 37. Besuch in der Stichprobe sind?
-Warum müssen Sie die Größe der Grundgesamtheit nicht kennen, um die oben genannten
Chancen zu berechnen?
-Kann man davon ausgehen, dass diese Stichprobe keinen Selektionsfehler in den Stichprobenprozess einführt?
einen Auswahlfehler in der Stichprobe verursacht? Warum oder warum nicht?
Einem kürzlich erschienenen Bericht zufolge landet die Hälfte der Gefangenen, die in einem bestimmten Jahr in den
Vereinigten Staaten innerhalb von drei Jahren wieder im Gefängnis landen. Ein anderer Bericht
der einzelne aus dem Gefängnis entlassene Häftlinge über einen Zeitraum von 20 Jahren verfolgt, ergab, dass
dass etwa 2/3 der entlassenen Häftlinge in ihrem gesamten Leben nie wieder ins Gefängnis
gesamten Lebenszeit. Wie ist das möglich?
46 | Kapitel 2: Simulation und Datendesign

-Beschreiben Sie für jeden Bericht sorgfältig die interessierende Population und den Zugang

Rahmen.

-Manche haben dieses scheinbare Paradoxon als den Unterschied zwischen einer

"ereignisbezogene" Stichprobe und eine "täterbezogene" Stichprobe. Was könnte das bedeuten?

-Der Begriff der größenabhängigen Stichprobe und der längenabhängigen Stichprobe kommt vor, wenn eine

eine größere/längere Einheit ist eher Teil der Stichprobe als eine andere
kleinere/kürzere Einheit. Erklären Sie, wie eine Längenverzerrung für diese
scheinbar widersprüchlichen Ergebnisse verantwortlich ist.
Übungen | 47
KAPITEL 3

Modellierung mit zusammenfassenden Statistiken

Ein Hinweis für die Leser von Early Release

Mit Early Release ebooks erhalten Sie Bücher in ihrer frühesten Form - den unbearbeiteten und
unbearbeiteten Inhalt, so dass Sie die Vorteile dieser Technologien schon lange
vor der offiziellen Veröffentlichung dieser Titel.
Dies wird das 4. Kapitel des endgültigen Buches sein. Bitte beachten Sie, dass das GitHub-Repositorium
zu einem späteren Zeitpunkt aktiviert wird.
Wenn Sie Anmerkungen haben, wie wir den Inhalt und/oder die Beispiele in diesem Buch verbessern können
diesem Buch verbessern könnten, oder wenn Sie fehlendes Material in diesem Kapitel bemerken, wenden Sie sich bitte an den
Autor unter mpotter@oreilly.com.
Ein Modell ist eine idealisierte Darstellung eines Systems. Wahrscheinlich verwenden Sie ständig Modelle

Zeit. Eine Wettervorhersage zum Beispiel ist ein Modell. Eine Wettervorhersage verwendet vergangene

Wetter, aktuelle Bedingungen und die Physik der Atmosphäre, um Vorhersagen zu treffen

über die Zukunft. Modelle stimmen nicht immer mit der Realität überein, wie Sie erfahren haben, wenn Sie

von Regen oder Schnee überrascht worden. Und selbst die kompliziertesten Modelle für das Wetter

können keine genauen Vorhersagen treffen, die mehr als ein paar Wochen in die Zukunft reichen. Dennoch, das Wetter

Vorhersagen sind so nützlich, dass wir die Vorhersage prüfen, bevor wir uns auf den Weg machen.

Tag.

Wir haben bereits in Kapitel 2 ein Modell namens Urnenmodell vorgestellt. Die Urne

Das Modell vergleicht den zugrunde liegenden Zufallsprozess bei der Datenerzeugung mit dem Ziehen von Murmeln.

aus einer Urne, und wir verwenden es, um Variationen zu untersuchen. In diesem Kapitel stellen wir eine weitere

eine Art Modell, das das Muster/Signal in den Daten beschreibt und nicht den Zufall

Variation. Dieser Prozess wird als Anpassung eines Modells an die Daten bezeichnet. Wir konzentrieren uns auf das einfachste der

49
1 Wir (die Autoren) erfuhren zum ersten Mal von den Busankunftsdaten durch eine Analyse eines Datenwissenschaftlers namens Jake
VanderPlas. Ihm zu Ehren haben wir den Protagonisten dieses Abschnitts benannt.
dieser Art von Modellen, das so genannte Konstantmodell. Es dient als ein nützlicher Baustein

zu den komplexeren Modellen, die später in diesem Buch erscheinen.

Das Konstantenmodell ermöglicht es uns, die Modellanpassung aus der Perspektive der Verlustmini-

misierung, die zusammenfassende Statistiken wie Mittelwert und Median mit umfassenderen Statistiken verknüpft.

plexe Modelle. Wir beginnen mit einem Beispiel, das Daten über die Wartezeiten auf einen Bus verwendet

um das Konstantmodell einzuführen.

Das konstante Modell
Ein Fahrgast, Jake, nimmt häufig den Bus der Linie C in Richtung Norden an der Bushaltestelle 3rd & Pike in

Stadtzentrum von Seattle^1. Der Bus soll alle 10 Minuten kommen, aber Jake bemerkt

dass er manchmal sehr lange auf den Bus warten muss. Er möchte wissen, wie spät der Bus

normalerweise ist. Jake war in der Lage, die geplanten und tatsächlichen Ankunftszeiten zu ermitteln.

das Washington State Transportation Center, damit er die Minuten berechnen kann, die

jeder Bus hat Verspätung an seiner Haltestelle. Wir lesen in diesen Daten.

Strecke Richtung geplant tatsächlich Minuten_verspätet
0 C nordgehend 2016-03-26 06:30:28 2016-03-26 06:26:04 -4.40
1 C Richtung Norden 2016-03-26 01:05:25 2016-03-26 01:10:15 4.83
2 C in nördlicher Richtung 2016-03-26 21:00:25 2016-03-26 21:05:00 4.58
... ... ... ... ... ...
1431 C in nördlicher Richtung 2016-04-10 06:15:28 2016-04-10 06:11:37 -3,85
1432 C in nördlicher Richtung 2016-04-10 17:00:28 2016-04-10 16:56:54 -3,57
1433 C Richtung Norden 2016-04-10 20:15:25 2016-04-10 20:18:21 2.93
1434 Zeilen × 5 Spalten

In der Spalte minutes_late der Datentabelle wird festgehalten, wie spät jeder Bus war. Hinweis

dass einige der Zeiten negativ sind, was bedeutet, dass der Bus früher angekommen ist. Plotten wir

ein Histogramm der Minuten, die jeder Bus Verspätung hat.

50 | Kapitel 3: Modellierung mit zusammenfassenden Statistiken

In den Daten sind bereits einige interessante Muster zu erkennen. Zum Beispiel, viele Busse

kommen früher als geplant und einige sind mehr als 20 Minuten zu spät. Wir sehen auch eine

Clear Mode (Höchstwert) bei 0, was bedeutet, dass viele Busse ungefähr pünktlich ankommen.

Um die Verspätung des Busses zu verstehen, fassen wir die Verspätung in einem

Konstante: Dies ist eine Statistik, eine einzelne Zahl, wie der Mittelwert, der Median oder der Modus. Lassen Sie uns

finden Sie jede dieser zusammenfassenden Statistiken für die minutes_late-Daten.

Anhand des Histogramms schätzen wir den Modus der Daten auf 0. Wir verwenden Python, um

den Mittelwert und den Median zu berechnen.

Mittelwert: 1,92 Min. Verspätung
Median: 0,74 Minuten zu spät
Modus: 0,00 Minuten zu spät
Natürlich wollen wir wissen, welche dieser Zahlen am besten als Zusammenfassung der Verspätung geeignet ist.

Statt uns auf Faustregeln zu verlassen, wählen wir einen eher formalen Ansatz. Wir machen eine

Konstantenmodell für die Verspätung von Bussen. Nennen wir diese Konstante θ (in der Modellierung wird θ oft als

als Parameter bezeichnet). Wenn wir zum Beispiel sagen, dass θ= 5 ist, ist die beste Schätzung unseres Modells

dass der Bus in der Regel 5 Minuten Verspätung hat.

Nun ist θ= 5 kein besonders guter Wert. Aus dem Histogramm der Ankunftszeiten ergibt sich

sah, dass es viel mehr Punkte gibt, die näher an 0 als an 5 liegen. Aber es ist nicht klar, dass θ= 0 (die

Modus) ist eine bessere Wahl als θ= 0. 74 (der Median), θ= 1. 92 (der Mittelwert), oder einige

etwas dazwischen. Um eine genaue Auswahl zwischen verschiedenen Werten von θ zu treffen, müssten wir

jedem Wert von θ eine Punktzahl zuweisen, die angibt, wie gut das Modell mit den Daten übereinstimmt.

Formaler ausgedrückt: Wir verwenden eine Verlustfunktion, um die beste

Parameter, θ, für ein konstantes Modell unserer Daten. Eine Verlustfunktion nimmt als Eingabe einen Wert

von θ und den Punkten in unserem Datensatz. Es gibt eine einzige Zahl aus, die wir zur Auswahl verwenden können

Das konstante Modell | 51
Im nächsten Abschnitt untersuchen wir, wie man Verlustfunktionen definiert und verwendet, um das Modell anzupassen.

dieses konstante Modell.

Verlustfunktionen
Wir modellieren die Verspätung des nach Norden fahrenden Busses C durch eine Konstante namens θ, und wir wollen

die Daten der tatsächlichen Ankunftszeiten zu verwenden, um einen guten Wert für θ zu ermitteln. Um θ zu finden, müssen wir

eine Verlustfunktion verwenden - eine Funktion, die misst, wie weit unser Modell von der Realität entfernt ist.

tatsächliche Daten.

Eine Verlustfunktion ist eine mathematische Funktion, die θ und einen Datenwert y annimmt. Sie ergibt

setzt eine einzelne Zahl, den Verlust, der misst, wie weit θ von y entfernt ist. Wir schreiben den

Verlustfunktion als 𝐀 θ,y.

Konventionell gibt die Verlustfunktion niedrigere Werte für bessere Werte von θ und größere

Werte für schlechtere θ. Um ein Modell an unsere Daten anzupassen, wählen wir dasjenige θ, das zu

einen geringeren durchschnittlichen Verlust über alle anderen Wahlmöglichkeiten hinweg; mit anderen Worten, wir finden den θ, der die

z ist der durchschnittliche Verlust für unsere Daten, y 1 , ...,yn. Wir schreiben den durchschnittlichen Verlust als

Lθ,y 1 ,y 2 , ...,yn, wobei

Lθ,y 1 ,y 2 , ...,yn = mean𝐀 θ,y 1 ,𝐀θ,y 2 , ...,𝐀θ,yn

=
1
n
∑
i= 1
n
𝐀θ,yi
Als Abkürzung definieren wir oft den Vektor 𝐀= y 1 ,y 2 , ...,yn. Dann können wir schreiben die

Durchschnittsverlust als:

Lθ,𝐀 =
1
n
∑
i= 1

n
𝐀θ,yi
Beachten Sie, dass 𝐀θ,y den Verlust des Modells für einen einzelnen Datenpunkt angibt
während Lθ,𝐀 den durchschnittlichen Verlust des Modells für alle unsere Daten
Punkte angibt. Das große L hilft uns zu erkennen, dass der durchschnittliche Verlust
mehrere kleinere 𝐀-Werte zusammenfasst.
Sobald wir eine Verlustfunktion definiert haben, können wir den Wert von θ finden, der den kleinsten

durchschnittlichen Verlust. Wir nennen diesen minimierenden Wert θ. Mit anderen Worten: Von allen möglichen θ

Werten ist θ derjenige, der den geringsten durchschnittlichen Verlust für unsere Daten erzeugt. Wir nennen dies

Prozess der Anpassung eines Modells an unsere Daten, da er das beste konstante Modell für eine gegebene

Verlustfunktion.

Als nächstes betrachten wir zwei spezifische Verlustfunktionen: den absoluten Fehler und den quadratischen Fehler. Unser Ziel

ist es, unser Modell anzupassen und θ für jede dieser Verlustfunktionen zu finden.

52 | Kapitel 3: Modellierung mit zusammenfassenden Statistiken

Mittlerer absoluter Fehler
Wir beginnen mit der Verlustfunktion des absoluten Fehlers. Die Idee hinter dem absoluten Verlust ist die folgende. Für

einen bestimmten Wert von θ und einen Datenwert y,

Ermitteln Sie den Fehler: y-θ, und
Nehmen Sie den absoluten Wert des Fehlers: y-θ.
Unsere Verlustfunktion ist also 𝐀θ,y = y-θ.

Der absolute Wert des Fehlers ist eine einfache Methode, um negative Fehler in

positive. Zum Beispiel ist der Punkt y= 4 gleich weit von θ= 2 und θ= 6 entfernt,

Die Fehler sind also gleichermaßen "schlimm".

Der durchschnittliche Verlust wird als mittlerer absoluter Fehler, kurz MAE, bezeichnet. Das heißt, der

MAE ist der Durchschnitt der einzelnen absoluten Fehler:

Lθ,𝐀 =

1
n
∑
i= 1
n
yi-θ
Beachten Sie, dass der Name MAE angibt, wie er zu berechnen ist: Nehmen Sie den Mittelwert der abso-

Laute Fehler.

Wir können eine einfache Python-Funktion schreiben, um diesen Verlust zu berechnen:

def mae_loss(theta, y_vals):
return np.mean(np.abs(y_vals - theta))
Schauen wir uns an, wie sich diese Verlustfunktion verhält, wenn wir nur fünf Punkte haben - 1, 0, 2, 5, 10.

Wir können verschiedene Werte von θ ausprobieren und sehen, was die Verlustfunktion für jeden Wert ausgibt.

Verlustfunktionen | 53
Wir empfehlen, einige dieser Verlustwerte von Hand zu überprüfen, um sicherzugehen, dass Sie sie verstehen

wie die MAE berechnet wird.

Von den ausprobierten Werten von θ hat θ= 2 den niedrigsten mittleren absoluten Wert.

Fehler. In diesem einfachen Beispiel ist (2) der Median der Datenwerte. Dies ist keine Münze...

Zufall - sehen wir uns nun an, was die Verlustfunktion für den ursprünglichen Datensatz der Busse ausgibt

Zeiten. Wir finden den MAE-Wert, wenn wir θ auf den Modus, den Median und den Mittelwert der Ankunftszeiten setzen

Zeiten.

Es zeigt sich erneut, dass der Median (mittleres Diagramm) einen geringeren Verlust ergibt als der Modus und

Mittelwert (linkes und rechtes Diagramm). In der Tat können wir beweisen, dass für den absoluten Verlust, die Minimierungs-.

ing θ ist der Mediany 1 ,y 2 , ...,yn. (Der Beweis erscheint in den Übungen.)

Bislang haben wir den besten Wert von θ durch einfaches Ausprobieren einiger Werte gefunden und dann

diejenige mit dem geringsten Verlust zu wählen. Um ein besseres Gefühl für die MAE als Funktion zu bekommen

von θ können wir viele weitere Werte von θ ausprobieren, um eine vollständige Kurve zu zeichnen, die zeigt, wie

Lθ,𝐀 ändert sich, wenn sich θ ändert. Wir zeichnen die Kurve für das Beispiel von oben mit der

fünf Datenwerte, - 1, 0, 2, 5, 10.

Das obige Diagramm zeigt, dass θ= 2 tatsächlich die beste Wahl für den kleinen Datensatz von fünf

Werte. Beachten Sie die Form der Kurve. Sie ist stückweise linear, wobei die Liniensegmente kon-

54 | Kapitel 3: Modellierung mit zusammenfassenden Statistiken

an der Stelle der Datenwerte (-1, 0, 2 und 5). Dies ist eine Eigenschaft der abso-

lute Wertfunktion. Bei einer großen Datenmenge sind die flachen Stücke weniger offensichtlich. Unsere Busdaten

haben über 1400 Punkte und ihre MAE-Kurve erscheint unten.

Anhand dieses Diagramms können wir bestätigen, dass der Median der Daten das Minimum ist

Wert, mit anderen Worten θ= 0,74.

Betrachten wir nun eine andere Verlustfunktion, den quadratischen Fehler.

Mittlerer quadratischer Fehler
Wir haben ein konstantes Modell für unsere Daten verwendet und festgestellt, dass der mittlere absolute Fehler,

der Minimierer ist der Median. Nun behalten wir unser Modell bei, wechseln aber zu einem anderen...

ent Verlustfunktion: quadratischer Fehler. Anstatt die absolute Differenz zwischen unseren

Datenwertes y und der Konstante θ wird der Fehler quadriert. Das heißt, für einen bestimmten Wert von θ und

Datenwert y,

Ermitteln Sie den Fehler: y-θ und
Nehmen Sie den quadrierten Wert des Fehlers: y-θ
2
.
Dies ergibt die Verlustfunktion 𝐀θ,y = y-θ^2.

Wie zuvor wollen wir alle unsere Daten verwenden, um das beste θ zu finden, also berechnen wir den Mittelwert

quadratischer Fehler, kurz MSE:

Lθ,𝐀 =Lθ,y 1 ,y 2 , ...,yn =

1
n
∑
i= 1
n
yi-θ
2
Wir können eine einfache Python-Funktion schreiben, um den MSE zu berechnen:

Verlustfunktionen | 55
def mse_loss(theta, y_vals):
return np.mean((y_vals - theta) ** 2)
Probieren wir noch einmal den Mittelwert, den Median und den Modus als potenzielle Minimierer des MSE aus.

Wenn wir nun das konstante Modell unter Verwendung des MSE-Verlustes anpassen, stellen wir fest, dass der Mittelwert (rechts

Diagramm) hat einen geringeren Verlust als der Modus und der Median (linkes und mittleres Diagramm).

Zeichnen wir die MSE-Kurve für verschiedene Werte von θ für unsere Daten. Wir können sehen, dass die

Minimierungswert θ liegt nahe bei 2.

Ein auffälliges Merkmal dieser Kurve ist, wie schnell die MSE wächst

im Vergleich zur MAE (beachten Sie den Bereich auf der vertikalen Achse). Dieses Wachstum hat zu tun mit

mit der Art der Fehlerquadrierung; sie führt zu einem viel höheren Verlust bei Datenwerten, die weiter entfernt sind

weg. Wenn θ= 10 und y= 110 ist, beträgt der quadratische Verlust 10 - 110
2
= 10000, während der abso-

lute Verlust ist 10 - 110 = 100. Aus diesem Grund reagiert der MSE empfindlicher auf ungewöhnlich große

Werte als MAE.

56 | Kapitel 3: Modellierung mit zusammenfassenden Statistiken

Wenn wir den MSE-Verlust verwenden, sehen wir, dass θ der Mittelwert𝐀 zu sein scheint. Auch dies ist kein bloßer

Koinzidenz; der Mittelwert der Daten ergibt immer den θ, der den MSE minimiert. Wir

können wir zeigen, wie dies zustande kommt, indem wir die quadratische Natur des MSE nutzen. Zu Beginn addieren wir

und subtrahieren Sie y in der Verlustfunktion und erweitern Sie das Quadrat wie folgt:

Lθ,𝐀 =

1
n
∑
i= 1
n
yi-θ^2
=
1
n
∑
i= 1
n
yi-y + y-θ
2
=
1
n
∑
i= 1
n
yi-y
2
+ 2yi-y y-θ + y-θ
2
Als Nächstes teilen wir den MSE in die Summe dieser drei Terme auf und stellen fest, dass der mittlere Term

ist aufgrund der einfachen Eigenschaft des Durchschnitts 0: ∑yi-y = 0.

1
n
∑
i= 1
n
yi-y
2
+
1
n
∑
i= 1
n
2 yi-y y-θ +
1
n
∑
i= 1
n
y-θ
2
=
1
ni∑= 1
n
yi-y
2
+ 2y-θ
1
ni∑= 1
n
yi-y +
1
ni∑= 1
n
y-θ
2
=
1
n
∑
i= 1
n
yi-y^2 + y-θ^2
Von den beiden verbleibenden Termen ist der erste nicht mit θ verbunden und der zweite ist immer

nicht-negativ:

Lθ,𝐀 =

1
n
∑
i= 1
n
yi-y
2
+
1
n
∑
i= 1
n
y-θ
2
≥
1
n
∑
i= 1
n
yi-y^2
Diese Ungleichung zeigt uns, dass der MSE minimiert wird, wenn θ gleich y ist, d. h. es gibt eine

einen einzigen Wert für θ, der für jeden Datensatz den kleinstmöglichen MSE-Wert ergibt, und dieser Wert ist

θ=y.

Wir haben gesehen, dass für den absoluten Verlust der Median die beste Konstante ist, aber für den quadratischen

Fehler, es ist der Mittelwert. Die Wahl der Verlustfunktion ist ein wichtiger Aspekt des Modells

Anprobe.

Auswahl von Verlustfunktionen
Nachdem wir nun mit zwei Verlustfunktionen gearbeitet haben, können wir zu unserer ursprünglichen Frage zurückkehren.

Frage: Warum sollten wir den Median, den Mittelwert oder den Modus gegenüber den anderen wählen? Da der

Verlustfunktionen | 57
2 Der Modus minimiert eine Verlustfunktion namens 0-1-Verlust. Obwohl wir diesen speziellen Verlust nicht behandelt haben, ist die Vorgehensweise
Obwohl wir uns mit diesem speziellen Verlust beschäftigt haben, ist das Verfahren identisch: Man wählt die Verlustfunktion aus und findet dann diejenige, die den Verlust minimiert.
Wenn die Statistik verschiedene Verlustfunktionen^2 minimiert, kann man auch fragen: Welches ist die verlustreichste Funktion?

geeignete Verlustfunktion für unser Problem? Um diese Frage zu beantworten, betrachten wir die

Zusammenhang mit unserem Problem.

Im Vergleich zur MAE liefert die MSE besonders große Verlustwerte, wenn der Bus

viel später (oder früher) als erwartet. Ein Busfahrer, der verstehen will, wie die

typische Wartezeiten würden MAE und den Median (0,74 Minuten Verspätung) verwenden, und ein Fahrgast

der unerwartete Wartezeiten verabscheut, würde die Wartezeit mit MSE zusammenfassen und

den Mittelwert (1,92 Minuten Verspätung).

Wenn wir das Modell noch mehr verfeinern wollen, können wir eine speziellere Verlustfunktion verwenden.

tion. Wenn zum Beispiel ein Bus zu früh kommt, wartet er an der Haltestelle, bis der planmäßige

Abfahrtszeit, dann sollten wir eine frühe Ankunft mit 0 Verlusten bewerten. Und, wenn ein wirklich

ein verspäteter Bus ein größeres Ärgernis ist als ein mäßig verspäteter, könnten wir einen asymmetrischen Bus wählen.

metrische Verlustfunktion, die besonders späte Ankünfte stärker bestraft.

Bei der Wahl einer Verlustfunktion kommt es im Wesentlichen auf den Kontext an. Durch sorgfältiges Nachdenken

darüber, wie wir das angepasste Modell verwenden wollen, können wir eine Verlustfunktion wählen, die uns hilft

anhand von Daten gute Entscheidungen zu treffen.

Zusammenfassung
Wir haben das Konstantenmodell eingeführt: ein Modell, das alle Daten durch eine

Einzelwert. Zur Anpassung des konstanten Modells wählten wir eine Verlustfunktion, die misst, wie

eine gegebene Konstante zu einem Datenwert passt, und wir berechneten den durchschnittlichen Verlust über alle

die Datenwerte. Wir haben gesehen, dass wir je nach Wahl der Verlustfunktion ein unterschiedli- ches Ergebnis erhalten.

ent minimierenden Wert. Wir haben festgestellt, dass der Mittelwert den durchschnittlichen quadratischen Fehler minimiert

(MSE) und der Median minimiert den durchschnittlichen absoluten Fehler (MAE). Wir dis-

erörtert, wie wir den Kontext und das Wissen über unser Problem einbeziehen können, um die Verluste

Funktionen.

Die Idee der Modellanpassung durch Verlustminimierung bindet einfache zusammenfassende Statistiken.

wie Mittelwert, Median und Modus - zu komplexeren Modellierungssituationen. Die Schritte

die wir zur Modellierung unserer Daten verwendet haben, gelten für viele Modellierungsszenarien:

Wählen Sie die Form eines Modells (z. B. das Konstantmodell)
Wählen Sie eine Verlustfunktion (z. B. absoluter Fehler)
Anpassen des Modells durch Minimieren des durchschnittlichen Verlusts für die Daten
58 | Kapitel 3: Modellierung mit zusammenfassenden Statistiken

Im weiteren Verlauf dieses Buches bauen alle unsere Modellierungstechniken auf einem oder mehreren der folgenden Punkte auf

diese Schritte. Wir führen neue Modelle (1), neue Verlustfunktionen (2) und neue Techniken ein

um den Verlust zu minimieren (3).

Im nächsten Kapitel wird die Studie über einen Bus, der mit Verspätung an seiner Haltestelle ankommt, erneut aufgegriffen. Diesmal geht es um

zurück, um alle Phasen des Lebenszyklus der Datenwissenschaft als Fallstudie zu besuchen.

Übungen
Eine andere Verlustfunktion namens Huber-Verlust kombiniert den absoluten und den quadrierten
Verlust, um eine Verlustfunktion zu erstellen, die sowohl glatt als auch robust gegenüber Ausreißern ist. Der
Huber-Verlust erreicht dies, indem er sich für θ-Werte nahe dem Minimum wie der quadratische Verlust verhält
Werte in der Nähe des Minimums verhält und bei θ-Werten, die weit vom Minimum entfernt sind, zum absoluten Verlust
mum. Nachstehend finden Sie eine Formel für eine vereinfachte Version des Huber-Verlusts. Verwenden Sie diese Definition
Definition des Huber-Verlusts
-Schreiben Sie eine Funktion namens mhe, um den mittleren Huber-Fehler zu berechnen.

Zeichnen Sie die glatte mhe-Kurve für die Buszeitdaten, wobei θ zwischen -2 und 8 liegt.

-Finden Sie durch Versuch und Irrtum die Minimierung von θ für die Buszeiten.

lθ,y =

1
2
y-θ
2
für y-θ ≤ 2
= 2 y-θ - 1 sonst.
Fahren Sie mit dem Huber-Verlust und der Funktion mhe aus dem vorherigen Problem fort:
-Zeichnen Sie die glatte mhe für die fünf Datenpunkte - 2, 0, 1, 5, 10.

-Beschreiben Sie die Kurve.

-Wie lautet für diese fünf Punkte die Minimierung von θ?

-Was passiert, wenn der Datenpunkt 10 durch 100 ersetzt wird? Vergleichen Sie den Mini-
mizer mit dem Mittelwert und dem Median der fünf Punkte.
Betrachten Sie eine Verlustfunktion, die 0 Verlust für negative Werte von y und quadratischen Verlust
für positive y-Werte hat.
-Schreiben Sie eine Funktion namens m0e, die den durchschnittlichen Verlust für diese Funktion berechnet.

Zeichnen Sie die m0e-Kurve für viele θs bei den Daten 𝐀= - 2, 0, 1, 5, 10

-Finde durch Versuch und Irrtum die Minimierung von θ.

-Intuitiv, was sollte der minimierende Wert sein? Was wäre, wenn wir den linearen Verlust
stattdessen?
Übungen | 59
In dieser Übung zeigen wir erneut, dass der Mittelwert den mittleren quadratischen Fehler minimiert,
aber wir werden stattdessen die Infinitesimalrechnung verwenden.
-Bilden Sie die Ableitung des mittleren Verlustes nach θ.

-Setzen Sie die Ableitung auf 0 und lösen Sie für θ.

-Zur Sicherheit nehmen Sie eine zweite Ableitung, um zu bestätigen, dass y ein Minimierer ist.
(Es sei daran erinnert, dass die quadratische Funktion konkav ist, wenn die zweite Ableitung positiv ist).
Führen Sie die folgenden Schritte aus, um festzustellen, dass MAE für den Median minimiert ist.
-Teilen Sie die Summation auf,
1
n∑i= 1
n y
i-θ in drei Terme auf, wenn yi-θ negativ
tive, 0 und positiv ist.
-Setze den mittleren Term auf 0, damit die Gleichungen leichter zu handhaben sind. Verwenden Sie die
Tatsache, dass die Ableitung des absoluten Wertes -1 oder +1 ist, um die
die beiden verbleibenden Terme nach θ.
-Setzen Sie die Ableitung auf 0 und vereinfachen Sie die Terme. Erklären Sie, warum bei einer ungeraden
Anzahl von Punkten die Lösung der Median ist.
-Erläutern Sie, warum bei einer geraden Anzahl von Punkten die Minimierung von θ
nicht eindeutig definiert ist (genau wie beim Median).
60 | Kapitel 3: Modellierung mit zusammenfassenden Statistiken

KAPITEL 4

Arbeiten mit Dataframes unter Verwendung von pandas

Ein Hinweis für die Leser von Early Release

Mit Early Release ebooks erhalten Sie die Bücher in ihrer frühesten Form, d. h. den unbearbeiteten und
unbearbeiteten Inhalt, so dass Sie die Vorteile dieser Technologien schon lange
vor der offiziellen Veröffentlichung dieser Titel.
Dies wird das 6. Kapitel des endgültigen Buches sein. Bitte beachten Sie, dass das GitHub-Repositorium
zu einem späteren Zeitpunkt aktiviert wird.
Wenn Sie Anmerkungen haben, wie wir den Inhalt und/oder die Beispiele in diesem Buch verbessern können
diesem Buch verbessern könnten, oder wenn Sie fehlendes Material in diesem Kapitel bemerken, wenden Sie sich bitte an den
Autor unter mpotter@oreilly.com.
Datenwissenschaftler arbeiten mit Daten, die in Tabellen gespeichert sind. In diesem Kapitel werden Dataframes vorgestellt,

eine der am weitesten verbreiteten Methoden zur Darstellung von Datentabellen. Wir werden auch pan

das, das Standard-Python-Paket für die Arbeit mit Dataframes. Hier ist ein Beispiel für

ein Datenrahmen, der Informationen über beliebte Hunderassen enthält:

Pflege Futter_Kosten Kinder Größe
Rasse
Labrador Retriever wöchentlich 466,0 hoch mittel
Deutscher Schäferhund wöchentlich 466,0 mittel groß
Beagle täglich 324,0 hoch klein
Golden Retriever wöchentlich 466,0 hoch mittel
Yorkshire Terrier täglich 324,0 niedrig klein
Bulldogge wöchentlich 466,0 mittel mittel
Boxer wöchentlich 466,0 hoch mittel
61
In einem Datenrahmen steht jede Zeile für einen einzelnen Datensatz - in diesem Fall für eine einzelne Hunderasse.

Jede Spalte stellt ein Merkmal des Datensatzes dar, z. B. die Spalte "Pflege".

Die Tabelle zeigt, wie oft die einzelnen Hunderassen gepflegt werden müssen.

Datenrahmen haben Beschriftungen sowohl für Spalten als auch für Zeilen. Zum Beispiel hat dieser Datenrahmen eine

eine Spalte mit der Bezeichnung "Pflege" und eine Zeile mit der Bezeichnung "Deutscher Schäferhund". Die Spalten und

Zeilen eines Datenrahmens angeordnet sind - wir können die Labrador Retriever-Zeile als

erste Zeile des Datenrahmens.

Innerhalb einer Spalte haben die Daten den gleichen Typ. Zum Beispiel enthält die Spalte food_cost

enthält Zahlen, und die Spalte Größe enthält Kategorien. Aber Datentypen können unterschiedlich sein.

ent innerhalb einer Reihe.

Aufgrund dieser Eigenschaften ermöglichen Dataframes alle möglichen nützlichen Operationen.

Als Datenwissenschaftler arbeiten Sie häufig mit Menschen
mit unterschiedlichen Hintergründen zusammen, die unterschiedliche Begriffe verwenden. Zum Beispiel,
sagen Informatiker, dass die Spalten eines Datenrahmens Merkmale der Daten darstellen
Merkmale der Daten darstellen, während Statistiker sie stattdessen als Variablen bezeichnen.
In anderen Fällen wird derselbe Begriff für etwas anderes verwendet.
unterschiedliche Dinge. Datentypen im Sinne der Programmierung beziehen sich darauf, wie ein
Computer Daten intern speichert. Zum Beispiel hat die Spalte Größe in
einen String-Datentyp in Python. Vom statistischen Standpunkt aus betrachtet speichert die
Spalte Größe geordnete kategoriale Daten (Ordinaldaten). Wir sprechen
mehr über diese spezielle Unterscheidung im Kapitel ???.
In diesem Kapitel werden wir Ihnen zeigen, wie Sie gängige Datenrahmenoperationen mit pan

das. Datenwissenschaftler verwenden die Pandas-Bibliothek, wenn sie mit Datenrahmen in Python arbeiten.

Zuerst werden wir die wichtigsten Objekte erklären, die Pandas zur Verfügung stellt: DataFrame und Series

Klassen. Dann zeigen wir Ihnen, wie Sie mit Pandas gängige Datenmanipulationen durchführen können.

tionsaufgaben, wie Zerlegen, Filtern, Sortieren, Gruppieren und Verbinden.

Teilmengenbildung
In diesem Abschnitt werden Operationen zur Bildung von Teilmengen von Datenrahmen vorgestellt. Wenn Data Scien-

Beim ersten Einlesen eines Datenrahmens wollen sie oft die spezifischen Daten, die sie planen, unterteilen.

zu verwenden. Ein Datenwissenschaftler kann zum Beispiel die zehn relevanten Merkmale aus einer Datenmenge herausschneiden.

Rahmen mit Hunderten von Spalten. Oder sie können einen Datenrahmen filtern, um Zeilen mit

unvollständige Daten. Im weiteren Verlauf dieses Kapitels werden wir Datenrahmenoperationen vorstellen

unter Verwendung eines Datenrahmens mit Babynamen.

62 | Kapitel 4: Arbeiten mit Dataframes unter Verwendung von pandas

Über die Daten
In einem Artikel der New York Times aus dem Jahr 2021 ist die Rede von Prinz Harry und Meghans

für den Namen ihrer neuen Tochter entschieden: Lilibet (Williams 2021). Die Kunst-.

cle hat ein Interview mit Pamela Redmond, einer Expertin für Babynamen, die über

über interessante Trends bei der Namensgebung für ihre Kinder. Sie sagt zum Beispiel, dass

Namen, die mit dem Buchstaben "L" beginnen, sind in den letzten Jahren sehr beliebt geworden, während

Namen, die mit dem Buchstaben "J" beginnen, waren in den 1970er und 1980er Jahren am beliebtesten. Sind

diese Behauptungen in den Daten widerspiegeln? Wir können Pandas verwenden, um das herauszufinden.

Importieren Sie zunächst das Paket als pd, die kanonische Abkürzung:

import pandas as pd
Wir haben einen Datensatz mit Babynamen in einer CSV-Datei (comma-separated values) gespeichert

namens babynames.csv. Verwenden Sie die Funktion pd.read_csv, um die Datei als pan zu lesen

das.DataFrame-Objekt.

baby = pd.read_csv('babynames.csv')
Baby
Name Geschlecht Anzahl Jahr
0 Liam M^196592020
1 Noah M^182522020
2 Oliver M^141472020
... ... ... ... ...
2020719 Verona F^51880
2020720 Vertie F^51880
2020721 Wilma F^51880
2020722 Zeilen × 4 Spalten

Die Daten in der Babytabelle stammen von der US-Sozialversicherungsbehörde, die

nimmt den Namen und das Geschlecht des Babys für die Geburtsurkunde auf. Sie machen die

Die Daten zu den Babynamen sind auf ihrer Website verfügbar (Abteilung 2021). Wir haben diese Daten geladen

in den Babytisch.

Auf der Website der Sozialversicherung gibt es eine Seite, auf der die Daten ausführlicher beschrieben werden (Link).

Wir werden in diesem Kapitel nicht näher auf die Einschränkungen der Daten eingehen, aber wir weisen darauf hin

dieses relevante Zitat von der Website:

Alle Namen stammen aus Anträgen auf Sozialversicherungskarten für Geburten, die in den
Vereinigten Staaten nach 1879. Beachten Sie, dass viele Menschen, die vor 1937 geboren wurden, nie einen Antrag auf eine
Sozialversicherungskarte beantragt haben, so dass ihre Namen in unseren Daten nicht enthalten sind. Für andere, die einen
die einen Antrag gestellt haben, kann es sein, dass unsere Aufzeichnungen den Geburtsort nicht enthalten, und auch ihre Namen sind nicht
in unseren Daten enthalten.
Unterauswahl | 63
Alle Daten stammen aus einer 100-prozentigen Stichprobe unserer Aufzeichnungen über Anträge auf Sozialversicherungskarten ab
vom März 2021.
DataFrames und Indizes
Schauen wir uns den Baby-Datenrahmen genauer an. Ein Datenrahmen hat Zeilen und Spalten.

Jede Zeile und Spalte hat eine Beschriftung, wie in Abbildung 4-1 hervorgehoben.

Abbildung 4-1. Der Baby-Datenrahmen hat Beschriftungen für Zeilen und Spalten (umrahmt).

Standardmäßig weist Pandas Zeilenbeschriftungen als aufsteigende Zahlen zu, beginnend bei 0. In

In diesem Fall haben die Daten in der Zeile mit der Bezeichnung 0 und der Spalte mit der Bezeichnung Name den Wert "Liam".

64 | Kapitel 4: Arbeiten mit Dataframes mit pandas

Datenrahmen können auch Zeichenketten als Zeilenbeschriftungen haben. Abbildung 4-2 zeigt einen Datenrahmen von Hund

Daten, bei denen die Zeilenbezeichnungen Zeichenketten sind.

Abbildung 4-2. Zeilenbeschriftungen in Datenrahmen können auch Strings sein. In diesem Beispiel ist jede Zeile

mit dem Namen der Hunderasse beschriftet.

Die Zeilenbeschriftungen haben einen besonderen Namen. Wir nennen sie den Index eines Datenrahmens, und pan

das speichert die Zeilenbeschriftungen in einem speziellen pandas.Index-Objekt. Wir werden nicht über das pan

Das.Index-Objekt, da es nicht sehr üblich ist, den Index selbst zu manipulieren. Aber, es ist

Wichtig ist, dass der Index zwar wie eine Datenspalte aussieht, die

Index wirklich Zeilenbeschriftungen und keine Daten darstellt. Zum Beispiel der Datenrahmen der Hunderassen

hat 4 Spalten mit Daten, nicht 5, da der Index nicht als Spalte zählt.

Slicing
Slicing ist ein Vorgang, bei dem ein neuer Datenrahmen erstellt wird, indem eine Teilmenge von Zeilen oder Spalten genommen wird.

umns aus einem anderen Datenrahmen. Denken Sie daran, wie Sie eine Tomate in Scheiben schneiden - Scheiben können sowohl

vertikal und horizontal. Um Slices eines Datenrahmens in Pandas zu erstellen, verwenden wir das .loc

und .iloc-Eigenschaften. Beginnen wir mit .loc.

Hier ist der vollständige Baby-Datenrahmen:

Baby
Name Geschlecht Anzahl Jahr
0 Liam M^196592020
Untermenge | 65
Name Geschlecht Anzahl Jahr
1 Noah M^182522020
2 Oliver M^141472020
... ... ... ... ...
2020719 Verona F^51880
2020720 Vertie F^51880
2020721 Wilma F^51880
2020722 Zeilen × 4 Spalten

.loc können Sie Zeilen und Spalten anhand ihrer Beschriftung auswählen. Zum Beispiel, um die Daten zu erhalten

in der Zeile mit der Bezeichnung 1 und der Spalte mit der Bezeichnung Name:

# Das erste Argument ist die Zeilenbezeichnung
# ↓
baby.loc[1, 'Name']
# ↓
# Das zweite Argument ist die Spaltenbeschriftung
'Noah'
Beachten Sie, dass .loc Klammern benötigt; die Ausführung von baby.loc(1, 'Name')
wird fehlschlagen.
Um mehrere Zeilen oder Spalten auszuschneiden, können Sie die Python-Slice-Syntax anstelle von indi-

viduelle Werte:

baby.loc[0:3, 'Name':'Anzahl']
Name Geschlecht Anzahl
0 Liam M^19659
1 Noah M^18252
2 Oliver M^14147
3 Elijah M^13034
Um eine ganze Datenspalte zu erhalten, übergeben Sie ein leeres Slice als erstes Argument:

baby.loc[:, 'Count']
0 19659
1 18252
2 14147
...
2020719 5
66 | Kapitel 4: Arbeiten mit Dataframes mit pandas

2020720 5
2020721 5
Name: Count, Länge: 2020722, dtype: int64
Beachten Sie, dass die Ausgabe nicht wie ein Datenrahmen aussieht, und das ist sie auch nicht. Auswählen

eine einzelne Zeile oder Spalte eines Datenrahmens erzeugt ein pd.Series-Objekt.

counts = baby.loc[:, 'Count']
counts.__class__.__name__
'Reihe'
Was ist der Unterschied zwischen einem pd.Series und pd.DataFrame Objekt? Im Wesentlichen ist ein

pd.DataFrame ist zweidimensional - es hat Zeilen und Spalten und stellt eine Tabelle mit

Daten. Eine pd.Series ist eindimensional - sie stellt eine Liste von Daten dar. pd.Series und

pd.DataFrame-Objekte haben viele Methoden gemeinsam, aber sie stellen eigentlich zwei

unterschiedliche Dinge. Eine Verwechslung der beiden kann zu Fehlern und Verwirrung führen.

Um bestimmte Spalten eines Datenrahmens auszuwählen, übergeben Sie eine Liste in .loc:

# Hier ist der ursprüngliche Datenrahmen
Baby
Name Geschlecht Anzahl Jahr
0 Liam M^196592020
1 Noah M^182522020
2 Oliver M^141472020
... ... ... ... ...
2020719 Verona F^51880
2020720 Vertie F^51880
2020721 Wilma F^51880
2020722 Zeilen × 4 Spalten

# Und hier ist der Datenrahmen mit nur den Spalten Name und Jahr
baby.loc[:, ['Name', 'Jahr']]
# └──────┬───────┘
# Liste der Spaltenbezeichnungen
Name Jahr
0 Liam^2020
1 Noah^2020
2 Oliver^2020
... ... ...
2020719 Verona^1880
2020720 Vertie^1880
Teilmenge | 67
Name Jahr
2020721 Wilma^1880
2020722 Zeilen × 2 Spalten

Die Auswahl von Spalten ist sehr häufig, daher gibt es eine Kurzschrift.

# Kurzform für baby.loc[:, 'Name']
baby['Name']
0 Liam
1 Noah
2 Oliver
...
2020719 Verona
2020720 Vertie
2020721 Wilma
Name: Name, Länge: 2020722, dtype: object
# Kurzzeichen für baby.loc[:, ['Name', 'Anzahl']]
baby[['Name', 'Anzahl']]
Name Anzahl
0 Liam^19659
1 Noah^18252
2 Oliver^14147
... ... ...
2020719 Verona^5
2020720 Vertie^5
2020721 Wilma^5
2020722 Zeilen × 2 Spalten

Das Slicen mit .iloc funktioniert ähnlich wie .loc, mit dem Unterschied, dass .iloc die Positionen von

Zeilen und Spalten statt Beschriftungen. Es ist am einfachsten, den Unterschied zwischen .iloc

und .loc, wenn der Index des Datenrahmens Zeichenketten enthält, so dass wir zu Demonstrationszwecken

einen Datenrahmen mit Informationen über Hunderassen betrachten:

Hunde = pd.read_csv('Hunde.csv', index_col='Rasse')
Hunde
Pflege Futter_Kosten Kinder Größe
Rasse
Labrador Retriever wöchentlich 466,0 hoch mittel
Deutscher Schäferhund wöchentlich 466,0 mittel groß
Beagle täglich 324.0 hoch klein
68 | Kapitel 4: Arbeiten mit Dataframes mit pandas

Pflege Futter_Kosten Kinder Größe
Rasse
Golden Retriever wöchentlich 466,0 hoch mittel
Yorkshire Terrier täglich 324.0 niedrig klein
Bulldogge wöchentlich 466,0 mittel mittel
Boxer wöchentlich 466,0 hoch mittel
Um die ersten drei Zeilen und die ersten zwei Spalten nach Position zu erhalten, verwenden Sie .iloc:

dogs.iloc[0:3, 0:2]
pflege lebensmittel_kosten
Rasse
Labrador Retriever wöchentlich 466.0
Deutscher Schäferhund wöchentlich 466.0
Beagle täglich 324.0
Für die gleiche Operation mit .loc müssen Sie die Datenrahmenbeschriftungen verwenden:

dogs.loc['Labrador Retriever':'Beagle', 'grooming':'food_cost']
Pflegekosten_Futter_kosten
Rasse
Labrador Retriever wöchentlich 466.0
Deutscher Schäferhund wöchentlich 466.0
Beagle täglich 324.0
Als Nächstes sehen wir uns das Filtern von Zeilen an.

Zeilen filtern
Bisher haben wir gezeigt, wie man .loc und .iloc verwendet, um einen Datenrahmen mit Hilfe von Labels und

Positionen.

Datenwissenschaftler möchten jedoch häufig Zeilen filtern, d. h. sie möchten Teilmengen von Zeilen auswählen.

anhand einiger Kriterien. Nehmen wir an, Sie möchten die beliebtesten Babynamen im Jahr 2020 finden.

Zu diesem Zweck können Sie die Zeilen filtern, um nur die Zeilen zu erhalten, in denen das Jahr 2020 steht.

Um zu filtern, können Sie 1) prüfen, ob jeder Wert in der Spalte Jahr gleich 1970 ist,

dann 2) nur diese Zeilen behalten.

Um jeden Wert in Year zu vergleichen, schneiden Sie die Spalte aus und führen einen booleschen Vergleich durch.

Sohn. (Dies ist vergleichbar mit dem, was Sie mit einem Numpy-Array tun würden).

Subsetting | 69
# Hier ist der Datenrahmen als Referenz
Baby
Name Geschlecht Anzahl Jahr
0 Liam M^196592020
1 Noah M^182522020
2 Oliver M^141472020
... ... ... ... ...
2020719 Verona F^51880
2020720 Vertie F^51880
2020721 Wilma F^51880
2020722 Zeilen × 4 Spalten

# Eine Serie mit den Jahresdaten erhalten
baby['Jahr']
0 2020
1 2020
2 2020
...
2020719 1880
2020720 1880
2020721 1880
Name: Jahr, Länge: 2020722, dtype: int64
# Vergleich mit 2020
baby['Jahr'] == 2020
0 Wahr
1 Wahr
2 Wahr
...
2020719 Falsch
2020720 Falsch
2020721 Falsch
Name: Jahr, Länge: 2020722, dtype: bool
Beachten Sie, dass ein boolescher Vergleich mit einer Reihe eine Reihe von Booleschen ergibt. Dies ist fast

gleichwertig zum Schreiben:

is_2020 = []
for value in baby['Jahr']:
is_2020.append(wert == 2020)
Aber der boolesche Vergleich ist einfacher zu schreiben und viel schneller auszuführen als ein for

Schleife.

Jetzt weisen wir Pandas an, nur die Zeilen zu behalten, bei denen der Vergleich zu True ausgewertet wurde:

70 | Kapitel 4: Arbeiten mit Dataframes unter Verwendung von pandas

# Die Übergabe einer Serie von Booleschen Werten in .loc behält nur Zeilen bei, in denen die Serie
# einen wahren Wert hat.
# ↓
baby.loc[baby['Jahr'] == 2020, :]
Name Geschlecht Anzahl Jahr
0 Liam M^196592020
1 Noah M^182522020
2 Oliver M^141472020
... ... ... ... ...
31267 Zylynn F^52020
31268 Zynique F^52020
31269 Zynlee F^52020
31270 Zeilen × 4 Spalten

# Filtern hat eine Kurzschrift. Dies berechnet die gleiche Tabelle wie das obige Snippet
# ohne Verwendung von .loc
baby[baby['Jahr'] == 2020]
Name Geschlecht Anzahl Jahr
0 Liam M^196592020
1 Noah M^182522020
2 Oliver M^141472020
... ... ... ... ...
31267 Zylynn F^52020
31268 Zynique F^52020
31269 Zynlee F^52020
31270 Zeilen × 4 Spalten

Um schließlich die häufigsten Namen im Jahr 2020 zu finden, sortieren Sie den Datenrahmen nach Anzahl in

absteigender Reihenfolge.

# Wenn Sie einen langen Ausdruck haben, können Sie ihn in Klammern einschließen, dann fügen Sie
# Zeilenumbrüche hinzufügen, um ihn lesbarer zu machen.
(baby[baby['Jahr'] == 2020]
.sort_values('Count', ascending=False)
.head(7) # Nimm die ersten sieben Zeilen
)
Name Geschlecht Anzahl Jahr
0 Liam M^196592020
1 Noah M^182522020
Teilmenge | 71
Name Geschlecht Anzahl Jahr
13911 Emma F^155812020
2 Oliver M^141472020
13912 Ava F^130842020
3 Elijah M^130342020
13913 Charlotte F^130032020
Wir sehen, dass Liam, Noah und Emma die beliebtesten Babynamen im Jahr 2020 sind.

Beispiel: Seit wann ist Luna ein beliebter Name?
In dem Artikel der New York Times wird erwähnt, dass der Name "Luna" fast nicht existent war

vor 2000, hat sich aber seitdem zu einem sehr beliebten Namen für Mädchen entwickelt. Wenn

Wie genau ist Luna populär geworden? Wir können dies mithilfe von Slicing und Filterung überprüfen.

Wenn Sie eine Aufgabe zur Datenmanipulation angehen, empfehlen wir, das Problem aufzuteilen

in kleinere Schritte zu unterteilen. Wir könnten zum Beispiel denken:

Filter: nur Zeilen mit 'Luna' in der Spalte Name behalten.
Filter: nur Zeilen mit 'F' in der Spalte Geschlecht beibehalten.
Slice: Behalte die Spalten Count und Year.
Jetzt geht es darum, jeden Schritt in Code zu übersetzen.

luna = baby[baby['Name'] == 'Luna'] # [1]
luna = luna[luna['Geschlecht'] == 'F'] # [2]
luna = luna[['Count', 'Year']] # [3]
luna
Anzahl Jahr
13923^77702020
45366^77722019
77393^69292018
... ... ...
2014083^171883
2018187^181881
2020223^151880
128 Zeilen × 2 Spalten

In diesem Buch verwenden wir eine Bibliothek namens Plotly zum Plotten. Wir werden nicht in die Tiefe gehen in Plot-

Wir werden hier nicht weiter darauf eingehen, da wir in "Plotten" mehr über das Plotten sprechen. Denken Sie daran, dass Sie im Moment

Verwenden Sie px.line(), um ein einfaches Liniendiagramm zu erstellen.

72 | Kapitel 4: Arbeiten mit Dataframes mit pandas

px.line(luna, x='Jahr', y='Anzahl', width=350, height=250)
Es ist genau so, wie es im Artikel steht. Luna war bis zum Jahr 2000 oder so überhaupt nicht populär. Denk an

Wenn dir jemand sagt, dass sein Name Luna ist, kannst du dir ziemlich sicher sein, dass er

ihr Alter zu schätzen, auch ohne weitere Informationen über sie!

Nur so zum Spaß, hier ist die gleiche Handlung für den Namen Siri.

# Die Verwendung von .query ähnelt der Verwendung von .loc mit einer booleschen Reihe. query() hat mehr
Beschränkungen für die Art der Filterung, die Sie durchführen können, # aber sie kann als
# Kurzschrift.
siri = (baby.query('Name == "Siri"')
.query('Geschlecht == "F"'))
px.line(siri, x='Jahr', y='Anzahl', width=350, height=250)
Teilmenge | 73
Warum könnte die Popularität nach 2010 so plötzlich gesunken sein? Nun, Siri ist zufällig

ist der Name von Apples Sprachassistent und wurde 2011 eingeführt. Ziehen wir einen Strich

für das Jahr 2011 und werfen Sie einen Blick darauf...

fig = px.line(siri, x='Jahr', y='Anzahl', width=350, height=250)
fig.add_vline(x=2011, line_color='red', line_width=4)
Es sieht so aus, als ob die Eltern nicht wollen, dass ihre Kinder verwirrt sind, wenn andere Leute sagen: "Hey

Siri" auf ihre Telefone.

Mitbringsel
In diesem Abschnitt werden Dataframes in Pandas vorgestellt. Wir haben die üblichen Wege behandelt, die

Datenwissenschaftler unterteilen Datenrahmen mit Beschriftungen und filtern mit Hilfe eines booleschen Parameters

Bedingung. Im nächsten Abschnitt wird erklärt, wie man Zeilen zusammenfasst.

Aggregieren
In diesem Abschnitt werden Operationen zum Aggregieren von Zeilen in einem Datenrahmen vorgestellt. Data Scien-

tisten fassen Zeilen zusammen, um Zusammenfassungen von Daten zu erstellen. Zum Beispiel kann ein Datensatz kon-

Die täglichen Umsätze können aggregiert werden, um stattdessen die monatlichen Umsätze anzuzeigen. Genauer gesagt, werden wir

Gruppierung und Pivotierung, zwei gängige Operationen zur Aggregation von Daten, einführen.

Wir arbeiten mit den Babynamen-Daten, die im vorigen Abschnitt vorgestellt wurden:

baby = pd.read_csv('babynames.csv')
Baby
Name Geschlecht Anzahl Jahr
0 Liam M^196592020
74 | Kapitel 4: Arbeiten mit Datenrahmen mit pandas

Name Geschlecht Anzahl Jahr
1 Noah M^182522020
2 Oliver M^141472020
2020719 Verona F^51880
2020720 Vertie F^51880
2020721 Wilma F^51880
2020722 Zeilen × 4 Spalten

Grundlegendes Gruppen-Aggregat
Angenommen, wir wollen die Gesamtzahl der in diesen Daten erfassten Babys herausfinden.

Dies ist einfach die Summe der Spalte Count:

baby['Count'].sum()
352554503
Die Summierung der Namenszahlen ist eine einfache Möglichkeit, die Daten zu aggregieren - sie kombiniert

Daten aus mehreren Zeilen.

Aber nehmen wir an, wir wollen stattdessen eine interessantere Frage beantworten: Sind die Geburten in den USA

im Laufe der Zeit nach oben tendieren? Um diese Frage zu beantworten, können wir die Summe der Spalte Count

innerhalb jedes Jahres, anstatt die Summe über den gesamten Datensatz zu bilden. Mit anderen Worten,

Wir teilen die Daten nach Jahr in Gruppen ein und summieren dann die Zählwerte innerhalb der Gruppen.

jede Gruppe. Dieser Prozess ist in Abbildung 4-3 dargestellt.

Abbildung 4-3. Eine Darstellung der Gruppierung und anschließenden Aggregation von Beispieldaten.

Wir nennen diese Operation Gruppierung gefolgt von Aggregation. In Pandas schreiben wir:

baby.groupby('Jahr')['Anzahl'].sum()
Aggregieren | 75
Jahr
1880 194419
1881 185772
1882 213385
...
2018 3487193
2019 3437438
2020 3287724
Name: Count, Länge: 141, dtype: int64
Beachten Sie, dass der Code fast derselbe ist wie die nicht gruppierte Version, mit der Ausnahme, dass er

beginnt mit einem Aufruf von .groupby('Jahr').

Das Ergebnis ist eine pd.Series mit der Gesamtzahl der Babys, die für jedes Jahr in den Daten geboren wurden. Hinweis

dass der Index dieser Reihe die eindeutigen Jahreswerte enthält. Jetzt können wir die

zählt im Laufe der Zeit:

zählt_nach_Jahr = baby.groupby('Jahr')['Anzahl'].sum().reset_index()
px.line(zählt_nach_Jahr, x='Jahr', y='Anzahl', width=350, height=250)
Was sehen wir in diesem Diagramm? Zunächst fällt auf, dass es verdächtig wenige

Babys, die vor 1920 geboren wurden. Eine wahrscheinliche Erklärung ist, dass die Sozialversicherungsanstalt

Die Datenbank wurde 1935 eingerichtet, so dass die Daten für frühere Geburten weniger vollständig sein könnten.

76 | Kapitel 4: Arbeiten mit Datenrahmen mit pandas

Wir bemerken auch den Einbruch mit Beginn des Zweiten Weltkriegs 1939 und das Baby der Nachkriegszeit

Boomer-Ära von 1946-1964.

Hier ist das Grundrezept für die Gruppierung in Pandas:

(baby # der Datenrahmen
.groupby('Jahr') # Spalte(n) zum Gruppieren
['Count'] # Spalte(n), die aggregiert werden soll(en)
.sum() # wie aggregiert werden soll
)
Gruppieren nach mehreren Spalten
Wir übergeben mehrere Spalten in .groupby als eine Liste, um nach mehreren Spalten zu gruppieren

einmal. Dies ist nützlich, wenn wir unsere Gruppen weiter unterteilen müssen. Zum Beispiel, wir

kann nach Jahr und Geschlecht gruppiert werden, um zu sehen, wie viele männliche und weibliche Babys geboren wurden

im Laufe der Zeit.

counts_by_year_and_sex = (baby
.groupby(['Jahr', 'Geschlecht']) # Arg to groupby ist eine Liste von Spaltennamen
['Count']
.sum()
)
zählt_nach_Jahr_und_Geschlecht
Jahr Geschlecht
1880 F 83929
M 110490
1881 F 85034
...
2019 M 1785527
2020 F 1581301
M 1706423
Name: Count, Länge: 282, dtype: int64
Beachten Sie, dass sich der Code eng an das Gruppierungsrezept hält.

Die Reihe counts_by_year_and_sex hat einen so genannten mehrstufigen Index mit zwei

Ebenen, eine für jede Spalte, die gruppiert wurde. Es ist etwas einfacher zu sehen, wenn wir die

Serie in einen Datenrahmen:

# Das Ergebnis hat nur eine Spalte.
zählt_nach_Jahr_und_Geschlecht.to_frame()
Zählung
Jahr Geschlecht
1880 F^83929
M^110490
1881 F^85034
... ... ...
Aggregieren | 77
Anzahl
Jahr Geschlecht
2019 M^1785527
2020 F^1581301
M^1706423
282 Zeilen × 1 Spalte

Der Index besteht aus zwei Ebenen, da wir nach zwei Spalten gruppiert haben. Es kann ein bisschen

kompliziert, mit mehrstufigen Indizes zu arbeiten, so dass wir den Index zurücksetzen können, um zu einem Datenindex zurückzukehren.

Rahmen mit einem einzigen Index.

zählt_nach_Jahr_und_Geschlecht.reset_index()
Jahr Geschlecht Anzahl
0^1880 F^83929
1^1880 M^110490
2^1881 F^85034
279^2019 M^1785527
280^2020 F^1581301
281^2020 M^1706423
282 Zeilen × 3 Spalten

Benutzerdefinierte Aggregationsfunktionen
Nach der Gruppierung gibt uns Pandas flexible Möglichkeiten, die Daten zu aggregieren. Bis jetzt haben wir gesehen

wie man .sum() nach der Gruppierung verwendet:

(Baby
.groupby('Jahr')
['Count']
.sum() # Aggregieren durch Summieren
)
Jahr
1880 194419
1881 185772
1882 213385
...
2018 3487193
2019 3437438
2020 3287724
Name: Count, Länge: 141, dtype: int64
78 | Kapitel 4: Arbeiten mit Datenrahmen mit pandas

pandas bietet auch andere Aggregationsfunktionen, wie .mean(), .size(),

und .first(). Hier ist die gleiche Gruppierung mit .max():

(Baby
.groupby('Jahr')
['Count']
.max() # Aggregieren, indem das Maximum innerhalb jeder Gruppe genommen wird
)
Jahr
1880 9655
1881 8769
1882 9557
...
2018 19924
2019 20555
2020 19659
Name: Count, Länge: 141, dtype: int64
Aber manchmal hat Pandas nicht die genaue Aggregationsfunktion, die wir verwenden wollen. In

In diesen Fällen können wir eine benutzerdefinierte Aggregationsfunktion definieren und verwenden. pandas ermöglicht uns

dies durch .agg(fn), wobei fn eine von uns definierte Funktion ist.

Wenn wir zum Beispiel die Differenz zwischen dem größten und dem kleinsten Wert ermitteln wollen

innerhalb jeder Gruppe (dem Bereich der Daten), könnten wir zunächst eine Funktion namens

data_range, dann übergeben Sie diese Funktion an .agg().

# Die Eingabe für diese Funktion ist ein pd.Series-Objekt, das eine einzelne Spalte
# von Daten enthält. Sie wird für jede Gruppe einmal aufgerufen.
def data_range(counts):
return counts.max() - counts.min()
(Baby
.groupby('Jahr')
['Count']
.agg(daten_bereich) # Aggregieren mit benutzerdefinierter Funktion
)
Jahr
1880 9650
1881 8764
1882 9552
...
2018 19919
2019 20550
2020 19654
Name: Count, Länge: 141, dtype: int64
Aggregieren | 79
Beispiel: Sind die Menschen bei Babynamen kreativer geworden?
Sind die Menschen im Laufe der Zeit bei Babynamen kreativer geworden? Eine Möglichkeit zur Messung

ob die Zahl der einzigartigen Babynamen pro Jahr im Laufe der Jahre gestiegen ist.

Zeit.

Wir beginnen mit der Definition einer count_unique-Funktion, die die Anzahl der eindeutigen Werte zählt.

ues in einer Serie. Dann übergeben wir diese Funktion an .agg().

def count_unique(s):
return len(s.unique())
unique_names_by_year = (baby
.groupby('Jahr')
['Name']
.agg(count_unique) # aggregieren mit der benutzerdefinierten Funktion count_unique
)
eindeutige_Namen_nach_Jahr
Jahr
1880 1889
1881 1829
1882 2012
...
2018 29619
2019 29417
2020 28613
Name: Name, Länge: 141, dtype: int64
px.line(unique_names_by_year.reset_index(),
x='Jahr', y='Name',
labels={'Name': '# eindeutige Namen'},
width=350, height=250)
80 | Kapitel 4: Arbeiten mit Datenrahmen mit pandas

Wir sehen, dass die Zahl der eindeutigen Namen im Laufe der Zeit allgemein zugenommen hat, auch

Allerdings hat sich die Zahl der Geburten seit den 1960er Jahren weitgehend stabilisiert.

Pivotierung
Die Pivotierung ist im Wesentlichen eine bequeme Möglichkeit, die Ergebnisse einer Gruppe und eines Aggregats zu ordnen.

bei der Gruppierung mit zwei Spalten. Weiter oben in diesem Abschnitt haben wir das Baby gruppiert

Namensdaten nach Jahr und Geschlecht:

counts_by_year_and_sex = (baby
.groupby(['Jahr', 'Geschlecht'])
['Count']
.sum()
)
zählt_nach_Jahr_und_Geschlecht.to_frame()
Anzahl
Jahr Geschlecht
1880 F^83929
M^110490
1881 F^85034
... ... ...
2019 M^1785527
Aggregieren | 81
Anzahl
Jahr Geschlecht
2020 F^1581301
M^1706423
282 Zeilen × 1 Spalte

Dies ergibt eine pd.Series mit den Zählungen. Wir können uns die gleichen Daten auch mit

die Sex-Indexebene, die auf die Spalten eines Datenrahmens "gedreht" wurde. Es ist einfacher zu sehen mit einer

Beispiel:

mf_pivot = pd.pivot_table(
baby,
index='Jahr', # Spalte, die in einen neuen Index umgewandelt werden soll
columns='Geschlecht', # Spalte, die in neue Spalten umgewandelt werden soll
values='Count', # Spalte für die Aggregation der Werte
aggfunc=sum) # Aggregationsfunktion
mf_pivot
Geschlecht F M
Jahr
1880^83929110490
1881^85034100738
1882^99699113686
... ... ...
2018^16768841810309
2019^16519111785527
2020^15813011706423
141 Zeilen × 2 Spalten

Beachten Sie, dass die Datenwerte in der Pivot-Tabelle und in der erstellten Tabelle identisch sind

mit .groupby(); die Werte sind nur anders angeordnet. Pivot-Tabellen sind nützlich für

schnelle Zusammenfassung von Daten anhand von zwei Attributen und werden häufig in Artikeln und

Papiere.

Die Funktion px.line() funktioniert zufällig auch gut mit Pivot-Tabellen, da die Funktio-.

zeichnet eine Zeile für jede Datenspalte in der Tabelle:

px.line(mf_pivot, width=350, height=250)
82 | Kapitel 4: Arbeiten mit Datenrahmen mit pandas

Mitbringsel
Dieser Abschnitt behandelt gängige Methoden zur Aggregation von Daten in Pandas unter Verwendung der Funktion .groupby()

Funktion mit einer oder mehreren Spalten oder mit der Funktion pd.pivot_table(). Unter

Im nächsten Abschnitt wird erklärt, wie man Datenrahmen miteinander verbindet.

Verbinden
Datenwissenschaftler wollen sehr häufig zwei oder mehr Datenrahmen miteinander verbinden, um

um Datenwerte über Datenrahmen hinweg zu verbinden. Eine Online-Buchhandlung könnte zum Beispiel

einen Datenrahmen mit den Büchern haben, die jeder Benutzer bestellt hat, und einen zweiten Datenrahmen

mit den Genres der einzelnen Bücher. Durch die Verbindung der beiden Datenrahmen können die Datenwissenschaftler

tist kann sehen, welche Genres die einzelnen Nutzer bevorzugen.

Wir werden die Daten zu den Babynamen weiter untersuchen. Wir werden Joins verwenden, um einige Trends zu überprüfen

in einem Artikel der New York Times über Babynamen erwähnt (Williams 2021). Die

In dem Artikel geht es darum, wie bestimmte Namenskategorien mehr oder weniger populär geworden sind.

im Laufe der Zeit. So wird beispielsweise erwähnt, dass mythologische Namen wie Julius und Cassius

populär geworden, während die Namen der Babyboomer wie Susan und Debbie zu

weniger beliebt. Wie hat sich die Beliebtheit dieser Kategorien im Laufe der Zeit verändert?

Verbinden | 83
Wir haben die Namen und Kategorien aus dem NYT-Artikel genommen und in eine kleine

Datenrahmen:

nyt = pd.read_csv('nyt_names.csv')
nyt
nyt_name Kategorie
0 Luzifer verboten
1 Lilith verboten
2 Danger verboten
20 Venus himmlisch
21 Celestia himmlisch
22 Skye himmlisch
23 Zeilen × 2 Spalten

Um zu sehen, wie beliebt die Namenskategorien sind, verbinden wir den nyt-Datenrahmen mit dem

baby dataframe, um die Anzahl der Namen von baby zu erhalten.

baby = pd.read_csv('babynames.csv')
Baby
Name Geschlecht Anzahl Jahr
0 Liam M^196592020
1 Noah M^182522020
2 Oliver M^141472020
2020719 Verona F^51880
2020720 Vertie F^51880
2020721 Wilma F^51880
2020722 Zeilen × 4 Spalten

Stellen Sie sich vor, Sie gehen jede Zeile in baby durch und fragen: Ist dieser Name in der nyt-Tabelle? Wenn ja,

und fügen dann den Wert in der Kategoriespalte zu der Zeile hinzu. Das ist die grundlegende Idee hinter einer

verbinden. Betrachten wir zunächst ein paar einfachere Beispiele.

Innere Verknüpfungen
Wir werden kleinere Versionen der Tabellen baby und nyt erstellen, damit man besser sehen kann, was passiert.

Stifte, wenn wir Tabellen miteinander verbinden.

nyt_small = nyt.iloc[[11, 12, 14]].reset_index(drop=True)
nyt_small
84 | Kapitel 4: Arbeiten mit Datenrahmen mit pandas

nyt_name Kategorie
0 Karen Boomer
1 Julius-Mythologie
2 Freya-Mythologie
names_to_keep = ['Julius', 'Karen', 'Noah']
baby_small = (baby
.query("Jahr == 2020 und Name in @names_to_keep")
.reset_index(drop=True)
)
baby_klein
Name Geschlecht Anzahl Jahr
0 Noah M^182522020
1 Julius M^9602020
2 Karen M^62020
3 Karen F^3252020
4 Noah F^3052020
Um Tabellen in Pandas zu verbinden, verwenden wir die Methode .merge():

baby_small.merge(nyt_small,
left_on='Name', # Spalte in der linken Tabelle muss übereinstimmen
right_on='nyt_name') # Spalte in der rechten Tabelle, die übereinstimmen soll
Name Geschlecht Anzahl Jahr nyt_name Kategorie
0 Julius M^9602020 Julius Mythologie
1 Karen M^62020 Karen Boomer
2 Karen F^3252020 Karen Boomer
Beachten Sie, dass die neue Tabelle die Spalten der beiden Tabellen baby_small und nyt_small enthält.

Die Zeilen mit dem Namen Noah sind verschwunden. Und die verbleibenden Zeilen haben ihre passenden

Kategorie von nyt_small.

Wenn wir zwei Tabellen miteinander verbinden, teilen wir Pandas die Spalte(n) aus jeder Tabelle mit, die

die wir für die Verknüpfung verwenden wollen (die Argumente left_on und right_on). pandas findet Zeilen

zusammen, wenn die Werte in den Verbindungsspalten übereinstimmen, wie in Abbildung 4-4 dargestellt.

Verbinden | 85
Abbildung 4-4. Beim Joinen gleicht Pandas Zeilen anhand der Werte in den Feldern Name und nyt_name

Spalten. Bei inneren Verknüpfungen (Standardeinstellung) werden Zeilen, die keine übereinstimmenden Werte aufweisen, gelöscht.

Standardmäßig führt Pandas eine innere Verknüpfung durch. Wenn eine der beiden Tabellen Zeilen hat, die nicht übereinstimmen

in der anderen Tabelle, entfernt Pandas diese Zeilen aus dem Ergebnis. In diesem Fall wird die Noah

Zeilen in baby_small haben keine Übereinstimmungen in nyt_small, also werden sie fallen gelassen. Auch die

Die Freya-Reihe in nyt_small hat keine Entsprechungen in baby_small, daher wird sie ebenfalls fallen gelassen.

Nur die Zeilen, die in beiden Tabellen übereinstimmen, werden in das Endergebnis aufgenommen.

Linke, rechte und äußere Verknüpfungen
Manchmal werden wir Zeilen ohne Übereinstimmung behalten wollen, anstatt sie zu löschen

vollständig. Es gibt andere Arten von Joins - linke, rechte und äußere -, bei denen die Zeilen gleichmäßig

wenn sie kein Streichholz haben.

86 | Kapitel 4: Arbeiten mit Dataframes unter Verwendung von pandas

Bei einer linken Verknüpfung werden Zeilen in der linken Tabelle, die nicht übereinstimmen, im Endergebnis beibehalten, da

in Abbildung 4-5 dargestellt.

Abbildung 4-5. Bei einer linken Verknüpfung werden Zeilen in der linken Tabelle, die keine übereinstimmenden Werte haben, beibehalten.

Um eine linke Verknüpfung in Pandas durchzuführen, verwenden Sie how='left' in dem Aufruf von .merge():

baby_small.merge(nyt_small,
left_on='Name',
right_on='nyt_name',
how='left') # linke Verknüpfung anstelle der inneren
Name Geschlecht Anzahl Jahr nyt_name Kategorie
0 Noah M^182522020 NaN NaN
1 Julius M^9602020 Julius Mythologie
2 Karen M^62020 Karen Boomer
3 Karen F^3252020 Karen Bumer
4 Noah F^3052020 NaN NaN
Beachten Sie, dass die Noah-Zeilen in der endgültigen Tabelle enthalten sind. Da diese Zeilen nicht mit einem

im Datenrahmen nyt_small übereinstimmen, lässt die Verknüpfung NaN-Werte in den Feldern nyt_name und

Kategorie-Spalten. Beachten Sie auch, dass die Freya-Zeile in nyt_small immer noch weggelassen wird.

Eine rechte Verknüpfung funktioniert ähnlich wie die linke Verknüpfung, mit der Ausnahme, dass nicht übereinstimmende Zeilen in der

rechte Tabelle anstelle der linken Tabelle beibehalten werden:

baby_small.merge(nyt_small,
left_on='Name',
right_on='nyt_name',
how='right')
Zusammenführen | 87
Name Geschlecht Anzahl Jahr nyt_name Kategorie
0 Karen M 6,0 2020,0 Karen Boomer
1 Karen F 325,0 2020,0 Karen Boomer
2 Julius M 960,0 2020,0 Julius Mythologie
3 NaN NaN NaN NaN Freya-Mythologie
Bei einer äußeren Verknüpfung werden schließlich Zeilen aus beiden Tabellen beibehalten, auch wenn sie nicht übereinstimmen.

baby_small.merge(nyt_small,
left_on='Name',
right_on='nyt_name',
how='outer')
Name Geschlecht Anzahl Jahr nyt_name Kategorie
0 Noah M 18252.0 2020.0 NaN NaN
1 Noah F 305,0 2020,0 NaN NaN
2 Julius M 960.0 2020.0 Julius Mythologie
3 Karen M 6.0 2020.0 Karen Boomer
4 Karen F 325.0 2020.0 Karen-Boomer
5 NaN NaN NaN NaN Freya-Mythologie
Beispiel: Beliebtheit der NYT-Namenskategorien
Kehren wir nun zu den vollständigen Datenrahmen baby und nyt zurück.

Baby
Name Geschlecht Anzahl Jahr
0 Liam M^196592020
1 Noah M^182522020
2 Oliver M^141472020
... ... ... ... ...
2020719 Verona F^51880
2020720 Vertie F^51880
2020721 Wilma F^51880
2020722 Zeilen × 4 Spalten

nyt
nyt_name Kategorie
0 Luzifer verboten
1 Lilith verboten
88 | Kapitel 4: Arbeiten mit Dataframes unter Verwendung von pandas

nyt_name Kategorie
2 Verbotene Gefahr
... ... ...
20 Venus himmlisch
21 Celestia himmlisch
22 Skye himmlisch
23 Zeilen × 2 Spalten

Wir wollen wissen, wie sich die Beliebtheit der Namenskategorien in nyt im Laufe der Jahre verändert hat.

Zeit. Um diese Frage zu beantworten:

Inner join baby mit nyt.
Gruppieren Sie die Tabelle nach Kategorie und Jahr
Aggregieren Sie die Zählungen mit einer Summe.
cate_counts = (
baby.merge(nyt, left_on='Name', right_on='nyt_name') # [1]
.groupby(['Kategorie', 'Jahr']) # [2]
['Anzahl'] # [3]
.sum() # [3]
.reset_index()
)
cate_counts
Kategorie Jahr Anzahl
0 boomer^1880292
1 boomer^1881298
2 boomer^1882326
... ... ... ...
647 Mythologie^20182944
648 Mythologie^20193320
649 Mythologie^20203489
650 Zeilen × 3 Spalten

Jetzt können wir die Beliebtheit von Boomer-Namen und mythologischen Namen aufzeichnen:

Beitritt | 89
Wie in dem NYT-Artikel behauptet wird, sind "Babyboomer"-Namen weniger beliebt geworden, nachdem

2000, während mythologische Namen beliebter geworden sind.

Wir können auch die Popularität aller Kategorien auf einmal darstellen. Werfen Sie einen Blick auf die Diagramme

und sehen Sie nach, ob sie die Behauptungen im Artikel der New York Times bestätigen.

90 | Kapitel 4: Arbeiten mit Datenrahmen mit pandas

Mitbringsel
Beim Zusammenfügen von Datenrahmen werden Zeilen mit der Funktion .merge() abgeglichen. Sie ist

Es ist wichtig, bei der Verknüpfung von Daten die Art der Verknüpfung (inner, links, rechts oder außen) zu berücksichtigen.

Rahmen. Im nächsten Abschnitt wird erklärt, wie man Werte in einem Datenrahmen transformiert.

Transformieren
Datenwissenschaftler transformieren Datenrahmenspalten, wenn sie jeden Wert in

ein Merkmal auf die gleiche Weise. Zum Beispiel, wenn ein Merkmal die Höhe von Personen in Fuß enthält,

könnte ein Datenwissenschaftler die Höhen in Zentimeter umwandeln wollen. In diesem Abschnitt,

werden wir apply einführen, eine Operation, die Datenspalten mit Hilfe eines benutzerdefinierten Parameters umwandelt.

definierte Funktion.

baby = pd.read_csv('babynames.csv')
Baby
Name Geschlecht Anzahl Jahr
0 Liam M^196592020
1 Noah M^182522020
2 Oliver M^141472020
... ... ... ... ...
2020719 Verona F^51880
Umwandlung | 91
Name Geschlecht Anzahl Jahr
2020720 Vertie F^51880
2020721 Wilma F^51880
2020722 Zeilen × 4 Spalten

In dem Artikel über Babynamen in der New York Times (Williams 2021), erwähnt Pamela, dass

Namen, die mit dem Buchstaben "L" und "K" beginnen, wurden nach 2000 populär. Auf der anderen

Hand, Namen mit dem Anfangsbuchstaben "J" erreichten in den 1970er und 1980er Jahren ihren Höhepunkt

und haben seitdem an Popularität eingebüßt. Wir können diese Behauptungen anhand des Babys überprüfen

Datensatz.

Wir gehen dieses Problem in folgenden Schritten an:

Wandeln Sie die Spalte Name in eine neue Spalte um, die die Anfangsbuchstaben
jedes Wertes in Name enthält.
Gruppieren Sie den Datenrahmen nach dem ersten Buchstaben und dem Jahr.
Aggregieren Sie die Anzahl der Namen durch Aufsummieren.
Um den ersten Schritt abzuschließen, wenden wir eine Funktion auf die Spalte Name an.

Anwenden
pd.Series-Objekte enthalten eine .apply()-Methode, die eine Funktion aufnimmt und sie anwendet

auf jeden Wert in der Reihe. Um zum Beispiel die Längen der einzelnen Namen zu ermitteln, wenden wir

die Funktion len.

names = baby['Name']
names.apply(len)
0 4
1 4
2 6
..
2020719 6
2020720 6
2020721 5
Name: Name, Länge: 2020722, dtype: int64
Um den ersten Buchstaben eines jeden Namens zu extrahieren, definieren Sie eine benutzerdefinierte Funktion und übergeben Sie diese

in .apply().

# Das Argument der Funktion ist ein einzelner Wert in der Reihe.
def erster_buchstabe(zeichenkette):
return string[0]
names.apply(erster_buchstabe)
92 | Kapitel 4: Arbeiten mit Datenrahmen mit pandas

0 L
1 N
2 O
2020719 V
2020720 V
2020721 W
Name: Name, Länge: 2020722, dtype: object
Die Verwendung von .apply() ist vergleichbar mit der Verwendung einer for-Schleife. Der obige Code ist ungefähr gleichwertig

zum Schreiben:

result = []
for name in names:
result.append(erster_buchstabe(name))
Nun können wir die Anfangsbuchstaben einer neuen Spalte im Datenrahmen zuweisen:

Buchstaben = baby.assign(Firsts=names.apply(first_letter))
Buchstaben
Name Geschlecht Anzahl Jahr Vornamen
0 Liam M^196592020 L
1 Noah M^182522020 N
2 Oliver M^141472020 O
2020719 Verona F^51880 V
2020720 Vertie F^51880 V
2020721 Wilma F^51880 W
2020722 Zeilen × 5 Spalten

Um eine neue Spalte in einem Datenrahmen zu erstellen, können Sie auch
diese Syntax:
baby['Firsts'] = names.apply(first_letter)
Dadurch wird die Baby-Tabelle verändert, indem eine neue Spalte mit dem Namen "Vornamen" hinzugefügt wird.
Im obigen Code verwenden wir .assign(), wodurch die Tabelle baby
Tabelle selbst nicht verändert; stattdessen wird ein neuer Datenrahmen erstellt. Das Ändern von Datenframes
ist nicht falsch, kann aber eine häufige Quelle von Fehlern sein. Aus diesem Grund
werden wir in diesem Buch hauptsächlich .assign() verwenden.
Beispiel: Beliebtheit von "L"-Namen
Nun können wir den Buchstaben-Datenframe verwenden, um die Beliebtheit der Anfangsbuchstaben im Laufe der Zeit zu sehen.

letter_counts = (Buchstaben
.groupby(['Erstlinge', 'Jahr'])
Transforming | 93
['Count']
.sum()
.reset_index()
)
Buchstaben_Zahlen
Anzahl der ersten Jahre
0 A^188016740
1 A^188116257
2 A^188218790
... ... ... ...
3638 Z^201855996
3639 Z^201955293
3640 Z^202054011
3641 Zeilen × 3 Spalten

fig = px.line(letter_counts.loc[letter_counts['Firsts'] == 'L'],
x='Jahr', y='Anzahl', title='Beliebtheit von 'L'-Namen',
width=350, height=250)
margin(fig, t=30)
94 | Kapitel 4: Arbeiten mit Dataframes mit pandas

Die Grafik zeigt, dass "L"-Namen in den 1960er Jahren sehr beliebt waren und in den Jahrzehnten danach wieder abnahmen,

haben aber nach 2000 wieder an Popularität gewonnen.

Was ist mit "J"-Namen?

fig = px.line(letter_counts.loc[letter_counts['Firsts'] == 'J'],
x='Jahr', y='Anzahl', title='Beliebtheit von 'J'-Namen',
width=350, height=250)
margin(fig, t=30)
Der NYT-Artikel besagt, dass "J"-Namen in den 1970er und 80er Jahren sehr beliebt waren. Die Grafik

stimmt dem zu und zeigt auch, dass sie nach dem Jahr 2000 weniger beliebt geworden sind.

Der Preis von Apply
Die Stärke von .apply() ist seine Flexibilität - Sie können es mit jeder Funktion aufrufen, die in

einen einzelnen Datenwert und gibt einen einzelnen Datenwert aus.

Diese Flexibilität hat allerdings ihren Preis. Die Verwendung von .apply() kann langsam sein, da Pandas nicht opti-

mize beliebige Funktionen. Zum Beispiel ist die Verwendung von .apply() für numerische Berechnungen

viel langsamer als die Verwendung vektorisierter Operationen direkt auf pd.Series-Objekten:

%%timeit
# Berechnung der Dekade mit vektorisierten Operatoren
baby['Jahr'] // 10 * 10
Transformieren | 95
20,5 ms ± 442 μs pro Schleife (Mittelwert ± std. Abweichung von 7 Läufen, je 10 Schleifen)
%%timeit
def decade(yr):
return yr // 10 * 10
# Berechnen der Dekade mit apply
baby['Jahr'].apply(jahrzehnt)
549 ms ± 35,7 ms pro Schleife (Mittelwert ± std. Abweichung von 7 Durchläufen, je 1 Schleife)
Die Version mit .apply() ist 30 Mal langsamer! Vor allem für numerische Operationen,

empfehlen wir, direkt mit pd.Series-Objekten zu arbeiten.

Mitbringsel
Um Werte in einem Datenrahmen zu transformieren, verwenden wir üblicherweise die Funktionen .apply() und .assign()

Funktionen. Im nächsten Abschnitt werden wir Dataframes mit anderen Darstellungsformen vergleichen

und Datentabellen zu manipulieren.

Wie unterscheiden sich Dataframes von anderen Datendarstellungen?
Dataframes sind nur eine Möglichkeit, in einer Tabelle gespeicherte Daten darzustellen. In der Praxis werden die Daten sci-

entisten begegnen vielen anderen Arten von Datentabellen, wie Tabellenkalkulationen, Matrizen und

Beziehungen. In diesem Abschnitt werden wir den Datenrahmen mit anderen Relationen vergleichen und gegenüberstellen.

um zu erklären, warum Datenrahmen so häufig für die Datenanalyse verwendet werden.

Wir werden auch auf Szenarien hinweisen, in denen andere Darstellungen angemessener sein könnten.

ate.

Datenrahmen und Tabellenkalkulationen
Tabellenkalkulationen sind Computeranwendungen, bei denen die Benutzer Daten in ein Raster eingeben und mit

Formeln, um Berechnungen durchzuführen. Ein berühmtes Beispiel ist heute Microsoft Excel,

obwohl Tabellenkalkulationen mindestens bis 1979 mit VisiCalc (Grad 2007) zurückreichen. Tabellenkalkulation -

machen es einfach, Daten zu sehen und direkt zu bearbeiten. Diese Eigenschaften machen

Tabellenkalkulationen sehr beliebt - einer Schätzung aus dem Jahr 2005 zufolge gibt es über 55 Millionen Tabellenkalkulationsprogramme.

Blattnutzer im Vergleich zu 3 Millionen professionellen Programmierern in der Industrie (Scaffidi et

al. 2005).

Dataframes haben gegenüber Tabellenkalkulationen mehrere entscheidende Vorteile. Das Schreiben von Dataframe-Code in

Ein computergestütztes Notizbuch wie Jupyter erzeugt natürlich einen Datenstrang. Jemand

der das Notizbuch öffnet, kann die Eingabedateien für das Notizbuch sehen und wie die Daten

geändert wurden. Tabellenkalkulationen machen eine Datenabfolge nicht sichtbar; wenn eine Person manuell

Datenwerte in einer Zelle bearbeitet, ist es für künftige Benutzer schwierig zu erkennen, welche Werte geändert wurden.

96 | Kapitel 4: Arbeiten mit Datenrahmen mit pandas

bearbeitet wurden oder wie sie bearbeitet wurden. Dataframes können auch größere Datensätze verarbeiten als

Tabellenkalkulationen, und die Benutzer können auch verteilte Programmierwerkzeuge verwenden, um mit

riesige Datensätze, die nur sehr schwer in eine Tabellenkalkulation geladen werden können.

Datenframes und Matrizen
Eine Matrix ist eine zweidimensionale Anordnung von Daten, die hauptsächlich für lineare Algebra-Operationen verwendet wird.

tionen. Im folgenden Beispiel ist 𝐀 eine Matrix mit drei Zeilen und zwei Spalten.

𝐀=

1 0
0 4
0 0
Matrizen sind mathematische Objekte, die durch die Operatoren, die sie zulassen, definiert sind. Für

Beispielsweise können Matrizen addiert oder multipliziert werden. Matrizen haben auch eine Trans-

aufstellen. Diese Operatoren haben sehr nützliche Eigenschaften, auf die sich Datenwissenschaftler verlassen, um

statistische Modellierung.

Ein wichtiger Unterschied zwischen einer Matrix und einem Datenrahmen: Bei der Behandlung als Mathe-

ematisches Objekt, Matrizen können nur Zahlen enthalten. Dataframes hingegen,

können auch andere Datentypen wie Text enthalten. Dies macht Datenrahmen nützlicher für

Laden und Verarbeiten von Rohdaten, die alle möglichen Datentypen enthalten können. In der Tat,

Datenwissenschaftler verwenden häufig Dataframes, um Matrizen zu erstellen. In diesem Buch werden wir im Allgemeinen Folgendes verwenden

Datenrahmen für die explorative Datenanalyse und Datenbereinigung, dann die Verarbeitung der Daten in

Matrizen für Modelle des maschinellen Lernens.

Datenwissenschaftler bezeichnen Matrizen nicht nur als mathematische Objekte,
sondern auch als Programmobjekte. Zum Beispiel hat die Programmiersprache R
Programmiersprache R ein Matrix-Objekt, während wir in Python eine
eine Matrix durch ein zweidimensionales Numpy-Array darstellen. Matrizen, wie
und R implementiert sind, können neben Zahlen auch andere Datentypen enthalten
Zahlen enthalten, verlieren dabei aber ihre mathematischen Eigenschaften. Dies ist
ein weiteres Beispiel dafür, wie sich Domänen mit demselben Begriff auf unterschiedliche Dinge
mit ein und demselben Begriff bezeichnen können.
Dataframes und Relationen
Die Relation ist eine Datentabellendarstellung, die in Datenbanksystemen verwendet wird, insbesondere in SQL

Systeme wie SQLite und PostgreSQL. (Wir behandeln Beziehungen und SQL in Kapitel 5

Kapitel in diesem Buch). Beziehungen haben viele Ähnlichkeiten mit Datenrahmen; beide verwenden

Zeilen zur Darstellung von Datensätzen und Spalten zur Darstellung von Merkmalen. Beide haben Spalten

Namen und Daten innerhalb einer Spalte haben denselben Typ.

Ein wesentlicher Vorteil von Datenrahmen ist, dass sie keine Zeilen zur Darstellung von Datensätzen benötigen

und Spalten zur Darstellung von Merkmalen. Oft liegen die Rohdaten nicht in einer praktischen

Format, das direkt in eine Beziehung gesetzt werden kann. In diesen Szenarien verwenden die Datenwissenschaftler

Wie unterscheiden sich Dataframes von anderen Datendarstellungen? | 97
1 https://www.postgresql.org/about/
den Datenrahmen, um Daten zu laden und zu verarbeiten, da Datenrahmen in dieser Hinsicht flexibler sind

beachten. Häufig laden Datenwissenschaftler Rohdaten in einen Datenrahmen und verarbeiten dann die

Daten in ein Format zu bringen, das leicht in einer Beziehung gespeichert werden kann.

Ein wesentlicher Vorteil von Relationen gegenüber Datenrahmen besteht darin, dass Relationen verwendet werden von

relationale Datenbanksysteme wie PostgreSQL^1, die sehr nützliche Funktionen für Daten haben

Speicherung und Verwaltung. Stellen Sie sich einen Datenwissenschaftler in einem Unternehmen vor, das eine große

Website für soziale Medien. Die Datenbank kann Daten enthalten, die viel zu groß sind, um sie in einem

Datenrahmen von Pandas auf einmal zu erstellen; stattdessen verwenden Datenwissenschaftler SQL-Abfragen, um Untergruppen zu bilden und

Daten zu aggregieren, da Datenbanksysteme besser in der Lage sind, große Datenmengen zu verarbeiten.

Außerdem aktualisieren die Nutzer der Website ständig ihre Daten, indem sie Beiträge verfassen, hochladen

Bilder und die Bearbeitung ihres Profils. Hier ermöglichen Datenbanksysteme den Datenwissenschaftlern die Wiederverwendung

ihre bestehenden SQL-Abfragen, um ihre Analysen mit den neuesten Daten zu aktualisieren, anstatt

immer wieder große CSV-Dateien herunterladen zu müssen.

Für eine genauere Beschreibung des Unterschieds zwischen Datenrahmen und Beziehungen,

siehe (Petersohn et al. 2020).

Zusammenfassung
In diesem Kapitel wird erklärt, was Dataframes sind, warum sie nützlich sind und wie man mit ihnen arbeitet.

mit ihnen unter Verwendung von Pandas-Code. Subsetting, Aggregieren, Verbinden und Transformieren sind

die bei fast jeder Datenanalyse nützlich sind. Wir werden diese Operationen in den folgenden Abschnitten häufig verwenden

das Buch, insbesondere in den Kapiteln 6, ???, und ???.

Übungen
Verwenden Sie die Babydaten, um ein Diagramm zu erstellen, das zeigt, wie beliebt Ihr Name im Laufe der Zeit war. Wenn
wenn Sie diese Grafik verwenden würden, um Ihr Alter zu schätzen, was würden Sie schätzen? Ist das
nahe an Ihrem tatsächlichen Alter? Überlegen Sie sich einen möglichen Grund.
In diesem Kapitel haben wir darüber gesprochen, wie man .loc und .iloc für Slicing verwendet. Wir haben auch
ein paar Kurzbefehle gezeigt. Konvertieren Sie für jeden dieser Shorthand-Codeschnipsel
in den entsprechenden Code, der .loc oder .iloc verwendet.
baby['Name']
baby[0:5]
baby[['Name', 'Anzahl']]
baby[baby['Anzahl'] < 10]
98 | Kapitel 4: Arbeiten mit Datenrahmen mit pandas

Was ist der Unterschied zwischen running:
baby['Name']
und:
baby[[['Name']]
Und warum funktioniert dieser Code?
baby[['Name']].iloc[0:5, 0]
aber dieser Code schlägt fehl?
baby['Name'].iloc[0:5, 0]
Das erste Codeschnipsel unten erzeugt einen Datenrahmen mit 6 Zeilen, aber das zweite
erzeugt einen Datenrahmen mit 5 Zeilen. Warum?
baby.loc[0:5]
baby.iloc[0:5]
Wenn Sie männliche und weibliche Babynamen über die Zeit aufzeichnen, werden Sie feststellen, dass
nach 1950 generell mehr männliche Babys geboren wurden. Spiegelt sich dieser Trend in den U.S.
Volkszählungsdaten wider? Gehen Sie auf die Website der Volkszählung (https://data.census.gov/cedsci/) und
prüfen.
Suchen Sie die fünf Namen mit der höchsten Standardabweichung der jährlichen Zählungen. Was
könnte eine große Standardabweichung etwas über die Beliebtheit dieser Namen
im Laufe der Zeit sagen?
Finden Sie die fünf Namen mit dem höchsten Interquartilsabstand der jährlichen Zählungen. Der
Interquartilsbereich ist das 75. Perzentil minus das 25. Perzentil der Daten.
Die Funktion pd.Series.quantile() kann Ihnen dabei helfen (Link zur Dokumentation).
tion). Sind diese Namen anders als die Namen mit der höchsten Standardabweichung?
tion? Warum kann das passieren?
Wir haben diese Syntax für die Gruppierung gezeigt:
baby.groupby('Jahr')['Anzahl'].sum()
Dieser Code bewirkt das Gleiche:
baby.groupby(baby['Jahr'])['Anzahl'].sum()
Die zweite Syntax übergibt eine pd.Series an .groupby(). Sie ist ein wenig ausführlicher
aber bietet auch mehr Flexibilität. Warum ist diese Syntax flexibler?
Hinweis: Was bewirkt dieser Code?
baby.groupby(baby['Jahr'] // 10 * 10)['Anzahl'].sum()
Übungen | 99
Angenommen, Sie möchten die beliebtesten männlichen und weiblichen Babynamen jedes
Jahr finden. Sie könnten dies schreiben:
(Baby
.groupby([['Jahr', 'Geschlecht']])
[['Anzahl', 'Name']]
.max()
)
Aber dieser Code liefert nicht das richtige Ergebnis. Warum?

Schreiben Sie nun einen Code, um die beliebtesten männlichen und weiblichen Namen jedes Jahres zu ermitteln,
zusammen mit ihrer Anzahl. Tipp: Sie können sich die Tatsache zunutze machen, dass die Namen innerhalb jedes Jahres und
Geburtsgeschlecht die Namen in absteigender Reihenfolge ihrer Beliebtheit sortiert sind.
Denken Sie sich ein realistisches Datenbeispiel aus, bei dem ein Datenwissenschaftler einen
Inner Join gegenüber einem Left Join bevorzugen würde, und ein Beispiel, bei dem ein Datenwissenschaftler einen Left
Join einem Inner Join vorziehen würde.
In dem Abschnitt über Joins hat die Tabelle nyt keine doppelten Namen. Aber ein
Name könnte aber durchaus zu mehreren Kategorien gehören, z. B. ist Elisabeth ein
Name aus der Bibel und ein Name für ein Königshaus. Nehmen wir an, wir haben einen Datenrahmen namens
multi_cat, der einen Namen mehrfach auflisten kann - einmal für jede Kategorie, zu der er gehört
zugehörig ist:
multi_cat = pd.DataFrame([
['Elizabeth', 'bible'],
['Elisabeth', 'königlich'],
['Arjun', 'hindu'],
['Arjun', 'mythologisch'],
], columns=nyt_small.columns)
multi_cat
nyt_name Kategorie
0 Elisabeth Bibel
1 Elisabeth königlich
2 Arjun hinduistisch
3 Arjun mythologisch
Was passiert, wenn wir Baby mit dieser Tabelle verbinden? Was geschieht im Allgemeinen, wenn

es mehrere übereinstimmende Zeilen in der linken und rechten Tabelle gibt?

Bei einem Self-Join wird eine Tabelle mit sich selbst verknüpft. Die Tabelle "Freunde" zum Beispiel
enthält Paare von Personen, die miteinander befreundet sind.
100 | Kapitel 4: Arbeiten mit Datenrahmen mit Pandas

friends = pd.DataFrame([
['Jim', 'Scott'],
['Scott', 'Philip'],
['Philip', 'Tricia'],
['Philip', 'Ailie'],
], columns=['self', 'other'])
Freunde
selbst andere
0 Jim Scott
1 Scott Philip
2 Philip Tricia
3 Philipp Ailie
Warum könnte ein Datenwissenschaftler den folgenden Self-Join nützlich finden?

friends.merge(friends, left_on='other', right_on='self')
self_x andere_x self_y andere_y
0 Jim Scott Scott Philip
1 Scott Philip Philip Tricia
2 Scott Philip Philip Ailie
Sind die Namen im Laufe der Zeit im Durchschnitt länger geworden? Erstelle ein Diagramm zur Beantwortung dieser
Frage zu beantworten.
In diesem Kapitel haben wir festgestellt, dass man das Alter einer Person
wenn man nur ihren Namen kennt. Der Name "Luna" hat zum Beispiel nach 2000 stark an
Popularität nach dem Jahr 2000 stark zugenommen, so dass man vermuten kann, dass eine Person namens "Luna"
etwa nach 2000 geboren wurde. Können Sie das Alter einer Person allein anhand des ersten Buchstabens
dem ersten Buchstaben ihres Namens? Schreiben Sie einen Code, um zu sehen, ob dies möglich ist und welche
Anfangsbuchstaben die meisten Informationen über das Alter einer Person liefern.
Übungen | 101
KAPITEL 5

Arbeiten mit Beziehungen mit SQL

Ein Hinweis für die Leser von Early Release

Mit Early Release ebooks erhalten Sie Bücher in ihrer frühesten Form - den unbearbeiteten und
unbearbeiteten Inhalt, so dass Sie die Vorteile dieser Technologien schon lange
vor der offiziellen Veröffentlichung dieser Titel.
Dies wird das 7. Kapitel des endgültigen Buches sein. Bitte beachten Sie, dass das GitHub-Repositorium
zu einem späteren Zeitpunkt aktiviert wird.
Wenn Sie Anmerkungen haben, wie wir den Inhalt und/oder die Beispiele in diesem Buch verbessern können
diesem Buch verbessern könnten, oder wenn Sie fehlendes Material in diesem Kapitel bemerken, wenden Sie sich bitte an den
Autor unter mpotter@oreilly.com.
In diesem Kapitel werden die Datenanalysen aus Kapitel 4 wiederholt
unter Verwendung von Relationen und SQL anstelle von Dataframes und Python. Die
Datensätze, Datenmanipulationen und Schlussfolgerungen sind nahezu identisch
den beiden Kapiteln nahezu identisch, so dass es für den Leser einfacher ist zu sehen, wie
die gleichen Datenmanipulationen sowohl in Pandas als auch in
SQL DURCHGEFÜHRT WERDEN.
Wenn Sie das Kapitel über Datenrahmen bereits gelesen haben, können Sie Ihre
auf diesen Abschnitt konzentrieren, in dem wir die Relation vorstellen, und die
spezifischen SQL-Code-Beispiele in den folgenden Abschnitten.
Datenwissenschaftler arbeiten mit Daten, die in Tabellen gespeichert sind. Dieses Kapitel stellt Relationen vor, eine

eine der am häufigsten verwendeten Methoden zur Darstellung von Datentabellen. Wir werden auch SQL vorstellen, die

Standardprogrammiersprache für die Arbeit mit Beziehungen. Hier ist ein Beispiel für eine

Beziehung, die Informationen über beliebte Hunderassen enthält:

103
Rasse Pflege Futter_Kosten Kinder Größe
Labrador Retriever wöchentlich 466,0 hoch mittel
Deutscher Schäferhund wöchentlich 466,0 mittel groß
Beagle täglich 324,0 hoch klein
Golden Retriever wöchentlich 466,0 hoch mittel
Yorkshire Terrier täglich 324,0 niedrig klein
Bulldogge wöchentlich 466,0 mittel mittel
Boxer wöchentlich 466,0 hoch mittel
In einer Beziehung steht jede Zeile für einen einzelnen Datensatz - in diesem Fall für eine einzelne Hunderasse.

Jede Spalte stellt ein Merkmal des Datensatzes dar, z. B. die Spalte "Pflege".

Die Tabelle zeigt, wie oft die einzelnen Hunderassen gepflegt werden müssen.

Beziehungen haben Bezeichnungen für Spalten. Zum Beispiel hat diese Beziehung eine Spalte mit der Bezeichnung

Pflegen. Innerhalb einer Spalte haben die Daten den gleichen Typ. Zum Beispiel ist die Spalte food_cost

Spalte enthält Zahlen, und die Spalte Größe enthält Kategorien. Aber Datentypen

können innerhalb einer Zeile unterschiedlich sein.

Aufgrund dieser Eigenschaften ermöglichen Beziehungen alle Arten von nützlichen Operationen.

Als Datenwissenschaftler arbeiten Sie häufig mit Menschen
mit unterschiedlichen Hintergründen zusammen, die unterschiedliche Begriffe verwenden. Zum Beispiel,
Informatiker sagen, dass die Spalten einer Beziehung Merkmale der Daten darstellen.
turen der Daten darstellen, während Statistiker sie stattdessen als Variablen bezeichnen.
In anderen Fällen wird derselbe Begriff für etwas anderes verwendet.
unterschiedliche Dinge. Datentypen im Sinne der Programmierung beziehen sich darauf, wie ein
Computer Daten intern speichert. Zum Beispiel hat die Spalte Größe in
einen String-Datentyp in Python. Vom statistischen Standpunkt aus betrachtet speichert die
Spalte Größe geordnete kategoriale Daten (Ordinaldaten). Wir sprechen
mehr über diese spezielle Unterscheidung im Kapitel ???.
In diesem Kapitel zeigen wir Ihnen, wie Sie gängige Beziehungsoperationen mit SQL durchführen können.

Zunächst wird die Struktur von SQL-Abfragen erklärt. Dann zeigen wir, wie Sie SQL verwenden, um

gängige Datenverarbeitungsaufgaben wie Zerschneiden, Filtern, Sortieren und Gruppieren durchführen,

und Beitritt.

Teilmengenbildung
In diesem Abschnitt werden Operationen zur Bildung von Teilmengen von Beziehungen vorgestellt. Wenn Data Scien-

Wenn Wissenschaftler mit einer Beziehung arbeiten, wollen sie oft die spezifischen Daten, die sie benötigen, unterteilen.

die sie zu nutzen gedenken. Ein Datenwissenschaftler kann zum Beispiel die zehn wichtigsten Merkmale herausfiltern

aus einer Beziehung mit Hunderten von Spalten. Oder sie können eine Beziehung filtern, um Folgendes zu entfernen

104 | Kapitel 5: Arbeiten mit Beziehungen mit SQL

Zeilen mit unvollständigen Daten. Im weiteren Verlauf dieses Kapitels werden wir die Beziehungsoperatio- nen einführen.

mit Hilfe eines Datensatzes von Babynamen.

Um mit Beziehungen zu arbeiten, werden wir eine domänenspezifische Programmiersprache einführen

genannt SQL (Structured Query Language). Wir sprechen "SQL" üblicherweise wie folgt aus

"Fortsetzung", anstatt das Akronym zu buchstabieren. SQL ist eine Fachsprache für Arbeits- und

SQL hat daher eine eigene Syntax, die das Schreiben von Pro- grammen erleichtert.

gramme, die mit relationalen Daten arbeiten.

Über die Daten
In einem Artikel der New York Times aus dem Jahr 2021 ist die Rede von Prinz Harry und Meghans

für den Namen ihrer neuen Tochter entschieden: Lilibet (Williams 2021). Die Kunst-.

cle hat ein Interview mit Pamela Redmond, einer Expertin für Babynamen, die über

über interessante Trends bei der Namensgebung für ihre Kinder. Sie sagt zum Beispiel, dass

Namen, die mit dem Buchstaben "L" beginnen, sind in den letzten Jahren sehr beliebt geworden, während

Namen, die mit dem Buchstaben "J" beginnen, waren in den 1970er und 1980er Jahren am beliebtesten. Sind

diese Behauptungen in den Daten widerspiegeln? Wir können SQL verwenden, um das herauszufinden.

In diesem Kapitel werden wir SQL-Abfragen in Python-Programmen verwenden. Dies veranschaulicht eine Kom-

mon workflow - Datenwissenschaftler verarbeiten und unterteilen Daten oft in SQL, bevor sie sie laden

die Daten in Python zur weiteren Analyse. SQL-Datenbanken erleichtern die Arbeit mit

große Datenmengen im Vergleich zu Pandas-Programmen. Allerdings ist das Laden von Daten in pan

Das erleichtert die Visualisierung der Daten und die Erstellung statistischer Modelle.

In diesem Kapitel werden wir also die Funktion pandas.read_sql verwenden, die eine SQL-Abfrage ausführt

und speichert die Ausgabe in einem Datenrahmen. Die Verwendung dieser Funktion erfordert einige Vorbereitungen. Wir

importieren Sie zunächst die Python-Pakete pandas und sqlalchemy.

importiere pandas als pd
import sqlalchemy
Unsere Datenbank ist in einer Datei namens babynames.db gespeichert. Diese Datei ist eine SQLite-Datenbank

(2021), also werden wir ein Sqlalchemy-Objekt einrichten, das dieses Format verarbeiten kann.

db = sqlalchemy.create_engine('sqlite:///babynames.db')
Untergliederung | 105
SQL ist eine Programmiersprache, die je nach Datenbanksystem unterschiedlich implementiert
unterschiedlichen Datenbanksystemen implementiert wird. In diesem Buch verwenden wir SQLite, ein beliebtes
Datenbanksystem. Andere Systeme gehen andere Kompromisse ein, die
die für verschiedene Bereiche nützlich sind. Zum Beispiel sind PostgreSQL und MySQL
Systeme, die sich für große Webanwendungen eignen, bei denen viele
Endbenutzer gleichzeitig Daten schreiben.
Um die Sache noch komplizierter zu machen, hat jedes SQL-System leichte
Unterschiede. In diesem Buch werden wir uns auf die Kernbestandteile der SQL-Syntax verlassen, die
die sich bei verschiedenen Implementierungen wahrscheinlich nicht ändern werden. Wir werden zwar nicht
wir nicht im Detail auf andere Systeme eingehen, aber wir werden aufzeigen, wo sich verschiedene SQL
Systeme sich in ihren Fähigkeiten unterscheiden können.
Jetzt können wir pd.read_sql verwenden, um SQL-Abfragen auf dieser Datenbank auszuführen. Diese Datenbank hat

zwei Beziehungen: baby und nyt. Hier ist ein einfaches Beispiel, das das gesamte Baby einliest

Beziehung.

# SQL-Abfrage in einem Python-String gespeichert
Abfrage = '''
SELECT *
FROM baby;
'''
pd.read_sql(abfrage, db)
Name Geschlecht Anzahl Jahr
0 Liam M^196592020
1 Noah M^182522020
2 Oliver M^141472020
... ... ... ... ...
2020719 Verona F^51880
2020720 Vertie F^51880
2020721 Wilma F^51880
2020722 Zeilen × 4 Spalten

Der Text innerhalb der Abfragevariable enthält SQL-Code. SELECT und FROM sind SQL-Schlüssel.

Worte. Wir lesen die Anfrage oben wie:

SELECT * -- Holt alle Spalten...
FROM baby; -- ...aus der Beziehung baby
Die Daten in der Baby-Relation stammen von der US-Sozialversicherungsbehörde, die

nimmt den Namen und das Geschlecht des Babys für die Geburtsurkunde auf. Sie machen die

Die Daten zu den Babynamen sind auf ihrer Website verfügbar (Abteilung 2021).

106 | Kapitel 5: Arbeiten mit Beziehungen mit SQL

Auf der Website der Sozialversicherung gibt es eine Seite, auf der die Daten ausführlicher beschrieben werden (Link).

Wir werden in diesem Kapitel nicht näher auf die Einschränkungen der Daten eingehen, aber wir weisen darauf hin

dieses einschlägige Zitat von der Website:

Alle Namen stammen aus Anträgen auf Sozialversicherungskarten für Geburten, die in den
Vereinigten Staaten nach 1879. Beachten Sie, dass viele Menschen, die vor 1937 geboren wurden, nie einen Antrag auf eine
Sozialversicherungskarte beantragt haben, so dass ihre Namen in unseren Daten nicht enthalten sind. Für andere, die einen
die einen Antrag gestellt haben, kann es sein, dass unsere Aufzeichnungen den Geburtsort nicht enthalten, und auch ihre Namen sind nicht in unseren Daten enthalten.
in unseren Daten enthalten.
Alle Daten stammen aus einer 100-prozentigen Stichprobe unserer Aufzeichnungen über Anträge auf Sozialversicherungskarten zum
vom März 2021.
Was ist eine Verwandtschaft?
Schauen wir uns die Baby-Relation genauer an. Eine Relation hat Zeilen und Spalten.

Jede Spalte hat eine Beschriftung, wie in Abbildung 5-1 dargestellt. Anders als bei Datenrahmen,

Die einzelnen Zeilen einer Relation haben keine Beschriftungen. Im Gegensatz zu Datenrahmen haben die Zeilen einer Rela-

tion nicht bestellt werden.

Abbildung 5-1. Die Baby-Beziehung hat Beschriftungen für die Spalten (in Kästen).

Beziehungen haben eine lange Geschichte. Formalere Abhandlungen über Beziehungen verwenden den Begriff

"Tupel" für die Zeilen einer Beziehung und "Attribut" für die Spalten. Dort

ist auch eine strenge Methode zur Definition von Datenoperationen mit Hilfe der relationalen Algebra, die

abgeleitet von der mathematischen Mengenalgebra. Interessierte Datenwissenschaftler können eine mehr in-...

tiefe Behandlung von Beziehungen in Büchern über Datenbanksysteme wie dem von Garcia-

Molina, Ullman, und Widom (Garcia-Molina et al. 2008.

Unterteilung | 107
Slicing
Slicing ist eine Operation, die eine neue Relation aus einer Teilmenge von Zeilen oder Spalten erstellt.

umns aus einer anderen Beziehung. Denken Sie daran, wie Sie eine Tomate in Scheiben schneiden - die Scheiben können sowohl verti-

und waagerecht. Um Spalten einer Beziehung aufzuteilen, geben wir der SELECT-Anweisung die

Spalten, die wir wollen.

Abfrage = '''
SELECT Name
FROM baby;
'''
pd.read_sql(abfrage, db)
Name
0 Liam
1 Noah
2 Oliver
... ...
2020719 Verona
2020720 Vertie
2020721 Wilma
2020722 Zeilen × 1 Spalten

Abfrage = '''
SELECT Name, Anzahl
FROM baby;
'''
pd.read_sql(abfrage, db)
Name Anzahl
0 Liam^19659
1 Noah^18252
2 Oliver^14147
... ... ...
2020719 Verona^5
2020720 Vertie^5
2020721 Wilma^5
2020722 Zeilen × 2 Spalten

Um eine bestimmte Anzahl von Zeilen auszuschneiden, verwenden Sie das Schlüsselwort LIMIT:

108 | Kapitel 5: Arbeiten mit Beziehungen mit SQL

Abfrage = '''
SELECT Name
FROM baby
LIMIT 10;
'''
pd.read_sql(abfrage, db)
Name
0 Liam
1 Noah
2 Oliver
7 Lukas
8 Heinrich
9 Alexander
10 Zeilen × 1 Spalte

Zusammenfassend lässt sich sagen, dass wir die Schlüsselwörter SELECT und LIMIT verwenden, um die Spalten und Zeilen einer Rela-

tion.

Zeilen filtern
Bisher haben wir gezeigt, wie man SELECT und LIMIT verwendet, um Spalten und Zeilen einer Rela- sion zu filtern.

tion.

Datenwissenschaftler möchten jedoch häufig Zeilen filtern, d. h. sie möchten Teilmengen von Zeilen auswählen.

anhand einiger Kriterien. Nehmen wir an, Sie möchten die beliebtesten Babynamen im Jahr 2020 finden.

Zu diesem Zweck können Sie die Zeilen filtern, um nur die Zeilen zu erhalten, in denen das Jahr 2020 steht.

Um eine Beziehung zu filtern, verwenden Sie das Schlüsselwort WHERE mit einem Prädikat:

Abfrage = '''
SELECT *
FROM baby
WHERE Jahr = 2020;
'''
pd.read_sql(abfrage, db)
Name Geschlecht Anzahl Jahr
0 Liam M^196592020
1 Noah M^182522020
2 Oliver M^141472020
... ... ... ... ...
Teilmenge | 109
Name Geschlecht Anzahl Jahr
31267 Zylynn F^52020
31268 Zynique F^52020
31269 Zynlee F^52020
31270 Zeilen × 4 Spalten

Beachten Sie, dass SQL beim Vergleich auf Gleichheit ein einzelnes Gleichheits
Zeichen:
SELECT *
FROM baby
WHERE Jahr = 2020;
-- ↓
-- Einfaches Gleichheitszeichen
In Python hingegen werden einfache Gleichheitszeichen für die Zuweisung von Variablen verwendet.
verwendet. Die Anweisung Year = 2020 weist der Variablen Year den Wert 2020 zu.
Variable Jahr zu. Um Gleichheit zu vergleichen, verwendet Python-Code doppelte
Gleichheitszeichen:
# Zuweisung
mein_jahr = 2021
# Vergleich, der zu False ausgewertet wird
mein_Jahr == 2020
Um dem Filter weitere Prädikate hinzuzufügen, verwenden Sie die Schlüsselwörter AND und OR. Zum Beispiel, um

die Namen zu finden, die entweder 2020 oder 2019 mehr als 10000 Babys haben, schreiben wir:

Abfrage = '''
SELECT *
FROM baby
WHERE Anzahl > 10000
AND (Jahr = 2020
OR Jahr = 2019);
'''
pd.read_sql(abfrage, db)
Name Geschlecht Anzahl Jahr
0 Liam M^196592020
1 Noah M^182522020
2 Oliver M^141472020
... ... ... ... ...
41 Mia F^124522019
42 Harper F^104642019
110 | Kapitel 5: Arbeiten mit Beziehungen mit SQL

Name Geschlecht Anzahl Jahr
43 Evelyn F^104122019
44 Zeilen × 4 Spalten

Um schließlich die zehn häufigsten Namen im Jahr 2020 zu finden, können wir den Datenrahmen sortieren nach

Zählung in absteigender Reihenfolge unter Verwendung des Schlüsselworts ORDER BY mit der Option DESC (kurz

für DESCending).

Abfrage = '''
SELECT *
FROM baby
WHERE Jahr = 2020
ORDER BY Count DESC
LIMIT 10;
'''
pd.read_sql(abfrage, db)
Name Geschlecht Anzahl Jahr
0 Liam M^196592020
1 Noah M^182522020
2 Emma F^155812020
7 Sophia F^129762020
8 Amelia F^127042020
9 William M^125412020
10 Zeilen × 4 Spalten

Wir sehen, dass Liam, Noah und Emma die beliebtesten Babynamen im Jahr 2020 sind.

Beispiel: Seit wann ist Luna ein beliebter Name?
In dem Artikel der New York Times wird erwähnt, dass der Name "Luna" fast nicht existent war

vor 2000, hat sich aber seitdem zu einem sehr beliebten Namen für Mädchen entwickelt. Wenn

Wie genau ist Luna populär geworden? Wir können dies mithilfe von Slicing und Filterung überprüfen.

Wenn Sie eine Aufgabe zur Datenmanipulation angehen, empfehlen wir, das Problem aufzuteilen

in kleinere Schritte zu unterteilen. Wir könnten zum Beispiel denken:

Filter: Behalte nur Zeilen mit 'Luna' in der Spalte Name und 'F' in der Spalte Geschlecht.
spalte.
Slice: Behalten Sie die Spalten Count und Year.
Jetzt geht es darum, jeden Schritt in Code zu übersetzen.

Subsetting | 111
Abfrage = '''
SELECT *
FROM baby
WHERE Name = "Luna"
AND Sex = "F";
'''
luna = pd.read_sql(abfrage, db)
Luna
Name Geschlecht Anzahl Jahr
0 Luna F^77702020
1 Luna F^77722019
2 Luna F^69292018
... ... ... ... ...
125 Luna F^171883
126 Luna F^181881
127 Luna F^151880
128 Zeilen × 4 Spalten

pd.read_sql gibt ein pandas.DataFrame-Objekt zurück, das wir zur Erstellung eines Plots verwenden können.

Dies veranschaulicht einen üblichen Arbeitsablauf: Verarbeiten der Daten mit SQL, Laden der Daten in eine Pandas

Datenrahmen und visualisieren Sie dann die Ergebnisse.

px.line(luna, x='Jahr', y='Anzahl', width=350, height=250)
Es ist genau so, wie es im Artikel steht. Luna war bis zum Jahr 2000 oder so überhaupt nicht populär. Denk an

Wenn dir jemand sagt, dass sein Name Luna ist, kannst du dir ziemlich sicher sein, dass er

ihr Alter zu schätzen, auch ohne weitere Informationen über sie!

112 | Kapitel 5: Arbeiten mit Beziehungen mit SQL

Nur so zum Spaß, hier ist die gleiche Handlung für den Namen Siri.

Abfrage = '''
SELECT *
FROM baby
WHERE Name = "Siri"
AND Sex = "F";
'''
siri = pd.read_sql(abfrage, db)
px.line(siri, x='Jahr', y='Anzahl', width=350, height=250)
Warum könnte die Popularität nach 2010 so plötzlich gesunken sein? Nun, Siri ist zufällig

ist der Name von Apples Sprachassistent und wurde 2011 eingeführt. Ziehen wir einen Strich

für das Jahr 2011 und werfen Sie einen Blick darauf...

fig = px.line(siri, x='Jahr', y='Anzahl', width=350, height=250)
fig.add_vline(x=2011, line_color='red', line_width=4)
Unterteilung | 113
Es sieht so aus, als ob Eltern nicht wollen, dass ihre Kinder verwirrt sind, wenn andere Leute sagen: "Hey

Siri" auf ihre Telefone.

Mitbringsel
In diesem Abschnitt werden Relationen in SQL vorgestellt. Wir haben die üblichen Wege behandelt, auf denen Daten

Beziehungen zwischen Wissenschaftlern - Aufteilung mit Spaltenbeschriftungen und Filterung mit einem Booleschen Wert

Bedingung. Im nächsten Abschnitt wird erklärt, wie man Zeilen zusammenfasst.

Aggregieren
In diesem Abschnitt werden Operationen zur Aggregation von Zeilen in einer Beziehung vorgestellt. Datenwissenschaftler

Zeilen zusammenfassen, um Zusammenfassungen von Daten zu erstellen. Ein Datensatz enthält zum Beispiel-

Die täglichen Umsätze können aggregiert werden, um stattdessen die monatlichen Umsätze anzuzeigen. Genauer gesagt, werden wir

die Gruppierung einführen, eine gängige Operation zur Aggregation von Daten.

Wir arbeiten mit den Babynamen-Daten, die im vorigen Abschnitt vorgestellt wurden:

# Verbindung zur Datenbank einrichten
sqlalchemy importieren
db = sqlalchemy.create_engine('sqlite:///babynames.db')
query = '''
SELECT *
FROM baby
LIMIT 10
'''
pd.read_sql(abfrage, db)
114 | Kapitel 5: Mit Beziehungen arbeiten mit SQL

Name Geschlecht Anzahl Jahr
0 Liam M^196592020
1 Noah M^182522020
2 Oliver M^141472020
7 Lukas M^112812020
8 Heinrich M^107052020
9 Alexander M^101512020
10 Zeilen × 4 Spalten

Grundlegendes Gruppen-Aggregat
Angenommen, wir wollen die Gesamtzahl der in diesen Daten erfassten Babys herausfinden.

Dies ist einfach die Summe der Spalte Count. SQL bietet Funktionen, die wir in

die SELECT-Anweisung, wie SUM:

Abfrage = '''
SELECT SUM(Count)
FROM baby
'''
pd.read_sql(abfrage, db)
SUM(Count)
0^352554503
Die Summierung der Namenszahlen ist eine einfache Möglichkeit, die Daten zu aggregieren - sie kombiniert

Daten aus mehreren Zeilen.

Aber nehmen wir an, wir wollen stattdessen eine interessantere Frage beantworten: Sind die Geburten in den USA

im Laufe der Zeit nach oben tendieren? Um diese Frage zu beantworten, können wir die Summe der Spalte Count

innerhalb jedes Jahres, anstatt die Summe über den gesamten Datensatz zu bilden. Mit anderen Worten,

Wir teilen die Daten nach Jahr in Gruppen ein und summieren dann die Zählwerte innerhalb der Gruppen.

jede Gruppe. Dieser Prozess ist in Abbildung 5-2 dargestellt.

Aggregieren | 115
Abbildung 5-2. Eine Darstellung der Gruppierung und anschließenden Aggregation von Beispieldaten.

Wir nennen diesen Vorgang Gruppierung und anschließend Aggregation. In SQL geben wir an, was

Spalte, die wir für die Gruppierung durch die GROUP BY-Klausel verwenden wollen, dann mit aggre-

gationsfunktionen in SELECT:

Abfrage = '''
SELECT Jahr, SUM(Anzahl)
FROM baby
GROUP BY Jahr
'''
pd.read_sql(abfrage, db)
Jahr SUM(Count)
0^1880194419
1^1881185772
2^1882213385
... ... ...
138^20183487193
139^20193437438
140^20203287724
141 Zeilen × 2 Spalten

Beachten Sie, dass der Code fast derselbe ist wie bei der nicht gruppierten Version, außer dass er

enthält eine GROUP BY-Klausel unter Verwendung der Spalte Jahr. Wir fügen die Spalte Jahr auch zu

die SELECT-Klausel, so dass jede Zeile des Ergebnisses auch das Jahr enthält.

Das Ergebnis ist eine Relation zur Gesamtzahl der Babys, die für jedes Jahr in den Daten geboren wurden. Beachten Sie, dass

die Spalte Jahr enthält die eindeutigen Jahreswerte - es gibt keine doppelten Jahreswerte

mehr, da wir sie in Gruppen zusammengefasst haben. Jetzt können wir die Zählungen über die Zeit darstellen:

116 | Kapitel 5: Arbeiten mit Beziehungen mit SQL

counts_by_year = pd.read_sql(query, db)
px.line(counts_by_year, x='Year', y='SUM(Count)', width=350, height=250)
Was sehen wir in diesem Diagramm? Zunächst fällt auf, dass es verdächtig viele

wenige vor 1920 geborene Kinder. Eine wahrscheinliche Erklärung ist, dass die Sozialversicherung

Die Verwaltung wurde 1935 gegründet, so dass ihre Daten für frühere Geburten weniger vollständig sein könnten.

Man könnte auch den Einbruch bei Beginn des Zweiten Weltkriegs 1939 und die Nachkriegszeit

Baby-Boomer-Ära von 1946-1964.

Hier ist das Grundrezept für die Gruppierung in SQL:

SELECT
col1, -- Spalte, die für die Gruppierung verwendet wird
SUM(col2) -- Aggregation einer anderen Spalte
FROM table_name -- zu verwendende Beziehung
GROUP BY col1 -- die Spalte(n), nach denen gruppiert werden soll
Gruppierung nach mehreren Spalten
Wir geben mehrere Spalten in GROUP BY ein, um nach mehreren Spalten auf einmal zu gruppieren. Dies

ist nützlich, wenn wir unsere Gruppen weiter untergliedern müssen. Zum Beispiel können wir gruppieren

nach Jahr und Geschlecht, um zu sehen, wie viele männliche und weibliche Babys im Laufe der Zeit geboren wurden.

Abfrage = '''
SELECT Jahr, Geschlecht, SUM(Anzahl)
FROM baby
GROUP BY Jahr, Geschlecht
'''
pd.read_sql(abfrage, db)
Aggregieren | 117
1 https://www.sqlite.org/lang_aggfunc.html
Jahr Geschlecht SUM(Count)
0^1880 F^83929
1^1880 M^110490
2^1881 F^85034
279^2019 M^1785527
280^2020 F^1581301
281^2020 M^1706423
282 Zeilen × 3 Spalten

Beachten Sie, dass sich der Code eng an das Gruppierungsrezept hält.

Andere Aggregationsfunktionen
Die SQLite-Datenbank verfügt über verschiedene andere eingebaute Aggregationsfunktionen, wie z. B. COUNT,

AVG, MIN, und MAX. Die vollständige Liste der Funktionen finden Sie auf der SQLite-Website^1.

Die Funktion SUMME haben wir bereits gesehen:

Abfrage = '''
SELECT Jahr, SUM(Anzahl)
FROM baby
GROUP BY Jahr
'''
pd.read_sql(abfrage, db)
Jahr SUM(Count)
0^1880194419
1^1881185772
2^1882213385
... ... ...
138^20183487193
139^20193437438
140^20203287724
141 Zeilen × 2 Spalten

Um eine andere Aggregationsfunktion zu verwenden, rufen wir sie in der SELECT-Klausel auf. Zum Beispiel können wir

kann MAX anstelle von SUM verwenden:

118 | Kapitel 5: Arbeiten mit Beziehungen mit SQL

2 https://www.postgresql.org/docs/current/functions-aggregate.html
Abfrage = '''
SELECT Jahr, MAX(Anzahl)
FROM baby
GROUP BY Jahr
'''
pd.read_sql(abfrage, db)
Jahr MAX(Anzahl)
0^18809655
1^18818769
2^18829557
138^201819924
139^201920555
140^202019659
141 Zeilen × 2 Spalten

Die verfügbaren Aggregationsfunktionen sind eine der ersten Stellen, an denen ein Datenwissenschaftler
Datenwissenschaftler auf Unterschiede in den SQL-Implementierungen stoßen kann. Für
SQLite verfügt beispielsweise über einen relativ minimalen Satz an Aggregationsfunktio- nen
tionen, während PostgreSQL über viel mehr verfügt^2. Die meisten SQL-Implemen- tationen
tionen bieten SUM, COUNT, MIN, MAX und AVG.
Beispiel: Sind die Menschen bei Babynamen kreativer geworden?
Sind die Menschen im Laufe der Zeit bei Babynamen kreativer geworden? Eine Möglichkeit zur Messung

ob die Zahl der einzigartigen Babynamen pro Jahr im Laufe der Jahre gestiegen ist.

Zeit.

Um diese Aggregation in SQL durchzuführen, verwenden wir die Funktion COUNT und das Schlüsselwort DISTINCT.

Das Schlüsselwort DISTINCT weist SQL an, nur die eindeutigen Werte innerhalb einer Gruppe von Spalten zu behalten.

rungen.

# Findet die einzigartigen Babynamen
query = '''
SELECT DISTINCT Name
FROM baby
'''
pd.read_sql(abfrage, db)
Aggregieren | 119
Name
0 Liam
1 Noah
2 Oliver
... ...
100361 Kreta
100362 Rolle
100363 Zilpah
100364 Zeilen × 1 Spalten

Um die Anzahl der eindeutigen Namen zu zählen, können wir mit der Funktion COUNT aggregieren.

Wir verwenden auch das Schlüsselwort AS, um die resultierende Spalte umzubenennen.

# Findet die Anzahl der eindeutigen Babynamen
query = '''
SELECT COUNT(DISTINCT Name) AS n_names
FROM baby
'''
pd.read_sql(abfrage, db)
n_names
0^100364
Schließlich gruppieren wir nach der Spalte Jahr, um über jedes Jahr zu aggregieren, anstatt über die

gesamten Datensatzes:

Abfrage = '''
SELECT Jahr, COUNT(DISTINCT Name) AS n_names
FROM baby
GROUP BY Jahr
'''
unique_names_by_year = pd.read_sql(abfrage, db)
unique_names_by_year
Jahr n_names
0^18801889
1^18811829
2^18822012
... ... ...
138^201829619
139^201929417
140^202028613
120 | Kapitel 5: Arbeiten mit Beziehungen mit SQL

3 https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.unstack.html
141 Zeilen × 2 Spalten

Jetzt können wir die Anzahl der eindeutigen Namen über die Zeit aufzeichnen:

px.line(eindeutige_Namen_nach_Jahr,
x='Jahr', y='n_names',
labels={'n_names': '# unique names'},
width=350, height=250)
Es zeigt sich, dass die Zahl der eindeutigen Namen im Laufe der Zeit allgemein gestiegen ist, auch wenn

Allerdings hat sich die Zahl der Geburten seit den 1960er Jahren weitgehend stabilisiert.

Im Gegensatz zu Pandas gibt es in SQLite keine einfache Möglichkeit, eine Relation zu drehen.
tion. Stattdessen verwenden wir GROUP BY für zwei Spalten in SQL, lesen das
Ergebnis in einen Datenrahmen ein und verwenden dann die unstack()-Datenrahmen
Methode^3.
Mitbringsel
In diesem Abschnitt wurden gängige Methoden zur Aggregation von Daten in SQL unter Verwendung des GROUP BY-Schlüssels behandelt.

Wort mit einer oder mehreren Spalten. Im nächsten Abschnitt werden wir erklären, wie man Rela-

tionen zusammen.

Aggregieren | 121
Verbinden
Datenwissenschaftler wollen sehr häufig zwei oder mehr Relationen miteinander verbinden, um

Datensätze zwischen Beziehungen verbinden. Eine Online-Buchhandlung könnte zum Beispiel eine

Beziehung mit den Büchern, die jeder Nutzer bestellt hat, und eine zweite Beziehung mit den Genres der

jedes Buch. Indem er die beiden Beziehungen miteinander verbindet, kann der Datenwissenschaftler sehen, welche Genres

jeder Benutzer bevorzugt.

Wir werden die Daten zu den Babynamen weiter untersuchen. Wir werden Joins verwenden, um einige Trends zu überprüfen

in einem Artikel der New York Times über Babynamen erwähnt (Williams 2021). Die

In dem Artikel geht es darum, wie bestimmte Namenskategorien mehr oder weniger populär geworden sind.

im Laufe der Zeit. So wird beispielsweise erwähnt, dass mythologische Namen wie Julius und Cassius

populär geworden, während die Namen der Babyboomer wie Susan und Debbie zu

weniger beliebt. Wie hat sich die Beliebtheit dieser Kategorien im Laufe der Zeit verändert?

Wir haben die Namen und Kategorien aus dem NYT-Artikel genommen und sie in einer kleinen Rela-

tion namens nyt:

# Verbindung zur Datenbank einrichten
sqlalchemy importieren
db = sqlalchemy.create_engine('sqlite:///babynames.db')
query = '''
SELECT *
FROM nyt;
'''
pd.read_sql(abfrage, db)
nyt_name Kategorie
0 Luzifer verboten
1 Lilith verboten
2 Gefahr verboten
... ... ...
20 Venus himmlisch
21 Celestia himmlisch
22 Skye himmlisch
23 Zeilen × 2 Spalten

122 | Kapitel 5: Arbeiten mit Beziehungen mit SQL

Beachten Sie, dass der obige Code eine Abfrage auf babynames.db ausführt, die
dieselbe Datenbank, die die größere Baby-Beziehung aus den vorangegangenen
vorangegangenen Abschnitten enthält. SQL-Datenbanken können mehr als eine Beziehung enthalten, was
sehr nützlich, wenn wir mit vielen Datentabellen gleichzeitig arbeiten
auf einmal arbeiten müssen. CSV-Dateien hingegen enthalten in der Regel nur eine Datentabelle
Wenn wir eine Datenanalyse durchführen, die zwanzig Datentabellen verwendet
Datenanalyse mit zwanzig Datentabellen durchführen, müssen wir uns möglicherweise die Namen, Speicherorte und
Versionen von zwanzig CSV-Dateien verfolgen. Stattdessen könnte es einfacher sein, alle
Datentabellen in einer SQLite-Datenbank zu speichern, die in einer einzigen Datei gespeichert ist.
Um zu sehen, wie beliebt die Namenskategorien sind, verknüpfen wir die Beziehung nyt mit der Beziehung

baby-Beziehung, um die Anzahl der Namen von baby zu erhalten. Wir beginnen mit der Anzeige der ersten paar

Zeilen der Baby-Beziehung:

Abfrage = '''
SELECT *
FROM baby
LIMIT 10;
'''
pd.read_sql(abfrage, db)
Name Geschlecht Anzahl Jahr
0 Liam M^196592020
1 Noah M^182522020
2 Oliver M^141472020
7 Lukas M^112812020
8 Heinrich M^107052020
9 Alexander M^101512020
10 Zeilen × 4 Spalten

Stellen Sie sich vor, Sie gehen jede Zeile in baby durch und fragen: Ist dieser Name in der nyt-Tabelle? Wenn ja,

und fügen dann den Wert in der Kategoriespalte zu der Zeile hinzu. Das ist der Grundgedanke hinter einer

verbinden. Betrachten wir zunächst ein paar einfachere Beispiele.

Innere Verknüpfungen
Wir haben kleinere Versionen der Baby- und Nyt-Tabellen erstellt, damit es einfacher ist, zu sehen, was passiert.

Stifte, wenn wir Tabellen miteinander verbinden. Die Beziehungen heißen baby_small und

nyt_small.

Abfrage = '''
SELECT *
Verknüpfung | 123
FROM baby_klein;
'''
pd.read_sql(abfrage, db)
Name Geschlecht Anzahl Jahr
0 Noah M^182522020
1 Julius M^9602020
2 Karen M^62020
3 Karen F^3252020
4 Noah F^3052020
Abfrage = '''
SELECT *
FROM nyt_small;
'''
pd.read_sql(abfrage, db)
nyt_name Kategorie
0 Karen Bumerang
1 Julius-Mythologie
2 Freya-Mythologie
Um Tabellen in SQL zu verbinden, verwenden wir die JOIN-Klausel, um anzugeben, welche Tabellen wir verbinden wollen, und

die ON-Klausel, um ein Prädikat für die Verknüpfung der Tabellen anzugeben. Hier ist ein Beispiel:

Abfrage = '''
SELECT *
FROM baby_small JOIN nyt_small
ON baby_klein.Name = nyt_klein.nyt_name
'''
pd.read_sql(abfrage, db)
Name Geschlecht Anzahl Jahr nyt_name Kategorie
0 Julius M^9602020 Julius Mythologie
1 Karen M^62020 Karen Boomer
2 Karen F^3252020 Karen Bumer
Beachten Sie, dass die neue Tabelle die Spalten der beiden Tabellen baby_small und nyt_small enthält.

Die Zeilen mit dem Namen Noah sind verschwunden. Und die verbleibenden Zeilen haben ihre passenden

Kategorie von nyt_small.

Die obige Abfrage kann wie folgt gelesen werden:

124 | Kapitel 5: Arbeiten mit Beziehungen mit SQL

SELECT *
FROM baby_small JOIN nyt_small -- die zu verbindenden Tabellen
ON baby_small.Name = nyt_small.nyt_name
-- nur Zeilen zusammenfügen, wenn die Namen gleich sind
Wenn wir zwei Tabellen miteinander verbinden, teilen wir SQL die Spalte(n) aus jeder Tabelle mit, die wir

mit einem Prädikat mit dem Schlüsselwort ON verknüpfen möchten. SQL vergleicht Zeilen miteinander

wenn die Werte in den Verbindungsspalten übereinstimmen, wie in Abbildung 5-3 dargestellt.

Abbildung 5-3. Für die Verknüpfung gleicht SQL Zeilen anhand der Werte in den Spalten Name und nyt_name ab.

umns. Bei inneren Verknüpfungen (Standard) werden Zeilen, die keine übereinstimmenden Werte aufweisen, verworfen.

SQL führt standardmäßig eine innere Verknüpfung durch. Wenn eine der beiden Tabellen Zeilen hat, die keine Übereinstimmungen in

die andere Tabelle, lässt SQL diese Zeilen aus dem Ergebnis fallen. In diesem Fall werden die Noah-Zeilen in

baby_small haben keine Übereinstimmungen in nyt_small, also werden sie fallen gelassen. Auch die Freya

Zeile in nyt_small hat keine Übereinstimmungen in baby_small, also wird sie ebenfalls gelöscht. Nur

die Zeilen, die in beiden Tabellen übereinstimmen, bleiben im Endergebnis.

Linke, rechte und äußere Verknüpfungen
Manchmal möchte man Zeilen, die nicht übereinstimmen, behalten, anstatt sie ganz wegzulassen.

Es gibt andere Arten von Joins - linke, rechte und äußere -, die Zeilen beibehalten, auch wenn sie

haben keine Übereinstimmung.

Bei einer linken Verknüpfung werden Zeilen in der linken Tabelle, die nicht übereinstimmen, im Endergebnis beibehalten, da

in Abbildung 5-4 dargestellt.

Verbinden | 125
Abbildung 5-4. Bei einer linken Verknüpfung werden Zeilen in der linken Tabelle, die keine übereinstimmenden Werte haben, beibehalten.

Um eine linke Verknüpfung in Pandas durchzuführen, verwenden Sie LEFT JOIN anstelle von JOIN:

Abfrage = '''
SELECT *
FROM baby_small LEFT JOIN nyt_small
ON baby_klein.Name = nyt_klein.nyt_name
'''
pd.read_sql(abfrage, db)
Name Geschlecht Anzahl Jahr nyt_name Kategorie
0 Noah M^182522020 Keine Keine
1 Julius M^9602020 Julius Mythologie
2 Karen M^62020 Karen Boomer
3 Karen F^3252020 Karen Bumer
4 Noah F^3052020 Keine Keine
Beachten Sie, dass die Noah-Zeilen in der endgültigen Tabelle enthalten sind. Da diese Zeilen nicht mit einem

im Datenrahmen nyt_small übereinstimmen, lässt SQL NULL-Werte in den Feldern nyt_name und cate

Spalten (die dann beim Einlesen in eine Pandas-Datei in None-Werte umgewandelt werden)

Datenrahmen). Beachten Sie auch, dass die Zeile "Freya" in "nyt_small" immer noch weggelassen wird.

Eine rechte Verknüpfung funktioniert ähnlich wie die linke Verknüpfung, mit der Ausnahme, dass nicht übereinstimmende Zeilen in der

rechten Tabelle anstelle der linken Tabelle beibehalten werden. SQLite unterstützt Right Joins nicht direkt,

aber wir können die gleiche Verknüpfung durchführen, indem wir die Reihenfolge der Beziehungen umkehren, wenn wir

LEFT JOIN:

Abfrage = '''
SELECT *
126 | Kapitel 5: Arbeiten mit Beziehungen mit SQL

FROM nyt_small LEFT JOIN baby_small
ON baby_klein.Name = nyt_klein.nyt_name
'''
pd.read_sql(abfrage, db)
nyt_name Kategorie Name Geschlecht Anzahl Jahr
0 Karen Boomer Karen F 325.0 2020.0
1 Karen Boomer Karen M 6,0 2020,0
2 Julius Mythologie Julius M 960,0 2020,0
3 Freya-Mythologie Keine Keine NaN NaN
Bei einer äußeren Verknüpfung werden schließlich Zeilen aus beiden Tabellen beibehalten, auch wenn sie nicht übereinstimmen.

SQLite hat kein eingebautes Schlüsselwort für Outer-Joins. In Fällen, in denen eine äußere Verknüpfung

benötigen, müssen wir entweder eine andere SQL-Engine verwenden oder eine äußere Verknüpfung über

Pandas. Nach unserer Erfahrung (der des Autors) werden äußere Joins in der Praxis jedoch nur selten verwendet.

im Vergleich zu inneren und linken Fugen.

Beispiel: Popularität der NYT-Namenskategorien
Kehren wir nun zu den vollständigen Beziehungen zwischen Baby und NYT zurück.

Abfrage = '''
SELECT *
FROM baby
LIMIT 10
'''
pd.read_sql(abfrage, db)
Name Geschlecht Anzahl Jahr
0 Liam M^196592020
1 Noah M^182522020
2 Oliver M^141472020
... ... ... ... ...
7 Lukas M^112812020
8 Heinrich M^107052020
9 Alexander M^101512020
10 Zeilen × 4 Spalten

Abfrage = '''
SELECT *
FROM nyt
'''
Verknüpfung | 127
pd.read_sql(abfrage, db)
nyt_name Kategorie
0 Luzifer verboten
1 Lilith verboten
2 Gefahr verboten
... ... ...
20 Venus himmlisch
21 Celestia himmlisch
22 Skye himmlisch
23 Zeilen × 2 Spalten

Wir wollen wissen, wie sich die Beliebtheit der Namenskategorien in nyt im Laufe der Jahre verändert hat.

Zeit. Um diese Frage zu beantworten:

Innere Verknüpfung baby mit nyt, Abgleich der Zeilen, in denen die Namen gleich sind.
Gruppieren Sie die Tabelle nach Kategorie und Jahr
Aggregieren Sie die Zählungen mit einer Summe.
Abfrage = '''
SELECT
Kategorie,
Jahr,
SUM(Count) AS count -- [3]
FROM baby JOIN nyt -- [1]
ON baby.Name = nyt.nyt_name -- [1]
GROUP BY kategorie, Jahr -- [2]
'''
cate_counts = pd.read_sql(abfrage, db)
cate_counts
Kategorie Jahr Anzahl
0 boomer^1880292
1 bumer^1881298
2 boomer^1882326
... ... ... ...
647 Mythologie^20182944
648 Mythologie^20193320
649 Mythologie^20203489
650 Zeilen × 3 Spalten

128 | Kapitel 5: Arbeiten mit Beziehungen mit SQL

Die Zahlen in Klammern ([1], [2], [3]) in der obigen Abfrage zeigen, wie jeder Schritt in unserer

Plan den Teilen der SQL-Abfrage zuordnet. Beachten Sie, dass die Zahlen nicht in der richtigen Reihenfolge erscheinen.

Wir denken oft an die SELECT-Anweisung als letzten Teil der Abfrage, der ausgeführt wird

obwohl sie zuerst erscheint.

Jetzt können wir die Beliebtheit der einzelnen Kategorien darstellen:

Wie in dem NYT-Artikel behauptet wird, sind "Baby-Boomer"-Namen weniger beliebt geworden, nachdem

2000, während mythologische Namen beliebter geworden sind.

Wir können auch die Popularität aller Kategorien auf einmal darstellen. Werfen Sie einen Blick auf die Diagramme

und sehen Sie nach, ob sie die Behauptungen im Artikel der New York Times bestätigen.

Beitritt | 129
Mitbringsel
Beim Verbinden von Beziehungen werden Zeilen mit dem Schlüsselwort JOIN und einem

boolesches Prädikat. SQL erlaubt auch die Angabe der Art der Verknüpfung (INNER, LEFT), wenn

Durchführung einer Verknüpfung. Im nächsten Abschnitt wird erklärt, wie man Werte in einem Rela-

tion.

Transformieren
Datenwissenschaftler transformieren Spalten, wenn sie jeden Wert in einem Merkmal ändern müssen

auf die gleiche Weise. Wenn ein Merkmal beispielsweise die Höhe von Personen in Fuß enthält, kann ein Daten

Wissenschaftler möchten vielleicht die Höhen in Zentimeter umrechnen. In diesem Abschnitt werden wir

zeigen, wie man Funktionen zur Umwandlung von Datenspalten mit SQL anwendet.

# Verbindung zur Datenbank einrichten
sqlalchemy importieren
db = sqlalchemy.create_engine('sqlite:///babynames.db')
query = '''
SELECT *
FROM baby
LIMIT 10;
'''
pd.read_sql(abfrage, db)
Name Geschlecht Anzahl Jahr
0 Liam M^196592020
1 Noah M^182522020
2 Oliver M^141472020
... ... ... ... ...
7 Lukas M^112812020
8 Heinrich M^107052020
9 Alexander M^101512020
10 Zeilen × 4 Spalten

In dem Artikel über Babynamen in der New York Times (Williams 2021), erwähnt Pamela, dass

Namen, die mit dem Buchstaben "L" und "K" beginnen, wurden nach 2000 populär. Auf der anderen

Namen, die mit dem Buchstaben "J" beginnen, erreichten in den 1970er und 1980er Jahren einen Höhepunkt der Popularität

und haben seitdem an Popularität eingebüßt. Wir können diese Behauptungen anhand des Babys überprüfen

Datensatz.

Wir gehen dieses Problem in folgenden Schritten an:

130 | Kapitel 5: Arbeiten mit Beziehungen mit SQL

4 https://www.sqlite.org/lang_corefunc.html
Wandeln Sie die Spalte Name in eine neue Spalte um, die die Anfangsbuchstaben der
jedes Wertes in Name enthält.
Gruppieren Sie den Datenrahmen nach dem ersten Buchstaben und dem Jahr.
Aggregieren Sie die Anzahl der Namen durch Aufsummieren.
Um den ersten Schritt abzuschließen, wenden wir eine Funktion auf die Spalte Name an.

SQL-Funktionen
SQLite bietet skalare Funktionen, d.h. Funktionen, die einzelne Datenwerte umwandeln.

Beim Aufruf auf einer Datenspalte wendet SQLite diese Funktionen auf jeden Wert in

die Spalte. Im Gegensatz dazu nehmen Aggregationsfunktionen wie SUM und COUNT eine Spalte von

Werte als Eingabe und berechnen einen einzigen Wert als Ausgabe.

SQLite bietet eine umfassende Liste der eingebauten skalaren Funktionen auf seiner Website^4.

Um zum Beispiel die Längen der einzelnen Namen zu ermitteln, verwenden wir die Funktion LENGTH.

Abfrage = '''
SELECT Name, LENGTH(Name)
FROM baby
LIMIT 10;
'''
pd.read_sql(abfrage, db)
Name LENGTH(Name)
0 Liam^4
1 Noah^4
2 Oliver^6
... ... ...
7 Lukas^5
8 Heinrich^5
9 Alexander^9
10 Zeilen × 2 Spalten

Beachten Sie, dass die Funktion LENGTH auf jeden Wert in der Spalte Name angewendet wird.

Umwandlung | 131
5 https://www.postgresql.org/docs/9.2/functions.html
Wie bei den Aggregationsfunktionen bietet jede SQL-Implementierung einen
einen unterschiedlichen Satz an skalaren Funktionen. SQLite hat einen relativ minimalen Satz
von Funktionen, während PostgreSQL über viel mehr verfügt^5. Die meisten SQL-Implemen- tationen
Implementierungen bieten ein Äquivalent zu den SQLite-Funktionen LENGTH, ROUND,
SUBSTR und LIKE Funktionen.
Der Aufruf einer skalaren Funktion verwendet die gleiche Syntax wie eine Aggregationsfunktion. Dies kann

zu einer verwirrenden Ausgabe führen, wenn beide in einer einzigen Abfrage vermischt werden:

Abfrage = '''
SELECT Name, LENGTH(Name), COUNT(Name)
FROM baby
LIMIT 10;
'''
pd.read_sql(abfrage, db)
Name LENGTH(Name) COUNT(Name)
0 Liam^42020722
Aus diesem Grund müssen wir vorsichtig sein, wenn wir skalare und Aggregationsfunktionen

zusammen in einer SELECT-Anweisung erscheinen.

Um den ersten Buchstaben eines jeden Namens zu extrahieren, können wir die Funktion SUBSTR (kurz für

"substring"). Wie in der Dokumentation beschrieben, benötigt die Funktion SUBSTR drei

Argumente. Das erste ist die Eingabezeichenkette, das zweite ist die Position, an der die Sub

String (1-indiziert), und die dritte ist die Länge des Substrings.

Abfrage = '''
SELECT Name, SUBSTR(Name, 1, 1)
FROM baby
LIMIT 10;
'''
pd.read_sql(abfrage, db)
Name SUBSTR(Name, 1, 1)
0 Liam L
1 Noah N
2 Oliver O
... ... ...
7 Lukas L
8 Henry H
132 | Kapitel 5: Arbeiten mit Beziehungen mit SQL

Name SUBSTR(Name, 1, 1)
9 Alexander A
10 Zeilen × 2 Spalten

Jetzt können wir das Schlüsselwort AS verwenden, um die Spalte umzubenennen:

Abfrage = '''
SELECT *, SUBSTR(Name, 1, 1) AS Firsts
FROM baby
LIMIT 10;
'''
pd.read_sql(abfrage, db)
Name Geschlecht Anzahl Jahr Erstgeborene
0 Liam M^196592020 L
1 Noah M^182522020 N
2 Oliver M^141472020 O
7 Lucas M^112812020 L
8 Heinrich M^107052020 H
9 Alexander M^101512020 A
10 Zeilen × 5 Spalten

Damit ist Schritt 1 unseres Analyseplans abgeschlossen. SQL bietet mehrere Optionen zum Aufbrechen

Abfragen in kleinere Schritte aufzuteilen, was bei einer komplexeren Analyse wie dieser hilfreich ist.

Eine Möglichkeit besteht darin, eine völlig neue Beziehung mit der Anweisung CREATE TABLE zu erstellen.

Eine andere Möglichkeit ist die Verwendung des Schlüsselworts WITH, um eine temporäre Beziehung nur für die

Abfrage zur Hand. Wir werden das Schlüsselwort WITH für dieses Beispiel verwenden.

Mehrstufige Abfragen mit einer WITH-Klausel
Mit der WITH-Klausel können wir jeder SELECT-Abfrage einen Namen zuweisen. Dann können wir diese

Abfrage, als ob sie als Beziehung in der Datenbank vorhanden wäre. Wir können zum Beispiel die

Abfrage oben, die den ersten Buchstaben jedes Namens berechnet und als Buchstaben bezeichnet:

abfrage = '''
WITH letters AS (
SELECT *, SUBSTR(Name, 1, 1) AS Firsts
FROM baby
)
SELECT *
FROM buchstaben
LIMIT 10;
Umwandlung | 133
'''
pd.read_sql(abfrage, db)
Name Geschlecht Anzahl Jahr Erste
0 Liam M^196592020 L
1 Noah M^182522020 N
2 Oliver M^141472020 O
7 Lucas M^112812020 L
8 Heinrich M^107052020 H
9 Alexander M^101512020 A
10 Zeilen × 5 Spalten

Wir können diese Abfrage wie folgt lesen:

-- Erstellen Sie eine temporäre Beziehung namens Buchstaben, indem Sie die ersten
-- Buchstaben für jeden Namen im Baby
WITH letters AS (
SELECT *,
SUBSTR(Name, 1, 1) AS Anfangsbuchstaben
FROM baby
)
-- Wählen Sie dann die ersten zehn Zeilen von letters aus
SELECT *
FROM briefe
LIMIT 10;
WITH-Anweisungen sind sehr nützlich, da sie miteinander verkettet werden können. Wir können erstellen

mehrere temporäre Relationen in einer WITH-Anweisung, die jeweils einen Teil der Arbeit an

das vorherige Ergebnis, so dass wir schrittweise komplizierte Abfragen aufbauen können.

Beispiel: Beliebtheit von "L"-Namen
Nun können wir die nächsten Schritte in unserer Analyse durchführen. Wir gruppieren die Beziehung nach dem

Anfangsbuchstabe und Jahr, dann aggregieren Sie die Spalte Anzahl mit einer Summe.

abfrage = '''
WITH letters AS (
SELECT *, SUBSTR(Name, 1, 1) AS Firsts
FROM baby
)
SELECT Vornamen, Jahr, SUM(Anzahl) AS Anzahl
FROM briefe
GROUP BY Vornamen, Jahr;
'''
134 | Kapitel 5: Arbeiten mit Beziehungen mit SQL

buchstaben_zählungen = pd.read_sql(abfrage, db)
buchstaben_zählungen
Anzahl der Erstjahre
0 A^188016740
1 A^188116257
2 A^188218790
... ... ... ...
3638 Z^201855996
3639 Z^201955293
3640 Z^202054011
3641 Zeilen × 3 Spalten

Schließlich verwenden wir Plotly, um die Beliebtheit von "L"-Namen im Zeitverlauf darzustellen:

fig = px.line(letter_counts.loc[letter_counts['Firsts'] == 'L'],
x='Jahr', y='Anzahl', title='Beliebtheit von 'L'-Namen',
width=350, height=250)
margin(fig, t=30)
Das Diagramm zeigt, dass "L"-Namen in den 1960er Jahren sehr beliebt waren und in den Jahrzehnten danach abnahmen,

haben aber nach 2000 wieder an Popularität gewonnen.

Was ist mit "J"-Namen?

fig = px.line(letter_counts.loc[letter_counts['Firsts'] == 'J'],
x='Jahr', y='Anzahl', title='Beliebtheit von 'J'-Namen',
width=350, height=250)
margin(fig, t=30)
Umwandlung | 135
Der NYT-Artikel besagt, dass "J"-Namen in den 1970er und 80er Jahren sehr beliebt waren. Die Grafik

stimmt dem zu und zeigt auch, dass sie nach dem Jahr 2000 weniger beliebt geworden sind.

Mitbringsel
Um Werte in einer Beziehung zu transformieren, verwenden wir üblicherweise SQL-Funktionen wie LENGTH() oder

SUBSTR(). Wir haben auch erklärt, wie man komplexe Abfragen mit Hilfe der WITH-Klausel erstellt.

Im nächsten Abschnitt werden wir Relationen mit anderen Darstellungs- und Bearbeitungsmöglichkeiten vergleichen.

späte Datentabellen.

Wie unterscheiden sich Relationen von anderen Datendarstellungen?
Relationen sind nur eine Möglichkeit, in einer Tabelle gespeicherte Daten darzustellen. In der Praxis wird die Datenwis-

tisten begegnen vielen anderen Arten von Datentabellen, wie Tabellenkalkulationen, Matrizen und Daten.

Rahmen. In diesem Abschnitt vergleichen und kontrastieren wir die Beziehungen zu anderen

Darstellungen, um zu erklären, warum Relationen so häufig für die Datenanalyse verwendet werden.

Wir werden auch auf Szenarien hinweisen, in denen andere Darstellungen angemessener sein könnten.

ate.

Beziehungen und Tabellenkalkulationen
Tabellenkalkulationen sind Computeranwendungen, bei denen die Benutzer Daten in ein Raster eingeben und mit

Formeln, um Berechnungen durchzuführen. Ein berühmtes Beispiel ist heute Microsoft Excel,

obwohl Tabellenkalkulationen mindestens bis 1979 mit VisiCalc (Grad 2007) zurückreichen. Tabellenkalkulation -

machen es einfach, Daten zu sehen und direkt zu bearbeiten. Diese Eigenschaften machen

Tabellenkalkulationen sehr beliebt - einer Schätzung aus dem Jahr 2005 zufolge gibt es über 55 Millionen Tabellenkalkulationsprogramme.

136 | Kapitel 5: Arbeiten mit Beziehungen mit SQL

Blattnutzer im Vergleich zu 3 Millionen professionellen Programmierern in der Industrie (Scaffidi et

al. 2005).

Beziehungen haben mehrere entscheidende Vorteile gegenüber Tabellenkalkulationen. Das Schreiben von SQL-Code in einer Kom-

Ein rationales Notizbuch wie Jupyter erzeugt natürlich einen Datenstamm. Jemand, der

das Notizbuch öffnet, kann die Eingabedateien für das Notizbuch sehen und wie die Daten

geändert. Tabellenkalkulationen machen eine Datenabfolge nicht sichtbar; wenn eine Person die Daten manuell bearbeitet

Datenwerte in einer Zelle, ist es für zukünftige Benutzer schwierig zu erkennen, welche Werte manuell

bearbeitet wurden oder wie sie bearbeitet wurden. Relationen können auch größere Datensätze verarbeiten als Spread-

Die Benutzer können SQL-Systeme verwenden, um mit großen Datenmengen zu arbeiten, die sehr schwer zu verarbeiten wären.

in eine Tabellenkalkulation zu laden.

Beziehungen und Matrizen
Eine Matrix ist eine zweidimensionale Anordnung von Daten, die hauptsächlich für lineare Algebra-Operationen verwendet wird.

tionen. Im folgenden Beispiel ist 𝐀 eine Matrix mit drei Zeilen und zwei Spalten.

𝐀=

1 0
0 4
0 0
Matrizen sind mathematische Objekte, die durch die Operatoren, die sie zulassen, definiert sind. Für

Beispielsweise können Matrizen addiert oder multipliziert werden. Matrizen haben auch eine Trans-

aufstellen. Diese Operatoren haben sehr nützliche Eigenschaften, auf die sich Datenwissenschaftler verlassen, um

statistische Modellierung.

Ein wichtiger Unterschied zwischen einer Matrix und einer Relation: Bei der Behandlung als Mathe-

matisches Objekt, können Matrizen nur Zahlen enthalten. Beziehungen hingegen können

auch andere Datentypen wie Text enthalten. Dies macht Beziehungen für das Laden nützlicher

und die Verarbeitung realer Daten, die alle Arten von Datentypen enthalten können.

Datenwissenschaftler bezeichnen Matrizen nicht nur als mathematische Objekte,
sondern auch als Programmobjekte. Zum Beispiel hat die Programmiersprache R
Programmiersprache R ein Matrix-Objekt, während wir in Python eine
eine Matrix durch ein zweidimensionales Numpy-Array darstellen. Matrizen, wie
und R implementiert sind, können neben Zahlen auch andere Datentypen enthalten
Zahlen enthalten, verlieren dabei aber ihre mathematischen Eigenschaften. Dies ist
ein weiteres Beispiel dafür, wie sich Domänen mit demselben Begriff auf unterschiedliche Dinge
mit ein und demselben Begriff bezeichnen können.
Beziehungen und Dataframes
Datenrahmen sind eine der gebräuchlichsten Methoden zur Darstellung von Datentabellen im Allgemeinen.

Programmiersprachen wie Python und R. (Wir behandeln Dataframes in den Kapiteln

ter 4 in diesem Buch). Dataframes haben viele Ähnlichkeiten mit Relationen; beide

Wie unterscheiden sich Beziehungen von anderen Datendarstellungen? | 137
6 https://www.postgresql.org/about/
verwenden Zeilen zur Darstellung von Datensätzen und Spalten zur Darstellung von Merkmalen. Beide haben Spalten

Namen und Daten innerhalb einer Spalte haben denselben Typ.

Ein wesentlicher Vorteil von Datenrahmen ist, dass sie keine Zeilen zur Darstellung von Datensätzen benötigen

und Spalten zur Darstellung von Merkmalen. Oft liegen die Rohdaten nicht in einer praktischen

Format, das direkt in eine Beziehung gesetzt werden kann. In diesen Szenarien verwenden die Datenwissenschaftler

den Datenrahmen zum Laden und Verarbeiten von Daten, da Datenrahmen in dieser Hinsicht flexibler sind

beachten. Häufig laden Datenwissenschaftler Rohdaten in einen Datenrahmen und verarbeiten dann die

Daten in ein Format zu bringen, das leicht in einer Beziehung gespeichert werden kann.

Ein wesentlicher Vorteil von Relationen gegenüber Datenrahmen besteht darin, dass Relationen verwendet werden von

relationale Datenbanksysteme wie PostgreSQL^6, die sehr nützliche Funktionen für Daten haben

Speicherung und Verwaltung. Stellen Sie sich einen Datenwissenschaftler in einem Unternehmen vor, das eine große

Website für soziale Medien. Die Datenbank kann Daten enthalten, die viel zu groß sind, um sie in einem

Datenrahmen von Pandas auf einmal zu erstellen; stattdessen verwenden Datenwissenschaftler SQL-Abfragen, um Untergruppen zu bilden und

Daten zu aggregieren, da Datenbanksysteme besser in der Lage sind, große Datenmengen zu verarbeiten.

Außerdem aktualisieren die Nutzer der Website ständig ihre Daten, indem sie Beiträge verfassen, hochladen

Bilder und die Bearbeitung ihres Profils. Hier ermöglichen Datenbanksysteme den Datenwissenschaftlern die Wiederverwendung

ihre bestehenden SQL-Abfragen, um ihre Analysen mit den neuesten Daten zu aktualisieren, anstatt

immer wieder große CSV-Dateien herunterladen zu müssen.

Für eine genauere Beschreibung des Unterschieds zwischen Datenrahmen und Beziehungen,

siehe (Petersohn et al. 2020).

Schlussfolgerung
In diesem Kapitel haben wir erklärt, was Beziehungen sind, warum sie nützlich sind und wie man mit ihnen arbeitet.

mit ihnen unter Verwendung von SQL-Code. Subsetting, Aggregieren, Verbinden und Transformieren sind

die bei fast jeder Datenanalyse nützlich sind. Wir werden diese Operationen in den folgenden Abschnitten häufig verwenden

das Buch, insbesondere in den Kapiteln 6, ???, und ???.

Übungen
Schreiben Sie eine SQL-Abfrage über die Baby-Relation, lesen Sie das Ergebnis in einen Pandas-Dataframe ein,
und erstellen Sie ein Diagramm über die Beliebtheit Ihres Namens im Laufe der Zeit. Wenn Sie dieses Diagramm verwenden würden
um Ihr Alter zu schätzen, was würden Sie schätzen? Ist das nahe an Ihrem tatsächlichen
Alter? Überlegen Sie sich einen möglichen Grund.
Wenn Sie männliche und weibliche Babynamen im Laufe der Zeit aufzeichnen, werden Sie feststellen, dass
nach 1950 generell mehr männliche Babys geboren wurden. Spiegelt sich dieser Trend in den Vereinigten Staaten wider?
138 | Kapitel 5: Arbeiten mit Beziehungen mit SQL

Volkszählungsdaten? Gehen Sie auf die Website der Volkszählung (https://data.census.gov/cedsci/) und
überprüfen.
Nehmen wir an, Sie möchten die beliebtesten männlichen und weiblichen Babynamen jedes
Jahr finden. Sie könnten diese Abfrage schreiben:
SELECT Jahr, Geschlecht, MAX(Anzahl), MAX(Name)
FROM baby
GROUP BY Jahr, Geschlecht
Aber dieser Code liefert nicht das richtige Ergebnis. Warum?

Schreiben Sie nun einen Code, um die beliebtesten männlichen und weiblichen Namen jedes Jahres zu ermitteln,
zusammen mit ihrer Anzahl. Hinweis: Die Antwort ist etwas einfacher, als Sie vielleicht erwarten
aufgrund einer speziellen SQLite-Regel (suchen Sie nach "Bare columns in an aggregate quer-
ies").
Denken Sie sich ein realistisches Datenbeispiel aus, bei dem ein Datenwissenschaftler einen
Inner Join gegenüber einem Left Join bevorzugen würde, und ein Beispiel, bei dem ein Datenwissenschaftler einen Left
Join einem Inner Join vorziehen würde.
In dem Abschnitt über Joins hat die Tabelle nyt keine doppelten Namen. Aber ein
Name könnte aber durchaus zu mehreren Kategorien gehören, z. B. ist Elisabeth ein
Name aus der Bibel und ein Name für ein Königshaus. Nehmen wir an, wir haben eine Relation namens
multi_cat, die einen Namen mehrfach auflisten kann - einmal für jede Kategorie, zu der er gehört
zugehörig ist:
query = '''
SELECT *
FROM multi_cat
'''
pd.read_sql(abfrage, db)
nyt_name Kategorie
0 Elisabeth Bibel
1 Elisabeth königlich
2 Arjun hinduistisch
3 Arjun mythologisch
Was passiert, wenn wir Baby mit dieser Tabelle verbinden? Was geschieht im Allgemeinen, wenn

es mehrere übereinstimmende Zeilen in der linken und rechten Tabelle gibt?

Bei einem Self-Join wird eine Tabelle mit sich selbst verbunden. Zum Beispiel enthält die Freundesrela-
tion enthält Paare von Personen, die miteinander befreundet sind.
Übungen | 139
Abfrage = '''
SELECT *
FROM Freunde
'''
pd.read_sql(abfrage, db)
selbst andere
0 Jim Scott
1 Scott Philip
2 Philip Tricia
3 Philip Ailie
Warum könnte ein Datenwissenschaftler den folgenden Self-Join nützlich finden?

Abfrage = '''
SELECT *
FROM freunde AS f1
INNER JOIN Freunde AS f2
ON f1.andere = f2.selbst
'''
pd.read_sql(abfrage, db)
selbst andere selbst andere
0 Jim Scott Scott Philip
1 Scott Philip Philip Ailie
2 Scott Philip Philip Tricia
Die Self-Join-Abfrage in der vorherigen Übung verwendet das AS-Schlüsselwort, um jede Beziehung umzubenennen.
Beziehung umzubenennen. Warum müssen wir die Relationen in dieser Abfrage umbenennen? Ganz allgemein,
Unter welchen Umständen müssen wir Beziehungen in der FROM-Klausel umbenennen?
Sind die Namen im Laufe der Zeit im Durchschnitt länger geworden? Schreiben Sie eine SQL-Abfrage, lesen Sie das
Ergebnis in einen Pandas-Dataframe ein, und erstellen Sie dann ein Diagramm, um diese Frage zu beantworten.
Was errechnet die folgende SQL-Abfrage? Was enthält die stat-Spalte?
tain?
Abfrage = '''
WITH yearly_avgs AS (
SELECT Name, AVG(Count) AS avg
FROM baby
GROUP BY Name
),
140 | Kapitel 5: Arbeiten mit Beziehungen mit SQL

sq_diffs AS (
SELECT b.Name, POWER(Count - avg, 2) AS sq_diff
FROM baby AS b JOIN yearly_avgs AS y
ON b.Name = y.Name
)
SELECT Name, POWER(AVG(sq_diff), 0.5) AS stat
FROM sq_diffs
GROUP BY Name
ORDER BY stat DESC
LIMIT 10
'''
pd.read_sql(abfrage, db)
Name stat
0 Michael 28296,71
1 Robert 26442,69
2 Jakobus 26434,03
... ... ...
7 Richard 15849,65
8 Patricia 13428,25
9 Matthäus 13165,15
10 Zeilen × 2 Spalten

In diesem Kapitel haben wir herausgefunden, dass man das Alter einer Person gut einschätzen kann
wenn man nur den Namen kennt. Zum Beispiel ist der Name "Luna" nach 2000 stark
Popularität nach dem Jahr 2000 stark zugenommen, so dass man annehmen kann, dass eine Person namens "Luna"
etwa nach 2000 geboren wurde. Können Sie das Alter einer Person allein anhand des ersten Buchstabens
dem ersten Buchstaben ihres Namens?
Beantworten Sie diese Frage, indem Sie eine SQL-Abfrage schreiben, das Ergebnis in einen Pandas
Datenrahmen einlesen und dann Diagramme erstellen, um zu sehen, ob dies möglich ist. Sehen Sie dann, welche
Anfangsbuchstaben die meisten Informationen über das Alter einer Person liefern.
Übungen | 141
KAPITEL 6

Dateien streiten

Ein Hinweis für die Leser von Early Release

Mit Early Release ebooks erhalten Sie die Bücher in ihrer frühesten Form, d. h. den unbearbeiteten und
unbearbeiteten Inhalt, so dass Sie die Vorteile dieser Technologien schon lange
vor der offiziellen Veröffentlichung dieser Titel.
Dies wird das 8. Kapitel des endgültigen Buches sein. Bitte beachten Sie, dass das GitHub-Repositorium
zu einem späteren Zeitpunkt aktiviert wird.
Wenn Sie Anmerkungen haben, wie wir den Inhalt und/oder die Beispiele in diesem Buch verbessern können
diesem Buch verbessern könnten, oder wenn Sie fehlendes Material in diesem Kapitel bemerken, wenden Sie sich bitte an den
Autor unter mpotter@oreilly.com.
Bevor Sie mit Daten in Python arbeiten können, ist es hilfreich, die Dateien zu verstehen, in denen

die Quelle der Daten. Sie möchten Antworten auf ein paar grundlegende Fragen wie:

Wie viele Daten haben Sie?
Wie ist die Quelldatei formatiert?
Die Antworten auf diese Fragen können sehr hilfreich sein. Wenn Ihre Datei zu groß ist, müssen Sie möglicherweise

spezielle Methoden, um sie einzulesen. Wenn Ihre Datei nicht so formatiert ist, wie Sie es erwarten, können Sie

kann nach dem Laden in einen Datenrahmen auf fehlerhafte Werte stoßen.

Obwohl es viele Arten von Strukturen gibt, die Daten darstellen können, werden wir in diesem Buch

arbeiten hauptsächlich mit Datentabellen, wie Pandas DataFrames und SQL-Relationen. (Aber

Bitte beachten Sie, dass das Programm weniger strukturierte Textdaten untersucht und mit hierarchischen Daten arbeitet.

Datenstrukturen). Wir konzentrieren uns aus mehreren Gründen auf Datentabellen. Erstens, die Forschung darüber, wie man

Speicherung und Bearbeitung von Datentabellen hat zu stabilen und effizienten Werkzeugen für die Arbeit

143
1 https://www.datafiles.samhsa.gov/study-series/drug-abuse-warning-network-dawn-nid13516
mit Tabellen. Zweitens sind Daten in Tabellenform enge Verwandte von Matrizen, die Mathe-

matischen Objekten aus dem unermesslich reichen Gebiet der linearen Algebra. Schließlich sind Datentabellen

sehr häufig.

In diesem Kapitel stellen wir typische Dateiformate und Kodierungen für reinen Text und

beschreiben Maße für die Dateigröße, und wir verwenden Python-Tools zur Untersuchung von Quelldateien. Später

in diesem Kapitel stellen wir einen alternativen Ansatz für die Arbeit mit Dateien vor, die Shell

Interpreter. Shell-Befehle geben uns einen programmatischen Weg, um Informationen über eine

Datei außerhalb der Python-Umgebung, und die Shell kann bei großen Datenmengen sehr nützlich sein.

Schließlich überprüfen wir die Form der Datentabelle (die Anzahl der Zeilen und Spalten) und

Granularität (was eine Zeile darstellt). Diese einfachen Prüfungen sind der Ausgangspunkt für

Bereinigen und Analysieren Ihrer Daten.

Der erste Abschnitt enthält kurze Beschreibungen von zwei Datensätzen, die wir als Beispiele verwenden

in diesem Kapitel.

Beispiele für Datenquellen
In diesem Kapitel verwenden wir zwei Datensätze als Beispiele: eine staatliche Erhebung über Drogen

Missbrauch und Verwaltungsdaten der Stadt San Francisco über Restaurants

Inspektionen. In den folgenden Abschnitten zeigen wir, wie die Bestandsaufnahme des Formats, der Kodierung und der

und Größe der Dateien, die die "Rohdaten" enthalten, können Probleme beim Laden der Daten verhindern.

der Quelldatei in einen Datenrahmen. Bevor wir beginnen, möchten wir eine

Überblick über diese Datensätze und deren Umfang (Kapitel 1).

Drug Abuse Warning Network (DAWN) Umfrage
DAWN ist eine nationale Umfrage im Gesundheitswesen, die Trends im Drogenmissbrauch und die

das Aufkommen neuer missbräuchlicher Substanzen. Die Erhebung zielt auch darauf ab, die Auswirkungen der

Drogenmissbrauchs auf das Gesundheitssystem des Landes und zur Verbesserung der Notfallmaßnahmen

Abteilungen überwachen Krisen im Zusammenhang mit Drogenmissbrauch. DAWN wird von der U.S. Sub-

stance Abuse and Medical Health Services Administration (SAMHSA). DAWN wurde

die von 1998 bis 2011 jährlich durchgeführt wurde. Dies ist zum Teil auf die Opioid-Epidemie zurückzuführen,

die DAWN-Erhebung wurde 2018 neu gestartet. Wir untersuchen die Daten von 2011, die

die über das SAMHSA-Datenarchiv^1 zugänglich sind.

Die Zielpopulation der Erhebung sind alle drogenbedingten Besuche in der Notaufnahme der

In den USA werden diese Besuche durch eine Reihe von Notaufnahmen in Krankenhäusern (und

ihre Aufzeichnungen). Die Krankenhäuser werden für die Erhebung anhand von Wahrscheinlichkeitsstichproben ausgewählt

(siehe Kapitel 2) und alle drogenbedingten Besuche in der Notaufnahme des Krankenhauses der Stichprobe

144 | Kapitel 6: Mit Dateien hantieren

2 https://data.sfgov.org/Health-and-Social-Services/Restaurant-Scores-LIVES-Standard/pyih-qa8i/data
3 Im Jahr 2020 begann die Stadt damit, Restaurants mit farbcodierten Plaketten auszustatten, die anzeigen, ob das Restaurant die Inspektion
(grün), "bedingt bestanden" (gelb) oder "nicht bestanden" (rot) hat. Auf diesen neuen Schildern wird nicht mehr eine
numerische Inspektionsnote. Die Bewertungen und Verstöße eines Restaurants sind jedoch nach wie vor auf DataSF verfügbar.
Zimmer sind in der Erhebung enthalten. Alle Arten von drogenbedingten Besuchen werden erfasst, wie z. B.

Drogenmissbrauch, Missbrauch, versehentliche Einnahme, Selbstmordversuche, böswillige Vergiftungen und

unerwünschte Wirkungen. Bei jedem Besuch können bis zu 16 verschiedene Medikamente erfasst werden,

einschließlich illegaler Drogen, verschreibungspflichtiger Medikamente und frei verkäuflicher Arzneimittel.

Die Quelldatei für diesen Datensatz ist ein Beispiel für eine Formatierung mit fester Breite, die eine

Codebuch zu entschlüsseln. Außerdem handelt es sich um eine relativ große Datei, die das Thema der

wie man die Größe einer Datei ermittelt. Und die Granularität ist ungewöhnlich, weil ein Besuch in der Notaufnahme, nicht eine Per-

son, ist Gegenstand der Untersuchung und aufgrund des komplexen Erhebungsdesigns.

Die Restaurantdateien von San Francisco weisen noch weitere Merkmale auf, die sie zu einem guten

Beispiel für dieses Kapitel.

Lebensmittelsicherheit in Restaurants in San Francisco
Das Gesundheitsamt von San Francisco führt routinemäßig unangekündigte Besuche durch

zu den Restaurants und prüft sie auf Lebensmittelsicherheit. Der Inspektor errechnet eine Punktzahl

auf der Grundlage der festgestellten Verstöße und liefert Beschreibungen der Verstöße, die

gefunden. Die Zielpopulation sind alle Restaurants in der Stadt San Francisco.

Der Zugang zu diesen Restaurants erfolgt über einen Rahmen von Restaurantinspektionen, die

die zwischen 2013 und 2016 durchgeführt wurden. Einige Restaurants haben mehrere Inspektionen in einem

Jahr, und nicht alle der über 7000 Restaurants werden jährlich inspiziert.

Die Ergebnisse zur Lebensmittelsicherheit sind über die Open-Data-Initiative der Stadt, genannt DataSF, verfügbar.

DataSF ist ein Beispiel für Stadtverwaltungen auf der ganzen Welt, die ihre Daten veröffentlichen.

Die Aufgabe von DataSF besteht darin, "die Nutzung von Daten für die Entscheidungsfindung zu fördern

und Dienstleistungserbringung" mit dem Ziel, die Lebens- und Arbeitsqualität der Bewohner zu verbessern.

Schüler, Arbeitgeber, Arbeitnehmer und Besucher^2.

Die Stadt San Francisco verlangt von Restaurants, dass sie ihre Bewertungen öffentlich aushängen (siehe

Abbildung 6-1 unten für ein Beispielplakat)^3. Diese Daten sind ein Beispiel für mehrere

Dateien mit unterschiedlicher Struktur, Feldern und Granularität. Ein Datensatz enthält eine Zusammenfassung

Ergebnisse von Inspektionen, ein anderer enthält Einzelheiten über Verstöße, die bei einer

Inspektion und eine dritte enthält Informationen über die Restaurants. Die Verstöße

sowohl schwerwiegende Probleme im Zusammenhang mit der Übertragung von durch Lebensmittel übertragbaren Krankheiten als auch

kleinere Probleme wie das nicht ordnungsgemäße Anbringen der Prüfplakette.

Beispiele für Datenquellen | 145
Abbildung 6-1. Eine Scorecard zur Lebensmittelsicherheit, die in einem Restaurant angezeigt wird. Die Punktezahl liegt zwischen 0 und

146 | Kapitel 6: Mit Dateien hantieren

Sowohl die Daten der DAWN-Umfrage als auch die Daten der Restaurantinspektionen in San Francisco sind

online als einfache Textdateien verfügbar. Ihre Formate sind jedoch unterschiedlich, und in der

nächsten Abschnitt zeigen wir, wie man ein Dateiformat herausfindet, damit man die

Daten in einen Datenrahmen.

Dateiformate
Ein Dateiformat beschreibt, wie Daten auf einem Computer gespeichert werden. Das Verständnis der Datei

Format hilft uns herauszufinden, wie wir die Daten in Python einlesen können, um mit ihnen zu arbeiten

als Tabelle. In diesem Abschnitt werden mehrere gängige Formate für die Speicherung von Daten vorgestellt

Tabellen.

Das Dateiformat und die Struktur der Daten sind zwei verschiedene
Dinge. Die Struktur der Daten ist eine mentale Darstellung der Daten
und sagt uns, welche Arten von Operationen wir durchführen können. Zum Beispiel entspricht eine
Tabellenstruktur entspricht den Datenwerten, die in Zeilen und Spalten
Spalten. Dieselbe Tabelle kann jedoch in vielen verschiedenen Dateiformaten gespeichert werden.
Dateiformaten gespeichert werden.
Abgegrenztes Format
Abgegrenzte Formate verwenden ein bestimmtes Zeichen, um Datenwerte zu trennen. In der Regel sind diese Trennzeichen

ratoren sind entweder: ein Komma (Comma-Separated-Values oder kurz CSV), ein Tabulator (Tab-

Separated Values oder TSV), Leerzeichen oder ein Doppelpunkt. Diese Formate sind natürlich für

Daten speichern, die eine Tabellenstruktur haben. Jede Zeile in der Datei stellt einen Datensatz dar, der

werden durch Zeilenumbrüche (\n oder \r\n) abgegrenzt. Und innerhalb einer Zeile wird der Datensatz

Die Informationen werden durch das Komma (,) für CSV oder das Tabulatorzeichen

(\t) für TSV, und so weiter. Die erste Zeile dieser Dateien enthält oft die Namen der

die Spalten/Merkmale der Tabelle.

Die Bewertungen der Restaurants in San Francisco werden in Dateien im CSV-Format gespeichert. Wir wollen anzeigen

die ersten paar Zeilen der Datei inspections.csv. In Python ist die eingebaute Bibliothek pathlib

hat ein nützliches Path-Objekt, um Pfade zu Dateien und Ordnern anzugeben, die plattformübergreifend funktionieren.

Formulare. Die Daten sind in der Datei data/inspections.csv gespeichert, so dass wir Path() verwenden, um

den vollständigen Pfadnamen erstellen. Das nachstehende Path-Objekt hat viele nützliche Methoden, wie z. B.

open() wie unten gezeigt.

from pathlib import Path
# Pfad zu unserer Datendatei erstellen
insp_path = Path() / 'data' / 'inspections.csv'
with insp_path.open() as f:
# Die ersten fünf Zeilen der Datei anzeigen
Dateiformate | 147
for _ in range(5):
print(f.readline(), end='')
"business_id", "score", "date", "type"
19,"94","20160513","routine"
19,"94","20171211","routine"
24,"98","20171101","routine"
24,"98","20161005","routine"
Pfade sind knifflig, wenn man über verschiedene Betriebssysteme hinweg arbeitet
(OS). Ein typischer Pfad in Windows könnte zum Beispiel so aussehen: C:
\files\data.csv, während ein Pfad unter Unix oder MacOS wie folgt aussehen könnte
~/files/data.csv. Aus diesem Grund kann Code, der auf einem Betriebssystem funktioniert
auf anderen Betriebssystemen nicht ausgeführt werden.
Die Python-Bibliothek pathlib wurde entwickelt, um betriebssystemspezifische Pfad
Probleme zu vermeiden. Durch ihre Verwendung ist der hier gezeigte Code portabler - er funktioniert
unter Windows, MacOS und Unix.
Die Anzeige der ersten Zeilen einer Datei ist etwas, das wir oft benötigen, daher erstellen wir eine Funktion, die die

tion als Abkürzung:

def head(dateipfad, n=5):
'''Druckt die ersten n Zeilen von filepath'''
with dateipfad.open() as f:
for _ in range(n):
print(f.readline(), end='')
head(insp_path)
"business_id", "score", "date", "type"
19,"94","20160513","routine"
19,"94","20171211","routine"
24,"98","20171101","routine"
24,"98","20161005","routine"
Beachten Sie, dass die Feldnamen in der ersten Zeile der Datei erscheinen; sie sind durch Komma getrennt.

getrennt und in Anführungszeichen. Wir sehen vier Felder: die Unternehmenskennung, die Restaurantbezeichnung

Punktzahl, das Datum der Prüfung und die Art der Prüfung. Jede Zeile in der Datei kor-

reagiert auf eine Inspektion, und die Werte für ID, Punktzahl, Datum und Typ sind durch

Kommas. Zusätzlich zur Identifizierung des Dateiformats wollen wir auch das Format identifizieren

der Merkmale. Zwei Dinge sind erwähnenswert: Die Partituren und die Daten erscheinen beide als Zeichenketten.

Wir wollen die Ergebnisse in Zahlen umwandeln, um eine zusammenfassende Statistik berechnen zu können

und erstellen ein Histogramm der Punktwerte. Und wir konvertieren das Datum in eine Datumszeit für...

mat, so dass wir Zeitseriendiagramme erstellen können. Wir zeigen, wie man diese Transfor- mationen durchführt.

in den Niederlanden.

Alle drei Restaurant-Quelldateien sind im CSV-Format. Andererseits ist die

Die DAWN-Quelle hat ein Format mit fester Breite. Wir beschreiben dieses Format im Folgenden.

148 | Kapitel 6: Mit Dateien hantieren

Format mit fester Breite (fixed-width)
Das Format mit fester Breite (FWF) verwendet keine Trennzeichen, um Datenwerte zu trennen.

Stattdessen erscheinen die Werte für ein bestimmtes Feld in jeder Zeile an genau derselben Stelle.

Die DAWN-Quelldatei hat dieses Format. Jede Zeile der Datei ist sehr lang. Für die Anzeige

werden nur die ersten paar Zeichen der ersten 5 Zeilen der Datei angezeigt.

dawn_path = Pfad() / 'data' / 'DAWN-Data.txt'
Breite = 65
with dawn_path.open() as f:
for _ in range(5):
print(f.readline()[:width])
1 2251082 .9426354082 3 4 1 2201141 2 865 105 1102005 1
2 2291292 5.9920106887 911 1 3201134 12077 81 82 283-8
3 7 7 251 4.7231718669 611 2 2201143 12313 1 12 -7-8
410 8 292 4.0801470012 6 2 1 3201122 1 234 358 99 215 2
5 122 942 5.1777093467 10 6 1 3201134 3 865 105 1102005 1
Beachten Sie, dass sich die Werte von einer Zeile zur nächsten anzugleichen scheinen. Beachten Sie auch, dass sie

scheinen zusammengedrängt und ohne Trennzeichen zu sein. Wir müssen die genaue Position kennen

der einzelnen Informationen in einer Zeile, um sie zu verstehen. SAMHSA bietet

ein 2.000-seitiges Codebuch mit allen Informationen, die zum Lesen der Datei erforderlich sind. In der

Codebuchs finden wir, dass das Altersfeld an den Positionen 34-35 erscheint und in Interposition kodiert wird.

vals von 1 bis 11. Die ersten beiden oben gezeigten Datensätze haben zum Beispiel folgende Alterskategorien

von 4 und 11, und das Codebuch sagt uns, dass ein Code von 4 für die Altersgruppe "6 bis

11", und 11 steht für "65+".

Eine weit verbreitete Konvention ist die Verwendung der Dateinamenerweiterung, wie
wie .csv, .tsv und .txt, zu verwenden, um das Format des Inhalts der Datei anzugeben.
der Datei anzugeben. Bei Dateinamen, die auf .csv enden, wird erwartet, dass sie
Komma-getrennte Werte, .tsv Tab-getrennte Werte und .txt
txt ist in der Regel reiner Text ohne ein bestimmtes Format. Allerdings sind diese
Erweiterungsnamen sind jedoch nur Vorschläge. Selbst wenn eine Datei die Erweiterung .csv
Erweiterung hat, kann es sein, dass der eigentliche Inhalt nicht richtig formatiert ist! Es ist
ist es ratsam, den Inhalt der Datei zu prüfen, bevor man sie in einen
in einen Datenrahmen laden. Wenn die Datei nicht zu groß ist, können Sie sie mit einem Texteditor öffnen und
mit einem einfachen Texteditor öffnen und untersuchen. Andernfalls können Sie ein paar
Zeilen mit .readline() oder Shell-Befehlen.
Andere gängige Textformate sind hierarchische Formate und lockere

strukturierte Formate (im Gegensatz zu Formaten, die Tabellenstrukturen direkt unterstützen).

Diese werden in anderen Kapiteln ausführlicher behandelt, doch der Vollständigkeit halber sei kurz darauf hingewiesen

beschreiben sie hier.

Dateiformate | 149
Hierarchische Formate
Hierarchische Datenformate speichern Daten in einer verschachtelten Struktur. Zum Beispiel ist das Java-

Script Object Format (JSON) ist ein gebräuchliches Format für die Kommunikation von Web

Server. JSON-Dateien haben eine hierarchische Struktur mit Schlüsseln und Werten, ähnlich wie eine

Python-Wörterbuch. Jeder Datensatz in einer JSON-Datei kann verschiedene Felder und Datensätze haben

können andere Datensätze enthalten. Die eXtensible Markup Language (XML) und HyperText

Markup Language (HTML) sind weitere gängige Formate für die Speicherung von Dokumenten im Internet.

Internet. Wie JSON enthalten auch diese Dateien Daten in einem hierarchischen Schlüssel-Wert-Format.

Wir behandeln beide Formate (JSON und XML) ausführlicher in ?

Als Nächstes beschreiben wir kurz andere Textdateien, die in keine der vorherigen Kategorien fallen

Kategorien, haben aber dennoch eine gewisse Struktur, die es uns ermöglicht, sie zu lesen und zu extrahieren

Informationen.

Lose strukturierte Formate
Webprotokolle, Messwerte von Messgeräten und Programmprotokolle liefern in der Regel Daten im Klartext.

Im Folgenden sehen Sie zum Beispiel eine Zeile eines Webprotokolls (wir haben sie auf mehrere Zeilen aufgeteilt, um

Lesbarkeit). Sie enthält Informationen wie Datum und Uhrzeit sowie die Art der Anfrage

auf der Website vorgenommen.

169.237.46.168 - -
[26/Jan/2004:10:47:58 -0800]"GET /stat141/Winter04 HTTP/1.1" 301 328
"http://anson.ucdavis.edu/courses"
"Mozilla/4.0 (kompatibel; MSIE 6.0; Windows NT 5.0; .NET CLR 1.1.4322)"
Es ist eine Struktur vorhanden, aber nicht in einem einfachen, abgegrenzten Dateiformat, wie wir es

mit "locker strukturiert" meinen. Wir sehen, dass das Datum und die Uhrzeit zwischen den Quadraten erscheinen

Klammern und der Art der Anfrage (in diesem Fall GET) folgt die Datums- und Zeitangabe

und erscheint in Anführungszeichen. Später in diesem Abschnitt werden wir diese Beobachtungen über die Logs verwenden.

Struktur- und Stringmanipulationstools, um die interessierenden Werte in eine Datenstruktur zu extrahieren

Tisch.

Ein weiteres Beispiel ist eine einzelne Aufzeichnung von Messungen, die mit einer Draht-

weniger Gerät. Das Gerät meldet den Zeitstempel, die Kennung, den Standort des Geräts und

die Signalstärken, die es von anderen Geräten empfängt. Diese Informationen werden mit einem Kom-

Kombination von Formaten: Schlüssel=Wert-Paare, durch Semikolon getrennte und durch Komma getrennte Werte

ues.

t=1139644637174;id=00:02:2D:21:0F:33;pos=2.0,0.0,0.0;degree=45.5;
00:14:bf:b1:97:8a=-33,2437000000,3;00:14:bf:b1:97:8a=-38,2437000000,3;
Wie bei den Webprotokollen können wir die Zeichenkettenmanipulation und die Muster in den

Aufzeichnungen, um Merkmale in eine Tabelle zu extrahieren.

Obwohl wir uns in diesem Kapitel auf Datentabellen konzentrieren, soll das alles nur zeigen: Es gibt viele

Arten von Dateiformaten, die Daten speichern!

150 | Kapitel 6: Mit Dateien hantieren

Bisher haben wir den Begriff "reiner Text" im weitesten Sinne für Formate verwendet, die angezeigt werden können

mit einem Texteditor. Eine einfache Textdatei kann jedoch viele verschiedene Kodierungen haben, und

Wenn wir die Kodierung nicht korrekt angeben, können die Werte im Datenrahmen Folgendes enthalten

gibbersih. Im Folgenden geben wir einen Überblick über die Dateikodierung.

Datei-Codierung
Moderne Computer speichern Daten als lange Sequenzen von Bits: 0 s und 1 s. Zeichencodierung

ings, wie ASCII, sagen dem Computer, wie er zwischen Bits und tatsächlichem Text übersetzen soll. Für

In ASCII stehen beispielsweise die Bits 100 001 für den Buchstaben A und 100 010 für B. Die

Die einfachste Art von Klartext unterstützt nur Standard-ASCII-Zeichen, die

umfasst die englischen Groß- und Kleinbuchstaben, Zahlen, Interpunktionszeichen und

Räume.

Die ASCII-Kodierung reicht nicht aus, um eine Vielzahl von Sonderzeichen und Zeichen darzustellen

aus anderen Sprachen. Andere, modernere Zeichenkodierungen haben viel mehr

Zeichen, die dargestellt werden können. Gängige Kodierungen für Dokumente und Web

Seiten sind Latin-1 (ISO-8859-1) und UTF-8. UTF-8 hat mehr als eine Million Zeichen,

und ist abwärtskompatibel zu ASCII, d. h. es werden dieselben Darstellungsformen verwendet.

tion für englische Buchstaben, Zahlen und Satzzeichen als ASCII.

Wenn wir eine Textdatei haben, müssen wir normalerweise ihre Kodierung herausfinden. Wenn wir die

falsche Kodierung in eine Datei einzulesen, liest Python entweder falsche Werte oder Fehler. Die

Der beste Weg, die Kodierung herauszufinden, ist, die Dokumentation der Daten zu prüfen, die oft

sagt ausdrücklich, wie die Kodierung lautet.

Wenn wir die Kodierung nicht kennen, müssen wir eine Vermutung anstellen. Der Chardet

Paket hat eine Funktion namens detect(), die auf die Kodierung einer Datei schließt. Da diese

unvollkommen sind, liefert die Funktion auch eine Konfidenz zwischen 0 und 1. Wir verwenden

diese Funktion, um die Dateien im Datenordner dieses Kapitels zu betrachten.

chardet importieren
line = '{:<25} {:<10} {}'.format
# für jede Datei den Namen, die Kodierung und das Vertrauen in die Kodierung ausgeben
print(line('File', 'Encoding', 'Confidence'))
for filepath in Path('data').glob('*'):
result = chardet.detect(dateipfad.read_bytes())
print(line(str(filepath), result['encoding'], result['confidence']))
Dateikodierung Konfidenz
daten/untersuchungen.csv ascii 1.0
daten/co2_mm_mlo.txt ascii 1.0
daten/uebertretungen.csv ascii 1.0
daten/DAWN-Daten.txt ascii 1.0
Dateiformate | 151
daten/legende.csv ascii 1.0
daten/geschaefte.csv ISO-8859-1 0.73
Die Erkennungsfunktion ist sich ziemlich sicher, dass alle Dateien bis auf eine ASCII-kodiert sind.

Eine Ausnahme bildet die Datei businesses.csv, die offenbar eine ISO-8859-1-Kodierung aufweist.

Wir stoßen auf Probleme, wenn wir diese Kodierung ignorieren und versuchen, die Geschäftsdatei in

Pandas ohne Angabe der speziellen Kodierung.

# naiv liest die Datei ohne Berücksichtigung der Kodierung
>>> pd.read_csv('daten/geschaefte.csv')
[...stack trace omitted...]
UnicodeDecodeError: 'utf-8' codec kann Byte 0xd1 in Position 8 nicht dekodieren
Position 8: ungültiges Fortsetzungsbyte
Um die Daten erfolgreich zu lesen, müssen wir die ISO-8859-1-Kodierung angeben.

bus = pd.read_csv('daten/unternehmen.csv', encoding='ISO-8859-1')
business_id name adresse post_code
0^19 NRGIZE LIFESTYLE CAFE 1200 VAN NESS AVE, 3RD FLOOR^94109
1^24 OMNI S.F. HOTEL - 2ND FLOOR PANTRY 500 CALIFORNIA ST, 2ND FLOOR^94104
2^31 NORMAN'S ICE CREAM AND FREEZES 2801 LEAVENWORTH ST^94133
3^45 CHARLIE'S DELI CAFE 3202 FOLSOM ST^94110
In diesem Abschnitt haben wir Formate für Klartextdaten vorgestellt, die häufig für folgende Zwecke verwendet werden

Speicherung und Austausch von Tabellen. Das Format mit kommagetrennten Werten ist das gängigste.

mon, aber auch andere, wie z. B. Tabulator-getrennte und feste Breiten, sind weit verbreitet. Auch

Auch wenn ein Dateiname die Erweiterung ".csv" hat, sollte man sich vergewissern, dass es sich um eine

tatsächlich eine CSV-Datei. Auch die Dateikodierung kann ein wenig rätselhaft sein, um herauszufinden, und

Wenn es keine Metadaten gibt, die uns die Kodierung explizit mitteilen, muss man raten.

spielen. Wenn eine Kodierung nicht zu 100 % bestätigt ist, ist es eine gute Idee, zusätzliche Informationen einzuholen.

Dokumentation.

Häufig werden CSV- und TSV-Dateien mit Tabellenkalkulationen verwechselt. Das liegt
zum Teil, weil die meisten Tabellenkalkulationsprogramme (wie Microsoft Excel)
eine CSV-Datei automatisch als Tabelle in einer Arbeitsmappe anzeigt.
Hinter den Kulissen prüft Excel das Dateiformat und die Kodierung, genau wie
wie wir es in diesem Abschnitt getan haben. Excel-Dateien haben jedoch ein anderes
Format als CSV- und TSV-Dateien, und wir müssen verschiedene pan
das-Funktionen verwenden, um diese Formate in Python zu lesen.
Ein weiterer, möglicherweise wichtiger Aspekt einer Quelldatei ist ihre Größe. Wenn eine Datei sehr groß ist, müssen wir

möglicherweise nicht in der Lage sein, sie in einen Datenrahmen einzulesen. Im nächsten Abschnitt besprechen wir, wie man

die Größe einer Quelldatei zu ermitteln.

152 | Kapitel 6: Mit Dateien hantieren

Dateigröße
Computer haben nur begrenzte Ressourcen. Wahrscheinlich haben Sie diese Grenzen aus erster Hand erfahren, wenn Sie

Ihr Computer ist langsamer geworden, weil zu viele Anwendungen gleichzeitig geöffnet sind.

Wir wollen sicherstellen, dass wir bei der Arbeit die Grenzen des Computers nicht überschreiten

mit Daten, und wir können eine Datei je nach ihrer Größe unterschiedlich untersuchen. Wenn wir wissen

dass unser Datensatz relativ klein ist, dann kann ein Texteditor oder eine Tabellenkalkulation vermittelt werden.

die Daten zu betrachten. Andererseits ist bei großen Datenbeständen ein programmatischeres

Explorations- oder sogar verteilte Datenverarbeitungswerkzeuge erforderlich sein.

In vielen Situationen analysieren wir Datensätze, die aus dem Internet heruntergeladen wurden. Diese Dateien

befinden sich auf dem Festplattenspeicher des Computers. Um mit Python Daten zu erforschen und zu manipu-

müssen wir die Daten in den Speicher des Computers einlesen, auch bekannt als

Speicher mit wahlfreiem Zugriff (RAM). Der gesamte Python-Code erfordert die Verwendung von RAM, egal

wie kurz der Code ist.

Der Arbeitsspeicher eines Computers ist in der Regel viel kleiner als der Festplattenspeicher eines Computers. Für

Ein Computermodell, das 2018 auf den Markt kam, hatte beispielsweise 32-mal mehr Speicherplatz als

RAM. Leider bedeutet dies, dass die Datendateien oft viel größer sein können, als sie sind.

möglich ist, in den Speicher zu lesen. Sowohl die Festplattenspeicher- als auch die RAM-Kapazität werden gemessen in

in Form von Bytes. Grob gesagt, fügt jedes Zeichen in einer Textdatei ein Byte zur Dateigröße hinzu.

Größe. Um die Größe größerer Dateien kurz zu beschreiben, verwenden wir die Präfixe, wie sie in

die folgende Tabelle 6-1.

Tabelle 6-1. Tabelle 8.1 Präfixe für gängige Dateigrößen.

Mehrfachnotation Anzahl der Bytes
Kibibyte KiB 1024
Mebibyte MiB 1024²
Gibibyte GiB 1024³
Tebibyte TiB 1024⁴
Pebibyte PiB 1024⁵
Eine Datei mit 52428800 Zeichen benötigt zum Beispiel

52428800/1024^2 = 50 Megabyte, oder 50 MiB auf der Festplatte.

Dateigröße | 153
Warum wird ein Vielfaches von 1024 statt eines einfachen Vielfachen von 1000 für
diese Präfixe? Dies ist ein historisches Ergebnis der Tatsache, dass die meisten Com- puter ein
Computer ein binäres Zahlenschema verwenden, bei dem Potenzen von 2 einfacher
darstellen lassen (1024 = 2^10 ). Sie werden auch die typischen SI-Präfixe sehen
verwendet, um die Größe zu beschreiben: Kilobyte, Megabyte und Gigabyte, zum Beispiel
zum Beispiel. Leider werden diese Präfixe nicht einheitlich verwendet.
Manchmal bezieht sich ein Kilobyte auf 1000 Byte, ein anderes Mal bezieht sich ein Kilobyte
auf 1024 Byte. Um Verwirrung zu vermeiden, werden wir uns an kibi-,
mebi- und gibibytes, die eindeutig Vielfache von 1024 darstellen.
Viele Computer haben viel mehr Festplattenspeicher als verfügbarer Arbeitsspeicher. Es ist nicht

Es ist nicht ungewöhnlich, dass eine Datei, die auf einem Computer gespeichert ist, den Speicherplatz überfüllt.

den Speicher des Computers, wenn wir versuchen, ihn mit einem Programm, einschließlich Python, zu manipulieren

Programme. Wir beginnen unsere Datenarbeit oft damit, dass wir uns vergewissern, dass die Dateien, die wir verwalten, in Ordnung sind.

schen Größe. Zu diesem Zweck können wir die eingebaute os-Bibliothek verwenden.

from pathlib import Pfad
importieren os
kib = 1024
line = '{:<25} {}'.format
print(line('Datei', 'Größe (KiB)'))
for filepath in Path('data').glob('*'):
size = os.path.getsize(dateipfad)
print(line(str(filepath), np.round(size / kib)))
Dateigröße (KiB)
daten/untersuchungen.csv 455.0
daten/co2_mm_mlo.txt 50.0
daten/rechtsverletzungen.csv 3639.0
daten/DAWN-Daten.txt 273531.0
daten/legende.csv 0.0
daten/geschaefte.csv 645.0
Wir sehen, dass die Datei businesses.csv 645 KiB auf der Festplatte beansprucht und damit gut innerhalb der

die Speicherkapazitäten der meisten Systeme. Obwohl die Datei violations.csv mehr als

3,6 MiB Festplattenspeicher, die meisten Rechner können sie problemlos in einen Pandas DataFrame einlesen

auch. Die Datei DAWN-Data.txt, die die Daten der DAWN-Erhebung enthält, ist jedoch viel größer.

Die DAWN-Datei beansprucht fast 270 MiB Speicherplatz auf der Festplatte, und während einige Computer

mit dieser Datei im Speicher arbeiten kann, kann dies andere Systeme verlangsamen. Um diese Daten zu

in Python einfacher zu handhaben ist, können wir eine Teilmenge der Spalten laden, anstatt alle Spalten

sie.

Manchmal sind wir an der Gesamtgröße eines Ordners interessiert und nicht an der Größe der einzelnen Ordner.

uale Dateien. Wenn wir zum Beispiel eine Datei mit Inspektionen für jeden Monat eines Jahres haben, können wir

möchte ich sehen, ob wir alle Daten in einem einzigen Datenrahmen zusammenfassen können.

154 | Kapitel 6: Mit Dateien hantieren

mib = 1024**2
gesamt = 0
for filepath in Path('data').glob('*'):
total += os.path.getsize(dateipfad) / mib
print(f'Der Ordner data/ enthält {total:.2f} MiB')
Der Ordner "data/" enthält 271,80 MiB
Als Faustregel gilt, dass das Einlesen einer Datei mit Pandas normalerweise
mindestens das Fünffache des verfügbaren Speichers als die Dateigröße. Zum Beispiel
Beispiel: Das Einlesen einer 1 GiB großen Datei erfordert in der Regel mindestens 5 GiB an
verfügbaren Speicher. Der Speicher wird von allen auf einem Computer laufenden Programmen
Computer, einschließlich des Betriebssystems, der Webbrowser und des
Jupyter-Notebook selbst. Ein Computer mit insgesamt 4 GiB RAM könnte
nur 1 GiB verfügbaren RAM haben, wenn viele Anwendungen ausgeführt werden.
Mit 1 GiB verfügbarem RAM ist es unwahrscheinlich, dass Pandas in der Lage sein wird
eine 1 GiB große Datei einlesen kann.
Als Nächstes werden wir Strategien für die Arbeit mit Daten diskutieren, die viel größer sind als das, was machbar ist.

in den Speicher zu laden.

Arbeiten mit großen Datensätzen
Der populäre Begriff "Big Data" bezieht sich im Allgemeinen auf ein Szenario, bei dem die Daten groß sind

so dass selbst Spitzencomputer die Daten nicht direkt in den Speicher einlesen können.

Dies ist ein übliches Szenario in wissenschaftlichen Bereichen wie der Astronomie, wo Teleskope

viele große Bilder vom Weltraum aufnehmen. Es ist auch üblich, dass Unternehmen, die eine Vielzahl von

Benutzer.

Herauszufinden, wie man aus großen Datenbeständen Erkenntnisse gewinnen kann, ist ein wichtiges Forschungsproblem.

lem, das die Bereiche Datenbanktechnik und verteiltes Rechnen motiviert.

In diesem Buch werden wir diese Bereiche zwar nicht ausführlich behandeln, aber wir können einen kurzen Überblick über...

Ansicht der grundlegenden Ansätze.

Teilmenge der Daten.

Ein einfacher Ansatz besteht darin, die Daten zu unterteilen. Anstatt die gesamte Quelle zu laden

Datei können wir entweder einen bestimmten Teil davon auswählen (z. B. die Daten eines Tages), oder wir können

eine Zufallsstichprobe des Datensatzes. Wegen seiner Einfachheit verwenden wir diesen Ansatz recht

oft in diesem Buch. Der natürliche Nachteil ist, dass bei diesem Ansatz viele der Vorteile verloren gehen.

der Analyse eines großen Datensatzes, z. B. zur Untersuchung seltener Ereignisse.

Verwenden Sie ein Datenbanksystem.

Wie wir in Kapitel 5 besprochen haben, sind relationale Datenbankmanagementsysteme (RDBMS)

wurden speziell für die Speicherung großer Datenmengen entwickelt. Diese Systeme können Daten manipulieren

Dateigröße | 155
die zu groß sind, um mit SQL-Abfragen in den Speicher zu passen. Aufgrund ihrer Vorteile,

RDBMS sind in Forschung und Industrie zur Datenspeicherung weit verbreitet. Ein Nachteil...

Seite ist, dass sie oft einen separaten Server für die Daten benötigen, der seine eigene Konfiguration benötigt.

rung. Ein weiterer Nachteil ist, dass SQL weniger flexibel ist, was die Berechnungen angeht, als

Python, was insbesondere für die Modellierung und Vorhersage von Bedeutung ist. Eine nützliche

Ein hybrider Ansatz besteht darin, SQL zu verwenden, um die Daten in Stapel zu unterteilen, zu aggregieren oder Stichproben zu bilden.

die klein genug sind, um sie in Python einzulesen. Dann können wir Python für anspruchsvollere...

tizierte Analysen.

Verwenden Sie ein verteiltes Rechensystem.

Ein weiterer Ansatz für die Verarbeitung komplexer Berechnungen auf großen Datenbeständen ist die Verwendung einer Dis-

verteiltes Rechensystem wie MapReduce, Spark oder Ray. Diese Systeme funktionieren am besten

bei Aufgaben, die in viele kleinere Teile aufgeteilt werden können, da sie große Datensätze aufspalten

in kleinere Datensätze zu unterteilen und dann Programme für alle kleineren Datensätze gleichzeitig auszuführen. Wegen der

Diese Systeme sind sehr flexibel und können in einer Vielzahl von Szenarien eingesetzt werden, z. B.

Modellierung und Vorhersage. Ihr größter Nachteil ist, dass sie viel Arbeit erfordern können

ordnungsgemäß zu installieren und zu konfigurieren, da diese Systeme in der Regel an verschiedenen Orten installiert werden.

viele Computer, die miteinander koordiniert werden müssen.

Zusammenfassend wurde in diesem Abschnitt die übliche Notation der Dateigröße eingeführt und gezeigt, wie man die

Dateigrößen in Python. Es ist praktisch, Python zu verwenden, um ein Dateiformat zu bestimmen, zu kodieren...

und Größe. Ein weiteres mächtiges Werkzeug für die Arbeit mit Dateien ist die Shell, die weit verbreitet ist.

verwendet und hat eine prägnantere Syntax als Python. Im nächsten Abschnitt stellen wir ein

einige Befehlszeilentools, die in der Shell verfügbar sind, um die gleichen Aufgaben wie die Suche zu erledigen.

Informationen über eine Datei auslesen, bevor sie in einen Datenrahmen eingelesen wird.

Die Shell und Befehlszeilen-Tools
Fast alle Computer bieten Zugang zu einem Shell-Interpreter, wie sh oder bash. Shell

Interpreter führen in der Regel Operationen an den Dateien auf einem Computer durch und haben

ihre eigene Sprache, Syntax und integrierten Befehle.

Wir verwenden den Begriff Befehlszeilenschnittstellen-Tools (CLI), um die verfügbaren Befehle zu bezeichnen.

in einem Shell-Interpreter verwenden können. Obwohl wir in diesem Abschnitt nur einige wenige CLI-Tools behandeln,

gibt es viele nützliche CLI-Tools, die alle Arten von Operationen mit Dateien ermöglichen. Für

Wenn Sie beispielsweise den folgenden Befehl in der Bash-Shell ausführen, erhalten Sie eine Liste aller

Dateien im Ordner figures/ mit Angabe ihrer Dateigrößen:

ls -l -h Zahlen/
Die grundlegende Syntax für einen Shell-Befehl lautet:

Befehl -Optionen arg1 arg2
156 Kapitel 6: Mit Dateien ringen

CLI-Tools nehmen oft ein oder mehrere Argumente entgegen, ähnlich wie Python-Funktionen ein

Argumente. In der Shell umschließen wir Argumente mit Leerzeichen, nicht mit Klammern und

Kommas. Die Argumente erscheinen am Ende der Befehlszeile und lauten normalerweise

den Namen einer Datei oder einen Text. Im obigen ls-Beispiel ist das Argument für ls fig

ures/. Außerdem unterstützen die CLI-Tools Flags, die zusätzliche Optionen bieten. Diese

Flags werden unmittelbar nach dem Befehlsnamen mit einem Bindestrich als Trennzeichen angegeben.

iter. Im obigen ls-Beispiel haben wir die Flags -l (um zusätzliche Informationen bereitzustellen)

über jede Datei) und -h (um die Dateigrößen in einem besser lesbaren Format anzugeben). Viele

Befehle haben Standardargumente und -optionen, und der Befehl man gibt eine Liste von

akzeptable Optionen, Beispiele und Standardwerte für jeden Befehl. Zum Beispiel: man ls

beschreibt die etwa 30 Flags, die für ls verfügbar sind.

Alle CLI-Tools, die wir in diesem Buch behandeln, sind spezifisch für den Shell-Interpreter sh
Interpreter, dem Standardinterpreter für Jupyter-Installationen auf MacOS
und Linux-Systemen zum Zeitpunkt des Schreibens. Windows-Systeme haben einen
einen anderen Interpreter und die in diesem Buch gezeigten Befehle
nicht unter Windows laufen, obwohl Windows über sein Linux-Subsystem Zugang zu einem sh-Interpreter bietet.
Interpreter durch sein Linux Subsystem bietet.
Die Befehle in diesem Abschnitt können in einer Terminalanwendung ausgeführt werden,
oder über ein von Jupyter geöffnetes Terminal ausgeführt werden.
Wir beginnen mit einer Erkundung des Dateisystems für dieses Kapitel, indem wir das Werkzeug ls verwenden.

ls
Daten zanken_granularität.ipynb
Zahlen wrangling_intro.ipynb
wrangling_Befehlszeile.ipynb wrangling_Struktur.ipynb
wrangling_datasets.ipynb wrangling_summary.ipynb
wrangling_formats.ipynb
Um tiefer einzutauchen und die Dateien im Verzeichnis data/ aufzulisten, geben wir den Verzeichnisnamen an

als Argument für ls.

ls -l -L -h data/
gesamt 556664
-rw-r--r-- 1 nolan staff 267M Dec 10 14:03 DAWN-Data.txt
-rw-r--r-- 1 nolan staff 645K Dec 10 14:01 businesses.csv
-rw-r--r-- 1 nolan staff 50K Jan 22 13:09 co2_mm_mlo.txt
-rw-r--r-- 1 nolan staff 455K Dec 10 14:01 inspections.csv
-rw-r--r-- 1 nolan staff 120B Dez 10 14:01 legend.csv
-rw-r--r-- 1 nolan staff 3.6M Dec 10 14:01 violations.csv
Außerdem haben wir dem Befehl die Option -l hinzugefügt, die das Format der Ausgabe angibt

um Informationen über jede Datei in einer separaten Zeile zusammen mit zusätzlichen Metadaten zu erhalten.

Die Shell und die Befehlszeilentools | 157
In der fünften Spalte der Auflistung wird insbesondere die Dateigröße angezeigt. Um die Dateigrößen

besser lesbar zu machen, haben wir das Flag -h verwendet. Wenn wir mehrere einfache Optionsflags haben wie -

l, -h und -L können wir sie als Abkürzung miteinander kombinieren:

ls -lLh data/
Wenn wir in diesem Buch mit Datensätzen arbeiten, wird unser Code oft
ein zusätzliches -L-Flag für ls und andere CLI-Tools, wie z.B. du. Wir tun dies
Wir tun dies, weil wir die Datensätze in unserem Buch mit Verknüpfungen
(genannt Symlinks) einrichten. Normalerweise wird Ihr Code das Flag -L nicht benötigen, es sei denn
Sie arbeiten auch mit Symlinks.
Andere CLI-Tools zur Überprüfung der Dateigröße sind wc und du. Der Befehl wc (kurz

für wordcount) liefert hilfreiche Informationen über die Größe einer Datei in Form der Anzahl

der Zeilen, Wörter und Zeichen in der Datei.

wc daten/DAWN-Daten.txt
229211 22695570 280095842 daten/DAWN-Daten.txt
Aus der Ausgabe geht hervor, dass DAWN-Data.txt 229211 Zeilen und 280095842 Zeilen hat

Zeichen. (Der mittlere Wert ist die Wortanzahl der Datei, was für Text nützlich ist, aber nicht für

sehr nützlich für Dateien mit Daten).

Das Tool ls berechnet nicht die kumulative Größe des Inhalts eines Ordners. Um die...

Um die Gesamtgröße eines Ordners, einschließlich der darin enthaltenen Dateien, zu berechnen, verwenden wir du

(kurz für Festplattennutzung). Standardmäßig zeigt das du-Tool die Größe in Einheiten an, die Blöcke genannt werden.

du -L Daten/
556664 daten/
Wir fügen üblicherweise auch das Flag -s zu du hinzu, um die Dateigrößen sowohl für Dateien als auch für fold- anzuzeigen

ers und das Flag -h, um Mengen im Standardformat KiB, MiB, GiB anzuzeigen. Die Seite

Sternchen in data/* bedeutet, dass du die Größe jedes Objekts im Datenordner anzeigen sollst.

du -Lsh Daten/*
267M daten/DAWN-Daten.txt
648K Daten/Geschäfte.csv
52K daten/co2_mm_mlo.txt
456K daten/inspektionen.csv
4.0K Daten/Legende.csv
3.6M daten/rechtsverletzungen.csv
Um die Formatierung einer Datei zu überprüfen, können wir die ersten Zeilen mit dem Befehl head untersuchen

Befehl oder die letzten Zeilen mit tail. Diese CLIs sind sehr nützlich, um einen Blick auf eine

den Inhalt der Datei, um festzustellen, ob sie als CSV, TSV usw. formatiert ist. Ein Beispiel,

Schauen wir uns die Datei inspections.csv an.

158 | Kapitel 6: Mit Dateien hantieren

Kopfdaten/Prüfungen.csv
"business_id", "score", "date", "type"
19,"94","20160513","routine"
19,"94","20171211","routine"
24,"98","20171101","routine"
24,"98","20161005","routine"
24,"96","20160311","routine"
31,"98","20151204","routine"
45,"78","20160104","routine"
45,"88","20170307","routine"
45,"85","20170914","routine"
Standardmäßig zeigt head die ersten 10 Zeilen einer Datei an. Wenn wir z.B. 6 Zeilen anzeigen wollen,

dann fügen wir die Option -n 6 zu unserem Befehl hinzu (oder kurz -6).

Wir können den gesamten Inhalt der Datei mit dem Befehl cat ausdrucken. Sie sollten jedoch

Seien Sie vorsichtig bei der Verwendung dieses Befehls, da das Drucken einer großen Datei dazu führen kann, dass der Browser

Absturz. Die Datei legend.csv ist klein, und wir können cat verwenden, um sie zu verketten und zu drucken

Inhalt.

cat daten/legend.csv
"Minimum_Score", "Maximum_Score", "Beschreibung"
0,70, "Schlecht"
71,85, "Verbesserungsbedürftig"
86,90, "Angemessen"
91,100, "Gut"
In vielen Fällen gibt uns die Verwendung von Kopf oder Schwanz allein einen ausreichenden Eindruck von der Datei

Struktur, um mit dem Laden in einen Datenrahmen fortzufahren.

Schließlich kann der Befehl file dabei helfen, die Kodierung einer Datei zu bestimmen.

Datei -I Daten/*
daten/DAWN-Daten.txt: text/plain; charset=us-ascii
daten/geschaefte.csv: anwendung/csv; zeichensatz=iso-8859-1
daten/co2_mm_mlo.txt: text/plain; charset=us-ascii
data/inspections.csv: anwendung/csv; charset=us-ascii
daten/legend.csv: anwendung/csv; zeichensatz=us-ascii
data/violations.csv: anwendung/csv; zeichensatz=us-ascii
Wir sehen (wieder), dass alle Dateien ASCII sind, mit Ausnahme der Datei businesses.csv, die ein

ISO-8859-1-Kodierung.

Normalerweise öffnen wir ein Terminalprogramm, um einen Shell-Interpreter zu starten.
Jupyter Notebooks bieten jedoch einen Komfort: Wenn eine Codezeile in einer
Codezeile in einer Python-Codezelle das Zeichen! vorangestellt ist, wird die Zeile
direkt an den Shell-Interpreter des Systems weitergeleitet. Zum Beispiel kann die Ausführung
ls in einer Python-Zelle listet beispielsweise die Dateien im aktuellen Verzeichnis auf.
Die Shell und Kommandozeilenwerkzeuge | 159
In diesem Abschnitt haben wir einige Kommandozeilenwerkzeuge vorgestellt: ls, du, wc, head, tail,

cat und file. Diese Werkzeuge helfen uns, das Format und die Struktur von Datendateien zu verstehen.

Wir können auch Shell-Tools verwenden, um sicherzustellen, dass die Datendatei klein genug ist, um sie zu lesen

pandas und um die richtige Kodierung zu erhalten. Sobald eine Datei in Pandas eingelesen wurde, haben wir eine

DataFrame und kann mit der Analyse fortfahren.

Shell-Befehle geben uns eine programmatische Möglichkeit, mit Dateien zu arbeiten, anstatt eine Punkt-

und klicken Sie auf "manuelles" Vorgehen. Sie sind nützlich für:

Dokumentation: wenn Sie aufzeichnen müssen, was Sie getan haben
Fehlerreduzierung: wenn Sie Tippfehler und andere einfache, aber potenziell schädliche Fehler reduzieren wollen
potenziell schädliche Fehler
Reproduzierbarkeit: Wenn Sie den gleichen Prozess in der Zukunft wiederholen müssen oder planen
Ihr Verfahren mit anderen zu teilen, haben Sie eine Aufzeichnung Ihrer Handlungen
Umfang: Wenn Sie viele sich wiederholende Vorgänge durchführen müssen, die Größe der Datei, mit der Sie
der Datei, mit der Sie arbeiten, groß ist, oder wenn Sie Dinge schnell erledigen müssen, können CLI-Tools
helfen.
Nachdem die Daten in einen Datenrahmen geladen worden sind, besteht unsere nächste Aufgabe darin, herauszufinden, welche

der Form und Granularität der Tabelle. Wir beginnen mit der Ermittlung der Anzahl der Zeilen und Spalten in

die Tabelle (ihre Form). Dann müssen wir verstehen, was eine Zeile darstellt, bevor wir

beginnen, die Qualität der Daten zu überprüfen. Wir werden diese Themen im nächsten Abschnitt behandeln.

Tabellenform und Granularität
In diesem Abschnitt stellen wir die Granularität vor: was jede Zeile in der Tabelle darstellt.

Wie bereits beschrieben, bezeichnen wir die Struktur eines Datensatzes als eine mentale Darstellung der

Daten, und insbesondere stellen wir Daten, die eine Tabellenstruktur haben, durch die Anordnung von Daten dar

Werte in Zeilen und Spalten. Nachdem wir nun die Restaurantinspektion untersucht haben

Dateien, laden wir sie in Datenrahmen und untersuchen ihre Formen.

bus = pd.read_csv("daten/unternehmen.csv", encoding='ISO-8859-1')
insp = pd.read_csv("daten/kontrollen.csv")
viol = pd.read_csv("daten/verstoesse.csv")
print(" Form der Unternehmen:", bus.shape)
print("Form der Überprüfungen:", insp.shape)
print(" Form der Verstöße:", viol.shape)
Form der Unternehmen: (6406, 9)
Inspektionen shape: (14222, 4)
Verstöße shape: (39042, 3)
160 | Kapitel 6: Mit Dateien hantieren

Wir stellen fest, dass die Tabelle der Unternehmen 6406 Zeilen und 9 Spalten hat. Nun wollen wir herausfinden

die Granularität dieser Tabelle, indem wir verstehen, was jede Zeile darstellt. Zu Beginn werden wir

sehen Sie sich die ersten beiden Zeilen an.

business_id name adresse ort ... postal_code latitude longitude phone_number
0^19 NRGIZE
LIFESTYLE
CAFE
1200 VAN
NESS AVE, 3RD
FLOOR
San
Francisco
... 94109 37.79 -122.42 +14157763262
1^24 OMNI S.F.
HOTEL - 2.
FLOOR
PANTRY
500
KALIFORNIEN
ST, 2ND FLOOR
San
Francisco
... 94104 37.79 -122.40 +14156779494
2 Reihen × 9 Spalten

Wenn man sich diese beiden Zeilen ansieht, bekommt man den Eindruck, dass jeder Datensatz eine

ein einziges Restaurant. Aber wir können anhand von nur zwei Datensätzen nicht sagen, ob es sich um die

Fall. Der Feldname business_id impliziert, dass es sich um den eindeutigen Bezeichner für das Busi-

ness (Restaurant). Wir können dies bestätigen, indem wir prüfen, ob die Anzahl der Datensätze in

bus entspricht der Anzahl der eindeutigen Werte in business_id.

print("Anzahl der Datensätze:", len(bus))
print("Anzahl der eindeutigen Geschäfts-IDs:", len(bus['business_id'].unique()))
Anzahl der Datensätze: 6406
Anzahl der eindeutigen Unternehmens-IDs: 6406
Die Anzahl der eindeutigen business_ids entspricht der Anzahl der Zeilen in der Tabelle. In

Mit anderen Worten: business_id identifiziert jeden Datensatz im DataFrame eindeutig. Wir nennen

business_id der Primärschlüssel für die Tabelle. Ein Primärschlüssel ist das Merkmal, das eine eindeutige

identifiziert jede Zeile in einer Tabelle. Wir verwenden Primärschlüssel, um Tabellen wie in Kapitel 4 zu verbinden.

Manchmal besteht ein Primärschlüssel aus zwei (oder mehr) Merkmalen. Dies ist der Fall bei dem

die beiden anderen Restaurant-Akten. Fahren wir mit der Untersuchung der Kontrollen und Verstöße fort.

Datenrahmen und ihre Granularität zu finden.

Granularität von Restaurantinspektionen und Verstößen
Wir haben weiter oben in diesem Kapitel gesehen, dass es in der Tabelle der Inspektionen viel mehr Zeilen gibt

im Vergleich zur Geschäftstabelle. Schauen wir uns die ersten Zeilen der Inspektionstabelle genauer an.

tionen.

business_id Punktzahl Datum Art
0^199420160513 Routine
1^199420171211 Routine
2^249820171101 Routine
Tabellenform und Granularität | 161
business_id Punktzahl Datum Typ
3^249820161005 Routine
Wie die Tabelle businesses enthält auch diese Tabelle ein Feld namens business_id, aber wir

sehen Sie doppelte Werte für die ID. Die beiden Datensätze für Geschäft Nr. 19 haben unterschiedliche Daten

Werte, was bedeutet, dass es für jede Inspektion eines Restaurants einen Datensatz gibt. Unter

Mit anderen Worten, die Granularität dieser Tabelle ist eine Restaurantkontrolle. Wenn dies tatsächlich

der Fall ist, würde das bedeuten, dass der eindeutige Bezeichner einer Zeile die Kombination aus

business_id und Datum. Das heißt, der Primärschlüssel besteht aus zwei Feldern.

Um zu bestätigen, dass die beiden Felder einen Primärschlüssel bilden, können wir insp nach dem Kombi-

nation von business_id und date, und ermitteln Sie dann die Größe jeder Gruppe. Wenn business_id

und Datum jede Zeile des Datenrahmens eindeutig definieren, dann sollte jede Gruppe über

Größe 1.

(insp
.groupby(['business_id', 'date'])
.size()
.sort_values(ascending=False)
.head(5)
)
business_id Datum
64859 20150924 2
87440 20160801 2
77427 20170706 2
19 20160513 1
71416 20171213 1
dtTyp: int64
Die Kombination aus ID und Datum identifiziert jeden Datensatz in den Inspektionen auf eindeutige Weise

Tabelle, mit Ausnahme von drei Restaurants, die zwei Datensätze für ihre ID-

Datumskombination. Zum Beispiel hat das Restaurant 64859 zwei Datensätze mit einer Inspektion

das Datum 20150924. Untersuchen wir diese Zeilen.

insp.query('business_id == 64859 und Datum == 20150924')
business_id score date type
7742^648599620150924 Routine
7744^648599120150924 Routine
Das Restaurant 64859 hat am 20150924 (24. September 2015) zwei verschiedene Inspektionsergebnisse erhalten.

Wie konnte das passieren? Es könnte sein, dass das Restaurant an einem Tag zweimal kontrolliert wurde,

oder es könnte sich um einen Fehler handeln. Wir befassen uns mit dieser Art von Fragen, wenn wir die Daten betrachten

Qualität in den Niederlanden. In diesem Fall gehen wir davon aus, dass ein Restaurant zwei Inspektionen haben kann

das gleiche Datum. Der Primärschlüssel für die Tabelle der Inspektionen ist also die Kombination aus res-

Restaurant-ID und Inspektionsdatum.

162 | Kapitel 6: Mit Dateien hantieren

Beachten Sie, dass das Feld business_id in der Inspektionstabelle als Verweis auf die pri-

Schlüssel in der Business-Tabelle. Aus diesem Grund wird business_id in insp als Fremdschlüssel

Schlüssel: Er verknüpft jeden Datensatz in der Inspektionstabelle mit einem Datensatz in der Geschäftstabelle. Diese

bedeutet, dass wir diese beiden Tabellen ohne weiteres miteinander verbinden können.

Als nächstes untersuchen wir die Granularität der Tabelle der Verstöße.

business_id Datum Beschreibung
0^1920171211 Unzureichende Kenntnisse im Bereich der Lebensmittelsicherheit oder fehlende...
1^1920171211 Nicht zugelassene oder nicht gewartete Ausrüstung oder Utensilien
2^1920160513 Nicht zugelassene oder nicht gewartete Ausrüstungen oder U...
39039^9423120171214 Ungezieferbefall mit hohem Risiko [ Datum des Verstoßes...
39040^9423120171214 Mäßiges Risiko der Lebensmitteltemperatur [ Datum...
39041^9423120171214 Wischtücher nicht sauber oder nicht ordnungsgemäß gelagert oder ...
39042 Zeilen × 3 Spalten

Ein Blick auf die ersten paar Datensätze in viol zeigt, dass jede Inspektion mehrere

Einträge. Die Granularität liegt auf der Ebene eines bei einer Inspektion festgestellten Verstoßes. Lesen

der Beschreibungen sehen wir, dass, wenn sie korrigiert werden, ein Datum in der Beschreibung aufgeführt wird innerhalb

eckige Klammern.

viol.loc[39039, 'Beschreibung']
'Ungezieferbefall mit hohem Risiko [ Datum des korrigierten Verstoßes: 15.12.2017 ]'
Zusammenfassend haben wir festgestellt, dass diese drei Tabellen unterschiedliche Granularitäten aufweisen. Wenn wir

an der Untersuchung von Inspektionen interessiert sind, können wir uns den Verstößen und Inspektionen anschließen

zusammen mit der Unternehmens-ID und dem Inspektionsdatum. Auf diese Weise können wir die Nummer finden

der bei jeder Inspektion festgestellten Verstöße.

Wir können die Prüftabelle auch reduzieren, indem wir die letzte Prüfung für

jedes Restaurant. Diese reduzierte Datentabelle hat im Wesentlichen eine Granularität von Restaurant,

was für eine Analyse in einem Restaurant nützlich sein kann. Diese Art von Aktionen verändern die

der Datentabelle, Spalten umwandeln und neue Spalten erstellen. Wir werden diese Operationen behandeln.

in den Niederlanden.

Wir schließen diesen Abschnitt mit einem Blick auf die Form und die Granularität der DAWN-Umfrage.

Daten zu überprüfen.

Form und Granularität der DAWN-Erhebung
Wie bereits in diesem Kapitel erwähnt, hat die DAWN-Datei eine feste Breite, und wir

Sie müssen sich auf ein Codebuch verlassen, um herauszufinden, wo die Felder sind. Zum Beispiel, ein Ausschnitt aus

Das Codebuch in Abbildung 6-2 zeigt uns, dass das Alter an den Positionen 34 und 35 in der Reihe erscheint,

Tabellenform und Granularität | 163
und ist in 11 Altersgruppen eingeteilt, 1 steht für 5 und darunter, 2 für 6 bis 11, ...,

und 11 für 65 und älter. Außerdem steht -8 für einen fehlenden Wert.

Abbildung 6-2. Screenshot eines Teils der DAWN-Codierung für das Alter.

Zuvor haben wir festgestellt, dass die Datei 200 Tausend Zeilen und über 280 Millionen Zeichen enthält.

Das bedeutet, dass im Durchschnitt etwa 1200 Zeichen pro Zeile vorhanden sind. Dies könnte der Grund sein, warum sie

eine Datei mit fester Breite anstelle einer CSV-Datei verwendet; die Datei wäre viel größer, wenn es

war ein Komma zwischen jedem Feld!

In Anbetracht der enormen Menge an Informationen in jeder Zeile lesen wir nur einige wenige Vari-

ables in einen Datenrahmen. Dazu können wir die Methode pandas.read_fwf verwenden. Wir spezi-

die genauen Positionen der zu extrahierenden Felder angeben, Namen für diese Felder angeben und

andere Informationen über den Kopf und den Index.

colspecs = [(0,6), (14,29), (33,35), (35, 37), (37, 39), (1213, 1214)]
varNames = ["id", "wt", "age", "sex", "race", "type"]
dawn = pd.read_fwf('data/DAWN-Data.txt', colspecs=colspecs,
header=None, index_col=0, names=varNames)
wt Alter Geschlecht Rasse Typ
id
1 0.94^4128
2 5.99^11134
3 4.72^11224
4 4.08^2134
5 5.18^6138
Wir können die Zeilen in der Tabelle mit der Anzahl der Zeilen in der Datei vergleichen.

dawn.shape
(229211, 5)
Die Anzahl der Zeilen im Datenrahmen entspricht der Anzahl der Zeilen in der Datei. Das ist

gut. Die Granularität des Datenrahmens ist aufgrund des Erhebungsdesigns ein wenig kompliziert.

Es sei daran erinnert, dass diese Daten Teil einer großen wissenschaftlichen Studie sind, die eine komplexe Stichprobe enthält

Schema. Eine Zeile steht für einen Besuch in der Notaufnahme, so dass die Granularität auf der Ebene der Notaufnahme liegt.

Ebene der Besuche in den Behandlungsräumen. Um jedoch den Stichprobenplan widerzuspiegeln und die

164 | Kapitel 6: Mit Dateien hantieren

der Grundgesamtheit zu ermitteln, sind Gewichte vorgesehen. Wir müssen für jede Person ein Gewicht anwenden

aufzeichnen, wenn wir zusammenfassende Statistiken berechnen, Histogramme erstellen und Modelle anpassen.

Das Feld wt enthält einen Wert, der die Wahrscheinlichkeit eines Notaufnahmebesuchs berücksichtigt, wie

der in der Stichprobe vorkommt. Mit "wie dieser" meinen wir einen Besuch mit ähnlichen Merkmalen.

wie Alter, Rasse, Aufenthaltsort und Tageszeit der Besucher. Wir untersuchen die Unter-

ent-Werte in Gew.-%.

dawn['wt'].value_counts()
0.94 1719
84.26 1617
1.72 1435
...
3.33 1
6.20 1
3.31 1
Name: wt, Länge: 3500, dtype: int64
Was bedeuten diese Gewichte? Ein vereinfachtes Beispiel: Angenommen
Sie führten eine Umfrage durch und 75 % der Befragten gaben ihr Geschlecht als
weiblich. Da Sie aus der Volkszählung wissen, dass etwa 50 % der
U.S.-Bevölkerung weiblich ist, können Sie Ihre Umfrageantworten anpassen, indem Sie
ein kleines Gewicht (kleiner als 1) für weibliche Antworten und ein größeres
Gewicht (größer als 1) für männliche Antworten. Die DAWN-Umfrage verwendet
Die DAWN-Umfrage verwendet dieselbe Idee, nur dass die Gruppen viel feiner aufgeteilt werden.
Es ist wichtig, die Gewichtung der Umfrage in Ihre Analyse einzubeziehen, um Daten zu erhalten, die Folgendes repräsentieren

der Gesamtbevölkerung. Wir können zum Beispiel die Berechnung der Proportionalität vergleichen.

der Frauen unter den Notaufnahmebesuchen mit und ohne Gewichte.

ungewichtet = np.average(dawn["sex"] == 2)
gewichtet = np.average(dawn["sex"] == 2, weights=dawn["wt"])
print(f'Ungewichtet % weiblich: {Ungewichtet:.1%}')
print(f' Gewichtet % weiblich: {gewichtet:.1%}')
Ungewichtet % weiblich: 48,0%
Gewichtetes % weiblich: 52,3%
Diese Zahlen unterscheiden sich um mehr als 4 Prozentpunkte. Die gewichtete Version ist ein besserer

genaue Schätzung des Anteils der Frauen an der Gesamtpopulation der Drogenabhängigen

Besuche in der Notaufnahme.

In diesem Abschnitt haben wir die Granularität für die Tabellen des Restaurant-Datensatzes und die

DAWN-Daten. Manchmal kann es schwierig sein, die Granularität herauszufinden, wie wir es bei

die Daten der Inspektionen. Und wir mussten die Stichprobengewichte berücksichtigen, wenn wir

Betrachtung der Granularität der DAWN-Daten. Diese Beispiele zeigen, wie wichtig es ist

Nehmen Sie sich Zeit und prüfen Sie die Datenbeschreibungen, bevor Sie mit der Analyse fortfahren.

Tabellenform und Granularität | 165
Zusammenfassung
Die Datenaufbereitung ist ein wesentlicher Bestandteil der Datenanalyse. Ohne sie riskieren wir, Folgendes zu übersehen

Probleme in den Daten, die erhebliche Auswirkungen auf künftige Analysen haben können. Dieses Kapitel

einen wichtigen ersten Schritt in der Datenverarbeitung behandelt: das Lesen von Daten aus einem einfachen Text

Quelldatei in einen Python DataFrame. Wir haben verschiedene Arten von Dateiformaten eingeführt

und Kodierungen, und wir haben Code geschrieben, der Daten aus diesen Formaten lesen kann. Wir haben geprüft

die Größe der Quelldateien und prüfte alternative Tools für die Arbeit mit großen Daten.

Sets.

Wir haben auch Kommandozeilen-Tools als Alternative zu Python eingeführt, um die

Format, Kodierung und Größe einer Datei. Diese CLI-Tools sind besonders praktisch für

dateisystemorientierte Aufgaben aufgrund ihrer einfachen Syntax. In diesem Kapitel haben wir nur

haben nur die Oberfläche dessen berührt, was CLI-Tools leisten können. In Wirklichkeit ist die Shell in der Lage, raffinierte...

Datenverarbeitung und ist es wert, gelernt zu werden.

Das Verständnis der Form und Granularität einer Tabelle gibt uns Aufschluss darüber, was eine Zeile in

eine Datentabelle darstellt. So können wir feststellen, ob die Granularität gemischt ist,

eine Aggregation erforderlich ist oder Gewichte benötigt werden. Nach Betrachtung der Granularität von

Ihres Datensatzes sollten Sie Antworten auf die folgenden Fragen haben.

Was stellt ein Datensatz dar?
Erfassen alle Datensätze in einer Tabelle dieselbe Granularität? Manchmal enthält eine Tabelle
zusätzliche Summenzeilen mit einer anderen Granularität enthalten.
Wenn die Daten aggregiert wurden, wie wurde die Aggregation durchgeführt? Summierung und
Mittelwertbildung sind übliche Arten der Aggregation.
Welche Arten von Aggregationen könnten wir mit den Daten durchführen?
Die Kenntnis der Granularität Ihrer Tabelle ist ein erster Schritt zur Bereinigung Ihrer Daten und gibt Aufschluss über

wie Sie Ihre Daten analysieren können. Wir haben zum Beispiel die Granularität der DAWN

Umfrage ist ein Besuch in der Notaufnahme. Das bringt uns natürlich dazu, über Vergleiche von Patienten nachzudenken

demographischen Verhältnissen in den USA insgesamt.

Die in diesem Kapitel beschriebenen Techniken helfen uns, Daten aus einer Quelldatei in eine

Datenrahmen zu erstellen und seine Struktur zu verstehen. Sobald wir einen Datenrahmen haben, können wir weitere Wran-

Um die Qualität der Daten zu beurteilen und sie für die Analyse vorzubereiten, ist eine gründliche Analyse erforderlich. Wir behandeln dies in

das nächste Kapitel.

166 | Kapitel 6: Mit Dateien hantieren

Über die Autoren
Sam Lau ist ein Doktorand an der UC San Diego. Er entwirft neuartige Schnittstellen für Lern- und

Seine Forschungsergebnisse wurden in hochrangigen Konferenzen veröffentlicht.

Erfahrungen in den Bereichen Mensch-Computer-Interaktion und Endbenutzerprogrammierung. Sam unterrichtete

und half bei der Gestaltung von Vorzeigekursen für Datenwissenschaft an der UC Berkeley. Diese Kurse haben

Tausende von Schülern werden jedes Jahr betreut, und ihr Lehrplan wird von Uni-

versitäten in der ganzen Welt.

Deborah (Deb) Nolan ist Professorin für Statistik und stellvertretende Dekanin für Undergraduate-

ate Studies in der Abteilung für Informatik, Datenwissenschaft und Gesellschaft an der Universität

von Kalifornien, Berkeley, wo sie den Zaffaroni-Familienlehrstuhl für Undergraduate

Ausbildung. Ihre Forschungsarbeiten befassen sich mit dem empirischen Prozess, hochdimensionalen Modi

und in jüngerer Zeit Technologie in der Bildung und reproduzierbare Forschung. Ihr

pädagogischen Ansatz, der Forschung, Praxis und Bildung miteinander verbindet, und sie ist Mitautorin

von 4 Lehrbüchern: Stat Labs, Teaching Statistics, Data Science in R, und Communicating

mit Daten.

Joseph (Joey) Gonzalez ist Assistenzprofessor in der EECS-Abteilung der UC

Berkeley und Gründungsmitglied des neuen UC Berkeley RISE Lab. Seine Forschung

Seine Interessen liegen an der Schnittstelle zwischen maschinellem Lernen und Datensystemen, einschließlich:

dynamische tiefe neuronale Netze für Transfer Learning, beschleunigtes tiefes Lernen für

hochauflösende Computer Vision und Software-Plattformen für autonome Fahrzeuge.

Joey ist außerdem Mitbegründer von Turi Inc. (ehemals GraphLab), das auf seinem

Arbeit an den Systemen GraphLab und PowerGraph. Turi wurde kürzlich erworben von

Apple Inc.

Dies ist ein Offline-Tool, Ihre Daten bleiben lokal und werden nicht an einen Server gesendet!
Feedback & Fehlerberichte