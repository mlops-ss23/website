Handbuch der Datenwissenschaft in Python
Jake VanderPlas

Python Daten
Wissenschaft
Handbuch
Grundlegende Werkzeuge für die Arbeit mit Daten
Zweite
Auflage
DATA
"Diese frisch aktualisierte
Ausgabe bietet klare, leicht verständliche
nachvollziehbare Beispiele, die
Ihnen helfen wird, erfolgreich
einrichten und verwenden
Data Science und
Tools für maschinelles Lernen."
-Anne Bonner,
Gründerin und CEO, Content Simplicity
Handbuch der Datenwissenschaft in Python

US $69.99 CAN $87.
ISBN: 978-1-098-12122-
Twitter: @oreillymedia
linkedin.com/Unternehmen/oreilly-media
youtube.com/oreillymedia
Python ist für viele Forscher ein erstklassiges Werkzeug, vor allem
vor allem wegen seiner Bibliotheken zum Speichern, Manipulieren und Gewinnen von
Einblicke aus Daten zu gewinnen. Es gibt mehrere Ressourcen für einzelne Teile
dieses Data Science Stacks, aber nur mit der neuen Ausgabe des
Python Data Science Handbook erhalten Sie sie alle - Python,
NumPy, Pandas, Matplotlib, Scikit-Learn und andere verwandte
Werkzeuge.
Berufstätige Wissenschaftler und Data Cruncher, die mit dem Lesen
und Schreiben von Python-Code vertraut sind, finden in der zweiten Auflage dieses
umfassenden Nachschlagewerkes ideal für die Bewältigung
zu bewältigen: Daten manipulieren, transformieren und bereinigen;
Visualisierung verschiedener Datentypen und Verwendung von Daten zur Erstellung
statistischen oder maschinellen Lernmodellen. Ganz einfach, dies ist
die unverzichtbare Referenz für wissenschaftliches Rechnen in Python.
Mit diesem Handbuch lernen Sie, wie:
- IPython und Jupyter bieten Berechnungsumgebungen
für Wissenschaftler, die Python verwenden
- NumPy enthält ndarray für die effiziente Speicherung und
Manipulation von dichten Datenarrays
- Pandas enthält den DataFrame für die effiziente Speicherung und
Manipulation von beschrifteten/spaltenweisen Daten
- Matplotlib enthält Funktionen für eine flexible Reihe von Daten
Visualisierungen
- Scikit-Learn hilft Ihnen bei der Erstellung effizienter und sauberer Python
Implementierungen der wichtigsten und etablierten
Algorithmen für maschinelles Lernen

Jake VanderPlas ist Software-Ingenieur
bei Google Research und arbeitet an Tools
die datenintensive Forschung unterstützen.
Er erstellt und entwickelt Python-Tools für
für die datenintensive Wissenschaft, darunter
Pakete wie Scikit-Learn, SciPy, Astropy,
Altair, JAX, und viele andere.
Großes Lob für Python Data Science Handbook,

Zweite Auflage

Es gibt heutzutage viele Bücher über Datenwissenschaft, aber ich finde das Buch von Jake VanderPlas
außergewöhnlich. Er behandelt ein sehr umfangreiches und komplexes Thema und bricht es so auf
und bricht es auf eine Weise herunter, die es leicht verständlich macht.
und Übungen, mit denen man die Konzepte schnell anwenden kann.
-Celeste Stinger, Ingenieurin für Standortzuverlässigkeit
Jake VanderPlas' Fachwissen und seine Leidenschaft, Wissen zu vermitteln, sind unbestreitbar.
Diese frisch aktualisierte Ausgabe bietet klare, leicht nachvollziehbare Beispiele, die Ihnen helfen werden
die Ihnen helfen, die wichtigsten Tools für Data Science und maschinelles Lernen erfolgreich einzurichten und zu nutzen. Wenn Sie
bereit sind, in die Kerntechniken für den Einsatz von Python-basierten Tools einzutauchen, um echte Erkenntnisse aus Ihren Daten zu gewinnen
aus Ihren Daten zu gewinnen, ist dies das richtige Buch für Sie!
-Anne Bonner, Gründerin und CEO, Content Simplicity
Das Python Data Science Handbook ist seit Jahren eines meiner Lieblingsbücher, das ich
Studenten der Datenwissenschaft. Die zweite Auflage verbessert ein bereits erstaunliches Buch
mit überzeugenden Jupyter-Notebooks, die es Ihnen ermöglichen, Ihr
Ihr Lieblingsrezept der Datenwissenschaft auszuführen, während Sie lesen.
-Noah Gift, Duke Executive in Residence
und Gründer von Pragmatic AI Labs

Diese aktualisierte Ausgabe ist eine hervorragende Einführung in die Bibliotheken, die Python zu einer
Sprache für Data Science und wissenschaftliches Rechnen machen, präsentiert in einem
Stil und mit großartigen Beispielen.
-Allen Downey, Autor von Think Python und Think Bayes
Das Python Data Science Handbook ist ein ausgezeichneter Leitfaden für Leser, die den Python
Data Science Stack zu lernen. Mit vollständigen praktischen Beispielen, die in einer zugänglichen
wird der Leser zweifellos lernen, wie man Daten effektiv speichert, manipuliert
und Erkenntnisse aus einem Datensatz zu gewinnen.
William Jamir Silva, leitender Software-Ingenieur, Adjust GmbH
Jake VanderPlas ist bekannt dafür, die wichtigsten Python-Konzepte und -Werkzeuge für
Python-Konzepte und -Werkzeuge für Einsteiger in die Datenwissenschaft zu erläutern, und in der zweiten Auflage von Python Data Science Handbook
hat er es wieder einmal geschafft. In diesem Buch gibt er einen Überblick über alle Tools, die man für den
Werkzeuge, die man für den Einstieg braucht, sowie den Hintergrund, warum bestimmte Dinge so sind, wie sie sind,
und das auf eine leicht verständliche Art und Weise.
Jackie Kazil, Schöpferin der Mesa-Bibliothek
und Leiterin der Datenwissenschaft

Jake VanderPlas
Handbuch der Datenwissenschaft in Python

Wichtige Tools für die Arbeit mit Daten

ZWEITE AUSGABE

PekingPeking BostonBoston FarnhamFarnham SebastopolSebastopol TokioTokio

978-1-098-12122-
[LSI]
Python Handbuch der Datenwissenschaft
von Jake VanderPlas

Urheberrecht © 2023 Jake VanderPlas. Alle Rechte vorbehalten.

Gedruckt in den Vereinigten Staaten von Amerika.

Veröffentlicht von O'Reilly Media, Inc. 1005 Gravenstein Highway North, Sebastopol, CA 95472.

O'Reilly-Bücher können für Bildungszwecke, geschäftliche Zwecke oder zur Verkaufsförderung erworben werden. Online-Ausgaben sind
sind für die meisten Titel ebenfalls erhältlich (http://oreilly.com). Weitere Informationen erhalten Sie von unserer Vertriebsabteilung für Unternehmen/Institutionen
Vertriebsabteilung: 800-998-9938 oder corporate@oreilly.com.

Akquisitions-Editor: Aaron Black
Entwicklungsredakteur: Jill Leonard
Produktionsredakteurin: Katherine Tozer
Redakteurin: Rachel Head
Korrekturleser: James Fraleigh

Indexierer: WordCo Indexing Services, Inc.
Innenarchitekt: David Futato
Umschlagdesigner: Karen Montgomery
Illustratorin: Kate Dullea
Dezember 2022: Zweite Auflage

Revisionsübersicht für die zweite Auflage
2022-12-06: Erste Veröffentlichung

Siehe http://oreilly.com/catalog/errata.csp?isbn=9781098121228 für Einzelheiten zur Veröffentlichung.

Das O'Reilly-Logo ist eine eingetragene Marke von O'Reilly Media, Inc. Python Data Science Handbook, das
Titelbild und die zugehörige Aufmachung sind Marken von O'Reilly Media, Inc.

Die in diesem Werk geäußerten Ansichten sind die des Autors und geben nicht die Meinung des Herausgebers wieder. Während
Der Herausgeber und der Autor haben sich nach bestem Wissen und Gewissen bemüht, die Richtigkeit der Informationen und Anleitungen in diesem Werk sicherzustellen.
Obwohl der Herausgeber und der Autor sich nach bestem Wissen und Gewissen bemüht haben, die Richtigkeit der in diesem Werk enthaltenen Informationen und Anweisungen zu gewährleisten, lehnen der Herausgeber und der Autor jede Verantwortung für Fehler oder Auslassungen ab.
Auslassungen, einschließlich und ohne Einschränkung der Verantwortung für Schäden, die aus der Nutzung oder dem Vertrauen auf
oder Vertrauen auf dieses Werk. Die Nutzung der in diesem Werk enthaltenen Informationen und Anleitungen erfolgt auf eigene Gefahr. Falls irgendwelche
Codebeispiele oder andere Technologien, die dieses Werk enthält oder beschreibt, Open-Source-Lizenzen oder den
Open-Source-Lizenzen oder den Rechten am geistigen Eigentum anderer unterliegen, liegt es in Ihrer Verantwortung, sicherzustellen, dass Ihre Nutzung dieser
mit diesen Lizenzen und/oder Rechten übereinstimmt.

Python Data Science Handbook ist verfügbar unter der Creative Commons Attribution-Noncommercial-No
Derivative 4.0 International Public License. Der Autor unterhält eine Online-Version unter https://github.com/
jakevdp/PythonDataScienceHandbook.

Inhaltsübersicht
Vorwort...................................................................... xix

Erste Schritte in IPython und Jupyter. Teil I. Jupyter: Jenseits von normalem Python
Starten der IPython-Shell
Starten des Jupyter-Notizbuchs
Hilfe und Dokumentation in IPython
Zugriff auf die Dokumentation mit?
Zugriff auf den Quellcode mit ??
Erkunden von Modulen mit Tabulatorvervollständigung
Tastaturkürzel in der IPython-Shell
Navigationskurzbefehle
Tastenkombinationen für die Texteingabe
Tastaturkürzel für die Befehlshistorie
Verschiedene Tastaturkürzel
Verbesserte interaktive Funktionen.
IPython Magic-Befehle
Ausführen von externem Code: %run
Zeitmessung der Code-Ausführung: %timeit
Hilfe zu Magic-Funktionen: ?, %magic, und %lsmagic
Eingabe- und Ausgabegeschichte
IPython's In und Out Objekte
Unterstrichene Abkürzungen und vorherige Ausgaben
Ausgabe unterdrücken
Verwandte Magic-Befehle
IPython und Shell-Befehle
Kurze Einführung in die Shell
Shell-Befehle in IPython
Übergabe von Werten an und von der Shell
Shell-verwandte Magic-Befehle
Debugging und Profiling.
Fehler und Debugging
Ausnahmen kontrollieren: %xmode
Fehlersuche: Wenn das Lesen von Tracebacks nicht ausreicht
Profiling und Zeitmessung von Code
Timing-Code-Schnipsel: %timeit und %time
Vollständige Skripte profilieren: %prun
Zeilenweises Profiling mit %lprun
Profiling der Speichernutzung: %memit und %mprun
Weitere IPython-Ressourcen
Web-Ressourcen
Bücher
Verstehen von Datentypen in Python. Teil II. Einführung in NumPy
Eine Python-Ganzzahl ist mehr als nur eine Ganzzahl
Eine Python-Liste ist mehr als nur eine Liste
Arrays festen Typs in Python
Arrays aus Python-Listen erstellen
Arrays von Grund auf neu erstellen
NumPy Standard-Datentypen
Die Grundlagen von NumPy-Arrays.
NumPy Array-Attribute
Array-Indizierung: Zugriff auf einzelne Elemente
Array-Slicing: Zugriff auf Subarrays
Eindimensionale Subarrays
Mehrdimensionale Subarrays
Subarrays als No-Copy-Ansichten
Kopien von Arrays erstellen
Umformung von Arrays
Array-Verkettung und Aufteilung
Verkettung von Arrays
Aufteilung von Arrays
Berechnungen auf NumPy-Arrays: Universelle Funktionen.
Die Langsamkeit von Schleifen
Einführung in Ufuncs
NumPy's Ufuncs erforschen
Array-Arithmetik
Absoluter Wert
Trigonometrische Funktionen
Exponenten und Logarithmen
Spezialisierte Ufuncs
Erweiterte Ufunc-Funktionen
Spezifizierung der Ausgabe
Aggregationen
Äußere Produkte
Ufuncs: Mehr lernen
Aggregationen: min, max, und alles dazwischen.
Summierung der Werte in einem Array
Minimum und Maximum
Mehrdimensionale Aggregate
Andere Aggregationsfunktionen
Beispiel: Wie hoch ist die durchschnittliche Größe der US-Präsidenten?
Berechnungen auf Arrays: Übertragungen.
Einführung in das Broadcasting
Regeln für Broadcasting
Beispiel für Broadcasting
Broadcasting-Beispiel
Beispiel für Rundfunk
Broadcasting in der Praxis
Zentrieren eines Arrays
Plotten einer zweidimensionalen Funktion
Vergleiche, Masken und Boolesche Logik.
Beispiel: Zählen von Regentagen
Vergleichsoperatoren als Ufuncs
Arbeiten mit booleschen Arrays
Zählen von Einträgen
Boolesche Operatoren
Boolesche Arrays als Masken
Verwendung der Schlüsselwörter und/oder im Gegensatz zu den Operatoren &/|
Ausgefallene Indizierung.
Fancy Indizierung erforschen
Kombinierte Indizierung
Beispiel: Zufällige Punkte auswählen
Ändern von Werten mit Fancy Indexing
Beispiel: Daten einteilen
Arrays sortieren.
Schnelles Sortieren in NumPy: np.sort und np.argsort
Sortieren entlang von Zeilen oder Spalten
Partielle Sortierungen: Partitionieren
Beispiel: k-Nächste Nachbarn
Strukturierte Daten: NumPy's strukturierte Arrays.
Erforschung der strukturierten Array-Erstellung
Weitergehende zusammengesetzte Typen
Datensatz-Arrays: Strukturierte Arrays mit einem Twist
Weiter zu Pandas
Einführung in Pandas-Objekte. Teil III. Datenmanipulation mit Pandas
Das Pandas-Reihenobjekt
Reihen als verallgemeinertes NumPy-Array
Reihen als spezialisiertes Wörterbuch
Konstruktion von Reihen-Objekten
Das Pandas DataFrame-Objekt
DataFrame als verallgemeinertes NumPy-Array
DataFrame als spezialisiertes Wörterbuch
Konstruktion von DataFrame-Objekten
Das Pandas Index-Objekt
Index als unveränderliches Array
Index als geordnete Menge
Datenindizierung und -auswahl.
Datenauswahl in Serien
Reihen als Dictionary
Reihen als eindimensionales Array
Indexer: loc und iloc
Datenauswahl in DataFrames
DataFrame als Wörterbuch
DataFrame als zweidimensionales Array
Zusätzliche Indexierungskonventionen
Operieren auf Daten in Pandas.
Ufuncs: Index-Erhaltung
Ufuncs: Index-Ausrichtung
Indexausrichtung in Serien
Indexausrichtung in DataFrames
Ufuncs: Operationen zwischen DataFrames und Serien
Umgang mit fehlenden Daten.
Kompromisse bei Konventionen für fehlende Daten
Fehlende Daten in Pandas
None als Sentinel-Wert
NaN: Fehlende numerische Daten
NaN und None in Pandas
Pandas nullbare D-Typen
Operieren mit Nullwerten
Erkennen von Nullwerten
Nullwerte fallen lassen
Füllen von Nullwerten
Hierarchische Indizierung.
Eine mehrfach indizierte Reihe
Der schlechte Weg
Der bessere Weg: Der Pandas MultiIndex
MultiIndex als zusätzliche Dimension
Methoden der MultiIndex-Erzeugung
Explizite MultiIndex-Konstruktoren
MultiIndex-Ebenennamen
MultiIndex für Spalten
Indizierung und Slicing eines MultiIndex
Mehrfach indizierte Reihen
Mehrfach indizierte DatenFrames
Neuordnung von MultiIndizes
Sortierte und unsortierte Indizes
Stapeln und Entstapeln von Indizes
Index setzen und zurücksetzen
Kombinieren von Datensätzen: concat und append.
Rückruf: Verkettung von NumPy-Arrays
Einfache Verkettung mit pd.concat
Doppelte Indizes
Verkettung mit Joins
Die append-Methode
Kombinieren von Datensätzen: merge und join.
Relationale Algebra
Kategorien von Joins
Eins-zu-Eins-Joins
Viele-zu-Eins-Joins
Viele-zu-Viele-Joins
Spezifikation des Merge-Schlüssels
Das on-Schlüsselwort
Die Schlüsselwörter left_on und right_on
Die Schlüsselwörter left_index und right_index
Festlegen der Mengenarithmetik für Joins
Überlappende Spaltennamen: Das Schlüsselwort suffixes
Beispiel: US-Staaten-Daten
Aggregation und Gruppierung.
Planeten-Daten
Einfache Aggregation in Pandas
groupby: Teilen, Anwenden, Kombinieren
Aufteilen, Anwenden, Kombinieren
Das GroupBy-Objekt
Aggregieren, Filtern, Transformieren, Anwenden
Den Split-Schlüssel spezifizieren
Beispiel für Gruppierung
Pivot-Tabellen.
Motivierende Pivot-Tabellen
Pivot-Tabellen von Hand
Pivot-Tabellen-Syntax
Mehrstufige Pivot-Tabellen
Zusätzliche Pivot-Tabellen-Optionen
Beispiel: Geburtenraten-Daten
Vektorisierte String-Operationen.
Einführung in Pandas String-Operationen
Tabellen von Pandas String-Methoden
Methoden ähnlich den Python-String-Methoden
Methoden mit regulären Ausdrücken
Verschiedene Methoden
Beispiel: Rezeptdatenbank
Ein einfacher Rezept-Empfehlungsdienst
Weitergehen mit Rezepten
Arbeiten mit Zeitreihen.
Datums- und Zeitangaben in Python
Native Python-Datums- und Zeitangaben: datetime und dateutil
Typisierte Arrays von Zeiten: NumPy's datetime64
Datums- und Zeitangaben in Pandas: Das Beste aus beiden Welten
Pandas Zeitreihen: Indizierung nach Zeit
Pandas Zeitreihen-Datenstrukturen
Reguläre Sequenzen: pd.date_range
Frequenzen und Offsets
Neuabtastung, Verschiebung und Fensterung
Neuabtastung und Konvertierung von Frequenzen
Zeitverschiebungen
Rollierende Fenster
Beispiel: Visualisierung der Fahrradzählungen in Seattle
Visualisierung der Daten
Eingehen auf die Daten
Leistungsstarke Pandas: eval und Abfrage.
Motivierende Abfrage und eval: Zusammengesetzte Ausdrücke
pandas.eval für effiziente Operationen
DataFrame.eval für spaltenweise Operationen
Zuweisung in DataFrame.eval
Lokale Variablen in DataFrame.eval
Die Methode DataFrame.query
Leistung: Wann diese Funktionen zu verwenden sind
Weitere Ressourcen
Allgemeine Matplotlib-Tipps. Teil IV. Visualisierung mit Matplotlib
Matplotlib importieren
Stile einstellen
Anzeigen oder nicht anzeigen? Wie man seine Plots anzeigt
Plotten aus einem Skript
Plotten aus einer IPython-Shell
Plotten aus einem Jupyter Notebook
Speichern von Zahlen in einer Datei
Zwei Schnittstellen zum Preis von einer
Einfache Linienplots.
Anpassen des Plots: Linienfarben und Stile
Anpassen des Plots: Achsenbegrenzungen
Plots beschriften
Matplotlib-Gotchas
Einfache Streudiagramme.
Streudiagramme mit plt.plot
Streudiagramme mit plt.scatter
Plot Versus Scatter: Ein Hinweis zur Effizienz
Visualisierung von Unsicherheiten
Einfache Fehlerbalken
Kontinuierliche Fehler
Dichte- und Konturdiagramme.
Visualisierung einer dreidimensionalen Funktion
Histogramme, Binnings und Dichte
Zweidimensionale Histogramme und Binnings
plt.hist2d: Zweidimensionales Histogramm
plt.hexbin: Sechseckige Binnings
Kernel-Dichte-Schätzung
Anpassen der Legenden von Diagrammen.
Auswahl der Elemente für die Legende
Legende für die Größe der Punkte
Mehrere Legenden
Farbbalken anpassen.
Anpassen von Farbbalken
Auswählen der Farbkarte
Farbbegrenzungen und Erweiterungen
Diskrete Farbbalken
Beispiel: Handgeschriebene Ziffern
Mehrere Teilplots.
plt.axes: Handschriftliche Unterplots
plt.subplot: Einfache Raster von Unterplots
plt.subplots: Das ganze Raster in einem Zug
plt.GridSpec: Kompliziertere Anordnungen
Text und Beschriftung.
Beispiel: Auswirkung von Feiertagen auf US-Geburten
Transformationen und Textposition
Pfeile und Beschriftungen
Anpassen von Häkchen.
Große und kleine Häkchen
Ausblenden von Häkchen oder Beschriftungen
Verringern oder Erhöhen der Anzahl der Häkchen
Ausgefallene Tick-Formate
Zusammenfassung der Formatierer und Locatoren
Matplotlib anpassen: Konfigurationen und Stylesheets.
Plotanpassung von Hand
Ändern der Standardeinstellungen: rcParams
Stylesheets
Standard-Stil
FiveThiryEight-Stil
ggplot-Stil
Bayes'sche Methoden für Hacker Stil
Dunkler Hintergrund-Stil
Grayscale-Stil
Seaborn-Stil
Dreidimensionales Plotten in Matplotlib.
Dreidimensionale Punkte und Linien
Dreidimensionale Konturplots
Wireframes und Oberflächenplots
Oberflächen-Triangulationen
Beispiel: Visualisierung eines Möbiusbandes
Visualisierung mit Seaborn.
Erforschung von Seaborn-Plots
Histogramme, KDE und Dichten
Paar-Diagramme
Facettierte Histogramme
Kategorische Diagramme
Gemeinsame Verteilungen
Balkendiagramme
Beispiel: Untersuchung von Marathon-Zielzeiten
Weitere Ressourcen
Andere Python-Visualisierungsbibliotheken
Was ist maschinelles Lernen? Teil V. Maschinelles Lernen
Kategorien des maschinellen Lernens
Qualitative Beispiele für Anwendungen des maschinellen Lernens
Klassifizierung: Vorhersage von diskreten Bezeichnungen
Regression: Vorhersage von kontinuierlichen Kennzeichnungen
Clustering: Ableitung von Kennzeichnungen aus unmarkierten Daten
Dimensionalitätsreduktion: Ableitung der Struktur von unmarkierten Daten
Zusammenfassung
Einführung in Scikit-Learn.
Datenrepräsentation in Scikit-Learn
Die Merkmalsmatrix
Das Ziel-Array
Die Estimator-API
Grundlagen der API
Beispiel für überwachtes Lernen: Einfache lineare Regression
Beispiel für überwachtes Lernen: Iris-Klassifikation
Beispiel für unüberwachtes Lernen: Dimensionalität der Iris
Beispiel für unüberwachtes Lernen: Iris-Clustering
Anwendung: Untersuchen handgeschriebener Ziffern
Laden und Visualisieren der Zifferndaten
Beispiel für unüberwachtes Lernen: Dimensionalitätsreduktion
Klassifizierung auf Ziffern
Zusammenfassung
Hyperparameter und Modellvalidierung.
Nachdenken über Modellvalidierung
Modellvalidierung auf die falsche Art
Modellvalidierung auf die richtige Art: Holdout-Sets
Modellvalidierung durch Kreuzvalidierung
Auswählen des besten Modells
Der Bias-Varianz-Kompromiss
Validierungskurven in Scikit-Learn
Lernkurven
Validierung in der Praxis: Gittersuche
Zusammenfassung
Merkmalstechnik.
Kategorische Merkmale
Text-Merkmale
Bild-Merkmale
Abgeleitete Merkmale
Imputation von fehlenden Daten
Merkmals-Pipelines
Vertiefung: Naive Bayes-Klassifikation.
Bayes'sche Klassifikation
Gaußsche Naive Bayes
Multinomial Naive Bayes
Beispiel: Klassifizierung von Text
Wann wird Naive Bayes verwendet?
Vertiefung: Lineare Regression.
Einfache lineare Regression
Basisfunktions-Regression
Polynomielle Basisfunktionen
Gaußsche Basisfunktionen
Regularisierung
Ridge-Regression (L 2 Regularisierung)
Lasso-Regression (L 1 Regularisierung)
Beispiel: Vorhersage des Fahrradverkehrs
Vertiefung: Support-Vektor-Maschinen.
Motivation für Support-Vektor-Maschinen
Support-Vektor-Maschinen: Maximierung der Gewinnspanne
Anpassen einer Support-Vektor-Maschine
Jenseits linearer Grenzen: Kernel-SVM
Abstimmung der SVM: Abschwächen der Ränder
Beispiel: Erkennung von Gesichtern
Zusammenfassung
Vertiefung: Entscheidungsbäume und Zufallsforste.
Motivation für Random Forests: Entscheidungsbäume
Erstellen eines Entscheidungsbaums
Entscheidungsbäume und Overfitting
Ensembles von Schätzern: Zufällige Wälder
Random-Forest-Regression
Beispiel: Random Forest zur Klassifizierung von Ziffern
Zusammenfassung
Vertiefung: Hauptkomponentenanalyse.
Einführung in die Principal Component Analysis
PCA als Dimensionalitätsreduktion
PCA für die Visualisierung: Handgeschriebene Ziffern
Was bedeuten die Komponenten?
Auswahl der Anzahl der Komponenten
PCA als Rauschfilterung
Beispiel: Eigenflächen
Zusammenfassung
Vertiefung: Vielfältiges Lernen.
Vielfältiges Lernen: "HALLO"
Multidimensionale Skalierung
MDS als Manifold Learning
Nichtlineare Einbettungen: Wo MDS scheitert
Nichtlineare Mannigfaltigkeiten: Lokale lineare Einbettung
Einige Gedanken zu Manifold-Methoden
Beispiel: Isomap auf Gesichtern
Beispiel: Visualisierung von Strukturen in Ziffern
Vertiefung: k-Means Clustering.
Einführung in k-Means
Erwartungs-Maximierung
Beispiele
Beispiel 1: k-Means auf Ziffern
Beispiel 2: k-Means für Farbkomprimierung
Vertiefung: Gaußsche Mischungsmodelle.
Motivation für Gaußsche Mischungen: Schwachstellen von k-Means
Verallgemeinerung von E-M: Gaußsche Mischungsmodelle
Die Wahl des Kovarianztyps
Gaußsche Mischungsmodelle als Dichteschätzung
Beispiel: GMMs zur Generierung neuer Daten
Vertiefung: Kernel-Dichte-Schätzung.
Motivation der Kernel-Dichte-Schätzung: Histogramme
Kernel-Dichte-Schätzung in der Praxis
Auswahl der Bandbreite durch Cross-Validierung
Beispiel: Nicht so Naive Bayes
Anatomie eines benutzerdefinierten Schätzers
Verwendung unseres angepassten Schätzers
Anwendung: Eine Gesichtserkennungs-Pipeline.
HOG-Merkmale
HOG in Aktion: Ein einfacher Gesichtsdetektor
Erhalten Sie einen Satz positiver Trainingsproben
Gewinnung eines Satzes negativer Trainingsproben
Kombinieren Sie die Gruppen und extrahieren Sie HOG-Merkmale
Trainieren einer Support-Vektor-Maschine
Gesichter in einem neuen Bild finden
Vorbehalte und Verbesserungen
Weitere Ressourcen zum maschinellen Lernen
Index.
Vorwort
Was ist Datenwissenschaft?
Dies ist ein Buch über Data Science mit Python, was sofort die Frage aufwirft

Frage: Was ist Datenwissenschaft? Es ist überraschend schwer, eine Definition zu finden, vor allem
vor allem, wenn man bedenkt, wie allgegenwärtig der Begriff geworden ist. Lautstarke Kritiker haben den Begriff unterschiedlich dis-

als überflüssige Bezeichnung (denn welche Wissenschaft hat nicht mit Daten zu tun?) oder als
einfaches Schlagwort, das nur dazu dient, Lebensläufe zu schmücken und die Aufmerksamkeit übereifriger Techniker auf sich zu ziehen

Anwerber.

Meiner Meinung nach gehen diese Kritiken an einer wichtigen Sache vorbei. Data Science ist trotz seiner Hype-
ist vielleicht die beste Bezeichnung, die wir für die interdisziplinären Fähigkeiten haben, die

die in vielen Anwendungen in Industrie und Wissenschaft zunehmend an Bedeutung gewinnen
Wissenschaft. Dieser disziplinübergreifende Teil ist der Schlüssel: Meiner Meinung nach ist die beste bestehende Definition

der Datenwissenschaft wird durch Drew Conways Data Science Venn Diagram illustriert, das erstmals veröffentlicht wurde.

die er im September 2010 in seinem Blog veröffentlichte (Abbildung P-1).

Auch wenn einige der Beschriftungen der Kreuzungen ein wenig ironisch gemeint sind, zeigt dieses Diagramm

die Essenz dessen, was die Leute meiner Meinung nach meinen, wenn sie "Datenwissenschaft" sagen: Es ist grundlegend interdisziplinär.
grundsätzlich ein interdisziplinäres Thema. Die Datenwissenschaft umfasst drei verschiedene, sich überschneidende Bereiche.

ping-Bereiche: die Fähigkeiten eines Statistikers, der weiß, wie man modelliert und zusammenfasst

Datenmengen (die immer größer werden); die Fähigkeiten eines Informatikers, der
Algorithmen zu entwerfen und anzuwenden, um diese Daten effizient zu speichern, zu verarbeiten und zu visualisieren; und die

Fachwissen - das, was man als "klassische" Ausbildung in einem Fach bezeichnen könnte - ist
um die richtigen Fragen zu formulieren und die Antworten in einen Kontext zu stellen.

In diesem Sinne möchte ich Sie ermutigen, Datenwissenschaft nicht als eine neue Technologie zu betrachten.

Wissensgebiet zu erlernen, sondern eine Reihe neuer Fähigkeiten, die Sie in Ihrem
Ihrem derzeitigen Fachgebiet anwenden können. Ob Sie nun über Wahlergebnisse berichten, Prognosen erstellen

Aktienrenditen, die Optimierung von Online-Werbeklicks, die Identifizierung von Mikroorganismen in Mikroskopfotos
Fotos, die Suche nach neuen Klassen von astronomischen Objekten oder die Arbeit mit Daten in allen

xix
Ziel dieses Buches ist es, Sie in die Lage zu versetzen, neue Fragen zu stellen und zu beantworten.

über den von Ihnen gewählten Themenbereich.

Abbildung P-1. Drew Conways Venn-Diagramm der Datenwissenschaft (Quelle: Drew Conway, verwendet

mit Genehmigung)

Für wen ist dieses Buch gedacht?
In meiner Lehrtätigkeit sowohl an der University of Washington als auch bei verschiedenen technisch orientierten

Auf Konferenzen und Treffen ist eine der häufigsten Fragen, die ich höre, diese:
"Wie sollte ich Python lernen?" Die Fragesteller sind im Allgemeinen technisch versiert

Studenten, Entwickler oder Forscher, die häufig bereits über gute Kenntnisse im Schreiben von
Code zu schreiben und Berechnungs- und numerische Werkzeuge zu verwenden. Die meisten dieser Leute wollen nicht

Python an sich zu lernen, sondern die Sprache mit dem Ziel zu erlernen, sie als

Werkzeug für datenintensive und rechnergestützte Wissenschaft. Obwohl ein großer Flickenteppich von Videos,
Blogbeiträgen und Tutorials für diese Zielgruppe online verfügbar ist, war ich lange Zeit frustriert

durch das Fehlen einer einzigen guten Antwort auf diese Frage; das hat dieses Buch inspiriert.

Das Buch ist nicht als Einführung in Python oder in die Programmierung im Allgemeinen gedacht.

Ich gehe davon aus, dass der Leser mit der Sprache Python vertraut ist, einschließlich der Definition von

Funktionen, Zuweisung von Variablen, Aufruf von Methoden von Objekten, Steuerung des Programmablaufs
Programmablaufs und anderer grundlegender Aufgaben. Stattdessen soll es Python-Benutzern helfen, den Umgang mit

Pythons Data Science Stack - Bibliotheken wie die im folgenden Abschnitt erwähnten
und verwandte Tools, um Daten effektiv zu speichern, zu bearbeiten und Erkenntnisse daraus zu gewinnen.

xx | Vorwort

Warum Python?
Python hat sich in den letzten Jahrzehnten zu einem erstklassigen Werkzeug für wissenschaftliche Anwendungen entwickelt.

Datenverarbeitungsaufgaben, einschließlich der Analyse und Visualisierung großer Datenmengen. Dies mag
frühen Befürworter der Sprache Python überrascht: Die Sprache

selbst wurde nicht speziell für die Datenanalyse oder das wissenschaftliche Rechnen entwickelt.
Die Nützlichkeit von Python für die Datenwissenschaft ergibt sich in erster Linie aus den großen und aktiven

Ökosystem von Drittanbieter-Paketen: NumPy für die Manipulation von homogenen Array-

basierten Daten, Pandas für die Manipulation von heterogenen und markierten Daten, SciPy für
allgemeine wissenschaftliche Berechnungsaufgaben, Matplotlib für Visualisierungen in Publikationsqualität,

IPython für die interaktive Ausführung und gemeinsame Nutzung von Code, Scikit-Learn für maschinelles Lernen
und viele weitere Tools, die auf den folgenden Seiten erwähnt werden.

Wenn Sie nach einem Leitfaden für die Sprache Python selbst suchen, empfehle ich Ihnen die Schwester

Projekt zu diesem Buch, A Whirlwind Tour of Python. Dieser kurze Bericht gibt einen Überblick über
die wesentlichen Merkmale der Sprache Python und richtet sich an Datenwissenschaftler, die bereits

Vertrautheit mit einer oder mehreren anderen Programmiersprachen.

Gliederung des Buches
Jeder nummerierte Teil dieses Buches konzentriert sich auf ein bestimmtes Paket oder Werkzeug, das einen
einen grundlegenden Teil der Python-Datenwissenschaft beiträgt, und ist in kurze Abschnitte

in sich abgeschlossene Kapitel, die jeweils ein einzelnes Konzept behandeln:

Teil I, "Jupyter: Jenseits von normalem Python" führt in IPython und Jupyter ein. Diese
Pakete bieten die Berechnungsumgebung, in der viele Python nutzende
Datenwissenschaftler arbeiten.
Teil II, "Einführung in NumPy" konzentriert sich auf die NumPy-Bibliothek, die
ndarray für die effiziente Speicherung und Manipulation von dichten Datenarrays in
Python bietet.
Teil III, "Datenmanipulation mit Pandas" führt in die Pandas-Bibliothek ein, die
DataFrame für die effiziente Speicherung und Manipulation von beschrifteten/gesammelten
narischen Daten in Python bietet.
Teil IV, "Visualisierung mit Matplotlib" konzentriert sich auf Matplotlib, eine Bibliothek, die
eine Bibliothek, die Möglichkeiten für eine flexible Palette von Datenvisualisierungen in Python bietet.
Teil V, "Maschinelles Lernen", konzentriert sich auf die Bibliothek Scikit-Learn, die
effiziente und saubere Python-Implementierungen der wichtigsten und etablierten
etablierten Algorithmen für maschinelles Lernen bietet.
Die PyData-Welt ist sicherlich viel größer als diese sechs Pakete, und sie wächst weiter

jeden Tag. In diesem Sinne bemühe ich mich, in diesem Buch
Verweise auf andere interessante Bemühungen, Projekte und Pakete zu geben, die die

Vorwort | xxi
Grenzen dessen, was man in Python tun kann. Dennoch konzentrieren sich die Pakete, die ich

on sind derzeit grundlegend für einen Großteil der Arbeit im Bereich der Python-Datenwissenschaft.
und ich erwarte, dass sie auch dann wichtig bleiben, wenn das Ökosystem weiter wächst.

um sie herum wachsen.

Überlegungen zur Installation
Die Installation von Python und der Bibliotheken, die wissenschaftliche Berechnungen ermöglichen, ist

geradlinig. In diesem Abschnitt werden einige der Dinge beschrieben, die Sie bei der
Ihren Computer einrichten.

Obwohl es verschiedene Möglichkeiten gibt, Python zu installieren, würde ich für die Verwendung in der
Datenwissenschaft empfehle, ist die Anaconda-Distribution, die in ähnlicher Weise funktioniert, egal ob Sie die

Windows, Linux oder macOS. Die Anaconda-Distribution gibt es in zwei Varianten:

Miniconda bietet Ihnen den Python-Interpreter selbst sowie ein Kommandozeilenwerkzeug namens
Werkzeug namens conda, das als plattformübergreifender Paketmanager für Python-Pakete arbeitet
auf Python-Pakete ausgerichtet ist, ähnlich wie die apt- oder yum-Tools, die Linux-Nutzer
vertraut sein könnten.
Anaconda enthält sowohl Python als auch conda und bündelt zusätzlich eine Reihe von
anderen vorinstallierten Paketen, die auf wissenschaftliche Berechnungen ausgerichtet sind. Aufgrund der
Größe dieses Pakets sollten Sie damit rechnen, dass die Installation mehrere Gigabytes an
Speicherplatz.
Jedes der in Anaconda enthaltenen Pakete kann auch manuell zusätzlich zu

Miniconda; aus diesem Grund schlage ich vor, mit Miniconda zu beginnen.

Um loszulegen, laden Sie das Miniconda-Paket herunter und installieren Sie es - stellen Sie sicher, dass Sie eine
Version mit Python 3 und installieren Sie dann die in diesem Buch verwendeten Kernpakete:

[~]$ conda install numpy pandas scikit-learn matplotlib seaborn jupyter
Im Laufe des Textes werden wir auch andere, spezialisiertere Werkzeuge aus dem Python-Programm verwenden

wissenschaftliches Ökosystem; die Installation ist normalerweise so einfach wie die Eingabe von conda install package

Name. Sollten Sie jemals auf Pakete stoßen, die nicht in der Standard-Conda

Channel zu nutzen, sollten Sie sich die conda-forge ansehen, ein breites, von der Community betriebenes Repository von

conda-Pakete.

Für weitere Informationen zu conda, einschließlich Informationen zur Erstellung und Verwendung von

conda-Umgebungen (was ich sehr empfehlen würde), finden Sie in der Online-Dokumentation von conda
Dokumentation.

xxii | Vorwort

In diesem Buch verwendete Konventionen
Die folgenden typografischen Konventionen werden in diesem Buch verwendet:

Kursiv
Kennzeichnet neue Begriffe, URLs, E-Mail-Adressen, Dateinamen und Dateierweiterungen.

Konstante Breite

Wird für Programmlistings sowie innerhalb von Absätzen verwendet, um auf Programmelemente zu verweisen.
Programmteile wie Variablen- oder Funktionsnamen, Datenbanken, Datentypen, Umgebungsvariablen
Variablen, Anweisungen und Schlüsselwörter.
Konstante Breite fett

Zeigt Befehle oder anderen Text an, der vom Benutzer wörtlich eingegeben werden muss.

Konstante Breite kursiv

Zeigt Text an, der durch vom Benutzer eingegebene Werte oder durch vom Kontext bestimmte Werte ersetzt werden soll.
durch den Kontext bestimmt werden.
Dieses Element kennzeichnet einen allgemeinen Hinweis.
Verwendung von Codebeispielen
Ergänzendes Material (Code-Beispiele, Abbildungen usw.) steht zum Download bereit unter

http://github.com/jakevdp/PythonDataScienceHandbook.

Wenn Sie eine technische Frage oder ein Problem mit den Codebeispielen haben, senden Sie bitte

E-Mail an bookquestions@oreilly.com.

Dieses Buch soll Ihnen helfen, Ihre Arbeit zu erledigen. Im Allgemeinen gilt: Wenn Beispielcode angeboten wird

mit diesem Buch, können Sie es in Ihren Programmen und Dokumentationen verwenden. Sie müssen nicht

Sie müssen uns nicht um Erlaubnis bitten, es sei denn, Sie reproduzieren einen großen Teil des
des Codes. Wenn Sie zum Beispiel ein Programm schreiben, das mehrere Teile des Codes aus diesem

Buches ist keine Genehmigung erforderlich. Für den Verkauf oder die Verbreitung von Beispielen aus O'Reilly
Büchern erfordert eine Erlaubnis. Beantworten einer Frage durch Zitieren dieses Buches und Zitieren von

Beispielcode ist nicht genehmigungspflichtig. Die Einbeziehung einer erheblichen Menge von

Beispielcode aus diesem Buch in die Dokumentation Ihres Produkts aufzunehmen, ist
Erlaubnis.

Wir sind dankbar für eine Namensnennung, verlangen sie aber im Allgemeinen nicht. Eine Namensnennung umfasst normalerweise
den Titel, den Autor, den Verlag und die ISBN. Zum Beispiel: "Python Datenwissenschaft

Vorwort | xxiii
Handbook, 2. Auflage, von Jake VanderPlas (O'Reilly). Copyright 2023 Jake Vander-

Plas, 978-1-098-12122-8."

Wenn Sie der Meinung sind, dass Ihre Verwendung von Code-Beispielen nicht unter die Fair Use-Regelung oder die erteilte Genehmigung fällt

kontaktieren Sie uns bitte unter permissions@oreilly.com.

O'Reilly Online Lernen
Seit mehr als 40 Jahren bietet O'Reilly Media technologische und
Technologie- und Geschäftsschulungen, Wissen und Einblicke, die den
Unternehmen zum Erfolg zu verhelfen.
Unser einzigartiges Netzwerk von Experten und Innovatoren teilt sein Wissen und seine Erfahrung

durch Bücher, Artikel und unsere Online-Lernplattform. Die Online-Lernplattform von O'Reilly
Plattform von O Reilly bietet Ihnen On-Demand-Zugang zu Live-Trainingskursen, vertiefenden Lerninhalten

Pfade, interaktive Programmierumgebungen und eine umfangreiche Sammlung von Texten und Videos von
O'Reilly und 200+ anderen Verlagen. Weitere Informationen finden Sie unter https://oreilly.com.

Wie Sie uns kontaktieren können
Bitte richten Sie Kommentare und Fragen zu diesem Buch an den Herausgeber:

O'Reilly Media, Inc.
1005 Gravenstein Highway Nord
Sebastopol, CA 95472
800-998-9938 (in den Vereinigten Staaten oder Kanada)
707-829-0515 (international oder lokal)
707-829-0104 (Fax)
Wir haben eine Webseite für dieses Buch, auf der wir Errata, Beispiele und zusätzliche Informationen auflisten.
Informationen. Sie können diese Seite unter https://oreil.ly/python-data-science-handbook aufrufen.

Senden Sie eine E-Mail an bookquestions@oreilly.com, um Kommentare oder technische Fragen zu diesem Buch zu stellen.
Buch zu stellen.

Neuigkeiten und Informationen über unsere Bücher und Kurse finden Sie unter https://oreilly.com.

Finden Sie uns auf LinkedIn: https://linkedin.com/company/oreilly-media.

Folgen Sie uns auf Twitter: https://twitter.com/oreillymedia.

Sehen Sie uns auf YouTube: https://youtube.com/oreillymedia.

xxiv | Vorwort

TEIL I
Jupyter: Jenseits von normalem Python

Es gibt viele Optionen für Entwicklungsumgebungen für Python, und ich werde oft gefragt
gefragt, welche ich bei meiner eigenen Arbeit verwende. Meine Antwort überrascht die Leute manchmal: mein

bevorzugte Umgebung ist IPython plus ein Texteditor (in meinem Fall Emacs oder VSCode
abhängig von meiner Stimmung). Jupyter hat seinen Ursprung in der IPython-Shell, die erstellt wurde

wurde 2001 von Fernando Perez als erweiterter Python-Interpreter entwickelt und hat sich seitdem

in ein Projekt umgewandelt, das, wie Perez es ausdrückt, "Werkzeuge für den gesamten Lebenszyklus von
Forschungsinformatik". Wenn Python der Motor unserer datenwissenschaftlichen Aufgabe ist, könnte man meinen

von Jupyter als interaktives Bedienfeld.

Jupyter ist nicht nur eine nützliche interaktive Schnittstelle zu Python, sondern bietet auch eine Reihe von...

Wir werden eine Reihe nützlicher syntaktischer Erweiterungen der Sprache behandeln.

Ergänzungen hier. Die vielleicht bekannteste Schnittstelle, die das Jupyter-Projekt bietet, ist
das Jupyter Notebook, eine browserbasierte Umgebung, die für die Entwicklung nützlich ist,

Zusammenarbeit, gemeinsame Nutzung und sogar Veröffentlichung von Ergebnissen der Datenwissenschaft. Als Beispiel für die
für die Nützlichkeit des Notizbuchformats ist die Seite, die Sie gerade lesen:

Das gesamte Manuskript für dieses Buch wurde in Form von Jupyter-Notebooks verfasst.

In diesem Teil des Buches werden zunächst einige der Funktionen von Jupyter und IPython
Funktionen, die für die Praxis der Datenwissenschaft nützlich sind, und konzentriert sich dabei besonders auf die Syn-

Steuer, die sie über die Standardfunktionen von Python hinaus bieten. Als Nächstes werden wir etwas mehr in die Tiefe gehen
einige der nützlicheren magischen Befehle, die gängige Aufgaben beschleunigen können

bei der Erstellung und Verwendung von Data-Science-Code. Schließlich werden wir auf einige der Funktionen eingehen

des Notizbuchs, die es für das Verständnis von Daten und den Austausch von Ergebnissen nützlich machen.

KAPITEL 1

Erste Schritte in IPython und Jupyter
Wenn ich Python-Code für Data Science schreibe, wechsle ich im Allgemeinen zwischen drei Arbeitsmodi
Arbeitsweise: Ich verwende die IPython-Shell, um kurze Befehlssequenzen auszuprobieren, die Jupyter

Notebook für längere interaktive Analysen und für den Austausch von Inhalten mit anderen, und

interaktive Entwicklungsumgebungen (IDEs) wie Emacs oder VSCode zur Erstellung
wiederverwendbare Python-Pakete. Dieses Kapitel konzentriert sich auf die ersten beiden Modi: die IPython

Shell und das Jupyter Notebook. Die Verwendung einer IDE für die Softwareentwicklung ist ein wichtiges
wichtiges drittes Werkzeug im Repertoire eines Datenwissenschaftlers, aber wir werden uns nicht direkt mit diesem

hier.

Starten der IPython-Shell
Der Text in diesem Teil ist, wie der größte Teil dieses Buches, nicht dazu gedacht, passiv aufgenommen zu werden. I

empfehlen, dass Sie während des Lesens mitmachen und mit den
Werkzeugen und der Syntax, die wir behandeln, zu experimentieren: Das Muskelgedächtnis, das Sie dabei aufbauen, wird Ihnen

weitaus nützlicher als das bloße Lesen darüber. Beginnen Sie mit dem Start von IPython

Interpreter, indem Sie ipython auf der Kommandozeile eingeben; alternativ können Sie, wenn Sie ein

Distributionen wie Anaconda oder EPD gibt es möglicherweise einen für Ihr System spezifischen Launcher.

Sobald Sie dies getan haben, sollten Sie eine Aufforderung wie die folgende sehen:

Python 3.9.2 (v3.9.2:1a79785e3e, Feb 19 2021, 09:06:10)
Geben Sie 'copyright', 'credits' oder 'license' für weitere Informationen ein
IPython 7.21.0 -- Ein verbessertes interaktives Python. Tippen Sie '?' für Hilfe.

In [1]:

Damit sind Sie bereit, der Sache zu folgen.

3
Starten des Jupyter-Notebooks
Das Jupyter Notebook ist eine browserbasierte grafische Schnittstelle zur IPython-Shell und

baut darauf eine Reihe von dynamischen Anzeigefunktionen auf. Neben der Ausführung von Python/
IPython-Anweisungen erlauben Notebooks dem Benutzer, formatierten Text, statische und

dynamische Visualisierungen, mathematische Gleichungen, JavaScript-Widgets und vieles mehr.
Außerdem können diese Dokumente so gespeichert werden, dass andere Personen sie öffnen können

und führen den Code auf ihren eigenen Systemen aus.

Obwohl Sie Jupyter-Notizbücher über Ihr Webbrowser-Fenster anzeigen und bearbeiten,
müssen sie sich mit einem laufenden Python-Prozess verbinden, um Code auszuführen. Sie können

starten Sie diesen Prozess (bekannt als "Kernel"), indem Sie den folgenden Befehl in Ihrer
System-Shell ausführen:

$ jupyter lab
Mit diesem Befehl wird ein lokaler Webserver gestartet, der für Ihren Browser sichtbar sein wird. Er

spuckt sofort ein Protokoll aus, das zeigt, was es tut; dieses Protokoll sieht etwa so aus
so:

$ jupyter lab
[ServerApp] Serving notebooks from local directory: /Users/jakevdp/ \
PythonDataScienceHandbuch
[ServerApp] Jupyter Server 1.4.1 wird ausgeführt unter:
[ServerApp] http://localhost:8888/lab?token=dd852649
[ServerApp] Verwenden Sie Control-C, um diesen Server zu stoppen und alle Kernel herunterzufahren
(zweimal, um die Bestätigung zu überspringen).
Nach dem Ausführen des Befehls sollte sich Ihr Standardbrowser automatisch öffnen und

navigieren Sie zu der aufgeführten lokalen URL; die genaue Adresse hängt von Ihrem System ab. Wenn die

Browser nicht automatisch öffnet, können Sie ein Fenster öffnen und diese Adresse manuell öffnen
Adresse (in diesem Beispiel http://localhost:8888/lab/).

Hilfe und Dokumentation in IPython
Wenn Sie keinen anderen Abschnitt in diesem Kapitel lesen, lesen Sie diesen: Ich finde die besprochenen Werkzeuge

Hier sind die wichtigsten Beiträge von IPython zu meinem täglichen Arbeitsablauf.

Wenn eine technikbegeisterte Person gebeten wird, einem Freund, Familienmitglied oder

Wenn ein Kollege ein Computerproblem hat, kommt es meist weniger darauf an, die

Antwort zu finden, als zu wissen, wie man eine unbekannte Antwort schnell findet. In der Datenwissenschaft ist es
dasselbe: durchsuchbare Webressourcen wie Online-Dokumentation, Mailinglisten

Threads und Stack Overflow-Antworten enthalten eine Fülle von Informationen, auch (vor allem?)
cially?) über Themen, zu denen Sie schon einmal etwas gesucht haben. Ein effektiver

der Datenwissenschaft geht es weniger darum, das Tool oder den Befehl auswendig zu lernen, den man

für jede mögliche Situation verwenden sollte, sondern vielmehr darum, zu lernen, wie man effektiv

4 | Kapitel 1: Erste Schritte in IPython und Jupyter

die Informationen, die Sie nicht kennen, sei es über eine Suchmaschine im Internet oder eine andere

bedeutet.

Eine der nützlichsten Funktionen von IPython/Jupyter ist es, die Lücke zwischen der

Benutzer und die Art der Dokumentation und Suche, die ihnen bei ihrer Arbeit helfen wird

effektiv. Die Suche im Internet spielt zwar immer noch eine Rolle bei der Beantwortung komplizierter Fragen,
kann eine erstaunliche Menge an Informationen allein durch IPython gefunden werden. Einige

Beispiele für Fragen, die IPython mit ein paar Tastenanschlägen beantworten kann, sind:

Wie kann ich diese Funktion aufrufen? Welche Argumente und Optionen hat sie?
Wie sieht der Quellcode dieses Python-Objekts aus?
Was befindet sich in dem von mir importierten Paket?
Welche Attribute oder Methoden hat dieses Objekt?
Hier werden wir die Werkzeuge besprechen, die in der IPython-Shell und im Jupyter Notebook zur Verfügung stehen, um

schnell auf diese Informationen zugreifen, und zwar mit dem Zeichen?

die ??-Zeichen, um den Quellcode zu erkunden, und die Tabulatortaste für die automatische Vervollständigung.

Zugriff auf die Dokumentation mit?
Die Sprache Python und ihr Data-Science-Ökosystem wurden mit Blick auf den Benutzer entwickelt,
und ein wichtiger Teil davon ist der Zugang zur Dokumentation. Jedes Python-Objekt enthält eine

einen Verweis auf eine Zeichenkette, den so genannten docstring, der in den meisten Fällen eine knappe

Zusammenfassung des Objekts und wie es zu verwenden ist. Python hat eine eingebaute Hilfefunktion, die

auf diese Informationen zugreifen und die Ergebnisse ausdrucken. Zum Beispiel, um die Dokumentation zu sehen

der eingebauten Funktion len, können Sie Folgendes tun:

In [1]: help(len)
Hilfe zur eingebauten Funktion len im Modul builtins:

len(obj, /)
Gibt die Anzahl der Elemente in einem Container zurück.

Abhängig von Ihrem Interpreter können diese Informationen als Inline-Text oder in

ein separates Pop-up-Fenster.

Da die Suche nach Hilfe zu einem Objekt so häufig und nützlich ist, haben IPython und Jupyter

Zeichen als Abkürzung für den Zugriff auf diese Dokumentation und andere
relevanten Informationen:

In [2]: len?
Signatur: len(obj, /)
Zeichenkette: Gibt die Anzahl der Elemente in einem Container zurück.
Typ: builtin_function_or_method

Hilfe und Dokumentation in IPython | 5
Diese Notation funktioniert für so gut wie alles, einschließlich Objektmethoden:

In [3]: L = [1, 2, 3]
In [4]: L.insert?
Unterschrift: L.insert(index, objekt, /)
Zeichenkette: Objekt vor Index einfügen.
Typ: builtin_function_or_method

oder sogar Objekte selbst, mit der Dokumentation ihres Typs:

In [5]: L?
Typ: Liste
String-Form: [1, 2, 3]
Länge: 3
Zeichenkette:
Eingebaute veränderbare Sequenz.

Wenn kein Argument angegeben wird, erzeugt der Konstruktor eine neue leere Liste.
Das Argument muss eine iterable sein, falls angegeben.

Wichtig ist, dass dies auch für Funktionen oder andere Objekte funktioniert, die Sie selbst erstellen!

Hier werden wir eine kleine Funktion mit einem Docstring definieren:

In [6]: def square(a):
....: """Return the square of a."""
....: return a ** 2
....:

Um einen Docstring für unsere Funktion zu erstellen, haben wir einfach ein Stringliteral in

die erste Zeile. Da docstrings in der Regel mehrzeilig sind, haben wir aus Konvention die
Pythons Notation in dreifachen Anführungszeichen für mehrzeilige Strings.

Jetzt werden wir den? verwenden, um diesen Docstring zu finden:

In [7]: square?
Signatur: square(a)
Zeichenkette: Gibt das Quadrat von a zurück.
Datei: <ipython-input-6>
Typ: Funktion

Dieser schnelle Zugriff auf die Dokumentation über docstrings ist einer der Gründe, warum Sie sich für die

Angewohnheit, dem von Ihnen geschriebenen Code stets eine solche Inline-Dokumentation hinzuzufügen.

Zugriff auf den Quellcode mit ??
Da die Sprache Python so leicht lesbar ist, kann eine andere Ebene der Einsicht normalerweise

durch das Lesen des Quellcodes des Objekts, an dem Sie interessiert sind, gewonnen werden. IPython und

Jupyter bietet mit dem doppelten Fragezeichen (??) eine Abkürzung zum Quellcode:

In [8]: square??
Signatur: Quadrat(a)
Quelle:
def square(a):

6 | Kapitel 1: Erste Schritte in IPython und Jupyter

"""Gib das Quadrat von a zurück."""
return a ** 2
Datei: <ipython-input-6>
Typ: Funktion

Für einfache Funktionen wie diese kann das doppelte Fragezeichen einen schnellen Einblick in

die Details unter der Haube.

Wenn Sie viel damit spielen, werden Sie feststellen, dass das Suffix ?? manchmal keinen Quellcode anzeigt.
keinen Quellcode anzeigt: Das liegt im Allgemeinen daran, dass das betreffende Objekt nicht in

Python, sondern in C oder einer anderen kompilierten Erweiterungssprache. Wenn dies der Fall ist, muss die ??

Suffix ergibt die gleiche Ausgabe wie das? Suffix. Sie finden dies insbesondere bei vielen der

Pythons eingebaute Objekte und Typen, einschließlich der bereits erwähnten Funktion len:

In [9]: len??
Signatur: len(obj, /)
Doku-String: Gibt die Anzahl der Elemente in einem Container zurück.
Typ: builtin_function_or_method

Die Verwendung von ? und/oder ?? ist ein leistungsfähiges und schnelles Mittel, um Informationen darüber zu finden, was eine

Python-Funktion oder -Modul tut.

Erkunden von Modulen mit der Tabulatorvervollständigung
Eine weitere nützliche Schnittstelle ist die Verwendung der Tabulatortaste zur automatischen Vervollständigung und Erkundung

des Inhalts von Objekten, Modulen und Namespaces. In den folgenden Beispielen werde ich

verwenden, um anzuzeigen, wann die Tabulatortaste gedrückt werden soll.

Tabulatorische Vervollständigung des Objektinhalts

Jedes Python-Objekt hat verschiedene Attribute und Methoden, die mit ihm verbunden sind. Wie die

Hilfe-Funktion hat Python eine eingebaute dir-Funktion, die eine Liste zurückgibt

von diesen, aber in der Praxis ist die Schnittstelle zur Vervollständigung der Registerkarten viel einfacher zu benutzen. Zur Anzeige einer
Liste aller verfügbaren Attribute eines Objekts zu sehen, können Sie den Namen des Objekts fol-

wird durch einen Punkt (.) und die Tabulatortaste abgeschlossen:

In [10]: L.
append() count insert reverse
löschen erweitern pop sortieren
kopieren index entfernen

Um die Liste einzugrenzen, können Sie den ersten Buchstaben oder mehrere Buchstaben des Wortes

Name, und die Tabulatortaste findet die passenden Attribute und Methoden:

In [10]: L.c
clear() count()
kopieren()

In [10]: L.co
copy() count()

Hilfe und Dokumentation in IPython | 7
Wenn es nur eine einzige Option gibt, wird die Zeile durch Drücken der Tabulatortaste vervollständigt. Für

Beispiel wird das Folgende sofort durch L.count ersetzt:

In [10]: L.cou

Obwohl Python keine streng erzwungene Unterscheidung zwischen öffentlichen/externen

Attributen und privaten/internen Attributen zu unterscheiden, wird üblicherweise ein vorangestellter Unterstrich
verwendet, um letztere zu kennzeichnen. Der Klarheit halber sind diese privaten Methoden und speziellen Methoden

aus der Liste ausgelassen, aber es ist möglich, sie aufzulisten, indem man explizit die

Unterstrich:

In [10]: L._
hinzufügen delattr eq
Klasse delitem format()
class_getitem() dir() ge >
enthält doc getattribute

Der Kürze halber habe ich nur die ersten paar Spalten der Ausgabe gezeigt. Die meisten davon sind

Pythons spezielle Double-Underscore-Methoden (oft auch als "Dunder"-Methoden bezeichnet).

Tabulatorvervollständigung beim Importieren

Die Tabulatorvervollständigung ist auch beim Importieren von Objekten aus Paketen nützlich. Hier werden wir sie verwenden

um alle möglichen Importe im itertools-Paket zu finden, die mit co beginnen:

In [10]: from itertools import co
kombinationen() komprimieren()
Kombinationen_mit_Ersetzung() count()

In ähnlicher Weise können Sie mit der Tabulatorvervollständigung sehen, welche Importe auf Ihrem System verfügbar sind.

tem (dies ändert sich je nachdem, welche Skripte und Module von Drittanbietern sichtbar sind

zu Ihrer Python-Sitzung):

In [10]: importieren
abc anyio
activate_this appdirs
aifc appnope >
antigravitation argon2

In [10]: import h
hashlib html
heapq http
hmac

Jenseits der Tabulatorvervollständigung: Wildcard-Abgleich

Die Tabulatorvervollständigung ist nützlich, wenn Sie die ersten Zeichen des Objektnamens kennen

oder Attribut, nach dem Sie suchen, ist aber wenig hilfreich, wenn Sie Zeichen in der
Mitte oder am Ende des Namens suchen. Für diesen Anwendungsfall bieten IPython und Jupyter eine

ein Platzhalter für Namen, die das Zeichen * verwenden.

8 | Kapitel 1: Erste Schritte in IPython und Jupyter

Damit können wir zum Beispiel jedes Objekt im Namensraum auflisten, dessen Name auf

mit Warnung:

In [10]: *Warnung?
BytesWarnung LaufzeitWarnung
DeprecationWarnung SyntaxWarnung
ZukunftsWarnung UnicodeWarnung
ImportWarnung UserWarnung
PendingDeprecationWarning Warnung
RessourcenWarnung

Beachten Sie, dass das Zeichen * auf jede beliebige Zeichenfolge passt, auch auf die leere Zeichenfolge.

Ähnlich verhält es sich, wenn wir nach einer String-Methode suchen, die das Wort find enthält

irgendwo in seinem Namen. Wir können auf diese Weise danach suchen:

In [11]: str.find?
str.find
str.rfind

Ich finde, dass diese Art der flexiblen Platzhaltersuche nützlich sein kann, um eine bestimmte Kom-

mand, wenn ich ein neues Paket kennenlerne oder mich mit einem vertrauten Paket wieder vertraut mache.
vertrautem Paket.

Tastaturkürzel in der IPython-Shell
Wenn Sie auch nur ein bisschen Zeit am Computer verbringen, haben Sie wahrscheinlich eine Verwendung für Tastenkombinationen gefunden.

Board-Shortcuts in Ihrem Arbeitsablauf. Am bekanntesten sind vielleicht Cmd-c und Cmd-v (oder
Ctrl-c und Ctrl-v), die zum Kopieren und Einfügen in einer Vielzahl von Programmen und

Systeme. Power-User neigen dazu, noch weiter zu gehen: beliebte Texteditoren wie Emacs, Vim,

und andere bieten den Benutzern eine unglaubliche Bandbreite an Operationen durch komplizierte Tastenkombinationen.
Kombinationen von Tastenanschlägen.

Die IPython-Shell geht nicht so weit, bietet aber eine Reihe von Tastaturkürzeln
Tastaturkürzel für die schnelle Navigation bei der Eingabe von Befehlen. Während einige dieser Abkürzungen

in den browserbasierten Notizbüchern zu arbeiten, geht es in diesem Abschnitt vor allem um Shortcuts im

IPython-Shell.

Sobald Sie sich daran gewöhnt haben, können sie sehr nützlich sein, um schnell

bestimmte Befehle, ohne dass Sie Ihre Hände von der "Home"-Tastaturposition wegbewegen müssen.
Wenn Sie ein Emacs-Benutzer sind oder Erfahrung mit Linux-ähnlichen Shells haben, können Sie die folgenden...

werden Ihnen sehr vertraut sein. Ich werde diese Abkürzungen in ein paar Kategorien einteilen: Navigation

Tastenkombinationen, Tastenkombinationen für die Texteingabe, Tastenkombinationen für die Befehlshistorie und verschiedene Tastenkombinationen.

Tastaturkürzel in der IPython-Shell | 9
Navigationskurzbefehle
Während die Verwendung der linken und rechten Pfeiltasten zum Vorwärts- und Rückwärtsbewegen in der

Zeile recht offensichtlich ist, gibt es andere Optionen, bei denen man die Hände nicht aus der
aus der Ausgangsposition der Tastatur zu bewegen:

Tastendruck Aktion
Strg-a Cursor an den Anfang der Zeile bewegen
Strg-e Cursor an das Ende der Zeile bewegen
Strg-b oder die linke Pfeiltaste Cursor ein Zeichen zurückbewegen
Strg-f oder die rechte Pfeiltaste Bewegen des Cursors um ein Zeichen vorwärts
Tastenkombinationen für die Texteingabe
Jeder kennt die Rücktaste, um das vorherige Zeichen zu löschen.
Die Rücktaste ist zwar jedem geläufig, um das vorherige Zeichen zu löschen, aber der Griff zu dieser Taste erfordert oft ein wenig Fingergymnastik, und sie ist nur

löscht ein einzelnes Zeichen auf einmal. In IPython gibt es mehrere Tastenkombinationen zum Entfernen
um einen Teil des Textes zu entfernen, den Sie gerade tippen; die unmittelbar nützlichsten davon sind

die Befehle zum Löschen ganzer Textzeilen. Sie wissen, dass diese Befehle sekundär geworden sind.

Natur, wenn Sie eine Kombination aus Strg-b und Strg-d verwenden, anstatt die
nach der Rücktaste zu greifen, um das vorherige Zeichen zu löschen!

Tastendruck Aktion
Rückschritttaste Vorheriges Zeichen in der Zeile löschen
Strg-d Nächstes Zeichen in der Zeile löschen
Strg-k Text vom Cursor bis zum Ende der Zeile ausschneiden
Strg-u Text vom Anfang der Zeile bis zur Schreibmarke ausschneiden
Strg-y Ausschneiden (d. h. Einfügen) von Text, der zuvor ausgeschnitten wurde
Strg-t Transponieren (d. h. Umschalten) der beiden vorherigen Zeichen
Tastenkombinationen für die Befehlshistorie
Die hier besprochenen Tastenkombinationen sind vielleicht die einflussreichsten, die IPython bietet

für die Navigation in der Befehlshistorie. Diese Befehlshistorie geht über Ihre aktuelle
IPython-Sitzung hinaus: Ihre gesamte Befehlshistorie wird in einer SQLite-Datenbank in

Ihr IPython-Profilverzeichnis.

10 | Kapitel 1: Erste Schritte in IPython und Jupyter

Der einfachste Weg, auf vorherige Befehle zuzugreifen, ist die Verwendung der Tasten up und

Pfeiltasten nach unten, um durch die Historie zu gehen, aber es gibt auch andere Optionen:

Tastendruck Aktion
Strg-p (oder die Pfeiltaste nach oben) Vorherigen Befehl im Verlauf aufrufen
Strg-n (oder die Pfeiltaste nach unten) Zugriff auf den nächsten Befehl in der Historie
Strg-r Rückwärtssuche in der Befehlshistorie
Die Option Rückwärtssuche kann besonders nützlich sein. Erinnern Sie sich, dass wir zuvor eine

Funktion namens Quadrat. Lassen Sie uns unsere Python-Historie von einem neuen IPython aus rückwärts durchsuchen

Shell und finden Sie diese Definition wieder. Wenn Sie Strg-r im IPython-Terminal drücken,

sehen Sie die folgende Aufforderung:

In [1]:
(reverse-i-search)`':

Wenn Sie an dieser Eingabeaufforderung mit der Eingabe von Zeichen beginnen, füllt IPython automatisch den letzten
Befehl, falls vorhanden, der mit diesen Zeichen übereinstimmt:

In [1]:
(Reverse-i-search)`sqa': square??

Sie können jederzeit weitere Zeichen hinzufügen, um die Suche zu verfeinern, oder erneut Strg-r drücken

um weiter nach einem anderen Befehl zu suchen, der der Abfrage entspricht. Wenn Sie vorhin mitgemacht haben
früher verfolgt haben, ergibt zweimaliges Drücken von Ctrl-r mehr:

In [1]:
(reverse-i-search)`sqa': def square(a):
"""Liefert das Quadrat von a"""
return a ** 2

Wenn Sie den gesuchten Befehl gefunden haben, drücken Sie die Eingabetaste und die Suche
wird beendet. Sie können dann den gefundenen Befehl verwenden und mit Ihrer Sitzung fortfahren:

In [1]: def square(a):
"""Return the square of a"""
return a ** 2

In [2]: Quadrat(2)
Out[2]: 4

Beachten Sie, dass Sie mit Strg-p/Strg-n oder den Pfeiltasten nach oben/unten die

Ihre Historie auf ähnliche Weise, aber nur durch den Abgleich von Zeichen am Anfang der

Zeile. Das heißt, wenn Sie def eingeben und dann Strg-p drücken, wird die letzte Kom-

mand (falls vorhanden) in Ihrer Geschichte, die mit dem Zeichen def beginnt.

Tastaturkürzel in der IPython-Shell | 11
Verschiedene Tastenkombinationen
Schließlich gibt es noch ein paar verschiedene Tastenkombinationen, die in keine der vorangegangenen Kategorien passen

Kategorien, sind aber dennoch nützlich zu wissen:

Tastendruck Aktion
Ctrl-l Terminalbildschirm löschen
Ctrl-c Aktuellen Python-Befehl unterbrechen
Strg-d Beenden der IPython-Sitzung
Insbesondere die Tastenkombination Strg-c kann nützlich sein, wenn Sie versehentlich einen sehr

langwierige Arbeit.

Auch wenn einige der hier besprochenen Abkürzungen auf den ersten Blick etwas obskur erscheinen mögen, so sind sie doch

werden mit etwas Übung schnell automatisch. Sobald Sie dieses Muskelgedächtnis entwickelt haben, kann ich

Vermutlich werden Sie sich sogar wünschen, sie wären in anderen Zusammenhängen verfügbar.

12 | Kapitel 1: Erste Schritte in IPython und Jupyter

KAPITEL 2

Verbesserte interaktive Funktionen
Ein Großteil der Leistungsfähigkeit von IPython und Jupyter ergibt sich aus den zusätzlichen interaktiven
Werkzeuge, die sie zur Verfügung stellen. Dieses Kapitel behandelt eine Reihe dieser Werkzeuge, darunter

so genannte magische Befehle, Werkzeuge zur Untersuchung des Eingabe- und Ausgabeverlaufs und Werkzeuge zur

mit der Shell interagieren.

Magische IPython-Befehle
Das vorherige Kapitel hat gezeigt, wie Sie mit IPython Python effizient und interaktiv nutzen und erforschen können.
effizient und interaktiv nutzen und erforschen können. Hier werden wir einige der Erweiterungen besprechen, die

IPython ergänzt die normale Python-Syntax. Diese sind in IPython bekannt als

magische Befehle, denen das Zeichen % vorangestellt ist. Diese magischen Befehle sind

entwickelt, um verschiedene häufige Probleme bei der Standarddatenanalyse kurz und bündig zu lösen.

Magische Befehle gibt es in zwei Varianten: zeilenmagische Befehle, die mit einem einzigen %
Präfix gekennzeichnet sind und auf eine einzelne Eingabezeile wirken, und Zellmagie, die durch ein

doppelten %%-Präfix und arbeiten mit mehreren Eingabezeilen. Ich demonstriere und diskutiere hier
einige kurze Beispiele demonstrieren und besprechen, und dann auf eine gezieltere Diskussion mehrerer nützlicher

magische Befehle später.

Externen Code ausführen: %run
Wenn Sie anfangen, umfangreicheren Code zu entwickeln, werden Sie wahrscheinlich in

IPython für die interaktive Erkundung sowie einen Texteditor zum Speichern von Code, den Sie
wiederverwenden möchten. Anstatt diesen Code in einem neuen Fenster auszuführen, kann es bequemer sein, das Programm

innerhalb Ihrer IPython-Sitzung. Dies kann mit dem Befehl %run magic geschehen.

13
Stellen Sie sich zum Beispiel vor, Sie haben eine Datei myscript.py mit folgendem Inhalt erstellt:

# Datei: myscript.py
def square(x):
"""Quadrat einer Zahl"""
return x ** 2
for N in range(1, 4):
print(f"{N} quadriert ist {square(N)}")
Sie können dies von Ihrer IPython-Sitzung aus wie folgt ausführen:

In [1]: % myscript.py ausführen
1 zum Quadrat ist 1
2 quadriert ist 4
3 quadriert ist 9

Beachten Sie auch, dass nach dem Ausführen dieses Skripts alle darin definierten Funktionen zur Verfügung stehen
zur Verwendung in Ihrer IPython-Sitzung zur Verfügung stehen:

In [2]: Quadrat(5)
Heraus[2]: 25

Es gibt mehrere Optionen zur Feinabstimmung, wie Ihr Code ausgeführt wird; Sie können die Doku-

auf die normale Art und Weise, indem Sie %run? in den IPython-Interpreter eingeben.

Zeitgesteuerte Code-Ausführung: %timeit
Ein weiteres Beispiel für eine nützliche magische Funktion ist %timeit, die automatisch
die Ausführungszeit der darauf folgenden einzeiligen Python-Anweisung ermittelt. Für

Wir möchten beispielsweise die Leistung eines Listenverstehens überprüfen:

In [3]: % timeit L = [n ** 2 for n in range(1000)]
430 μs ± 3,21 μs pro Schleife (Mittelwert ± Standardabweichung von 7 Läufen mit je 1000 Schleifen)

Der Vorteil von %timeit ist, dass es bei kurzen Befehlen automatisch mehrere Befehle ausführt.

dreimalige Durchläufe, um robustere Ergebnisse zu erzielen. Bei mehrzeiligen Anweisungen ist das Hinzufügen eines

Das zweite %-Zeichen verwandelt dies in eine magische Zelle, die mehrere Eingabezeilen verarbeiten kann.

Hier ist zum Beispiel die entsprechende Konstruktion mit einer for-Schleife:

In [4]: %%timeit
...: L = []
...: for n in range(1000):
...: L.append(n ** 2)
...:
484 μs ± 5,67 μs pro Schleife (Mittelwert ± Standardabweichung von 7 Läufen mit je 1000 Schleifen)

Wir können sofort sehen, dass die Listenverarbeitung etwa 10% schneller ist als die

gleichwertige for-Schleifen-Konstruktion in diesem Fall. Wir werden %timeit und andere

nähert sich dem Timing- und Profiling-Code unter "Profiling- und Timing-Code" auf Seite 26.

14 | Kapitel 2: Verbesserte interaktive Funktionen

Hilfe zu Magie-Funktionen: ?, %magisch, und %lsmagisch
Wie normale Python-Funktionen haben auch die magischen Funktionen von IPython Dokumentationsstrings, und dieser

können nützliche Unterlagen auf die übliche Weise abgerufen werden. So können Sie zum Beispiel

um die Dokumentation der magischen Funktion %timeit zu lesen, geben Sie einfach Folgendes ein:

In [5]: %timeit?

Die Dokumentation für andere Funktionen kann auf ähnliche Weise aufgerufen werden. Um auf eine allgemeine
Beschreibung der verfügbaren magischen Funktionen, einschließlich einiger Beispiele, zu gelangen, können Sie dies eingeben:

In [6]: % magisch

Um eine schnelle und einfache Liste aller verfügbaren magischen Funktionen zu erhalten, geben Sie dies ein:

In [7]: % lsmagic

Abschließend möchte ich noch erwähnen, dass es recht einfach ist, eigene magische Funktionen zu definieren.

wenn Sie es wünschen. Ich werde hier nicht darauf eingehen, aber wenn Sie daran interessiert sind, lesen Sie die Referenzen
die in "Weitere IPython-Ressourcen" auf Seite 31 aufgeführt sind.

Eingabe- und Ausgabeverlauf
Sie haben bereits gesehen, dass die IPython-Shell den Zugriff auf frühere Befehle ermöglicht

mit den Pfeiltasten nach oben und unten oder mit den Tastenkombinationen Strg-p/Strg-n. Zusätzlich
Darüber hinaus bietet IPython sowohl in der Shell als auch in den Notebooks mehrere Möglichkeiten, die

Ausgabe vorheriger Befehle, sowie String-Versionen der Befehle selbst

selbst. Wir werden sie hier erforschen.

IPython's In und Out Objekte
Ich nehme an, dass Sie inzwischen mit dem In [1]:/Out[1]: Stil von
Aufforderungen, die von IPython verwendet werden. Aber es stellt sich heraus, dass diese nicht nur hübsche Dekoration sind:

Sie geben einen Hinweis darauf, wie Sie in Ihrer aktuellen Sitzung auf frühere Ein- und Ausgaben zugreifen können.
Sitzung zugreifen können. Nehmen wir an, wir starten eine Sitzung, die wie folgt aussieht:

In [1]: math importieren

In [2]: math.sin(2)
Out[2]: 0.9092974268256817

In [3]: math.cos(2)
Out[3]: -0.4161468365471424

Wir haben das integrierte Mathematikpaket importiert und dann den Sinus und den Kosinus von

die Nummer 2. Diese Ein- und Ausgänge werden in der Shell mit In/Out-Etiketten angezeigt,

aber das ist noch nicht alles: IPython erstellt einige Python-Variablen namens In und Out

die automatisch aktualisiert werden, um diesen Verlauf widerzuspiegeln:

Eingabe- und Ausgabegeschichte | 15
In [4]: In
Out[4]: ['', 'import math', 'math.sin(2)', 'math.cos(2)', 'In']

In [5]: Out
Out[5]:
{2: 0.9092974268256817,
3: -0.4161468365471424,
4: ['', 'import math', 'math.sin(2)', 'math.cos(2)', 'In', 'Out']}

Das In-Objekt ist eine Liste, in der die Befehle der Reihe nach aufgeführt sind (der erste Eintrag in

die Liste ist ein Platzhalter, damit sich In [1] auf den ersten Befehl beziehen kann):

In [6]: print(In[1])
math importieren

Das Out-Objekt ist keine Liste, sondern ein Wörterbuch, das die eingegebenen Zahlen ihren Ausgaben zuordnet

(falls vorhanden):

In [7]: print(Out[2])
.9092974268256817

Beachten Sie, dass nicht alle Operationen eine Ausgabe haben: z.B. Import-Anweisungen und print

Anweisungen haben keinen Einfluss auf die Ausgabe. Letzteres mag überraschend sein, macht aber Sinn, wenn

Wenn man bedenkt, dass print eine Funktion ist, die keinen Wert zurückgibt, kann man der Einfachheit halber sagen, dass jeder Befehl

die None zurückgibt, wird nicht zu Out hinzugefügt.

Dies kann nützlich sein, wenn Sie mit früheren Ergebnissen interagieren möchten. Nehmen wir zum Beispiel an

Prüfen Sie die Summe von sin(2) ** 2 und cos(2) ** 2 anhand der zuvor berechneten

Ergebnisse:

In [8]: Out[2] ** 2 + Out[3] ** 2
Out[8]: 1.0

Das Ergebnis ist 1,0, wie wir es aufgrund der bekannten trigonometrischen Identität erwarten würden. In diesem

Fall ist es wahrscheinlich nicht notwendig, diese früheren Ergebnisse zu verwenden, aber es kann ziemlich schwierig werden.

praktisch, wenn Sie eine sehr teure Berechnung durchführen und vergessen, das Ergebnis einer
Variable zuzuweisen.

Unterstrichene Tastenkombinationen und vorherige Ausgaben
Die Standard-Python-Shell enthält nur einen einfachen Shortcut für den Zugriff auf vorherige Ausgaben

Ausgabe: Die Variable _ (d. h. ein einzelner Unterstrich) wird mit der vorherigen Ausgabe aktualisiert.
Ausgabe. Dies funktioniert auch in IPython:

In [9]: print(_)
.0

16 | Kapitel 2: Verbesserte interaktive Funktionen

Aber IPython geht noch einen Schritt weiter - Sie können einen doppelten Unterstrich verwenden, um auf die

vorletzte Ausgabe und einen dreifachen Unterstrich, um auf die drittletzte Ausgabe zuzugreifen (Überspringen
ping alle Befehle ohne Ausgabe):

In [10]: print(__)
-0.4161468365471424

In [11]: print(___)
.9092974268256817

IPython hört hier auf: Bei mehr als drei Unterstrichen wird es etwas schwierig zu zählen,

und an diesem Punkt ist es einfacher, sich auf die Ausgabe anhand der Zeilennummer zu beziehen.

Es gibt jedoch noch eine weitere Abkürzung, die ich erwähnen sollte - die Abkürzung für Out[ X ] ist _ X
(d.h. ein einzelner Unterstrich gefolgt von der Zeilennummer):

In [12]: Out[2]
Out[12]: 0.9092974268256817

In [13]: _2
Out[13]: 0.9092974268256817

Unterdrückung der Ausgabe
Manchmal möchten Sie vielleicht die Ausgabe einer Anweisung unterdrücken (dies ist vielleicht
am häufigsten bei den Plotting-Befehlen, die wir in Teil IV untersuchen werden). Oder vielleicht

der Befehl, den Sie ausführen, ein Ergebnis liefert, das Sie lieber nicht in Ihrem

Ausgabehistorie, so dass sie vielleicht freigegeben werden kann, wenn andere Referenzen
entfernt werden. Der einfachste Weg, die Ausgabe eines Befehls zu unterdrücken, ist das Hinzufügen eines Semikolons

bis zum Ende der Leitung:

In [14]: math.sin(2) + math.cos(2);

Das Ergebnis wird stillschweigend berechnet, und die Ausgabe wird weder auf dem Bildschirm angezeigt noch

im Out-Wörterbuch gespeichert:

In [15]: 14 in Out
Out[15]: Falsch

Verwandte magische Befehle
Um einen Stapel früherer Eingaben auf einmal abzurufen, ist der magische Befehl %history

sehr hilfreich. Hier sehen Sie, wie Sie die ersten vier Eingaben ausdrucken können:

In [16]: % Geschichte -n 1-3
1: math importieren
2: math.sin(2)
3: math.cos(2)

Wie üblich können Sie %history? eingeben, um weitere Informationen und eine Beschreibung der Optionen zu erhalten

zur Verfügung (siehe Kapitel 1 für weitere Informationen über die Funktion ?) Andere nützliche Magie

Eingabe- und Ausgabeverlauf | 17
Befehle sind %rerun, wodurch ein Teil der Befehlshistorie erneut ausgeführt wird,

und %save, das einen Teil des Befehlsverlaufs in einer Datei speichert).

IPython und Shell-Befehle
Bei der interaktiven Arbeit mit dem Standard-Python-Interpreter ist eine der Frustra-

tionen ist die Notwendigkeit, zwischen mehreren Fenstern zu wechseln, um auf Python-Tools und System
tem-Befehlszeilen-Tools. IPython überbrückt diese Lücke und gibt Ihnen eine Syntax für

Shell-Befehle direkt aus dem IPython-Terminal auszuführen. Der magische Zufall...

Stifte mit Ausrufezeichen: alles, was nach! in einer Zeile steht, wird ausgeführt

nicht durch den Python-Kernel, sondern durch die Kommandozeile des Systems.

Im Folgenden wird davon ausgegangen, dass Sie mit einem Unix-ähnlichen System arbeiten, z. B. Linux oder

macOS. Einige der folgenden Beispiele werden unter Windows nicht funktionieren, da hier standardmäßig eine andere Shell verwendet wird.
eine andere Art von Shell verwendet, aber wenn Sie das Windows Subsystem für Linux verwenden, wird die Prüfung

ples hier sollten korrekt ausgeführt werden. Wenn Sie mit Shell-Befehlen nicht vertraut sind, würde ich vorschlagen

das Unix-Shell-Tutorial, das von der stets ausgezeichneten Software Car-
pentry Foundation zusammengestellt wurde.

Kurze Einführung in die Shell
Eine vollständige Einführung in die Verwendung der Shell/Terminals/Befehlszeile würde den Rahmen dieses Kapitels bei weitem sprengen.
Rahmen dieses Kapitels, aber für die Uneingeweihten werde ich hier eine kurze Einführung geben.

Die Shell ist eine Möglichkeit zur textuellen Interaktion mit Ihrem Computer. Seit Mitte der 1980er Jahre,
als Microsoft und Apple die ersten Versionen ihrer inzwischen allgegenwärtigen

grafischen Betriebssystemen haben die meisten Computerbenutzer mit ihren Betriebssystemen interagiert.

Systeme durch die vertrauten Menüauswahlen und Drag-and-Drop-Bewegungen.
Aber Betriebssysteme gab es schon lange vor diesen grafischen Benutzeroberflächen, und sie waren

in erster Linie durch Texteingabefolgen gesteuert: An der Eingabeaufforderung gibt der Benutzer
einen Befehl ein, und der Computer tat, was der Benutzer ihm befahl. Diese frühen

Prompt-Systeme waren die Vorläufer der Shells und Terminals, die die meisten Datenwissenschaftler

die auch heute noch verwendet werden.

Jemand, der mit der Shell nicht vertraut ist, könnte sich fragen, warum man sich die Mühe machen sollte, wenn

können viele der gleichen Ergebnisse durch einfaches Anklicken von Symbolen und Menüs erreicht werden.
Menüs. Ein Shell-Benutzer könnte mit einer anderen Frage antworten: Warum nach Symbolen und Menüs suchen

wenn man Dinge viel einfacher durch Tippen erledigen kann? Es mag zwar

wie eine typische technische Sackgasse klingen, aber wenn man über grundlegende Aufgaben hinausgeht, wird
wird schnell klar, dass die Shell viel mehr Kontrolle über fortgeschrittene Aufgaben bietet -

obwohl die Lernkurve zugegebenermaßen einschüchternd sein kann.

Hier ist ein Beispiel für eine Linux/macOS-Shell-Sitzung, bei der ein Benutzer

erforscht, erstellt und verändert Verzeichnisse und Dateien auf ihrem System (osx:~ $ ist die

18 | Kapitel 2: Verbesserte interaktive Funktionen

Prompt, und alles nach dem $ ist der eingegebene Befehl; Text, dem ein #
vorangestellt ist, ist nur als Beschreibung gedacht und nicht als etwas, das Sie tatsächlich eintippen würden):

osx:~ $ echo "hello world" # echo ist wie Pythons print-Funktion
hallo Welt
osx:~ $ pwd # pwd = Arbeitsverzeichnis drucken
/home/jake # Dies ist der "Pfad", in dem wir uns befinden.
osx:~ $ ls # ls = Inhalt des Arbeitsverzeichnisses auflisten
notebooks projekte
osx:~ $ cd projects/ # cd = Verzeichnis wechseln
osx:projects $ pwd
/home/jake/projects
osx:projekte $ ls
datasci_book mpld3 meinprojekt.txt
osx:projects $ mkdir myproject # mkdir = neues Verzeichnis anlegen
osx:projects $ cd myproject/
osx:myproject $ mv ../myproject.txt ./ # mv = Datei verschieben. Hier verschieben wir die
# Datei myproject.txt von einem Verzeichnis
# nach oben (../) in das aktuelle Verzeichnis (./).
osx:myproject $ ls
myproject.txt
Beachten Sie, dass dies alles nur ein kompakter Weg ist, um bekannte Operationen auszuführen (Navigieren in einem

Verzeichnisstruktur, Erstellen eines Verzeichnisses, Verschieben einer Datei usw.) durch Eingabe von Befehlen

statt auf Icons und Menüs zu klicken. Mit nur wenigen Befehlen (pwd, ls, cd, mkdir,

und cp) können Sie viele der gebräuchlichsten Datei-Operationen durchführen, aber es ist, wenn Sie sich

über diese Grundlagen hinaus wird der Shell-Ansatz wirklich leistungsfähig.

Shell-Befehle in IPython
Jeder Standard-Shell-Befehl kann direkt in IPython verwendet werden, indem man ihm das Präfix

das! Zeichen. Die Befehle ls, pwd und echo können zum Beispiel wie folgt ausgeführt werden:

In [1]: !ls
myproject.txt

In [2]: !pwd
/home/jake/projects/myproject

In [3]: !echo "Drucken aus der Shell"
Drucken von der Shell aus

IPython und Shell-Befehle | 19
Übergabe von Werten an und von der Shell
Shell-Befehle können nicht nur von IPython aus aufgerufen werden, sondern sie können auch in die

mit dem IPython-Namensraum agieren. Sie können zum Beispiel die Ausgabe einer beliebigen Shell speichern

Befehl mit dem Zuweisungsoperator = in eine Python-Liste einfügen:

In [4]: Inhalt = !ls

In [5]: print(contents)
['myproject.txt']

In [6]: Verzeichnis = !pwd

In [7]: print(directory)
['/Users/jakevdp/notebooks/tmp/myproject']

Diese Ergebnisse werden nicht als Listen zurückgegeben, sondern als ein spezieller Shell-Rückgabetyp, der in

IPython:

In [8]: type(directory)
IPython.utils.text.SList

Sie sieht aus wie eine Python-Liste und verhält sich auch so, hat aber zusätzliche Funktionen, wie zum Beispiel die

grep- und fields-Methoden sowie die s-, n- und p-Eigenschaften, mit denen Sie suchen, fil-

ter und zeigen die Ergebnisse auf praktische Weise an. Für weitere Informationen dazu können Sie
können Sie die in IPython eingebauten Hilfefunktionen verwenden.

Die Kommunikation in die andere Richtung - Übergabe von Python-Variablen an die Shell - ist

mit der Syntax { varname } möglich:

In [9]: message = "hallo von Python"

In [10]: !echo {message}
hallo von Python

Die geschweiften Klammern enthalten den Variablennamen, der im Shell-Befehl durch den Variablenkontext ersetzt wird.
im Shell-Befehl ersetzt wird.

Shell-bezogene magische Befehle
Wenn Sie eine Weile mit den Shell-Befehlen von IPython spielen, werden Sie feststellen, dass Sie-

nicht !cd verwenden, um im Dateisystem zu navigieren:

In [11]: !pwd
/home/jake/projects/myproject

In [12]: !cd ..

In [13]: !pwd
/home/jake/projects/myproject

20 | Kapitel 2: Verbesserte interaktive Funktionen

Der Grund dafür ist, dass die Shell-Befehle im Notebook in einem temporären Unterverzeichnis ausgeführt werden.

Shell, die ihren Zustand nicht von Befehl zu Befehl beibehält. Wenn Sie Folgendes ändern möchten

das Arbeitsverzeichnis auf dauerhaftere Weise zu ändern, können Sie den Befehl %cd magic verwenden:

In [14]: % cd ..
/home/jake/projects

Standardmäßig können Sie diese sogar ohne das %-Zeichen verwenden:

In [15]: cd myproject
/home/jake/projects/myproject

Dies ist eine so genannte automatische Funktion, und die Fähigkeit, solche Befehle auszuführen

ohne ein explizites % kann mit der magischen Funktion %automagic umgeschaltet werden.

Neben %cd gibt es weitere shell-ähnliche magische Funktionen wie %cat, %cp, %env, %ls und %man,

%mkdir, %more, %mv, %pwd, %rm und %rmdir, die alle ohne das % verwendet werden können

Zeichen, wenn Automagic eingeschaltet ist. Dies macht es so, dass Sie fast behandeln können die IPython

als wäre es eine normale Shell:

In [16]: mkdir tmp

In [17]: ls
myproject.txt tmp/

In [18]: cp myproject.txt tmp/

In [19]: ls tmp
meinProjekt.txt

In [20]: rm -r tmp

Dieser Zugriff auf die Shell aus demselben Terminalfenster wie Ihre Python-Sitzung
können Sie Python und die Shell in Ihren Arbeitsabläufen natürlicher kombinieren mit

weniger Kontextwechsel.

IPython und Shell-Befehle | 21
KAPITEL 3

Fehlersuche und Profiling
Zusätzlich zu den erweiterten interaktiven Werkzeugen, die im vorigen Kapitel besprochen wurden, bietet Jupyter eine Reihe von Möglichkeiten, den Code zu untersuchen und zu verstehen,
Jupyter bietet eine Reihe von Möglichkeiten, den Code, den Sie ausführen, zu untersuchen und zu verstehen.

ning, z. B. durch Aufspüren von Fehlern in der Logik oder unerwartet langsamer Ausführung. Diese

In diesem Kapitel werden einige dieser Instrumente vorgestellt.

Fehler und Fehlersuche
Die Entwicklung von Code und die Analyse von Daten erfordern immer ein wenig Versuch und Irrtum, und IPy-
thon enthält Werkzeuge zur Rationalisierung dieses Prozesses. Dieser Abschnitt behandelt kurz einige

Optionen zur Steuerung der Python-Ausnahmeberichte, gefolgt von der Untersuchung von Werkzeugen zur
Fehlerbeseitigung im Code.

Ausnahmen kontrollieren: %xmode
Wenn ein Python-Skript fehlschlägt, löst es in den meisten Fällen eine Ausnahme aus. Wenn der Inter-
preter auf eine dieser Ausnahmen stößt, können Informationen über die Ursache des Fehlers

im Traceback gefunden, auf den von Python aus zugegriffen werden kann. Mit der %xmode
können Sie in IPython die Menge der ausgegebenen Informationen steuern

wenn die Ausnahme ausgelöst wird. Betrachten Sie den folgenden Code:

In [1]: def func1(a, b):
return a / b

def func2(x):
a = x
b = x - 1
return func1(a, b)

In [2]: func2(1)
ZeroDivisionError Traceback (jüngster Aufruf zuletzt)

22
<ipython-input-2-b2e110f6fc8f> in ()
----> 1 func2(1)

<ipython-input-1-d849e34d61fb> in func2(x)
5 a = x
6 b = x - 1
----> 7 return func1(a, b)

<ipython-input-1-d849e34d61fb> in func1(a, b)
1 def func1(a, b):
----> 2 return a / b
3
4 def func2(x):
5 a = x

ZeroDivisionError : Division durch Null

Der Aufruf von func2 führt zu einem Fehler, und anhand des gedruckten Traces können wir genau sehen, was

passiert. Im Standardmodus enthält dieser Trace mehrere Zeilen, die den Kontext zeigen

der einzelnen Schritte, die zu dem Fehler geführt haben. Mit der magischen Funktion %xmode (kurz für excep-
(kurz für Ausnahmemodus) können wir ändern, welche Informationen gedruckt werden.

%xmode benötigt ein einziges Argument, den Modus, und es gibt drei Möglichkeiten: Einfach,

Kontext und Ausführlich. Die Standardeinstellung ist Context, was eine Ausgabe wie diese ergibt

gezeigt. Einfarbig ist kompakter und liefert weniger Informationen:

In [3]: % xmode Plain
Out[3]: Modus für die Meldung von Ausnahmen: Klartext

In [4]: func2(1)
Traceback (jüngster Aufruf zuletzt):

Datei "<ipython-input-4-b2e110f6fc8f>", Zeile 1, in
func2(1)

Datei "<ipython-input-1-d849e34d61fb>", Zeile 7, in func2
return func1(a, b)

Datei "<ipython-input-1-d849e34d61fb>", Zeile 2, in func1
return a / b

ZeroDivisionError : Division durch Null

Der Verbose-Modus fügt einige zusätzliche Informationen hinzu, einschließlich der Argumente für alle

Funktionen, die aufgerufen werden:

In [5]: % xmode Ausführlich
Aus [5]: Ausnahmemeldungsmodus: Ausführlich

In [6]: func2(1)
ZeroDivisionError Traceback (jüngster Aufruf zuletzt)
<ipython-input-6-b2e110f6fc8f> in ()

Fehler und Fehlersuche | 23
----> 1 func2(1)
global func2 = <Funktion func2 bei 0x103729320>

<ipython-input-1-d849e34d61fb> in func2(x=1)
5 a = x
6 b = x - 1
----> 7 return func1(a, b)
global func1 = <Funktion func1 bei 0x1037294d0>
a = 1
b = 0

<ipython-input-1-d849e34d61fb> in func1(a=1, b=0)
1 def func1(a, b):
----> 2 return a / b
a = 1
b = 0
3
4 def func2(x):
5 a = x

ZeroDivisionError : Division durch Null

Diese zusätzlichen Informationen können Ihnen helfen, die Ursache der Ausnahme einzugrenzen.

Warum also nicht die ganze Zeit den Verbose-Modus verwenden? Wenn der Code kompliziert wird, kann diese Art von
Traceback extrem lang werden. Je nach Kontext kann die Kürze von

Der Modus "Einfach" oder "Kontext" ist einfacher zu handhaben.

Fehlersuche: Wenn das Lesen von Tracebacks nicht ausreicht
Das Standard-Python-Werkzeug für die interaktive Fehlersuche ist pdb, der Python-Debugger. Dieses

Debugger kann der Benutzer Zeile für Zeile durch den Code gehen, um zu sehen, was möglicherweise

was einen schwierigeren Fehler verursacht. Die IPython-erweiterte Version davon ist ipdb, die

IPython-Debugger.

Es gibt viele Möglichkeiten, diese beiden Debugger zu starten und zu verwenden; wir werden sie nicht behandeln

vollständig hier. Weitere Informationen finden Sie in der Online-Dokumentation zu diesen beiden Dienstprogrammen.

In IPython ist die vielleicht bequemste Schnittstelle zum Debuggen die %debug-Magie

Befehl. Wenn Sie ihn aufrufen, nachdem Sie auf eine Ausnahme gestoßen sind, öffnet er automatisch ein Inter-

aktive Debugging-Eingabeaufforderung an der Stelle, an der die Ausnahme auftritt. Mit der ipdb-Eingabeaufforderung können Sie

den aktuellen Zustand des Stacks erkunden, die verfügbaren Variablen untersuchen und sogar
Python-Befehle ausführen!

Schauen wir uns die letzte Ausnahme an und erledigen dann einige grundlegende Aufgaben. Wir drucken den Wert...

Werte von a und b und geben Sie dann quit ein, um die Debugging-Sitzung zu beenden:

In [7]: % debug <ipython-input-1-d849e34d61fb>(2)func1()
1 def func1(a, b):
----> 2 return a / b
3

24 | Kapitel 3: Debugging und Profiling

ipdb> print(a)
1
ipdb> print(b)
0
ipdb> quit

Der interaktive Debugger erlaubt aber noch viel mehr als das - wir können sogar noch weiter gehen

und nach unten durch den Stapel und erkunden Sie die Werte der Variablen dort:

In [8]: % debug <ipython-input-1-d849e34d61fb>(2)func1()
1 def func1(a, b):
----> 2 return a / b
3

ipdb> up <ipython-input-1-d849e34d61fb>(7)func2()
5 a = x
6 b = x - 1
----> 7 return func1(a, b)

ipdb> print(x)
1
ipdb> up <ipython-input-6-b2e110f6fc8f>(1)()
----> 1 func2(1)

ipdb> down <ipython-input-1-d849e34d61fb>(7)func2()
5 a = x
6 b = x - 1
----> 7 return func1(a, b)

ipdb> Beenden

So können wir nicht nur schnell herausfinden, was den Fehler verursacht hat, sondern auch, welche Funktionsaufrufe
Funktionsaufrufe zu dem Fehler geführt haben.

Wenn Sie möchten, dass der Debugger automatisch gestartet wird, wenn eine Ausnahme ausgelöst wird,

können Sie die Funktion %pdb magic verwenden, um dieses automatische Verhalten zu aktivieren:

In [9]: % xmode Plain
% pdb ein
func2(1)
Modus für die Meldung von Ausnahmen: Einfach
Automatischer pdb-Aufruf wurde EINgeschaltet
ZeroDivisionError : Division durch Null <ipython-input-1-d849e34d61fb>(2)func1()
1 def func1(a, b):
----> 2 return a / b
3

ipdb> print(b)
0
ipdb> quit

Fehler und Fehlersuche | 25
Wenn Sie ein Skript haben, das Sie von Anfang an im interaktiven Modus ausführen möchten, können Sie

Modus können Sie es mit dem Befehl %run -d ausführen und mit dem nächsten Befehl den Schritt

interaktiv durch die Codezeilen.

Es gibt noch viel mehr Befehle für das interaktive Debugging, als ich hier gezeigt habe
hier gezeigt habe. Tabelle 3-1 enthält eine Beschreibung einiger der gängigsten und nützlichsten Befehle.

Tabelle 3-1. Unvollständige Liste der Debugging-Befehle

Befehl Beschreibung
l(ist) Zeigt die aktuelle Position in der Datei an
h(elp) Zeigt eine Liste von Befehlen an, oder sucht Hilfe zu einem bestimmten Befehl
q(uit) Beendet den Debugger und das Programm
c(ontinue) Beendet den Debugger und fährt im Programm fort
n(ext) Zum nächsten Schritt des Programms gehen
<enter> Wiederholung des vorherigen Befehls
p(rint) Variablen ausgeben
s(tep) Schritt in ein Unterprogramm
r(eturn) Rückkehr aus einem Unterprogramm
Weitere Informationen erhalten Sie mit dem Hilfe-Befehl im Debugger, oder schauen Sie unter

die Online-Dokumentation von ipdb.

Profiling und Zeitmessung von Code
Bei der Entwicklung von Code und der Erstellung von Datenverarbeitungspipelines gibt es
gibt es oft Kompromisse zwischen verschiedenen Implementierungen, die Sie eingehen können. Zu Beginn der Entwicklung

Ihr Algorithmus, kann es kontraproduktiv sein, sich über solche Dinge Gedanken zu machen. Wie Donald

Knuth witzelte: "Wir sollten kleine Effizienzen vergessen, etwa 97% der Zeit.
der Zeit: Verfrühte Optimierung ist die Wurzel allen Übels."

Aber sobald Ihr Code funktioniert, kann es nützlich sein, seine Effizienz ein wenig zu untersuchen.
Manchmal ist es sinnvoll, die Ausführungszeit eines bestimmten Befehls oder einer Gruppe von Befehlen zu prüfen.

mands; in anderen Fällen ist es nützlich, einen mehrzeiligen Prozess zu untersuchen und festzustellen, wo die

Der Engpass liegt in einer komplizierten Reihe von Operationen. IPython bietet Zugang zu einer
eine breite Palette von Funktionen für diese Art von Timing und Profiling von Code. Hier werden wir

die folgenden magischen IPython-Befehle diskutieren:

%Zeit

Zeit für die Ausführung einer einzelnen Anweisung

%timeit
Zeit für die wiederholte Ausführung einer einzelnen Anweisung für mehr Genauigkeit

26 | Kapitel 3: Fehlersuche und Profiling

%prun
Code mit dem Profiler ausführen

%lprun
Code mit dem Line-by-Line-Profiler ausführen

%memit

Messung des Speicherverbrauchs einer einzelnen Anweisung

%mprun

Code mit dem zeilenweisen Speicher-Profiler ausführen

Die letzten vier Befehle sind nicht im Lieferumfang von IPython enthalten; um sie zu verwenden, müssen Sie

die Erweiterungen line_profiler und memory_profiler, die wir in den folgenden Abschnitten besprechen
folgenden Abschnitten besprechen.

Code-Schnipsel zur Zeitmessung: %timeit und %time
Wir haben die %timeit-Zeilenmagie und %%timeit-Zellenmagie in der Einführung in die Magie gesehen

Funktionen in "IPython Magic Commands" auf Seite 13; diese können verwendet werden, um die
wiederholte Ausführung von Codeschnipseln:

In [1]: % timeit Summe(Bereich(100))
1,53 μs ± 47,8 ns pro Schleife (Mittelwert ± Standardabweichung von 7 Läufen mit jeweils 1000000 Schleifen)

Beachten Sie, dass %timeit aufgrund der Schnelligkeit dieses Vorgangs automatisch eine große Anzahl von

der Wiederholungen. Bei langsameren Befehlen wird %timeit automatisch angepasst und führt

weniger Wiederholungen:

In [2]: %%timeit
gesamt = 0
for i in range(1000):
for j in range(1000):
gesamt += i * (-1) ** j
536 ms ± 15,9 ms pro Schleife (Mittelwert ± Standardabweichung von 7 Läufen, je 1 Schleife)

Manchmal ist die Wiederholung einer Operation nicht die beste Option. Zum Beispiel, wenn wir eine
Liste haben, die wir sortieren möchten, könnten wir durch eine wiederholte Operation in die Irre geführt werden; das Sortieren einer Vor

sortierte Liste ist viel schneller als die Sortierung einer unsortierten Liste, so dass die Wiederholungen das
Ergebnis:

In [3]: import random
L = [random.random() for i in range(100000)]
% timeit L.sort()
Out[3]: 1,71 ms ± 334 μs pro Schleife (Mittelwert ± std. Abweichung von 7 Durchläufen, je 1000 Schleifen)

Profiling und Zeitmessung von Code | 27
Hierfür ist die magische Funktion %time möglicherweise die bessere Wahl. Sie ist auch eine gute Wahl für
länger laufende Befehle, wenn kurze, systembedingte Verzögerungen wahrscheinlich keine Auswirkungen auf

das Ergebnis. Lassen Sie uns die Sortierung einer unsortierten und einer vorsortierten Liste überprüfen:

In [4]: import random
L = [random.random() for i in range(100000)]
print("Sortieren einer unsortierten Liste:")
% time L.sort()
Out[4]: Sortieren einer unsortierten Liste:
CPU-Zeiten: user 31.3 ms, sys: 686 μs, gesamt: 32 ms
Wandzeit: 33.3 ms

In [5]: print("Sortieren einer bereits sortierten Liste:")
% Zeit L.sort()
Out[5]: Sortieren einer bereits sortierten Liste:
CPU-Zeiten: user 5.19 ms, sys: 268 μs, total: 5.46 ms
Wandzeit: 14.1 ms

Beachten Sie, wie viel schneller die vorsortierte Liste sortiert werden kann, aber auch wie viel länger

die Zeitmessung mit %time gegenüber %timeit, sogar für die vorsortierte Liste! Dies ist eine

Das liegt daran, dass %timeit einige clevere Dinge unter der Haube macht, um zu verhindern, dass Sys-

tem-Aufrufe das Timing nicht beeinträchtigen. So wird zum Beispiel verhindert, dass die Aufräumarbeiten von ungenutzten

Python-Objekte (bekannt als Garbage Collection), die sonst das Timing beeinträchtigen könnten.

Aus diesem Grund sind die %timeit-Ergebnisse in der Regel deutlich schneller als die %time-Ergebnisse.

Für %time, wie auch für %timeit, ermöglicht die Verwendung der magischen Syntax für %%-Zellen das Timing von mehrzeiligen

Skripte:

In [6]: %%zeit
gesamt = 0
for i in range(1000):
for j in range(1000):
gesamt += i * (-1) ** j
CPU-Zeiten: user 655 ms, sys: 5.68 ms, gesamt: 661 ms
Wandzeit: 710 ms

Weitere Informationen über %time und %timeit sowie die verfügbaren Optionen finden Sie unter

die IPython-Hilfefunktion (z. B. geben Sie %time? am IPython-Prompt ein).

Profiling vollständiger Skripte: %prun
Ein Programm besteht aus vielen einzelnen Anweisungen, und manchmal ist es wichtiger, diese
Diese Zustände im Kontext zu verfolgen ist manchmal wichtiger als sie einzeln zu verfolgen. Python enthält

einen eingebauten Code-Profiler (den Sie in der Python-Dokumentation nachlesen können), aber

IPython bietet eine viel bequemere Möglichkeit, diesen Profiler zu verwenden, und zwar in Form der

magische Funktion %prun.

Als Beispiel werden wir eine einfache Funktion definieren, die einige Berechnungen durchführt:

28 | Kapitel 3: Debugging und Profiling

In [7]: def sum_of_lists(N):
Summe = 0
for i in range(5):
L = [j ^ (j >> i) for j in range(N)]
Summe += Summe(L)
Summe zurückgeben

Jetzt können wir %prun mit einem Funktionsaufruf aufrufen, um die profilierten Ergebnisse zu sehen:

In [8]: % prun sum_of_lists(1000000)
14 Funktionsaufrufe in 0,932 Sekunden
Geordnet nach: interne Zeit
nAufrufe tottime proAufruf cumtime proAufruf filename:lineno(function)
5 0,808 0,162 0,808 0,162 <ipython-input-7-f105717832a2>:4()
5 0.066 0.013 0.066 0.013 {eingebaute Methode builtins.sum}
1 0.044 0.044 0.918 0.918 <ipython-input-7-f105717832a2>:1

(sum_of_lists)
1 0.014 0.014 0.932 0.932 :1()
1 0.000 0.000 0.932 0.932 {eingebaute Methode builtins.exec}
1 0.000 0.000 0.000 0.000 {Methode 'disable' von '_lsprof.Profiler'
Objekte}

Das Ergebnis ist eine Tabelle, die in der Reihenfolge der Gesamtzeit für jeden Funktionsaufruf angibt, wo

die Ausführung die meiste Zeit in Anspruch nimmt. In diesem Fall ist der Großteil der Ausführungszeit

in der Listenauffassung innerhalb von sum_of_lists. Von hier aus könnten wir anfangen zu überlegen
darüber nachdenken, welche Änderungen wir vornehmen könnten, um die Leistung des Algorithmus zu verbessern.

Weitere Informationen zu %prun und den verfügbaren Optionen finden Sie in der IPython-Hilfe

Funktionalität (d.h. geben Sie %prun? an der IPython-Eingabeaufforderung ein).

Zeilenweise Profilerstellung mit %lprun
Die funktionsweise Profilerstellung von %prun ist nützlich, aber manchmal ist es sinnvoller, einen
aber manchmal ist es sinnvoller, einen zeilenweisen Profilbericht zu erhalten. Dies ist nicht in Python oder IPython eingebaut,

aber es gibt ein line_profiler-Paket zur Installation, das diese Aufgabe übernehmen kann. Starten Sie

indem Sie das line_profiler-Paket mit dem Python-Paketierungswerkzeug pip installieren:

$ pip install line_profiler
Als nächstes können Sie IPython verwenden, um die IPython-Erweiterung line_profiler zu laden, die als

Teil dieses Pakets:

In [9]: % load_ext line_profiler

Mit dem Befehl %lprun können Sie nun ein zeilenweises Profiling einer beliebigen Funktion durchführen. In diesem

Fall müssen wir explizit angeben, welche Funktionen wir profilieren wollen:

In [10]: % lprun -f sum_of_lists sum_of_lists(5000)
Timer-Einheit: 1e-06 s

Gesamtzeit: 0.014803 s
File: <ipython-input-7-f105717832a2>

Profiling und Zeitmessung Code | 29
Funktion: sum_of_lists in Zeile 1

Zeile # Treffer Zeit pro Treffer % Zeit Zeileninhalt
1 def sum_of_lists(N):
2 1 6.0 6.0 0.0 Summe = 0
3 6 13.0 2.2 0.1 for i in range(5):
4 5 14242.0 2848.4 96.2 L = [j ^ (j >> i) for j
5 5 541,0 108,2 3,7 gesamt += Summe(L)
6 1 1.0 1.0 0.0 return gesamt

Die Informationen am oberen Rand geben uns den Schlüssel zum Lesen der Ergebnisse: die Zeit wird angegeben

in Mikrosekunden, und wir können sehen, wo das Programm die meiste Zeit verbringt. Unter

können wir diese Informationen möglicherweise nutzen, um Aspekte des Skripts zu ändern und
damit es für unseren gewünschten Anwendungsfall besser funktioniert.

Weitere Informationen zu %lprun und den verfügbaren Optionen finden Sie in der IPython-Hilfe

Funktionalität (d.h. geben Sie %lprun? an der IPython-Eingabeaufforderung ein).

Profiling des Speicherverbrauchs: %memit und %mprun
Ein weiterer Aspekt der Profilerstellung ist die Menge an Speicher, die eine Operation verbraucht. Dies kann sein

mit einer anderen IPython-Erweiterung, dem memory_profiler, ausgewertet. Wie bei dem

line_profiler, beginnen wir mit der Pip-Installation der Erweiterung:

$ pip install memory_profiler
Dann können wir IPython verwenden, um es zu laden:

In [11]: % load_ext memory_profiler

Die Speicherprofilerweiterung enthält zwei nützliche magische Funktionen: %memit (die

bietet ein speichermessendes Äquivalent zu %timeit) und %mprun (das ein

das speichermessende Äquivalent zu %lprun). Die magische Funktion %memit kann verwendet werden

ziemlich einfach:

In [12]: % memit sum_of_lists(1000000)
Spitzenspeicher: 141.70 MiB, Inkrement: 75.65 MiB

Wir sehen, dass diese Funktion etwa 140 MB an Speicherplatz benötigt.

Für eine zeilenweise Beschreibung des Speicherverbrauchs können wir die magische Funktion %mprun verwenden.
Leider funktioniert dies nur für Funktionen, die in separaten Modulen definiert sind, und nicht für

das Notebook selbst, also beginnen wir mit der %%Dateizellen-Magie, um eine einfache

Modul namens mprun_demo.py, das unsere Funktion sum_of_lists enthält, mit einem
Hinzufügung, die unsere Ergebnisse der Speicherprofilierung deutlicher macht:

In [13]: %%file mprun_demo.py
def sum_of_lists(N):
Summe = 0

30 | Kapitel 3: Debugging und Profiling

for i in range(5):
L = [j ^ (j >> i) for j in range(N)]
Summe += Summe(L)
del L # Referenz auf L entfernen
return gesamt
Überschreiben von mprun_demo.py

Wir können nun die neue Version dieser Funktion importieren und die Speicherzeile ausführen

Profiler:

In [14]: from mprun_demo import sum_of_lists
% mprun -f sum_of_lists sum_of_lists(1000000)

Dateiname: /Users/jakevdp/github/jakevdp/PythonDataScienceHandbook/notebooks_v2/

m prun_demo.py

Zeile # Mem usage Increment Occurrences Zeile Inhalt
1 66.7 MiB 66.7 MiB 1 def sum_of_lists(N):
2 66.7 MiB 0.0 MiB 1 total = 0
3 75.1 MiB 8.4 MiB 6 for i in range(5):
4 105,9 MiB 30,8 MiB 5000015 L = [j ^ (j >> i) for j
5 109,8 MiB 3,8 MiB 5 gesamt += Summe(L)
6 75,1 MiB -34,6 MiB 5 del L # Referenz auf L entfernen
7 66,9 MiB -8,2 MiB 1 return gesamt

Hier gibt die Spalte "Increment" an, wie sehr jede Zeile den Gesamtspeicher beeinflusst

Budget: Beachten Sie, dass wir beim Erstellen und Löschen der Liste L etwa 30 MB hinzufügen.

an Speicherverbrauch. Dies ist zusätzlich zum Hintergrundspeicherverbrauch durch die Python

Interpreter selbst.

Weitere Informationen zu %memit und %mprun sowie zu den verfügbaren Optionen finden Sie unter

die IPython-Hilfefunktion (z. B. %memit? am IPython-Prompt eingeben).

Weitere IPython-Ressourcen
In diesen Kapiteln haben wir nur an der Oberfläche gekratzt, um IPython für die Datenverarbeitung zu nutzen

wissenschaftliche Aufgaben. Viele weitere Informationen sind sowohl in gedruckter Form als auch im Internet verfügbar, und
Ich werde hier einige andere Quellen auflisten, die Sie vielleicht hilfreich finden.

Web-Ressourcen
Die IPython-Website
Die IPython-Website bietet Links zur Dokumentation, zu Beispielen, Tutorials und einer
einer Vielzahl von anderen Ressourcen.

Weitere IPython-Ressourcen | 31
Die nbviewer-Website

Diese Seite zeigt statische Renderings aller im Internet verfügbaren Jupyter-Notizbücher.
Netz verfügbar sind. Auf der Titelseite finden Sie einige Beispiel-Notizbücher, die Sie durchstöbern können, um zu sehen
um zu sehen, wofür andere Leute IPython verwenden!
Eine kuratierte Sammlung von Jupyter-Notizbüchern
Diese ständig wachsende Liste von Jupyter-Notizbüchern, die von nbviewer unterstützt werden, zeigt die Tiefe und
Breite der numerischen Analyse, die Sie mit IPython durchführen können. Sie enthält alles
von kurzen Beispielen und Tutorials bis hin zu kompletten Kursen und Büchern, die im
dem Notebook-Format!

Video-Tutorials
Wenn Sie das Internet durchsuchen, werden Sie viele Video-Tutorials zu IPython finden. Ich würde besonders
Ich empfehle insbesondere die Tutorials der Konferenzen PyCon, SciPy und PyData
Konferenzen von Fernando Perez und Brian Granger, zwei der Hauptentwickler und
von IPython und Jupyter.

Bücher
Python für die Datenanalyse (O'Reilly)

Das Buch von Wes McKinney enthält ein Kapitel, das die Verwendung von IPython als Datenwissenschaftler
entist. Obwohl sich ein Großteil des Materials mit dem überschneidet, was wir hier besprochen haben,
ist eine andere Perspektive immer hilfreich.
Learning IPython for Interactive Computing and Data Visualization (Packt)
Dieses kurze Buch von Cyrille Rossant bietet eine gute Einführung in die Verwendung von IPython
für die Datenanalyse.

IPython Interaktive Datenverarbeitung und Visualisierung - Kochbuch (Packt)
Dieses Buch, das ebenfalls von Cyrille Rossant stammt, ist eine längere und fortgeschrittenere Behandlung der
Verwendung von IPython für die Datenwissenschaft. Trotz seines Namens geht es nicht nur um IPython; es
es geht auch auf eine breite Palette von Themen der Datenwissenschaft ein.

Zum Schluss noch eine Erinnerung daran, dass Sie die Hilfe auch selbst finden können: Die ?-basierte Hilfefunktionalität von IPython
basierte Hilfefunktion von IPython (die in Kapitel 1 besprochen wurde) kann sehr nützlich sein, wenn Sie sie gut und oft nutzen. Wie

Wenn Sie die Beispiele hier und anderswo durchgehen, können Sie sich damit mit allen
sich mit allen Werkzeugen vertraut zu machen, die IPython zu bieten hat.

32 | Kapitel 3: Debugging und Profiling

TEIL II
Einführung in NumPy

In diesem Teil des Buches sowie in Teil III werden Techniken zum effektiven Laden,
Speichern und Manipulieren von In-Memory-Daten in Python. Das Thema ist sehr breit gefächert: Daten- und

Sätze können aus einer Vielzahl von Quellen und in einer Vielzahl von Formaten stammen, darunter
Sammlungen von Dokumenten, Sammlungen von Bildern, Sammlungen von Soundclips, Sammlungen

von numerischen Messungen, oder fast alles andere. Trotz dieser scheinbaren Heterogenität

Viele Datensätze lassen sich im Grunde als Zahlenreihen darstellen.

Zum Beispiel können Bilder - insbesondere digitale Bilder - einfach als zwei- oder dreidimensionale Bilder betrachtet werden.

dimensionalen Arrays von Zahlen, die die Pixelhelligkeit im gesamten Bereich darstellen. Ton
Clips können als eindimensionale Arrays der Intensität im Verhältnis zur Zeit betrachtet werden. Text kann sein

auf verschiedene Weise in numerische Darstellungen umgewandelt werden, z. B. in binäre Ziffern.

die Häufigkeit bestimmter Wörter oder Wortpaare zu ermitteln. Ganz gleich, um welche Daten es sich handelt,
der erste Schritt, um sie auswertbar zu machen, besteht darin, sie in Zahlenreihen umzuwandeln.

(Wir werden einige konkrete Beispiele für diesen Prozess in Kapitel 40 erörtern).

Aus diesem Grund ist eine effiziente Speicherung und Bearbeitung von numerischen Arrays absolut notwendig.

grundlegend für den Prozess der Datenwissenschaft. Wir werfen nun einen Blick auf die speziellen...

Werkzeuge, die Python für den Umgang mit solchen numerischen Arrays bietet: das NumPy-Paket
und das Pandas-Paket (das in Teil III besprochen wird).

In diesem Teil des Buches wird NumPy im Detail behandelt. NumPy (kurz für Numerical
Python) bietet eine effiziente Schnittstelle zum Speichern und Bearbeiten von dichten Datenpuffern. Unter

In gewisser Weise sind NumPy-Arrays wie Pythons eingebauter Listentyp, aber NumPy-Arrays bieten
aber NumPy-Arrays bieten eine viel effizientere Speicherung und Datenoperationen, wenn die Arrays größer werden.

NumPy-Arrays bilden den Kern fast des gesamten Ökosystems von Data-Science-Tools in

Python, so dass die Zeit, die man in die effektive Nutzung von NumPy investiert, in jedem Fall wertvoll ist.

welcher Aspekt der Datenwissenschaft Sie interessiert.

Wenn Sie den Rat im Vorwort befolgt und den Anaconda-Stack installiert haben, können Sie

NumPy bereits installiert und einsatzbereit haben. Wenn Sie eher der Do-it-yourself-Typ sind,

können Sie zu NumPy.org gehen und den dortigen Installationsanweisungen folgen. Sobald
können Sie NumPy importieren und die Version doppelt überprüfen:

In [1]: importiere numpy
numpy.version
Out[1]: '1.21.2'

Für die hier besprochenen Teile des Pakets würde ich NumPy Version 1.8 oder

später. Sie werden feststellen, dass die meisten Leute in der SciPy/PyData-Welt aus Konvention

NumPy importieren und np als Alias verwenden:

In [2]: import numpy as np

In diesem Kapitel und auch im Rest des Buches werden Sie feststellen, dass dies die

Weise werden wir NumPy importieren und verwenden.

Erinnerung an die eingebaute Dokumentation
Wenn Sie diesen Teil des Buches durchlesen, vergessen Sie nicht, dass IPython Ihnen die Möglichkeit gibt
die Möglichkeit bietet, den Inhalt eines Pakets schnell zu erforschen (mit der Tabulator-Vervollständigung)
ture), sowie die Dokumentation verschiedener Funktionen (mit dem Zeichen?). Für eine
Auffrischung dieser Informationen finden Sie in Kapitel 1.
Um zum Beispiel den gesamten Inhalt des NumPy-Namensraums anzuzeigen, können Sie Folgendes eingeben:
In [3]: np.<TAB>
Und um die in NumPy integrierte Dokumentation anzuzeigen, können Sie dies verwenden:
In [4]: np?
Numpy bietet eine ausführlichere Dokumentation sowie Tutorials und andere Ressourcen.
KAPITEL 4

Verstehen von Datentypen in Python
Effektive datengesteuerte Wissenschaft und Berechnungen erfordern ein Verständnis dafür, wie Daten
gespeichert und manipuliert werden. Dieses Kapitel umreißt und kontrastiert, wie Arrays von Daten sind

die in der Sprache Python selbst behandelt werden, und wie NumPy dies verbessert. Unter-

Dieser Unterschied ist grundlegend für das Verständnis eines Großteils des Materials
des restlichen Buches.

Die Benutzer von Python werden oft von der einfachen Bedienung angezogen, zu der auch die dynamische
Typisierung. Während eine statisch typisierte Sprache wie C oder Java erfordert, dass jede Variable

explizit deklariert, eine dynamisch typisierte Sprache wie Python überspringt diese Angabe.

In C könnten Sie beispielsweise eine bestimmte Operation wie folgt angeben:

/* C-Code */
int Ergebnis = 0;
for ( int i=0; i<100; i++){
Ergebnis += i;
}
In Python könnte die entsprechende Operation folgendermaßen geschrieben werden:

# Python-Code
Ergebnis = 0
for i in range(100):
Ergebnis += i
Beachten Sie einen wesentlichen Unterschied: In C werden die Datentypen jeder Variablen explizit
deklariert, während in Python die Typen dynamisch hergeleitet werden. Das bedeutet zum Beispiel...

dass wir jeder Variablen jede Art von Daten zuweisen können:

# Python-Code
x = 4
x = "vier"
35
Hier haben wir den Inhalt von x von einer Ganzzahl in eine Zeichenkette umgewandelt. Das Gleiche in C
würde (je nach Compilereinstellungen) zu einem Kompilierungsfehler oder anderen ungewollten

ded Folgen:

/* C-Code */
int x = 4;
x = "vier"; // FAILS
Diese Art von Flexibilität ist ein Element, das Python und andere dynamisch typisierte

Sprachen bequem und einfach zu verwenden. Zu verstehen, wie das funktioniert, ist ein wichtiger
um zu lernen, wie man mit Python Daten effizient und effektiv analysieren kann. Aber was dies

Typflexibilität ist auch die Tatsache, dass Python-Variablen mehr sind als nur ihre

Werte; sie enthalten auch zusätzliche Informationen über den Typ des Wertes. Wir werden uns
in den folgenden Abschnitten näher eingehen.

Eine ganze Zahl in Python ist mehr als nur eine ganze Zahl
Die Standardimplementierung von Python ist in C geschrieben. Das bedeutet, dass jede Python

Objekt ist einfach eine geschickt getarnte C-Struktur, die nicht nur ihren Wert, sondern auch
sondern auch andere Informationen enthält. Wenn wir zum Beispiel eine ganze Zahl in Python definieren, wie zum Beispiel

x = 10000, ist x nicht nur eine "rohe" Ganzzahl. Es ist eigentlich ein Zeiger auf eine zusammengesetzte C-Struk
ture, die mehrere Werte enthält. Ein Blick in den Quellcode von Python 3.10 zeigt uns

finden, dass die Definition des Typs Integer (long) effektiv wie folgt aussieht (nachdem die C-Makros
ros erweitert sind):

struct _longobject {
long ob_refcnt;
PyTypeObject *ob_type;
size_t ob_size;
long ob_digit[1];
};
Eine einzelne Ganzzahl in Python 3.10 besteht eigentlich aus vier Teilen:

ob_refcnt, eine Referenzzählung, die Python hilft, Speicherzuweisungen und -freigaben stillschweigend zu behandeln.
Speicherzuweisung und -freigabe
ob_type, der den Typ der Variablen kodiert
ob_size, das die Größe der folgenden Datenelemente angibt
ob_digit, das den tatsächlichen Integer-Wert enthält, den die Python-Variable repräsentieren soll.
Variable repräsentieren soll
Das bedeutet, dass die Speicherung einer ganzen Zahl in Python mit einem gewissen Aufwand verbunden ist
im Vergleich zu einer kompilierten Sprache wie C, wie in Abbildung 4-1 dargestellt.

36 | Kapitel 4: Verstehen von Datentypen in Python

Abbildung 4-1. Der Unterschied zwischen C- und Python-Ganzzahlen

Hier ist PyObject_HEAD der Teil der Struktur, der die Referenzanzahl, den Typ
Code und andere bereits erwähnte Teile enthält.

Beachten Sie den Unterschied: eine C-Ganzzahl ist im Wesentlichen eine Bezeichnung für eine Position im Speicher

dessen Bytes einen Integer-Wert kodieren. Eine Python-Ganzzahl ist ein Zeiger auf eine Position im
Speicher, der alle Python-Objektinformationen enthält, einschließlich der Bytes, die den

den Integer-Wert zu erhalten. Diese zusätzlichen Informationen in der Python-Ganzzahlstruktur
erlaubt es, Python so frei und dynamisch zu kodieren. All diese zusätzlichen Informationen

in Python-Typen hat jedoch ihren Preis, was besonders deutlich wird in

Strukturen, die viele dieser Objekte kombinieren.

Eine Python-Liste ist mehr als nur eine Liste
Betrachten wir nun, was passiert, wenn wir eine Python-Datenstruktur verwenden, die
viele Python-Objekte enthält. Der standardmäßige veränderbare Multielement-Container in Python ist die

Liste. Wir können eine Liste von ganzen Zahlen wie folgt erstellen:

In [1]: L = Liste(Bereich(10))
L
Out[1]: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]

In [2]: Typ(L[0])
Ausgang[2]: int

Oder, ähnlich, eine Liste von Zeichenketten:

In [3]: L2 = [str(c) für c in L]
L2
Out[3]: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']

In [4]: type(L2[0])
Out[4]: str

Aufgrund der dynamischen Typisierung von Python können wir sogar heterogene Listen erstellen:

In [5]: L3 = [ True , "2", 3.0, 4]
[type(item) for item in L3]
Out[5]: [bool, str, float, int]

Diese Flexibilität hat jedoch ihren Preis: Um diese flexiblen Typen zu ermöglichen, muss jedes Element der Liste

muss seinen eigenen Typ, die Anzahl der Verweise und andere Informationen enthalten. Das heißt, jedes Element

Eine Python-Liste ist mehr als nur eine Liste | 37
ist ein vollständiges Python-Objekt. In dem speziellen Fall, dass alle Variablen vom gleichen Typ sind,

viele dieser Informationen sind redundant, so dass es viel effizienter sein kann, die Daten in einem
Daten in einem Array festen Typs zu speichern. Der Unterschied zwischen einer Liste vom dynamischen Typ und einem Array vom festen Typ

(NumPy-Style) Array ist in Abbildung 4-2 dargestellt.

Abbildung 4-2. Der Unterschied zwischen C- und Python-Listen

38 | Kapitel 4: Verstehen von Datentypen in Python

Auf der Implementierungsebene enthält das Array im Wesentlichen einen einzigen Zeiger auf einen Kon-

zusammenhängenden Block von Daten. Die Python-Liste hingegen enthält einen Zeiger auf einen
Block von Zeigern, von denen jeder wiederum auf ein vollständiges Python-Objekt wie das Python

Ganzzahl, die wir bereits gesehen haben. Auch hier ist der Vorteil der Liste die Flexibilität: Da jede Liste

Element eine vollständige Struktur ist, die sowohl Daten als auch Typinformationen enthält, kann die Liste
mit Daten jedes gewünschten Typs gefüllt werden. Arrays im NumPy-Stil mit festem Typ fehlt diese Flexibilität.

Sie sind jedoch sehr viel effizienter bei der Speicherung und Bearbeitung von Daten.

Arrays festen Typs in Python
Python bietet mehrere verschiedene Optionen für die Speicherung von Daten in effizienten Arrays festen Typs

Puffer. Das eingebaute Array-Modul (verfügbar seit Python 3.3) kann verwendet werden, um

dichte Arrays eines einheitlichen Typs:

In [6]: import array
L = Liste(Bereich(10))
A = array.array('i', L)
A
Out[6]: array('i', [0, 1, 2, 3, 4, 5, 6, 7, 8, 9])

Hier ist 'i' ein Typcode, der angibt, dass es sich bei den Inhalten um ganze Zahlen handelt.

Viel nützlicher ist jedoch das ndarray-Objekt aus dem NumPy-Paket. Während

Pythons Array-Objekt bietet eine effiziente Speicherung von Array-basierten Daten, NumPy ergänzt die

diese effizienten Operationen mit diesen Daten. Wir werden diese Operationen in späteren Kapiteln erforschen.
Als Nächstes zeige ich Ihnen ein paar verschiedene Möglichkeiten, ein NumPy-Array zu erstellen.

Arrays aus Python-Listen erstellen
Wir beginnen mit dem Standardimport von NumPy unter dem Alias np:

In [7]: import numpy as np

Jetzt können wir np.array verwenden, um Arrays aus Python-Listen zu erstellen:

In [8]: # Integer array
np.array([1, 4, 2, 5, 3])
Out[8]: array([1, 4, 2, 5, 3])

Denken Sie daran, dass NumPy-Arrays im Gegensatz zu Python-Listen nur Daten desselben Typs enthalten können
Typs enthalten können. Wenn die Typen nicht übereinstimmen, wird NumPy sie entsprechend seinem Typ promo-

tionsregeln; hier werden Ganzzahlen in Gleitkommazahlen umgewandelt:

In [9]: np.array([3.14, 4, 2, 3])
Out[9]: array([3.14, 4. , 2. , 3. ])

Arrays mit festem Typ in Python | 39
Wenn wir den Datentyp des resultierenden Arrays explizit festlegen wollen, können wir das Schlüsselwort dtype
Schlüsselwort verwenden:

In [10]: np.array([1, 2, 3, 4], dtype=np.float32)
Out[10]: array([1., 2., 3., 4.], dtype=float32)

Im Gegensatz zu Python-Listen, die immer eindimensionale Sequenzen sind, ist NumPy

Arrays können mehrdimensional sein. Hier ist eine Möglichkeit, ein mehrdimensionales Array zu initialisieren

Array mit einer Liste von Listen:

In [11]: # Verschachtelte Listen ergeben mehrdimensionale Arrays
np.array([range(i, i + 3) for i in [2, 4, 6]])
Out[11]: array([[2, 3, 4],
[4, 5, 6],
[6, 7, 8]])

Die inneren Listen werden als Zeilen des resultierenden zweidimensionalen Arrays behandelt.

Arrays von Grund auf neu erstellen
Besonders bei größeren Arrays ist es effizienter, Arrays von Grund auf mit rou-

in NumPy eingebauten Zinken. Hier sind einige Beispiele:

In [12]: # Erstelle ein Ganzzahl-Array der Länge 10, gefüllt mit 0en
np.zeros(10, dtype=int)
Out[12]: array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])

In [13]: # Ein 3x5-Gleitkomma-Array erstellen, das mit 1en gefüllt ist
np.ones((3, 5), dtype=float)
Out[13]: array([[1., 1., 1., 1., 1.],
[1., 1., 1., 1., 1.],
[1., 1., 1., 1., 1.]])

In [14]: # Erstelle ein 3x5-Array, das mit 3,14 gefüllt ist.
np.full((3, 5), 3.14)
Out[14]: array([[3.14, 3.14, 3.14, 3.14, 3.14],
[3.14, 3.14, 3.14, 3.14, 3.14],
[3.14, 3.14, 3.14, 3.14, 3.14]])

In [15]: _# Ein Array erstellen, das mit einer linearen Sequenz gefüllt ist

beginnend bei 0, endend bei 20, schrittweise um 2
(dies ist ähnlich wie die eingebaute Bereichsfunktion)_
np.arange(0, 20, 2)
Out[15]: array([ 0, 2, 4, 6, 8, 10, 12, 14, 16, 18])

In [16]: # Erstellen Sie ein Array mit fünf Werten, die gleichmäßig zwischen 0 und 1 verteilt sind.
np.linspace(0, 1, 5)
Out[16]: array([0. , 0.25, 0.5 , 0.75, 1. ])

In [17]: _# Erstellen Sie ein 3x3-Array gleichmäßig verteilter

pseudozufällige Werte zwischen 0 und 1_
np.random.random((3, 3))
Out[17]: array([[0.09610171, 0.88193001, 0.70548015],

40 | Kapitel 4: Verstehen von Datentypen in Python

[0.35885395, 0.91670468, 0.8721031 ],
[0.73237865, 0.09708562, 0.52506779]])
In [18]: _# Erzeugen eines 3x3-Arrays von normalverteilten Pseudozufallszahlen

Werte mit Mittelwert 0 und Standardabweichung 1_
np.random.normal(0, 1, (3, 3))
Out[18]: array([[-0.46652655, -0.59158776, -1.05392451],
[-1.72634268, 0.03194069, -0.51048869],
[ 1.41240208, 1.77734462, -0.43820037]])

In [19]: # Erstelle ein 3x3-Array mit pseudozufälligen ganzen Zahlen im Intervall [0, 10)
np.random.randint(0, 10, (3, 3))
Out[19]: array([[4, 3, 8],
[6, 5, 0],
[1, 1, 4]])

In [20]: # Erstelle eine 3x3-Identitätsmatrix
np.eye(3)
Out[20]: array([[1., 0., 0.],
[0., 1., 0.],
[0., 0., 1.]])

In [21]: _# Erstelle ein nicht initialisiertes Array mit drei Ganzzahlen; die Werte werden sein

was auch immer an diesem Speicherplatz bereits vorhanden ist_
np.empty(3)
Out[21]: array([1., 1., 1.])

NumPy Standard-Datentypen
NumPy-Arrays enthalten Werte eines einzigen Typs, daher ist es wichtig, detaillierte

Wissen über diese Typen und ihre Grenzen. Da NumPy in C aufgebaut ist, sind die
Typen den Benutzern von C, Fortran und anderen verwandten Sprachen vertraut sein.

Die Standard-NumPy-Datentypen sind in Tabelle 4-1 aufgeführt. Beachten Sie, dass beim Konstruieren von
eines Arrays können sie mit einer Zeichenkette angegeben werden:

np.zeros(10, dtype='int16')
Oder Sie verwenden das zugehörige NumPy-Objekt:

np.zeros(10, dtype=np.int16)
Eine erweiterte Typspezifikation ist möglich, z. B. die Angabe von Big- oder Little-Endian

Zahlen; weitere Informationen finden Sie in der NumPy-Dokumentation. NumPy unterstützt auch
unterstützt auch zusammengesetzte Datentypen, die in Kapitel 12 behandelt werden.

NumPy Standard-Datentypen | 41
Tabelle 4-1. NumPy-Standard-Datentypen

Datentyp Beschreibung
bool_ Boolescher Wert (Wahr oder Falsch), gespeichert als Byte
int_ Standard-Ganzzahltyp (wie C long; normalerweise entweder int64 oder int32)
intc Identisch mit C int (normalerweise int32 oder int64)
intp Ganzzahl, die zur Indexierung verwendet wird (wie C ssize_t; normalerweise entweder int32 oder int64)
int8 Byte (-128 bis 127)
int16 Ganze Zahl (-32768 bis 32767)
int32 Ganze Zahl (-2147483648 bis 2147483647)
int64 Ganze Zahl (-9223372036854775808 bis 9223372036854775807)
uint8 Ganze Zahl ohne Vorzeichen (0 bis 255)
uint16 Ganze Zahl ohne Vorzeichen (0 bis 65535)
uint32 Ganze Zahl ohne Vorzeichen (0 bis 4294967295)
uint64 Ganze Zahl ohne Vorzeichen (0 bis 18446744073709551615)
float_ Kurzbezeichnung für float64
float16 Halbpräzisions-Float: Vorzeichenbit, 5 Bits Exponent, 10 Bits Mantisse
float32 Einfachpräzisions-Float: Vorzeichenbit, 8 Bits Exponent, 23 Bits Mantisse
float64 Doppeltgenaue Fließkommazahl: Vorzeichenbit, 11 Bits Exponent, 52 Bits Mantisse
complex_ Kurzbezeichnung für complex128
complex64 Komplexe Zahl, dargestellt durch zwei 32-Bit-Fließkommazahlen
complex128 Komplexe Zahl, dargestellt durch zwei 64-Bit-Fließkommazahlen
42 | Kapitel 4: Verstehen von Datentypen in Python

KAPITEL 5

Die Grundlagen von NumPy-Arrays
Datenmanipulation in Python ist fast gleichbedeutend mit der Manipulation von NumPy-Arrays:
Selbst neuere Werkzeuge wie Pandas (Teil III) sind um das NumPy-Array herum aufgebaut. Dieses Kapitel...

ter wird mehrere Beispiele für die Verwendung von NumPy-Array-Manipulationen zum Zugriff auf Daten vorstellen

und Subarrays sowie zum Teilen, Umformen und Verbinden der Arrays. Während die hier gezeigten Arten von Operationen
ein wenig trocken und pedantisch erscheinen mögen, umfassen sie die Bausteine von

viele andere Beispiele, die im Buch verwendet werden. Lernen Sie sie gut kennen!

Wir werden hier einige Kategorien von grundlegenden Array-Manipulationen behandeln:

Attribute von Arrays

Bestimmung von Größe, Form, Speicherverbrauch und Datentypen von Arrays

Indizierung von Arrays

Abrufen und Setzen der Werte einzelner Array-Elemente

Aufteilung von Arrays

Abrufen und Setzen kleinerer Subarrays innerhalb eines größeren Arrays

Umformung von Arrays
Ändern der Form eines bestimmten Arrays

Kombinieren und Aufteilen von Arrays
Kombinieren mehrerer Arrays zu einem Array und Aufteilen eines Arrays in mehrere Arrays

43
NumPy Array-Attribute
Lassen Sie uns zunächst einige nützliche Array-Attribute besprechen. Wir beginnen mit der Definition zufälliger Arrays von

eine, zwei und drei Dimensionen. Wir werden den Zufallszahlengenerator von NumPy verwenden, der
mit einem bestimmten Wert geimpft wird, um sicherzustellen, dass die gleichen Zufallsarrays generiert werden.

jedes Mal, wenn dieser Code ausgeführt wird:

In [1]: import numpy as np
rng = np.random.default_rng(seed=1701) # seed für Reproduzierbarkeit

x1 = rng.integers(10, size=6) # eindimensionales Array
x2 = rng.integers(10, size=(3, 4)) # zweidimensionales Array
x3 = rng.integers(10, size=(3, 4, 5)) # dreidimensionales Array

Jedes Array hat Attribute wie ndim (die Anzahl der Dimensionen), shape (die Größe

jeder Dimension), size (die Gesamtgröße des Arrays) und dtype (der Typ jeder

Element):

In [2]: print("x3 ndim: ", x3.ndim)
print("x3 Form:", x3.Form)
print("x3 Größe: ", x3.Größe)
print("dtype: ", x3.dtype)
Out[2]: x3 ndim: 3
x3 Form: (3, 4, 5)
x3 Größe: 60
dTyp: int64

Weitere Informationen zu den Datentypen finden Sie in Kapitel 4.

Array-Indizierung: Zugriff auf einzelne Elemente
Wenn Sie mit der Standard-Listenindexierung von Python vertraut sind, wird sich die Indexierung in NumPy wie folgt anfühlen

recht vertraut. In einem eindimensionalen Array kann auf den i-ten Wert (von Null an gezählt) zugegriffen werden
zugreifen, indem man den gewünschten Index in eckigen Klammern angibt, genau wie bei Python-Listen:

In [3]: x1
Out[3]: array([9, 4, 0, 3, 8, 6])

Eingang [4]: x1[0]
Out[4]: 9

Eingang [5]: x1[4]
Ausgang[5]: 8

Um vom Ende des Arrays aus zu indizieren, können Sie negative Indizes verwenden:

In [6]: x1[-1]
Out[6]: 6

In [7]: x1[-2]
Out[7]: 8

44 | Kapitel 5: Die Grundlagen von NumPy-Arrays

In einem mehrdimensionalen Array kann auf die Elemente durch Komma getrennt zugegriffen werden ( Zeile ,

Spalte ) Tupel:

In [8]: x2
Out[8]: array([[3, 1, 3, 7],
[4, 0, 2, 3],
[0, 0, 6, 9]])

In [9]: x2[0, 0]
Out[9]: 3

In [10]: x2[2, 0]
Out[10]: 0

In [11]: x2[2, -1]
Out[11]: 9

Die Werte können auch unter Verwendung einer der vorstehenden Indexschreibweisen geändert werden:

In [12]: x2[0, 0] = 12
x2
Out[12]: array([[12, 1, 3, 7],
[ 4, 0, 2, 3],
[ 0, 0, 6, 9]])

Beachten Sie, dass NumPy-Arrays im Gegensatz zu Python-Listen einen festen Typ haben. Dies bedeutet,

Wenn Sie zum Beispiel versuchen, einen Fließkommawert in ein Integer-Array einzufügen,
der Wert stillschweigend abgeschnitten wird. Lassen Sie sich von diesem Verhalten nicht überrumpeln!

In [13]: x1[0] = 3.14159 # dies wird abgeschnitten!
x1
Out[13]: array([3, 4, 0, 3, 8, 6])

Array-Slicing: Zugriff auf Subarrays
Genauso wie wir eckige Klammern verwenden können, um auf einzelne Array-Elemente zuzugreifen, können wir auch

Sie können auf Subarrays mit der Slice-Notation zugreifen, die durch den Doppelpunkt (:) gekennzeichnet ist.

Die NumPy-Syntax für das Slicing folgt der der Standard-Python-Liste; um auf ein Slice von

ein Array x, verwenden Sie dies:

x[start:stop:step]
Wenn einer dieser Werte nicht angegeben wird, werden die Werte start=0, stop=<Größe der

Dimension>, Schritt=1. Sehen wir uns nun einige Beispiele für den Zugriff auf Subarrays in einer
Dimension und in mehreren Dimensionen.

Eindimensionale Subarrays
Hier sind einige Beispiele für den Zugriff auf Elemente in eindimensionalen Subarrays:

In [14]: x1
Out[14]: array([3, 4, 0, 3, 8, 6])

Array-Slicing: Zugriff auf Subarrays | 45
In [15]: x1[:3] # erste drei Elemente
Out[15]: array([3, 4, 0])

In [16]: x1[3:] # Elemente nach Index 3
Out[16]: array([3, 8, 6])

In [17]: x1[1:4] # mittleres Subarray
Out[17]: array([4, 0, 3])

In [18]: x1[::2] # jedes zweite Element
Out[18]: array([3, 0, 8])

In [19]: x1[1::2] # jedes zweite Element, beginnend bei Index 1
Out[19]: array([4, 3, 6])

Ein potenziell verwirrender Fall ist, wenn der Stufenwert negativ ist. In diesem Fall wird der

werden die Standardwerte für Start und Stopp vertauscht. Dies ist ein bequemer Weg, um die

ein Array:

In [20]: x1[::-1] # alle Elemente, invertiert
Out[20]: array([6, 8, 3, 0, 4, 3])

In [21]: x1[4::-2] # jedes zweite Element ab Index 4, invertiert
Out[21]: array([8, 0, 3])

Mehrdimensionale Subarrays
Mehrdimensionale Slices funktionieren auf die gleiche Weise, wobei mehrere Slices durch Kom-
mas. Zum Beispiel:

In [22]: x2
Out[22]: array([[12, 1, 3, 7],
[ 4, 0, 2, 3],
[ 0, 0, 6, 9]])

In [23]: x2[:2, :3] # erste zwei Zeilen und drei Spalten
Out[23]: array([[12, 1, 3],
[ 4, 0, 2]])

In [24]: x2[:3, ::2] # drei Zeilen, jede zweite Spalte
Out[24]: array([[12, 3],
[ 4, 2],
[ 0, 6]])

In [25]: x2[::-1, ::-1] # alle Zeilen und Spalten, invertiert
Out[25]: array([[ 9, 6, 0, 0],
[ 3, 2, 0, 4],
[ 7, 3, 1, 12]])

Eine häufig benötigte Routine ist der Zugriff auf einzelne Zeilen oder Spalten eines Arrays. Diese

kann durch die Kombination von Indizierung und Slicing erfolgen, wobei ein leeres Slice mit einem

einfacher Doppelpunkt (:):

In [26]: x2[:, 0] # erste Spalte von x2
Out[26]: array([12, 4, 0])

46 | Kapitel 5: Die Grundlagen von NumPy-Arrays

In [27]: x2[0, :] # erste Zeile von x2
Out[27]: array([12, 1, 3, 7])

Beim Zeilenzugriff kann die leere Scheibe weggelassen werden, um eine kompaktere Syntax zu erhalten:

In [28]: x2[0] # äquivalent zu x2[0, :]
Out[28]: array([12, 1, 3, 7])

Subarrays als kopierfreie Ansichten
Anders als Python-Listen-Slices werden NumPy-Array-Slices als Views und nicht als Kopien zurückgegeben

der Array-Daten. Betrachten wir unser zweidimensionales Array von vorhin:

In [29]: print(x2)
Out[29]: [[12 1 3 7]
[ 4 0 2 3]
[ 0 0 6 9]]

Extrahieren wir daraus ein 2 × 2 Unterfeld:

In [30]: x2_sub = x2[:2, :2]
print(x2_sub)
Out[30]: [[12 1]
[ 4 0]]

Wenn wir nun dieses Subarray verändern, sehen wir, dass das ursprüngliche Array verändert wurde! Beobachten Sie:

In [31]: x2_sub[0, 0] = 99
print(x2_sub)
Out[31]: [[99 1]
[ 4 0]]

In [32]: print(x2)
Out[32]: [[99 1 3 7]
[ 4 0 2 3]
[ 0 0 6 9]]

Dies mag für manche Benutzer überraschend sein, kann aber von Vorteil sein: zum Beispiel, wenn

Bei der Arbeit mit großen Datensätzen können wir auf Teile dieser Datensätze zugreifen und sie verarbeiten
zugreifen und verarbeiten, ohne dass der zugrunde liegende Datenpuffer kopiert werden muss.

Kopien von Arrays erstellen
Trotz der Funktionen von Array-Ansichten ist es manchmal nützlich, stattdessen explizit Kopien der

Daten innerhalb eines Arrays oder eines Subarrays. Dies ist am einfachsten mit der Kopiermethode zu bewerkstelligen:

In [33]: x2_sub_copy = x2[:2, :2].copy()
print(x2_sub_copy)
Out[33]: [[99 1]
[ 4 0]]

Array-Slicing: Zugriff auf Subarrays | 47
Wenn wir nun dieses Subarray verändern, bleibt das ursprüngliche Array unangetastet:

In [34]: x2_sub_copy[0, 0] = 42
print(x2_sub_copy)
Out[34]: [[42 1]
[ 4 0]]

In [35]: print(x2)
Out[35]: [[99 1 3 7]
[ 4 0 2 3]
[ 0 0 6 9]]

Umformung von Arrays
Ein weiterer nützlicher Operationstyp ist die Umformung von Arrays, die mit der Funktion

Methode umgestalten. Wenn Sie zum Beispiel die Zahlen 1 bis 9 in ein 3 × 3

Rasters können Sie Folgendes tun:

In [36]: grid = np.arange(1, 10).reshape(3, 3)
print(gitter)
Out[36]: [[1 2 3]
[4 5 6]
[7 8 9]]

Damit dies funktioniert, muss die Größe des anfänglichen Arrays mit der Größe der

umgestaltetes Array, und in den meisten Fällen wird die reshape-Methode eine kopierfreie Ansicht von

das ursprüngliche Array.

Eine gängige Umformungsoperation ist die Umwandlung einer eindimensionalen Matrix in eine zweidimensionale
dimensionalen Zeilen- oder Spaltenmatrix:

In [37]: x = np.array([1, 2, 3])
x.reshape((1, 3)) # Zeilenvektor über reshape
Out[37]: array([[1, 2, 3]])

In [38]: x.reshape((3, 1)) # Spaltenvektor durch Umformung
Out[38]: array([[1],
[2],
[3]])

Eine praktische Abkürzung hierfür ist die Verwendung von np.newaxis in der Slicing-Syntax:

In [39]: x[np.newaxis, :] # Zeilenvektor über newaxis
Out[39]: array([[1, 2, 3]])

In [40]: x[:, np.newaxis] # Spaltenvektor über newaxis
Out[40]: array([[1],
[2],
[3]])

Dieses Muster werden wir im weiteren Verlauf des Buches häufig verwenden.

48 | Kapitel 5: Die Grundlagen von NumPy-Arrays

Array-Verkettung und Aufteilung
Alle vorangegangenen Routinen haben mit einzelnen Arrays gearbeitet. NumPy bietet auch Werkzeuge zum

mehrere Arrays zu einem einzigen zu kombinieren und umgekehrt ein einzelnes Array in mehrere
Arrays.

Verkettung von Arrays
Die Verkettung oder das Zusammenfügen von zwei Arrays in NumPy wird hauptsächlich mit

die Routinen np.concatenate, np.vstack und np.hstack. np.concatenate nimmt ein
Tupel oder eine Liste von Arrays als erstes Argument, wie Sie hier sehen können:

In [41]: x = np.array([1, 2, 3])
y = np.array([3, 2, 1])
np.concatenate([x, y])
Out[41]: array([1, 2, 3, 3, 2, 1])

Sie können auch mehr als zwei Arrays auf einmal verketten:

In [42]: z = np.array([99, 99, 99])
print(np.concatenate([x, y, z]))
Out[42]: [ 1 2 3 3 2 1 99 99 99]

Und es kann für zweidimensionale Arrays verwendet werden:

In [43]: grid = np.array([[1, 2, 3],
[4, 5, 6]])

In [44]: # Verkettung entlang der ersten Achse
np.concatenate([grid, grid])
Out[44]: array([[1, 2, 3],
[4, 5, 6],
[1, 2, 3],
[4, 5, 6]])

In [45]: # Verkettung entlang der zweiten Achse (null-indiziert)
np.concatenate([grid, grid], axis=1)
Out[45]: array([[1, 2, 3, 1, 2, 3],
[4, 5, 6, 4, 5, 6]])

Für die Arbeit mit Arrays mit gemischten Dimensionen kann es übersichtlicher sein, die np.vstack

(vertikaler Stapel) und np.hstack (horizontaler Stapel):

In [46]: # die Arrays vertikal stapeln
np.vstack([x, grid])
Out[46]: array([[1, 2, 3],
[1, 2, 3],
[4, 5, 6]])

In [47]: # die Arrays horizontal stapeln
y = np.array([[99],
[99]])
np.hstack([grid, y])

Verkettung und Aufteilung von Arrays | 49
Out[47]: array([[ 1, 2, 3, 99],
[ 4, 5, 6, 99]])

In ähnlicher Weise stapelt np.dstack bei höherdimensionalen Arrays die Arrays entlang der dritten

Achse.

Aufteilung von Arrays
Das Gegenteil der Verkettung ist die Aufteilung, die durch die Funktionen

np.split, np.hsplit und np.vsplit. Für jede dieser Methoden können wir eine Liste von Indizes übergeben

mit den Splitpunkten:

In [48]: x = [1, 2, 3, 99, 99, 3, 2, 1]
x1, x2, x3 = np.split(x, [3, 5])
print(x1, x2, x3)
Out[48]: [1 2 3] [99 99] [3 2 1]

Es ist zu beachten, dass N Splitpunkte zu N + 1 Subarrays führen. Die zugehörigen Funktionen np.hsplit

und np.vsplit sind ähnlich:

In [49]: grid = np.arange(16).reshape((4, 4))
grid
Out[49]: array([[ 0, 1, 2, 3],
[ 4, 5, 6, 7],
[ 8, 9, 10, 11],
[12, 13, 14, 15]])

In [50]: oben, unten = np.vsplit(grid, [2])
print(oberes)
print(lower)
Out[50]: [[0 1 2 3]
[4 5 6 7]]
[[ 8 9 10 11]
[12 13 14 15]]

In [51]: links, rechts = np.hsplit(grid, [2])
print(left)
print(right)
Out[51]: [[ 0 1]
[ 4 5]
[ 8 9]
[12 13]]
[[ 2 3]
[ 6 7]
[10 11]
[14 15]]

In ähnlicher Weise teilt np.dsplit bei höherdimensionalen Arrays die Arrays entlang der dritten

Achse.

50 | Kapitel 5: Die Grundlagen von NumPy-Arrays

KAPITEL 6

Berechnungen auf NumPy-Arrays:
Universelle Funktionen
Bis jetzt haben wir einige der grundlegenden Funktionen von NumPy besprochen. In
den nächsten Kapiteln werden wir uns mit den Gründen befassen, warum NumPy so wichtig ist für die

Python-Data-Science-Welt: nämlich weil es eine einfache und flexible Schnittstelle bietet

zur Optimierung von Berechnungen mit Datenfeldern.

Berechnungen auf NumPy-Arrays können sehr schnell sein, aber auch sehr langsam. Der Schlüssel zu

schnell zu machen, ist die Verwendung von vektorisierten Operationen, die im Allgemeinen durch Num-
Py's universelle Funktionen (ufuncs). In diesem Kapitel wird die Notwendigkeit von NumPy's

ufuncs, die für wiederholte Berechnungen auf Array-Elementen verwendet werden können, viel

effizienter. Anschließend werden viele der häufigsten und nützlichsten arithmetischen
ufuncs, die im NumPy-Paket verfügbar sind.

Die Langsamkeit von Schleifen
Die Standardimplementierung von Python (bekannt als CPython) führt einige Operationen sehr

langsam. Dies liegt zum Teil an der dynamischen, interpretierten Natur der Sprache; Typen sind
Typen sind flexibel, so dass Sequenzen von Operationen nicht zu effizienten Maschinenoperationen kompiliert werden können.

Code wie in Sprachen wie C und Fortran. In letzter Zeit gab es verschiedene Versuche, die

diese Schwäche: Bekannte Beispiele sind das PyPy-Projekt, eine just-in-time
kompilierte Implementierung von Python; das Cython-Projekt, das Python in eine

Code in kompilierbaren C-Code umwandelt; und das Numba-Projekt, das Schnipsel von
Python-Code in schnellen LLVM-Bytecode umwandelt. Jedes dieser Projekte hat seine Stärken und Schwächen,

aber man kann mit Sicherheit sagen, dass keiner der drei Ansätze bisher über die Reichweite und

die Popularität der Standard-CPython-Engine.

51
Die relative Trägheit von Python macht sich im Allgemeinen in Situationen bemerkbar, in denen

viele kleine Operationen wiederholt werden, z. B. Schleifen über Arrays, um jedes Element zu bearbeiten
auf jedes Element. Stellen Sie sich zum Beispiel vor, Sie haben ein Array mit Werten und würden gerne

den Kehrwert von jedem berechnen. Ein einfacher Ansatz könnte wie folgt aussehen:

In [1]: import numpy as np
rng = np.random.default_rng(seed=1701)

def compute_reciprocals(values):
output = np.empty(len(values))
for i in range(len(values)):
output[i] = 1.0 / values[i]
return output

Werte = rng.integers(1, 10, size=5)
compute_reciprocals(values)
Out[1]: array([0.11111111, 0.25 , 1. , 0.33333333, 0.125 ])

Diese Implementierung fühlt sich für jemanden, der z.B. mit C oder Java arbeitet, wahrscheinlich ziemlich natürlich an.

Hintergrund. Wenn wir jedoch die Ausführungszeit dieses Codes für eine große Eingabe messen, sehen wir
sehen wir, dass dieser Vorgang sehr langsam ist - vielleicht sogar überraschend langsam! Wir werden einen Benchmark durchführen

mit der %timeit-Magie von IPython (besprochen in "Profiling und Timing von Code" auf Seite 26):

In [2]: big_array = rng.integers(1, 100, size=1000000)
% timeit compute_reciprocals(big_array)
Out[2]: 2,61 s ± 192 ms pro Schleife (Mittelwert ± std. Abweichung von 7 Durchläufen, je 1 Schleife)

Es dauert mehrere Sekunden, diese Million Operationen zu berechnen und das Ergebnis zu speichern!
Wenn selbst Mobiltelefone Verarbeitungsgeschwindigkeiten haben, die in Gigaflops gemessen werden (d. h. Milliarden von

numerische Operationen pro Sekunde), erscheint dies geradezu absurd langsam. Es stellt sich heraus, dass
dass der Engpass hier nicht die Operationen selbst sind, sondern die Typprüfung und die Funk- tionen.

die CPython bei jedem Zyklus der Schleife ausführen muss. Jedes Mal, wenn die rezipro-

cal berechnet wird, prüft Python zunächst den Typ des Objekts und sucht dynamisch nach der
die richtige Funktion für diesen Typ. Wenn wir stattdessen mit kompiliertem Code arbeiten würden,

wäre diese Typspezifikation bekannt, bevor der Code ausgeführt wird, und das Ergebnis könnte
viel effizienter berechnet werden.

Einführung in Ufuncs
Für viele Arten von Operationen bietet NumPy eine bequeme Schnittstelle für genau diese

Art von statisch typisierter, kompilierter Routine. Dies wird als vektorisierte Operation bezeichnet.

Für einfache Operationen wie die elementweise Division hier, ist die Vektorisierung so einfach
wie die Verwendung von Python-Arithmetikoperatoren direkt auf das Array-Objekt. Diese vektorisierte

Ansatz ist darauf ausgelegt, die Schleife in die kompilierte Schicht zu verschieben, die NumPy zugrunde liegt,
was zu einer viel schnelleren Ausführung führt.

52 | Kapitel 6: Berechnungen auf NumPy-Arrays: Universelle Funktionen

Vergleichen Sie die Ergebnisse der beiden folgenden Operationen:

In [3]: print(compute_reciprocals(values))
print(1.0 / Werte)
Out[3]: [0.11111111 0.25 1. 0.33333333 0.125 ]
[0.11111111 0.25 1. 0.33333333 0.125 ]

Betrachtet man die Ausführungszeit für unser großes Array, so stellt man fest, dass es um Größenordnungen
Größenordnung schneller als die Python-Schleife:

In [4]: % timeit (1.0 / big_array)
Out[4]: 2,54 ms ± 383 μs pro Schleife (Mittelwert ± Standardabweichung von 7 Läufen mit jeweils 100 Schleifen)

Vektorisierte Operationen in NumPy werden über ufuncs implementiert, deren Hauptzweck es ist

um schnell wiederholte Operationen auf Werte in NumPy-Arrays auszuführen. Ufuncs sind
extrem flexibel - bisher haben wir eine Operation zwischen einem Skalar und einem Array gesehen, aber wir

kann auch zwischen zwei Arrays operieren:

In [5]: np.arange(5) / np.arange(1, 6)
Out[5]: array([0. , 0.5 , 0.66666667, 0.75 , 0.8 ])

Und ufunc-Operationen sind nicht auf eindimensionale Arrays beschränkt. Sie können wirken auf

auch mehrdimensionale Arrays:

In [6]: x = np.arange(9).reshape((3, 3))
2 ** x
Out[6]: array([[ 1, 2, 4],
[ 8, 16, 32],
[ 64, 128, 256]])

Berechnungen mit Vektorisierung durch ufuncs sind fast immer effizienter

als ihre Gegenstücke, die mit Python-Schleifen implementiert werden, insbesondere wenn die Arrays
an Größe zunehmen. Jedes Mal, wenn Sie eine solche Schleife in einem NumPy-Skript sehen, sollten Sie Folgendes bedenken

ob er durch einen vektorisierten Ausdruck ersetzt werden kann.

NumPy's Ufuncs erforschen
Ufuncs gibt es in zwei Varianten: unäre Ufuncs, die auf eine einzige Eingabe wirken, und binäre

ufuncs, die auf zwei Eingänge wirken. Wir werden hier Beispiele für beide Arten von Funktionen sehen.
tionen sehen wir hier.

Array-Arithmetik
Die ufuncs von NumPy fühlen sich sehr natürlich an, weil sie die nativen arithmetischen Operatoren von Python verwenden
arithmetischen Operatoren verwenden. Die Standard-Addition, -Subtraktion, -Multiplikation und -Division

können alle verwendet werden:

In [7]: x = np.arange(4)
print("x =", x)
ausdrucken ("x + 5 =", x + 5)
print("x - 5 =", x - 5)

NumPy's Ufuncs erforschen | 53
print("x * 2 =", x * 2)
print("x / 2 =", x / 2)
print("x // 2 =", x // 2) # Bodenteilung
Out[7]: x = [0 1 2 3]
x + 5 = [5 6 7 8]
x - 5 = [-5 -4 -3 -2]
x * 2 = [0 2 4 6]
x / 2 = [0. 0.5 1. 1.5]
x // 2 = [0 0 1 1]

Außerdem gibt es einen unären ufunc für die Negation, einen **-Operator für die Potenzierung und einen %
Operator für den Modulus:

In [8]: print("-x = ", -x)
print("x ** 2 = ", x ** 2)
print("x % 2 = ", x % 2)
Out[8]: -x = [ 0 -1 -2 -3]
x ** 2 = [0 1 4 9]
x % 2 = [0 1 0 1]

Darüber hinaus können diese nach Belieben aneinandergereiht werden, wobei die Standardreihenfolge

der Operationen beachtet wird:

In [9]: -(0.5*x + 1) ** 2
Out[9]: array([-1. , -2.25, -4. , -6.25])

Alle diese arithmetischen Operationen sind einfach bequeme Umhüllungen für bestimmte

ufuncs, die in NumPy eingebaut sind. Zum Beispiel ist der Operator + ein Wrapper für die Ufunc add:

In [10]: np.add(x, 2)
Out[10]: array([2, 3, 4, 5])

Tabelle 6-1 listet die in NumPy implementierten arithmetischen Operatoren auf.

Tabelle 6-1. In NumPy implementierte arithmetische Operatoren

Operator Äquivalent ufunc Beschreibung
+ np.add Addition (z. B. 1 + 1 = 2)
np.subtract Subtraktion (z. B. 3 - 2 = 1)
np.negative Unäre Negation (z. B. -2)
np.multiply Multiplikation (z.B., 2 * 3 = 6)
/ np.divide Division (z. B. 3 / 2 = 1,5)
// np.floor_divide Bodenteilung (z. B. 3 // 2 = 1)
** np.power Potenzierung (z. B. 2 ** 3 = 8)
% np.mod Modulus/Rest (z. B. 9 % 4 = 1)
Darüber hinaus gibt es boolesche/bitweise Operatoren; wir werden diese in Kapitel 9 untersuchen.

54 | Kapitel 6: Berechnungen auf NumPy-Arrays: Universelle Funktionen

Absoluter Wert
Genauso wie NumPy die in Python eingebauten arithmetischen Operatoren versteht, versteht es auch die

Pythons eingebaute Absolutwertfunktion:

In [11]: x = np.array([-2, -1, 0, 1, 2])
abs(x)
Out[11]: array([2, 1, 0, 1, 2])

Die entsprechende NumPy-Ufunc ist np.absolute, die auch unter der

alias np.abs:

In [12]: np.absolute(x)
Out[12]: array([2, 1, 0, 1, 2])

In [13]: np.abs(x)
Out[13]: array([2, 1, 0, 1, 2])

Diese Ufunc kann auch komplexe Daten verarbeiten, in diesem Fall gibt sie den Betrag zurück:

In [14]: x = np.array([3 - 4j, 4 - 3j, 2 + 0j, 0 + 1j])
np.abs(x)
Out[14]: array([5., 5., 2., 1.])

Trigonometrische Funktionen
NumPy bietet eine große Anzahl nützlicher Funktionen, und einige der nützlichsten für die

Datenwissenschaftler sind die trigonometrischen Funktionen. Wir beginnen mit der Definition eines Arrays von
Winkeln:

In [15]: theta = np.linspace(0, np.pi, 3)

Nun können wir mit diesen Werten einige trigonometrische Funktionen berechnen:

In [16]: print("theta = ", theta)
print("sin(theta) = ", np.sin(theta))
print("cos(theta) = ", np.cos(theta))
print("tan(theta) = ", np.tan(theta))
Out[16]: theta = [0. 1.57079633 3.14159265]
sin(theta) = [0.0000000e+00 1.0000000e+00 1.2246468e-16]
cos(theta) = [ 1.000000e+00 6.123234e-17 -1.000000e+00]
tan(theta) = [ 0.00000000e+00 1.63312394e+16 -1.22464680e-16]

Die Werte werden mit der Genauigkeit einer Maschine berechnet, weshalb Werte, die

Null sein sollten, treffen nicht immer genau Null. Inverse trigonometrische Funktionen sind auch

verfügbar:

In [17]: x = [-1, 0, 1]
print("x = ", x)
print("arcsin(x) = ", np.arcsin(x))
print("arccos(x) = ", np.arccos(x))
print("arctan(x) = ", np.arctan(x))
Out[17]: x = [-1, 0, 1]
arcsin(x) = [-1.57079633 0. 1.57079633]

NumPy's Ufuncs erforschen | 55
arccos(x) = [3.14159265 1.57079633 0. ]
arctan(x) = [-0.78539816 0. 0.78539816]

Exponenten und Logarithmen
Andere gängige Operationen, die in NumPy ufuncs verfügbar sind, sind die Exponentiale:

In [18]: x = [1, 2, 3]
print("x =", x)
print("e^x =", np.exp(x))
print("2^x =", np.exp2(x))
print("3^x =", np.power(3., x))
Out[18]: x = [1, 2, 3]
e^x = [ 2.71828183 7.3890561 20.08553692]
2^x = [2. 4. 8.]
3^x = [ 3. 9. 27.]

Die Inverse der Exponentiale, die Logarithmen, sind ebenfalls verfügbar. Das grundlegende np.log

ergibt den natürlichen Logarithmus; wenn Sie es vorziehen, den Logarithmus zur Basis 2 oder den

Logarithmus zur Basis 10, sind diese ebenfalls verfügbar:

In [19]: x = [1, 2, 4, 10]
print("x =", x)
print("ln(x) =", np.log(x))
print("log2(x) =", np.log2(x))
print("log10(x) =", np.log10(x))
Out[19]: x = [1, 2, 4, 10]
ln(x) = [0. 0.69314718 1.38629436 2.30258509]
log2(x) = [0. 1. 2. 3.32192809]
log10(x) = [0. 0.30103 0.60205999 1. ]

Es gibt auch einige spezielle Versionen, die für die Aufrechterhaltung der Präzision nützlich sind

mit sehr geringem Aufwand:

In [20]: x = [0, 0,001, 0,01, 0,1]
print("exp(x) - 1 =", np.expm1(x))
print("log(1 + x) =", np.log1p(x))
Out[20]: exp(x) - 1 = [0. 0.0010005 0.01005017 0.10517092]
log(1 + x) = [0. 0.0009995 0.00995033 0.09531018]

Wenn x sehr klein ist, liefern diese Funktionen genauere Werte als die rohe np.log

oder np.exp verwendet werden sollte.

Spezialisierte Ufuncs
NumPy verfügt über viele weitere Ufuncs, unter anderem für hyperbolische Trigonometrie, Bit-

weise Arithmetik, Vergleichsoperationen, Umrechnung von Bogenmaß in Grad, Rundung

und Reste, und vieles mehr. Ein Blick in die NumPy-Dokumentation
zeigt eine Menge interessanter Funktionen.

56 | Kapitel 6: Berechnungen auf NumPy-Arrays: Universelle Funktionen

Eine weitere ausgezeichnete Quelle für speziellere Ufuncs ist das Submodul scipy.spe

cial. Wenn Sie eine obskure mathematische Funktion mit Ihren Daten berechnen wollen,

ist es wahrscheinlich, dass sie in scipy.special implementiert ist. Es gibt viel zu viele Funktionen, um

alle aufzulisten, aber der folgende Ausschnitt zeigt ein paar, die in einem statistischen Kontext auftauchen könnten
Kontext auftauchen könnten:

In [21]: from scipy import special

In [22]: # Gamma-Funktionen (verallgemeinerte Faktorielle) und verwandte Funktionen
x = [1, 5, 10]
print("gamma(x) =", special.gamma(x))
print("ln|gamma(x)| =", special.gammaln(x))
print("beta(x, 2) =", special.beta(x, 2))
Out[22]: gamma(x) = [1.0000e+00 2.4000e+01 3.6288e+05]
ln|gamma(x)| = [ 0. 3.17805383 12.80182748]
beta(x, 2) = [0,5 0,03333333 0,00909091]

In [23]: _# Fehlerfunktion (Integral von Gauß),

sein Komplement und seine Inverse_
x = np.array([0, 0.3, 0.7, 1.0])
print("erf(x) =", spezial.erf(x))
print("erfc(x) =", spezial.erfc(x))
print("erfinv(x) =", spezial.erfinv(x))
Out[23]: erf(x) = [0. 0.32862676 0.67780119 0.84270079]
erfc(x) = [1. 0.67137324 0.32219881 0.15729921]
erfinv(x) = [0. 0.27246271 0.73286908 inf]

Sowohl in NumPy als auch in scipy.special sind viele, viele weitere ufuncs verfügbar.

Da die Dokumentation dieser Pakete online verfügbar ist, findet eine Websuche nach
nach "gamma function python" in der Regel die relevanten Informationen finden.

Erweiterte Ufunc-Funktionen
Viele NumPy-Benutzer verwenden ufuncs, ohne jemals den vollen Funktionsumfang kennenzulernen.

Ich werde hier ein paar spezielle Funktionen von ufuncs beschreiben.

Ausgabe spezifizieren
Bei umfangreichen Berechnungen ist es manchmal nützlich, das Array angeben zu können, in dem die

das Ergebnis der Berechnung gespeichert werden soll. Für alle ufuncs kann dies über das Argument out
Argument der Funktion erfolgen:

In [24]: x = np.arange(5)
y = np.empty(5)
np.multiply(x, 10, out=y)
print(y)
Out[24]: [ 0. 10. 20. 30. 40.]

Erweiterte Ufunc-Funktionen | 57
Dies kann sogar mit Array-Ansichten verwendet werden. Zum Beispiel können wir die Ergebnisse einer

Berechnung auf jedes andere Element eines angegebenen Arrays:

In [25]: y = np.zeros(10)
np.power(2, x, out=y[::2])
print(y)
Out[25]: [ 1. 0. 2. 0. 4. 0. 8. 0. 16. 0.]

Hätten wir stattdessen y[::2] = 2 ** x geschrieben, hätte dies zu folgenden Ergebnissen geführt

eines temporären Arrays zur Aufnahme der Ergebnisse von 2 ** x, gefolgt von einer zweiten Operation

Kopieren dieser Werte in das y-Array. Dies macht keinen großen Unterschied für solche

eine kleine Berechnung, aber bei sehr großen Arrays sind die Speichereinsparungen durch die sorgfältige Verwendung von

kann das Out-Argument von Bedeutung sein.

Aggregate
Bei binären ufuncs können Aggregationen direkt aus dem Objekt berechnet werden. Zum Beispiel

Wenn wir ein Array mit einer bestimmten Operation verkleinern wollen, können wir die reduce
Methode einer beliebigen ufunc verwenden. Ein reduce wendet wiederholt eine bestimmte Operation auf die Elemente an

eines Arrays, bis nur noch ein einziges Ergebnis übrig ist.

Der Aufruf von reduce auf die add ufunc gibt zum Beispiel die Summe aller Elemente in der

Array:

In [26]: x = np.arange(1, 6)
np.add.reduce(x)
Out[26]: 15

In ähnlicher Weise ergibt der Aufruf von reduce für die Multiply-Ufunc das Produkt aller Array-Elemente.

ments:

In [27]: np.multiply.reduce(x)
Out[27]: 120

Wenn wir alle Zwischenergebnisse der Berechnung speichern möchten, können wir stattdessen

akkumulieren:

In [28]: np.add.accumulate(x)
Out[28]: array([ 1, 3, 6, 10, 15])

In [29]: np.multiply.accumulate(x)
Out[29]: array([ 1, 2, 6, 24, 120])

Beachten Sie, dass es für diese besonderen Fälle spezielle NumPy-Funktionen gibt, die die Berechnung von

die Ergebnisse (np.sum, np.prod, np.cumsum, np.cumprod), die wir in
Kapitel 7.

58 | Kapitel 6: Berechnungen auf NumPy-Arrays: Universelle Funktionen

Äußere Produkte
Schließlich kann jede ufunc die Ausgabe aller Paare von zwei verschiedenen Eingaben berechnen, indem sie

die äußere Methode. Dies ermöglicht es Ihnen, in einer Zeile Dinge zu tun, wie z. B. eine Multiplika-

tionstabelle:

In [30]: x = np.arange(1, 6)
np.multiply.outer(x, x)
Out[30]: array([[ 1, 2, 3, 4, 5],
[ 2, 4, 6, 8, 10],
[ 3, 6, 9, 12, 15],
[ 4, 8, 12, 16, 20],
[ 5, 10, 15, 20, 25]])

Die Methoden ufunc.at und ufunc.reduceat sind ebenfalls nützlich, und wir werden uns
sie in Kapitel 10.

Wir werden auch auf die Fähigkeit von ufuncs stoßen, zwischen Arrays unterschiedlicher
Formen und Größen zu operieren, eine Reihe von Operationen, die als Broadcasting bekannt sind. Dieses Thema ist wichtig

so wichtig, dass wir ihm ein ganzes Kapitel widmen werden (siehe Kapitel 8).

Ufuncs: Mehr lernen
Weitere Informationen zu universellen Funktionen (einschließlich der vollständigen Liste der verfügbaren Funktionen)

tionen) finden Sie auf den Dokumentationswebseiten von NumPy und SciPy.

Sie können auch direkt von IPython aus auf Informationen zugreifen, indem Sie -

der Pakete und die Verwendung der Tabulatorvervollständigung und der Hilfe (?) von IPython, als

beschrieben in Kapitel 1.

Ufuncs: Mehr lernen | 59
KAPITEL 7

Aggregationen: min, max und
Alles dazwischen
Ein erster Schritt bei der Untersuchung eines beliebigen Datensatzes besteht häufig darin, verschiedene zusammenfassende Statistiken zu berechnen.
Die vielleicht gebräuchlichsten zusammenfassenden Statistiken sind der Mittelwert und die Standardabweichung,

die es Ihnen ermöglichen, die "typischen" Werte in einem Datensatz zusammenzufassen, aber andere Aggregationen

sind ebenfalls nützlich (Summe, Produkt, Median, Minimum und Maximum, Quantile,
usw.).

NumPy hat schnelle integrierte Aggregationsfunktionen für die Arbeit mit Arrays; wir werden
und probieren einige von ihnen aus.

Summierung der Werte in einem Array
Ein kurzes Beispiel: Berechnen Sie die Summe aller Werte in einem Array. Python

Sie selbst können dies mit der eingebauten Summenfunktion tun:

In [1]: import numpy as np
rng = np.random.default_rng()

In [2]: L = rng.random(100)
sum(L)
Out[2]: 52.76825337322368

Die Syntax ist der der Summenfunktion von NumPy sehr ähnlich, und das Ergebnis ist dasselbe
im einfachsten Fall:

In [3]: np.sum(L)
Out[3]: 52.76825337322366

60
Da die Operation jedoch in kompiliertem Code ausgeführt wird, ist die NumPy-Version der

Operation wird viel schneller berechnet:

In [4]: big_array = rng.random(1000000)
% timeit sum(big_array)
% timeit np.sum(big_array)
Out[4]: 89.9 ms ± 233 μs pro Schleife (Mittelwert ± std. dev. von 7 Durchläufen, je 10 Schleifen)
521 μs ± 8,37 μs pro Schleife (Mittelwert ± Standardabweichung von 7 Läufen mit je 1000 Schleifen)

Aber Vorsicht: Die Funktion sum und die Funktion np.sum sind nicht identisch, was

können manchmal zu Verwirrung führen! Insbesondere haben ihre optionalen Argumente unterschiedliche...

ent bedeutet (sum(x, 1) initialisiert die Summe bei 1 , während np.sum(x, 1) die Summe entlang

Achse 1 ), und np.sum kennt mehrere Array-Dimensionen, wie wir im Folgenden sehen werden.

Abschnitt.

Minimum und Maximum
In ähnlicher Weise verfügt Python über eingebaute Min- und Max-Funktionen, mit denen sich der Mindestwert ermitteln lässt

und den Maximalwert eines bestimmten Arrays:

In [5]: min(big_array), max(big_array)
Out[5]: (2.0114398036064074e-07, 0.9999997912802653)

Die entsprechenden Funktionen von NumPy haben eine ähnliche Syntax und arbeiten ebenfalls viel schneller
schneller:

In [6]: np.min(big_array), np.max(big_array)
Out[6]: (2.0114398036064074e-07, 0.9999997912802653)

In [7]: % timeit min(big_array)
% timeit np.min(big_array)
Out[7]: 72 ms ± 177 μs pro Schleife (Mittelwert ± std. Abweichung von 7 Durchläufen, je 10 Schleifen)
564 μs ± 3,11 μs pro Schleife (Mittelwert ± Standardabweichung von 7 Läufen mit je 1000 Schleifen)

Für min, max, sum und verschiedene andere NumPy-Aggregate ist eine kürzere Syntax die Verwendung von
Methoden des Array-Objekts selbst:

In [8]: print(big_array.min(), big_array.max(), big_array.sum())
Out[8]: 2.0114398036064074e-07 0.9999997912802653 499854.0273321711

Wenn immer möglich, stellen Sie sicher, dass Sie die NumPy-Version dieser Aggregate verwenden.

Gates bei der Bearbeitung von NumPy-Arrays!

Mehrdimensionale Aggregate
Eine häufige Art der Aggregationsoperation ist ein Aggregat entlang einer Zeile oder Spalte.

Angenommen, Sie haben einige Daten in einem zweidimensionalen Array gespeichert:

In [9]: M = rng.integers(0, 10, (3, 4))
print(M)
Out[9]: [[0 3 1 2]

Minimum und Maximum | 61
[1 9 7 0]
[4 8 3 7]]
NumPy-Aggregationen werden auf alle Elemente eines mehrdimensionalen Arrays angewendet:

In [10]: M.sum()
Out[10]: 45

Aggregationsfunktionen benötigen ein zusätzliches Argument, das die Achse angibt, entlang derer
das Aggregat berechnet wird. Zum Beispiel können wir den Mindestwert innerhalb jeder

Spalte durch Angabe von Achse=0:

In [11]: M.min(Achse=0)
Out[11]: array([0, 3, 1, 0])

Die Funktion gibt vier Werte zurück, die den vier Zahlenspalten entsprechen.

Auf ähnliche Weise können wir den Höchstwert in jeder Zeile ermitteln:

In [12]: M.max(Achse=1)
Out[12]: array([3, 9, 8])

Die Art und Weise, wie die Achse hier spezifiziert wird, kann für Benutzer, die aus anderen Ländern kommen, verwirrend sein.

lichkeiten. Das Schlüsselwort axis gibt die Dimension des Arrays an, das kollabiert werden soll,

und nicht die Dimension, die zurückgegeben werden soll. Die Angabe von Achse=0 bedeutet also, dass die Achse

0 werden kollabiert: bei zweidimensionalen Arrays werden die Werte innerhalb jeder Spalte

aggregiert.

Andere Aggregationsfunktionen
NumPy bietet mehrere andere Aggregationsfunktionen mit einer ähnlichen API und zusätzlichen...

tionell haben die meisten ein NaN-sicheres Gegenstück, das das Ergebnis berechnet und dabei ignoriert

fehlende Werte, die durch den speziellen IEEE-Gleitkomma-NaN-Wert gekennzeichnet sind (siehe

Kapitel 16).

Tabelle 7-1 enthält eine Liste von nützlichen Aggregationsfunktionen, die in NumPy verfügbar sind.

Tabelle 7-1. In NumPy verfügbare Aggregationsfunktionen

Funktionsname NaN-sichere Version Beschreibung
np.sum np.nansum Berechnung der Summe der Elemente
np.prod np.nanprod Berechnet das Produkt der Elemente
np.mean np.nanmean Berechnung des Mittelwerts der Elemente
np.std np.nanstd Berechnung der Standardabweichung
np.var np.nanvar Berechnung der Varianz
np.min np.nanmin Minimalwert ermitteln
np.max np.nanmax Ermitteln des Höchstwertes
np.argmin np.nanargmin Ermitteln des Index des Minimalwertes
62 | Kapitel 7: Aggregationen: min, max, und alles dazwischen

Funktionsname NaN-sichere Version Beschreibung
np.argmax np.nanargmax Index des Maximalwerts finden
np.median np.nanmedian Berechnung des Medians von Elementen
np.percentile np.nanpercentile Berechnung der rangbasierten Statistik der Elemente
np.any N/A Auswertung, ob ein beliebiges Element wahr ist
np.all N/A Auswertung, ob alle Elemente wahr sind
Sie werden diese Aggregate im weiteren Verlauf des Buches häufig sehen.

Beispiel: Wie hoch ist die durchschnittliche Größe der US-Präsidenten?
Die in NumPy verfügbaren Aggregate können als zusammenfassende Statistiken für eine Gruppe von Werten dienen. Als ein

Ein kleines Beispiel: die Körpergröße aller US-Präsidenten. Diese Daten sind verfügbar in
der Datei president_heights.csv, die eine durch Kommata getrennte Liste von Bezeichnungen und Werten ist:

In [13]: !head -4 data/president_heights.csv
Out[13]: Reihenfolge,Name,Höhe(cm)
1,George Washington,189
2,John Adams,170
3,Thomas Jefferson,189

Wir werden das Pandas-Paket verwenden, das wir in Teil III genauer untersuchen werden, um die
Datei zu lesen und diese Informationen zu extrahieren (beachten Sie, dass die Höhen in Zentimetern gemessen werden):

In [14]: import pandas as pd
Daten = pd.read_csv('Daten/Präsidenten_Höhen.csv')
höhen = np.array(daten['höhe(cm)'])
print(höhen)
Out[14]: [189 170 189 163 183 171 185 168 173 183 173 173 175 178 183 193 178 173
174 183 183 168 170 178 182 180 183 178 182 188 175 179 183 193 182 183
177 185 188 188 182 185 191 182]

Jetzt, da wir diese Datenreihe haben, können wir eine Reihe von zusammenfassenden Statistiken berechnen:

In [15]: print("Mittlere Höhe: ", höhen.mittel())
print("Standardabweichung:", höhen.std())
print("Minimale Höhe: ", höhen.min())
print("Maximale Höhe: ", höhen.max())
Out[15]: Mittlere Höhe: 180.0454545454545453
Standardabweichung: 6.983599441335736
Minimale Höhe: 163
Maximale Höhe: 193

Beachten Sie, dass in jedem Fall die Aggregationsoperation das gesamte Array auf einen einzigen
zusammenfassenden Wert reduziert, der uns Informationen über die Verteilung der Werte liefert. Wir

können auch Quantile berechnen:

In [16]: print("25. Perzentil: ", np.percentile(höhen, 25))
print("Median: ", np.median(höhen))
print("75. Perzentil: ", np.percentile(höhen, 75))

Beispiel: Wie groß sind die US-Präsidenten im Durchschnitt? | 63
Ergebnis[16]: 25. Perzentil: 174,75
Median: 182.0
75. Perzentil: 183,5

Die durchschnittliche Körpergröße der US-Präsidenten liegt bei 182 cm, also bei knapp 1,80 m.

Natürlich ist es manchmal nützlicher, eine visuelle Darstellung dieser Daten zu sehen, die

können wir mit den Werkzeugen von Matplotlib erreichen (wir werden Matplotlib ausführlicher in

Teil IV). Dieser Code erzeugt zum Beispiel Abbildung 7-1:

In [17]: % matplotlib inline
import matplotlib.pyplot as plt
plt.style.use('seaborn-whitegrid')

In [18]: plt.hist(höhen)
plt.title('Größenverteilung der US-Präsidenten')
plt.xlabel('Höhe (cm)')
plt.ylabel('Anzahl');

Abbildung 7-1. Histogramm der Höhen der Präsidenten

64 | Kapitel 7: Aggregationen: min, max, und alles dazwischen

KAPITEL 8

Berechnungen auf Arrays: Übertragen von
In Kapitel 6 haben wir gesehen, wie die universellen Funktionen von NumPy verwendet werden können, um Operationen zu vektorisieren.
Operationen zu vektorisieren und dadurch langsame Python-Schleifen zu vermeiden. In diesem Kapitel geht es um Broadcasting: ein

eine Reihe von Regeln, mit denen NumPy binäre Operationen (z. B. Addition, Subtraktion) durchführen kann.

tion, Multiplikation usw.) zwischen Feldern unterschiedlicher Größe und Form.

Einführung in das Broadcasting
Erinnern Sie sich daran, dass bei Arrays gleicher Größe binäre Operationen auf einer
Element-für-Element-Basis durchgeführt werden:

In [1]: import numpy as np

In [2]: a = np.array([0, 1, 2])
b = np.array([5, 5, 5])
a + b
Out[2]: array([5, 6, 7])

Mit Hilfe von Broadcasting können diese Arten von Binäroperationen auf Arrays unterschiedlicher Größe durchgeführt werden.
verschiedenen Größen durchgeführt werden - zum Beispiel können wir genauso einfach einen Skalar hinzufügen (stellen Sie sich das als Null vor).

dimensionales Array) in ein Array:

In [3]: a + 5
Out[3]: array([5, 6, 7])

Wir können uns dies als eine Operation vorstellen, die den Wert 5 in den Bereich

Array [5, 5, 5], und addiert die Ergebnisse.

65
Wir können diese Idee auf ähnliche Weise auf Felder höherer Dimensionen ausweiten. Beachten Sie das Ergebnis

wenn wir ein eindimensionales Feld zu einem zweidimensionalen Feld hinzufügen:

In [4]: M = np.ones((3, 3))
M
Out[4]: array([[1., 1., 1.],
[1., 1., 1.],
[1., 1., 1.]])

In [5]: M + a
Out[5]: array([[1., 2., 3.],
[1., 2., 3.],
[1., 2., 3.]])

Hier wird das eindimensionale Array a über die zweite Ebene gestreckt bzw. übertragen.

um der Form von M zu entsprechen.

Während diese Beispiele relativ einfach zu verstehen sind, können kompliziertere Fälle
die Übertragung beider Arrays beinhalten. Betrachten Sie das folgende Beispiel:

In [6]: a = np.arange(3)
b = np.arange(3)[:, np.newaxis]

drucken(a)
print(b)
Out[6]: [0 1 2]
[[0]
[1]
[2]]

In [7]: a + b
Out[7]: array([[0, 1, 2],
[1, 2, 3],
[2, 3, 4]])

Wie zuvor haben wir einen Wert gestreckt oder übertragen, um ihn an die Form des anderen anzupassen,

Hier haben wir a und b gestreckt, um eine gemeinsame Form zu erhalten, und das Ergebnis ist ein zweidimensionales
dimensionales Feld! Die Geometrie dieser Beispiele wird in Abbildung 8-1 veranschaulicht.

Die Leuchtkästen stellen die ausgestrahlten Werte dar. Diese Art des Denkens über Broad-
kann Fragen über die Effizienz des Broadcasting in Bezug auf den Speicherverbrauch aufwerfen, aber Sorge

nicht: Beim NumPy-Broadcasting werden die gesendeten Werte nicht wirklich in den Speicher kopiert.

Dennoch kann dies ein nützliches mentales Modell sein, wenn wir über Rundfunk und Fernsehen nachdenken.

66 | Kapitel 8: Berechnungen auf Arrays: Übermittlung

1 Der Code zur Erstellung dieser Grafik ist im Online-Anhang zu finden.
Abbildung 8-1. Visualisierung von NumPy-Übertragungen (angepasst an eine in der astroML-Dokumentation veröffentlichte Quelle
der astroML-Dokumentation und mit Genehmigung verwendet)^1

Regeln für Broadcasting
Broadcasting in NumPy folgt einer strengen Reihe von Regeln, um die Interaktion zu bestimmen

zwischen den beiden Feldern:

Regel 1
Unterscheiden sich die beiden Arrays in der Anzahl ihrer Dimensionen, wird die Form des Arrays mit
mit weniger Dimensionen mit Einsen an der führenden (linken) Seite aufgefüllt.

Regel 2

Wenn die Form der beiden Arrays in einer Dimension nicht übereinstimmt, wird das Array mit
Form gleich 1 in dieser Dimension gestreckt, um der anderen Form zu entsprechen.
Regel 3

Wenn die Größen in irgendeiner Dimension nicht übereinstimmen und keine gleich 1 ist, wird ein Fehler ausgegeben.

Um diese Regeln zu verdeutlichen, wollen wir einige Beispiele im Detail betrachten.

Regeln des Rundfunks | 67
Broadcasting Beispiel 1
Angenommen, wir wollen ein zweidimensionales Feld zu einem eindimensionalen Feld hinzufügen:

In [8]: M = np.ones((2, 3))
a = np.arange(3)

Betrachten wir eine Operation mit diesen beiden Feldern, die die folgenden Formen haben:

M.shape ist (2, 3)
a.shape ist (3,)
Anhand von Regel 1 sehen wir, dass das Array a weniger Dimensionen hat, also füllen wir es auf der linken Seite mit
Einsen auf:

M.shape bleibt (2, 3)
a.shape wird zu (1, 3)
Nach Regel 2 sehen wir nun, dass die erste Dimension nicht übereinstimmt, also strecken wir diese Dimension

zu entsprechen:

M.Form bleibt (2, 3)
a.shape wird zu (2, 3)
Die Formen stimmen nun überein, und wir sehen, dass die endgültige Form (2, 3) sein wird:

In [9]: M + a
Out[9]: array([[1., 2., 3.],
[1., 2., 3.]])

Broadcasting Beispiel 2
Schauen wir uns nun ein Beispiel an, bei dem beide Arrays übertragen werden müssen:

In [10]: a = np.arange(3).reshape((3, 1))
b = np.arange(3)

Auch hier beginnen wir damit, die Formen der Arrays zu bestimmen:

a.Form ist (3, 1)
b.Form ist (3,)
Regel 1 besagt, dass wir die Form von b mit Einsen auffüllen müssen:

a.shape bleibt (3, 1)
b.Form wird (1, 3)
68 | Kapitel 8: Berechnungen auf Arrays: Übertragen

Und Regel 2 besagt, dass wir jede dieser 1en so erweitern müssen, dass sie der entsprechenden
Größe des anderen Arrays anzupassen:

a.Form wird (3, 3)
b.Form wird zu (3, 3)
Da die Ergebnisse übereinstimmen, sind diese Formen kompatibel. Wir können dies hier sehen:

In [11]: a + b
Out[11]: array([[0, 1, 2],
[1, 2, 3],
[2, 3, 4]])

Broadcasting Beispiel 3
Als Nächstes wollen wir uns ein Beispiel ansehen, bei dem die beiden Arrays nicht kompatibel sind:

In [12]: M = np.ones((3, 2))
a = np.arange(3)

Die Situation ist nur etwas anders als im ersten Beispiel: Die Matrix M ist

transponiert. Wie wirkt sich dies auf die Berechnung aus? Die Formen der Arrays sind wie folgt
wie folgt:

M.shape ist (3, 2)
a.shape ist (3,)
Wieder sagt uns Regel 1, dass wir die Form von a mit Einsen auffüllen müssen:

M.Form bleibt (3, 2)
a.Form wird (1, 3)
Nach Regel 2 wird die erste Dimension von a so gestreckt, dass sie der von M entspricht:

M.Form bleibt (3, 2)
a.shape wird zu (3, 3)
Jetzt treffen wir auf Regel 3 - die endgültigen Formen stimmen nicht überein, so dass diese beiden Arrays inkompatibel sind.
inkompatibel, wie wir durch diesen Vorgang feststellen können:

In [13]: M + a
ValueError : Operanden konnten nicht zusammen mit Formen übertragen werden (3,2) (3,)

Beachten Sie die mögliche Verwirrung hier: Sie könnten sich vorstellen, a und M kompatibel zu machen, indem Sie,

z. B. die Form von a mit Einsen auf der rechten Seite statt auf der linken Seite aufzufüllen. Aber das ist nicht die
die Übertragungsregeln funktionieren! Diese Art von Flexibilität mag in manchen Fällen nützlich sein, aber

würde das zu möglichen Unklarheiten führen. Wenn Sie die rechte Seite auffüllen möchten,

Regeln der Übertragung | 69
können Sie dies explizit tun, indem Sie das Array umformen (wir verwenden dazu das Schlüsselwort np.newaxis
das in Kapitel 5 eingeführt wurde):

In [14]: a[:, np.newaxis].shape
Out[14]: (3, 1)

In [15]: M + a[:, np.newaxis]
Out[15]: array([[1., 1.],
[2., 2.],
[3., 3.]])

Obwohl wir uns hier auf den Operator + konzentriert haben, gelten diese Übertragungsregeln auch für

jede binäre ufunc. Hier ist zum Beispiel die Funktion logaddexp(a, b), die eine

log(exp(a) + exp(b)) mit höherer Genauigkeit als der naive Ansatz:

In [16]: np.logaddexp(M, a[:, np.newaxis])
Out[16]: array([[1.31326169, 1.31326169],
[1.69314718, 1.69314718],
[2.31326169, 2.31326169]])

Weitere Informationen über die vielen verfügbaren Universalfunktionen finden Sie in Kapitel 6.

Broadcasting in der Praxis
Broadcasting-Vorgänge bilden den Kern vieler Beispiele, die Sie in diesem Buch sehen werden
Buches. Wir werden uns nun einige Beispiele ansehen, in denen sie nützlich sein können.

Ein Array zentrieren
In Kapitel 6 haben wir gesehen, dass ufuncs einem NumPy-Benutzer erlauben, die Notwendigkeit zu beseitigen, explizit
langsame Python-Schleifen zu schreiben. Broadcasting erweitert diese Fähigkeit. Eine häufig anzutreffende

Ein Beispiel aus der Datenwissenschaft ist das Subtrahieren des zeilenweisen Mittelwerts von einer Datenreihe.
Stellen Sie sich vor, Sie haben ein Array mit 10 Beobachtungen, von denen jede aus 3 Werten besteht.

Nach der Standardkonvention (siehe Kapitel 38) speichern wir dies in einem 10 × 3-Array:

In [17]: rng = np.random.default_rng(seed=1701)
X = rng.random((10, 3))

Wir können den Mittelwert jeder Spalte berechnen, indem wir das Mittelwertaggregat über die erste
Dimension:

In [18]: XMittel = X.Mittelwert(0)
XMittelwert
Out[18]: array([0.38503638, 0.36991443, 0.63896043])

Jetzt können wir das X-Array zentrieren, indem wir den Mittelwert subtrahieren (dies ist eine Übertragung

Betrieb):

In [19]: X_zentriert = X - XMittel

70 | Kapitel 8: Berechnungen auf Arrays: Übermittlung

Um zu überprüfen, ob wir das richtig gemacht haben, können wir prüfen, ob das zentrierte Array

hat einen Mittelwert nahe Null:

In [20]: X_centered.mean(0)
Out[20]: array([ 4.99600361e-17, -4.44089210e-17, 0.00000000e+00])

Mit maschineller Genauigkeit ist der Mittelwert nun Null.

Zeichnen einer zweidimensionalen Funktion
Ein Ort, an dem sich die Übertragung oft als nützlich erweist, ist die Anzeige von Bildern auf der Grundlage von

zweidimensionale Funktionen. Wenn wir eine Funktion z=fx,y definieren wollen, kann broadcasting
verwendet werden, um die Funktion über das gesamte Gitter zu berechnen:

In [21]: # x und y haben 50 Schritte von 0 bis 5
x = np.linspace(0, 5, 50)
y = np.linspace(0, 5, 50)[:, np.newaxis]

z = np.sin(x) ** 10 + np.cos(10 + y * x) * np.cos(x)

Wir verwenden Matplotlib, um dieses zweidimensionale Array darzustellen, wie in Abbildung 8-2 gezeigt (diese

Werkzeuge werden in Kapitel 28 ausführlich behandelt):

In [22]: % matplotlib inline
import matplotlib.pyplot as plt

In [23]: plt.imshow(z, origin='lower', extent=[0, 5, 0, 5])
plt.colorbar();

Abbildung 8-2. Visualisierung eines 2D-Arrays

Das Ergebnis ist eine überzeugende Visualisierung der zweidimensionalen Funktion.

Rundfunk in der Praxis | 71
KAPITEL 9

Vergleiche, Masken und Boolesche Logik
Dieses Kapitel behandelt die Verwendung von booleschen Masken zur Untersuchung und Manipulation von Werten
innerhalb von NumPy-Arrays. Masken kommen zum Einsatz, wenn Sie Werte extrahieren, ändern, zählen oder

Werte in einem Array auf der Grundlage eines Kriteriums anderweitig manipulieren: Sie können zum Beispiel

alle Werte zählen, die größer als ein bestimmter Wert sind, oder alle Ausreißer entfernen, die
über einem bestimmten Schwellenwert liegen. In NumPy ist die boolesche Maskierung oft die effizienteste

Art und Weise, diese Art von Aufgaben zu bewältigen.

Beispiel: Zählen von Regentagen
Stellen Sie sich vor, Sie haben eine Reihe von Daten, die die Niederschlagsmenge eines jeden Tages
für ein Jahr in einer bestimmten Stadt darstellt. Hier laden wir zum Beispiel die täglichen Niederschlagsstatistiken für

der Stadt Seattle im Jahr 2015 unter Verwendung von Pandas (siehe Teil III):

In [1]: import numpy as np
from vega_datasets import data

# Verwenden Sie DataFrame-Operationen, um die Niederschlagsmenge als NumPy-Array zu extrahieren.
rainfall_mm = np.array(
data.seattle_weather().set_index('date')['precipitation']['2015'])
len(rainfall_mm)
Out[1]: 365

Das Array enthält 365 Werte, die die tägliche Niederschlagsmenge in Millimetern vom 1. Januar bis

31. Dezember 2015.

Als erste schnelle Visualisierung betrachten wir das Histogramm der Regentage in Abbildung 9-1,

die mit Matplotlib erstellt wurde (wir werden dieses Tool in

Teil IV):

In [2]: % matplotlib inline
import matplotlib.pyplot as plt
plt.style.use('seaborn-whitegrid')

72
In [3]: plt.hist(Niederschlag_mm, 40);

Abbildung 9-1. Histogramm der Niederschlagsmenge 2015 in Seattle

Dieses Histogramm gibt uns eine allgemeine Vorstellung davon, wie die Daten aussehen: Trotz des Rufs der Stadt
regnerischen Rufs der Stadt gab es in Seattle an der überwiegenden Mehrheit der Tage fast keinen gemessenen Niederschlag

im Jahr 2015. Aber das ist nicht gut geeignet, um einige Informationen zu vermitteln, die wir gerne

siehe: Wie viele Regentage gab es zum Beispiel im Jahr? Wie hoch war die durchschnittliche
Niederschlag an diesen regnerischen Tagen? Wie viele Tage gab es mit mehr als 10 mm

der Niederschläge?

Eine Möglichkeit wäre, diese Fragen von Hand zu beantworten: Wir könnten eine Schleife

durch die Daten, wobei ein Zähler jedes Mal inkrementiert wird, wenn wir Werte in bestimmten gewünschten

Bereich. Aus den in diesem Kapitel genannten Gründen ist ein solcher Ansatz jedoch sehr
ineffizient, sowohl was die Zeit für das Schreiben von Code als auch die Zeit für die Berechnung der

Ergebnis. Wir haben in Kapitel 6 gesehen, dass die ufuncs von NumPy anstelle von Schleifen verwendet werden können, um
Schleifen verwendet werden können, um schnelle elementweise arithmetische Operationen auf Arrays durchzuführen; auf die gleiche Weise können wir andere

ufuncs, um elementweise Vergleiche über Arrays durchzuführen, und wir können dann die

Ergebnisse, um die Fragen zu beantworten, die wir haben. Wir lassen die Daten erst einmal beiseite und besprechen
einige allgemeine Werkzeuge in NumPy, um mit Hilfe von Maskierung diese Art von Fragen schnell zu

Fragen.

Vergleichsoperatoren als Ufuncs
In Kapitel 6 wurden ufuncs eingeführt, wobei der Schwerpunkt auf den arithmetischen Operatoren lag. Wir

gesehen, dass die Verwendung von +, -, *, / und anderen Operatoren auf Arrays zu elementweisen Operationen führt.

tionen. NumPy implementiert auch Vergleichsoperatoren wie < (weniger als) und >

(größer als) als elementweise ufuncs. Das Ergebnis dieser Vergleichsoperatoren ist
immer ein Array mit einem booleschen Datentyp. Alle sechs der Standard-Vergleichsopera-

tionen zur Verfügung:

In [4]: x = np.array([1, 2, 3, 4, 5])

In [5]: x < 3 # kleiner als
Out[5]: array([ True , True , False , False , False ])

In [6]: x > 3 # größer als
Out[6]: array([ Falsch , Falsch , Falsch , Wahr , Wahr ])

In [7]: x <= 3 # kleiner als oder gleich
Out[7]: array([ Wahr , Wahr , Wahr , Falsch , Falsch ])

In [8]: x >= 3 # größer als oder gleich
Out[8]: array([ Falsch , Falsch , Wahr , Wahr , Wahr ])

Vergleichsoperatoren als Ufuncs | 73
In [9]: x != 3 # nicht gleich
Out[9]: array([ True , True , False , True , True ])

In [10]: x == 3 # gleich
Out[10]: array([ Falsch , Falsch , Wahr , Falsch , Falsch ])

Es ist auch möglich, einen elementweisen Vergleich von zwei Arrays durchzuführen und die

zusammengesetzte Ausdrücke:

In [11]: (2 * x) == (x ** 2)
Out[11]: array([ Falsch , Wahr , Falsch , Falsch , Falsch ])

Wie bei den arithmetischen Operatoren sind auch die Vergleichsoperatoren implementiert als

ufuncs in NumPy; wenn Sie zum Beispiel x < 3 schreiben, verwendet NumPy intern

np.less(x, 3). Eine Zusammenfassung der Vergleichsoperatoren und ihrer äquivalenten ufuncs

wird hier gezeigt:

Operator Äquivalent ufunc Operator Äquivalent ufunc
== np.equal != np.not_equal
< np.less <= np.less_equal
> np.greater >= np.greater_equal
Genau wie im Fall der arithmetischen ufuncs funktionieren diese auf Arrays jeder Größe und Form
Form. Hier ist ein zweidimensionales Beispiel:

In [12]: rng = np.random.default_rng(seed=1701)
x = rng.integers(10, size=(3, 4))
x
Out[12]: array([[9, 4, 0, 3],
[8, 6, 3, 1],
[3, 7, 4, 0]])

In [13]: x < 6
Out[13]: array([[ False , True , True , True ],
[ Falsch , Falsch , Wahr , Wahr ],
[ Wahr , Falsch , Wahr , Wahr ]])

In jedem Fall ist das Ergebnis ein boolesches Array, und NumPy bietet eine Reihe von direkten
Muster für die Arbeit mit diesen booleschen Ergebnissen.

Arbeiten mit booleschen Arrays
Mit einem booleschen Array können Sie eine Vielzahl nützlicher Operationen durchführen. Wir arbeiten

mit x, dem zweidimensionalen Array, das wir zuvor erstellt haben:

In [14]: print(x)
Out[14]: [[9 4 0 3]
[8 6 3 1]
[3 7 4 0]]

74 | Kapitel 9: Vergleiche, Masken und Boolesche Logik

Zählen von Einträgen
Um die Anzahl der wahren Einträge in einem booleschen Array zu zählen, ist np.count_nonzero nützlich:

In [15]: # wie viele Werte kleiner als 6?
np.count_nonzero(x < 6)
Out[15]: 8

Wir sehen, dass es acht Einträge in der Matrix gibt, die kleiner als 6 sind. Ein anderer Weg, um dies herauszufinden

Information ist die Verwendung von np.sum; in diesem Fall wird False als 0 interpretiert, und True ist inter-

als 1 festgelegt:

In [16]: np.sum(x < 6)
Out[16]: 8

Der Vorteil von np.sum ist, dass wie bei anderen NumPy-Aggregationsfunktionen, diese Summen-

Die Information kann auch entlang von Zeilen oder Spalten erfolgen:

In [17]: # Wie viele Werte kleiner als 6 in jeder Zeile?
np.sum(x < 6, axis=1)
Out[17]: array([3, 2, 3])

Hier wird die Anzahl der Werte kleiner als 6 in jeder Zeile der Matrix gezählt.

Wenn wir schnell überprüfen wollen, ob ein oder alle Werte Wahr sind, können wir

verwenden Sie (Sie haben es erraten) np.any oder np.all:

In [18]: # Gibt es irgendwelche Werte größer als 8?
np.any(x > 8)
Out[18]: True

In [19]: # Gibt es irgendwelche Werte kleiner als Null?
np.any(x < 0)
Out[19]: False

In [20]: # Sind alle Werte kleiner als 10?
np.all(x < 10)
Out[20]: True

In [21]: # sind alle Werte gleich 6?
np.all(x == 6)
Out[21]: False

np.all und np.any können auch entlang bestimmter Achsen verwendet werden. Zum Beispiel:

In [22]: # Sind alle Werte in jeder Zeile kleiner als 8?
np.all(x < 8, Achse=1)
Out[22]: array([ Falsch , Falsch , Wahr ])

In diesem Fall sind alle Elemente in der dritten Reihe kleiner als 8, während dies nicht der Fall ist für

andere.

Zum Schluss noch eine kurze Warnung: Wie in Kapitel 7 erwähnt, hat Python eine eingebaute Summe, any,

und alle Funktionen. Diese haben eine andere Syntax als die NumPy-Versionen, und in

Arbeiten mit booleschen Feldern | 75
schlägt fehl oder führt zu unbeabsichtigten Ergebnissen, wenn es auf mehrdimensionale

Arrays. Stellen Sie sicher, dass Sie np.sum, np.any und np.all für diese Beispiele verwenden!

Boolesche Operatoren
Wir haben bereits gesehen, wie wir z. B. alle Tage mit weniger als 20 mm Regen zählen können oder
alle Tage mit mehr als 10 mm Regen. Was aber, wenn wir wissen wollen, wie viele Tage

es mehr als 10 mm und weniger als 20 mm Regen gab? Wir können erreichen

dies mit den bitweisen logischen Operatoren von Python, &, |, ^ und ~. Wie bei den Standard-Arithmetik-

metischen Operatoren überlädt NumPy diese als ufuncs, die elementweise auf (üblicherweise
booleschen) Arrays arbeiten.

Eine solche zusammengesetzte Frage kann zum Beispiel wie folgt beantwortet werden:

In [23]: np.sum((Niederschlag_mm > 10) & (Niederschlag_mm < 20))
Out[23]: 16

Daraus geht hervor, dass es 16 Tage mit Niederschlägen zwischen 10 und 20 Millimetern gab.

Die Klammern sind hier wichtig. Aufgrund der Vorrangregeln für Operatoren würde dieser Ausdruck ohne die
Klammern entfernt, würde dieser Ausdruck wie folgt ausgewertet werden, was zu folgendem Ergebnis führt

einen Fehler:

Niederschlag_mm > (10 & Niederschlag_mm) < 20
Lassen Sie uns einen komplizierteren Ausdruck demonstrieren. Mit den Gesetzen von De Morgan können wir
können wir das gleiche Ergebnis auf eine andere Weise berechnen:

In [24]: np.sum(~( (Niederschlag_mm <= 10) | (Niederschlag_mm >= 20) ))
Out[24]: 16

Die Kombination von Vergleichsoperatoren und booleschen Operatoren auf Arrays kann zu einer breiten

eine Reihe von effizienten logischen Operationen.

Die folgende Tabelle gibt einen Überblick über die bitweisen Booleschen Operatoren und ihre Entsprechung

ufuncs:

Operator Äquivalent ufunc Operator Äquivalent ufunc
& np.bitweise_und np.bitweise_oder
^ np.bitwise_xor ~ np.bitwise_not
Mit diesen Werkzeugen können wir beginnen, viele Fragen zu beantworten, die wir über
unsere Wetterdaten. Hier sind einige Beispiele für Ergebnisse, die wir durch Kombinieren berechnen können.

boolesche Operationen mit Aggregationen:

In [25]: print("Anzahl Tage ohne Regen: ", np.sum(regenmenge_mm == 0))
print("Anzahl der Tage mit Regen: ", np.sum(regenmenge_mm != 0))
print("Tage mit mehr als 10 mm: ", np.sum(regenmenge_mm > 10))
print("Regentage mit < 5 mm: ", np.sum((Niederschlag_mm > 0) &

76 | Kapitel 9: Vergleiche, Masken und Boolesche Logik

(Niederschlag_mm < 5)))
Out[25]: Anzahl der Tage ohne Regen: 221
Anzahl der Tage mit Regen: 144
Tage mit mehr als 10 mm: 34
Regentage mit < 5 mm: 83

Boolesche Arrays als Masken
Im vorangegangenen Abschnitt haben wir uns Aggregate angesehen, die direkt auf booleschen
Arrays berechnet werden. Ein leistungsfähigeres Muster ist die Verwendung boolescher Arrays als Masken, um bestimmte

Teilmengen der Daten selbst. Kehren wir zu unserem x-Array von vorhin zurück:

In [26]: x
Out[26]: array([[9, 4, 0, 3],
[8, 6, 3, 1],
[3, 7, 4, 0]])

Angenommen, wir wollen ein Array mit allen Werten im Array, die kleiner sind als, sagen wir, 5. Wir können
ein boolesches Array für diese Bedingung leicht erhalten, wie wir bereits gesehen haben:

In [27]: x < 5
Out[27]: array([[ False , True , True , True ],
[ Falsch , Falsch , Wahr , Wahr ],
[ Wahr , Falsch , Wahr , Wahr ]])

Um nun diese Werte aus dem Array auszuwählen, können wir einfach auf dieses boolesche Array indizieren;
Dies wird als Maskierungsoperation bezeichnet:

In [28]: x[x < 5]
Out[28]: array([4, 0, 3, 3, 1, 3, 4, 0])

Zurückgegeben wird ein eindimensionales Array, das mit allen Werten gefüllt ist, die diese Bedingungen erfüllen

Bedingung, d. h. alle Werte an Positionen, an denen das Maskenfeld Wahr ist.

Mit diesen Werten können wir dann nach Belieben arbeiten. Zum Beispiel können wir Folgendes berechnen

einige einschlägige Statistiken zu unseren Seattle-Regendaten:

In [29]: # eine Maske mit allen Regentagen konstruieren
regnerisch = (regenmenge_mm > 0)

# eine Maske aller Sommertage konstruieren (der 21. Juni ist der 172. Tag)
Tage = np.arange(365)
Sommer = (Tage > 172) & (Tage < 262)

print("Medianer Niederschlag an Regentagen im Jahr 2015 (mm): ",
np.median(niederschlag_mm[regnerisch]))
print("Median des Niederschlags an Sommertagen im Jahr 2015 (mm): ",
np.median(niederschlag_mm[sommer]))
print("Maximaler Niederschlag an Sommertagen im Jahr 2015 (mm): ",
np.max(niederschlag_mm[sommer]))
print("Median des Niederschlags an nicht-sommerlichen Regentagen (mm):",
np.median(niederschlag_mm[regnerisch & ~sommer]))

Boolesche Arrays als Masken | 77
Out[29]: Median des Niederschlags an Regentagen im Jahr 2015 (mm): 3.8
Mittlerer Niederschlag an Sommertagen im Jahr 2015 (mm): 0.0
Maximaler Niederschlag an Sommertagen im Jahr 2015 (mm): 32.5
Mittlerer Niederschlag an nicht-sommerlichen Regentagen (mm): 4.1

Durch die Kombination von booleschen Operationen, Maskierungsoperationen und Aggregaten können wir sehr

diese Art von Fragen zu unserem Datensatz schnell beantworten.

Verwendung der Schlüsselwörter und/oder im Gegensatz zu den Operatoren &/|
Ein häufiger Punkt der Verwirrung ist der Unterschied zwischen den Schlüsselwörtern and und or

auf der einen Seite und die Operatoren & und | auf der anderen. Wann würden Sie eines davon verwenden

gegen die andere?

Der Unterschied ist folgender: und und oder wirken auf das Objekt als Ganzes, während & und |

auf die Elemente innerhalb des Objekts wirken.

Wenn Sie and oder or verwenden, ist es gleichbedeutend mit der Aufforderung an Python, das Objekt wie ein sin-

gle Boolesche Einheit. In Python werden alle ganzen Zahlen, die nicht Null sind, als True ausgewertet. Daher:

In [30]: bool(42), bool(0)
Out[30]: ( Wahr , Falsch )

In [31]: bool(42 und 0)
Out[31]: False

In [32]: bool(42 oder 0)
Out[32]: True

Wenn Sie & und | für ganze Zahlen verwenden, wirkt der Ausdruck auf die bitweise Darstellung des Elements.
Repräsentation des Elements, indem er das und oder das oder auf die einzelnen Bits anwendet, aus denen das Element

Nummer:

In [33]: bin(42)
Ausgang [33]: '0b101010'

In [34]: bin(59)
Out[34]: '0b111011'

In [35]: bin(42 & 59)
Out[35]: '0b101010'

In [36]: bin(42 | 59)
Out[36]: '0b111011'

Beachten Sie, dass die entsprechenden Bits der binären Darstellung in der folgenden Reihenfolge verglichen werden

um das Ergebnis zu erhalten.

Wenn Sie ein Array mit booleschen Werten in NumPy haben, können Sie sich das wie eine

Zeichenfolge von Bits, wobei 1 = Wahr und 0 = Falsch ist, und & und | funktionieren ähnlich wie in

die vorangegangenen Beispiele:

78 | Kapitel 9: Vergleiche, Masken und Boolesche Logik

In [37]: A = np.array([1, 0, 1, 0, 1, 0], dtype=bool)
B = np.array([1, 1, 1, 0, 1, 1], dtype=bool)
A | B
Out[37]: array([ True , True , True , False , True , True ])

Aber wenn Sie oder auf diese Arrays anwenden, wird es versuchen, die Wahrheit oder Falschheit des
gesamten Array-Objekts zu bewerten, was kein wohldefinierter Wert ist:

In [38]: A oder B
ValueError : Der Wahrheitswert eines Arrays mit mehr als einem Element ist

zweideutig.
a.any() oder a.all()

In ähnlicher Weise sollten Sie bei der Auswertung eines booleschen Ausdrucks auf einem gegebenen Array | oder

& anstelle von oder oder und:

In [39]: x = np.arange(10)
(x > 4) & (x < 8)
Out[39]: array([ Falsch , Falsch , Falsch , Falsch , Falsch , Falsch , Wahr , Wahr , Wahr , Falsch ,
False ])

Der Versuch, die Wahrheit oder Unwahrheit des gesamten Feldes zu bewerten, führt zu demselben Ergebnis

ValueError, den wir zuvor gesehen haben:

In [40]: (x > 4) und (x < 8)
ValueError : Der Wahrheitswert eines Arrays mit mehr als einem Element ist

zweideutig.
a.any() oder a.all()

Denken Sie also daran: und und oder führen eine einzige boolesche Auswertung für eine ganze

Objekts, während & und | mehrere boolesche Auswertungen des Inhalts durchführen (die indi-

viduelle Bits oder Bytes) eines Objekts. Bei booleschen NumPy-Arrays ist letzteres fast
immer die gewünschte Operation.

Verwendung der Schlüsselwörter und/oder gegenüber den Operatoren &/| | 79
KAPITEL 10

Ausgefallene Indizierung
In den vorangegangenen Kapiteln wurde erörtert, wie man auf Teile von Arrays zugreift und diese verändert, indem man

einfache Indizes (z. B. arr[0]), Slices (z. B. arr[:5]) und boolesche Masken (z. B. arr[arr

0]). In diesem Kapitel werden wir uns eine andere Art der Array-Indizierung ansehen, bekannt als fancy oder

vektorisierte Indizierung, bei der wir Arrays von Indizes anstelle von einzelnen Skalaren übergeben. Diese
ermöglicht es uns, sehr schnell auf komplizierte Teilmengen der Werte eines Arrays zuzugreifen und diese zu ändern.

Erkundung der Fancy-Indizierung
Fancy Indexing ist konzeptionell einfach: Es bedeutet die Übergabe eines Arrays von Indizes für den Zugriff auf

mehrere Array-Elemente auf einmal. Betrachten Sie zum Beispiel das folgende Array:

In [1]: import numpy as np
rng = np.random.default_rng(seed=1701)

x = rng.integers(100, size=10)
print(x)
Out[1]: [90 40 9 30 80 67 39 15 33 79]

Angenommen, wir wollen auf drei verschiedene Elemente zugreifen. Wir könnten das so machen:

In [2]: [x[3], x[7], x[2]]
Out[2]: [30, 15, 9]

Alternativ können wir eine einzelne Liste oder ein Array von Indizes übergeben, um das gleiche Ergebnis zu erhalten:

In [3]: ind = [3, 7, 4]
x[ind]
Out[3]: array([30, 15, 80])

Bei der Verwendung von Arrays von Indizes spiegelt die Form des Ergebnisses die Form des Indexes wider

Arrays und nicht die Form des Arrays, das indiziert wird:

80
In [4]: ind = np.array([[3, 7],
[4, 5]])
x[ind]
Out[4]: array([[30, 15],
[80, 67]])

Fancy Indexing funktioniert auch in mehreren Dimensionen. Betrachten Sie das folgende Array:

In [5]: X = np.arange(12).reshape((3, 4))
X
Out[5]: array([[ 0, 1, 2, 3],
[ 4, 5, 6, 7],
[ 8, 9, 10, 11]])

Wie bei der Standardindizierung bezieht sich der erste Index auf die Zeile und der zweite auf die
Spalte:

In [6]: row = np.array([0, 1, 2])
col = np.array([2, 1, 3])
X[row, col]
Out[6]: array([ 2, 5, 11])

Beachten Sie, dass der erste Wert des Ergebnisses X[0, 2] ist, der zweite X[1, 1], und der

der dritte ist X[2, 3]. Die Paarung der Indizes in der Fancy-Indizierung folgt allen Übertragungsregeln
Regeln, die in Kapitel 8 erwähnt wurden. Wenn wir also zum Beispiel eine Spalte vec-

tor und einem Zeilenvektor innerhalb der Indizes, erhalten wir ein zweidimensionales Ergebnis:

In [7]: X[row[:, np.newaxis], col]
Out[7]: array([[ 2, 1, 3],
[ 6, 5, 7],
[10, 9, 11]])

Hier wird jeder Zeilenwert mit jedem Spaltenvektor abgeglichen, genau wie wir es bei der breit angelegten...

Gießen von arithmetischen Operationen. Zum Beispiel:

In [8]: Zeile[:, np.newaxis] * col
Out[8]: array([[0, 0, 0],
[2, 1, 3],
[4, 2, 6]])

Bei einer ausgefallenen Indizierung ist es immer wichtig, daran zu denken, dass der Rückgabewert die

die übertragene Form der Indizes und nicht die Form des Arrays, das indiziert wird.

Kombinierte Indizierung
Für noch leistungsfähigere Operationen kann die Fancy Indexing mit den anderen

Indexierungsschemata, die wir gesehen haben. Zum Beispiel, gegeben das Array X:

In [9]: print(X)
Out[9]: [[ 0 1 2 3]
[ 4 5 6 7]
[ 8 9 10 11]]

Kombinierte Indizierung | 81
Wir können ausgefallene und einfache Indizes kombinieren:

In [10]: X[2, [2, 0, 1]]
Out[10]: array([10, 8, 9])

Wir können auch eine ausgefallene Indizierung mit Slicing kombinieren:

In [11]: X[1:, [2, 0, 1]]
Out[11]: array([[ 6, 4, 5],
[10, 8, 9]])

Und wir können eine ausgeklügelte Indizierung mit Maskierung kombinieren:

In [12]: mask = np.array([ True , False , True , False ])
X[row[:, np.newaxis], mask]
Out[12]: array([[ 0, 2],
[ 4, 6],
[ 8, 10]])

Alle diese Indizierungsoptionen zusammengenommen führen zu einer sehr flexiblen Reihe von Operationen für

effizienter Zugriff auf und Änderung von Array-Werten.

Beispiel: Zufällige Auswahl von Punkten
Eine häufige Anwendung von Fancy Indexing ist die Auswahl von Teilmengen von Zeilen aus einer Matrix.
Beispielsweise könnte eine N × D-Matrix vorliegen, die N Punkte in D Dimensionen darstellt,

wie z. B. die folgenden Punkte, die aus einer zweidimensionalen Normalverteilung gezogen wurden:

In [13]: mean = [0, 0]
cov = [[1, 2],
[2, 5]]
X = rng.multivariate_normal(mean, cov, 100)
X.Form
Out[13]: (100, 2)

Mit den in Teil IV besprochenen Darstellungswerkzeugen können wir diese Punkte in einem
Streudiagramm darstellen (Abbildung 10-1).

In [14]: % matplotlib inline
import matplotlib.pyplot as plt
plt.style.use('seaborn-whitegrid')

plt.scatter(X[:, 0], X[:, 1]);

82 | Kapitel 10: Phantasievolle Indizierung

Abbildung 10-1. Normalverteilte Punkte

Verwenden wir eine ausgeklügelte Indizierung, um 20 zufällige Punkte auszuwählen. Dazu wählen wir zunächst 20

zufällige Indizes ohne Wiederholungen und Verwendung dieser Indizes zur Auswahl eines Teils der
ursprünglichen Arrays:

In [15]: indices = np.random.choice(X.shape[0], 20, replace= False )
Indizes
Out[15]: array([82, 84, 10, 55, 14, 33, 4, 16, 34, 92, 99, 64, 8, 76, 68, 18, 59,
80, 87, 90])

In [16]: selection = X[indices] # Hier wird eine schicke Indizierung vorgenommen
Auswahl.Form
Out[16]: (20, 2)

Um nun zu sehen, welche Punkte ausgewählt wurden, zeichnen wir große Kreise an den Stellen, an denen

die ausgewählten Punkte (siehe Abbildung 10-2).

In [17]: plt.scatter(X[:, 0], X[:, 1], alpha=0.3)
plt.scatter(Auswahl[:, 0], Auswahl[:, 1],
facecolor='none', edgecolor='black', s=200);

Beispiel: Auswählen von Zufallspunkten | 83
Abbildung 10-2. Zufällige Auswahl von Punkten

Diese Art von Strategie wird häufig verwendet, um Datensätze schnell zu partitionieren, wie es oft bei

Training/Test-Splitting zur Validierung statistischer Modelle (siehe Kapitel 39) und bei Stichproben
pling-Ansätze zur Beantwortung statistischer Fragen.

Ändern von Werten mit Fancy Indexing
Genauso wie Fancy Indexing verwendet werden kann, um auf Teile eines Arrays zuzugreifen, kann es auch verwendet werden, um

Teile eines Arrays ändern. Stellen Sie sich zum Beispiel vor, Sie haben ein Array mit Indizes und möchten
die entsprechenden Elemente in einem Array auf einen bestimmten Wert setzen:

In [18]: x = np.arange(10)
i = np.array([2, 1, 8, 4])
x[i] = 99
print(x)
Out[18]: [ 0 99 99 3 99 5 6 7 99 9]

Wir können dafür jeden beliebigen Zuweisungsoperator verwenden. Zum Beispiel:

In [19]: x[i] -= 10
print(x)
Out[19]: [ 0 89 89 3 89 5 6 7 89 9]

Beachten Sie jedoch, dass wiederholte Indizes mit diesen Operationen einige potenzielle Probleme verursachen können.

tiell unerwartete Ergebnisse. Betrachten Sie das Folgende:

In [20]: x = np.zeros(10)
x[[0, 0]] = [4, 6]
print(x)
Out[20]: [6. 0. 0. 0. 0. 0. 0. 0. 0. 0.]

84 | Kapitel 10: Phantasievolle Indizierung

Wo ist die 4 geblieben? Bei dieser Operation wird zunächst x[0] = 4 und anschließend x[0] = 6 zugewiesen. Die

Das Ergebnis ist natürlich, dass x[0] den Wert 6 enthält.

Gut und schön, aber bedenken Sie diesen Vorgang:

In [21]: i = [2, 3, 3, 4, 4, 4]
x[i] += 1
x
Out[21]: array([6., 0., 1., 1., 1., 0., 0., 0., 0., 0.])

Sie könnten erwarten, dass x[3] den Wert 2 und x[4] den Wert
Wert 3 enthalten, da jeder Index so oft wiederholt wird. Warum ist das nicht der Fall?

Der Grund dafür ist, dass x[i] += 1 als Abkürzung für x[i] = x[i] + 1 verstanden wird.

x[i] + 1 ausgewertet und das Ergebnis dann den Indizes in x zugewiesen wird.
nicht die Augmentation, sondern die Zuweisung mehrfach,

was zu den eher unintuitiven Ergebnissen führt.

Was aber, wenn Sie das andere Verhalten wünschen, bei dem der Vorgang wiederholt wird? Hierfür müssen Sie

kann die at-Methode von ufuncs verwenden und folgendes tun:

In [22]: x = np.zeros(10)
np.add.at(x, i, 1)
print(x)
Out[22]: [0. 0. 1. 2. 3. 0. 0. 0. 0. 0.]

Die at-Methode wendet den angegebenen Operator an der angegebenen Stelle

Indizes (hier: i) mit dem angegebenen Wert (hier: 1). Eine andere Methode, die ähnlich ist in

Geist ist die reduceat-Methode von ufuncs, über die Sie in der NumPy

Dokumentation.

Beispiel: Binning von Daten
Sie können diese Ideen nutzen, um benutzerdefinierte Berechnungen mit Daten in Binnings effizient durchzuführen. Für
Beispiel: Wir haben 100 Werte und möchten schnell herausfinden, wo sie einzuordnen sind

innerhalb einer Reihe von Bins. Wir könnten dies mit ufunc.at wie folgt berechnen:

In [23]: rng = np.random.default_rng(seed=1701)
x = rng.normal(size=100)

# ein Histogramm von Hand berechnen
bins = np.linspace(-5, 5, 20)
counts = np.zeros_like(bins)

# für jedes x das passende Bin finden
i = np.searchsorted(bins, x)

# 1 zu jedem dieser Bins hinzufügen
np.add.at(counts, i, 1)

Beispiel: Binning von Daten | 85
Die Zählungen spiegeln nun die Anzahl der Punkte innerhalb jedes Bins wider - mit anderen Worten: ein his-

togramm (siehe Abbildung 10-3).

In [24]: # Plotten der Ergebnisse
plt.plot(bins, counts, drawstyle='steps');

Abbildung 10-3. Ein von Hand berechnetes Histogramm

Natürlich wäre es lästig, dies jedes Mal tun zu müssen, wenn Sie eine

Histogramm. Aus diesem Grund bietet Matplotlib die Routine plt.hist an, die dasselbe in einer einzigen Zeile tut
in einer einzigen Zeile erledigt:

plt.hist(x, bins, histtype='step');
Mit dieser Funktion wird ein nahezu identisches Diagramm wie das soeben gezeigte erstellt. Zum Berechnen der

Binning, Matplotlib verwendet die Funktion np.histogram, die eine sehr ähnliche Kom-

zu dem, was wir vorher gemacht haben. Lassen Sie uns die beiden hier vergleichen:

In [25]: print(f "NumPy histogram ({len(x)} points):")
% timeit counts, edges = np.histogram(x, bins)

print(f "Benutzerdefiniertes Histogramm ({len(x)} Punkte):")
% timeit np.add.at(counts, np.searchsorted(bins, x), 1)
Out[25]: NumPy-Histogramm (100 Punkte):
33,8 μs ± 311 ns pro Schleife (Mittelwert ± std. Abweichung von 7 Durchläufen mit je 10000 Schleifen)
Benutzerdefiniertes Histogramm (100 Punkte):
17,6 μs ± 113 ns pro Schleife (Mittelwert ± Std.-Abweichung von 7 Durchläufen, jeweils 100000 Schleifen)

Unser eigener Ein-Zeilen-Algorithmus ist doppelt so schnell wie der optimierte Algorithmus in NumPy!

Wie kann das sein? Wenn Sie sich den Quellcode von np.histogram ansehen (Sie können dies in

IPython durch Eingabe von np.histogram??), werden Sie sehen, dass es ein bisschen komplizierter ist als

86 | Kapitel 10: Phantasievolle Indizierung

die einfache Suche und Zählung, die wir durchgeführt haben; das liegt daran, dass der Algorithmus von NumPy

flexibler und insbesondere für eine bessere Leistung bei einer großen Anzahl von Datenpunkten ausgelegt
von Datenpunkten groß wird:

In [26]: x = rng.normal(size=1000000)
print(f "NumPy Histogramm ({len(x)} Punkte):")
% timeit counts, edges = np.histogram(x, bins)

print(f "Benutzerdefiniertes Histogramm ({len(x)} Punkte):")
% timeit np.add.at(counts, np.searchsorted(bins, x), 1)
Out[26]: NumPy-Histogramm (1000000 Punkte):
84,4 ms ± 2,82 ms pro Schleife (Mittelwert ± std. Abweichung von 7 Durchläufen, jeweils 10 Schleifen)
Benutzerdefiniertes Histogramm (1000000 Punkte):
128 ms ± 2,04 ms pro Schleife (Mittelwert ± Standardabweichung von 7 Läufen mit je 10 Schleifen)

Dieser Vergleich zeigt, dass die Effizienz von Algorithmen fast nie eine einfache

Frage. Ein Algorithmus, der für große Datenmengen effizient ist, ist nicht immer die beste Wahl
für kleine Datensätze und umgekehrt (siehe Kapitel 11). Der Vorteil einer solchen Kodierung ist jedoch

Algorithmus selbst ist, dass mit dem Verständnis dieser grundlegenden Methoden der Himmel
Grenzen gesetzt: Sie sind nicht mehr auf eingebaute Routinen beschränkt, sondern können Ihre eigenen erstellen

Ansätze zur Erforschung der Daten. Der Schlüssel zur effizienten Nutzung von Python in datenintensiven

Anwendungen ist nicht nur das Wissen um allgemeine Komfortroutinen wie np.histo

gramme und wann sie angebracht sind, aber auch das Wissen, wie man die niedrigeren Ebenen

Funktionalität, wenn Sie ein gezielteres Verhalten benötigen.

Beispiel: Binning von Daten | 87
KAPITEL 11

Arrays sortieren
Bis zu diesem Punkt haben wir uns hauptsächlich mit Werkzeugen für den Zugriff auf und die Verarbeitung von
Array-Daten mit NumPy. Dieses Kapitel behandelt Algorithmen zum Sortieren von Werten in

NumPy-Arrays. Diese Algorithmen sind ein beliebtes Thema in Einführungen in die Informatik.

Wenn Sie jemals einen Kurs belegt haben, haben Sie wahrscheinlich Träume (oder, je nach
je nach Temperament, Albträume) über Einfüge-, Auswahl- und Zusammenführungssorten,

Schnellsorten, Blasensorten und viele, viele mehr. Alle sind Mittel zur Bewältigung einer
ähnliche Aufgabe: Sortieren der Werte in einer Liste oder einem Array.

Python hat eine Reihe von eingebauten Funktionen und Methoden zum Sortieren von Listen und anderen

iterierbare Objekte. Die Funktion sorted akzeptiert eine Liste und gibt eine sortierte Version davon zurück:

In [1]: L = [3, 1, 4, 1, 5, 9, 2, 6]
sorted(L) # gibt eine sortierte Kopie zurück
Out[1]: [1, 1, 2, 3, 4, 5, 6, 9]

Im Gegensatz dazu wird bei der Sortiermethode von Listen die Liste an Ort und Stelle sortiert:

In [2]: L.sort() # wirkt an Ort und Stelle und gibt keine zurück
print(L)
Out[2]: [1, 1, 2, 3, 4, 5, 6, 9]

Die Sortiermethoden von Python sind recht flexibel und können mit jedem iterierbaren Objekt umgehen. Für

Beispiel, hier sortieren wir eine Zeichenkette:

In [3]: sortiert('python')
Out[3]: ['h', 'n', 'o', 'p', 't', 'y']

Diese eingebauten Sortiermethoden sind praktisch, aber wie bereits erwähnt, sind sie aufgrund der
mismus von Python-Werten, dass sie weniger leistungsfähig sind als Routinen, die

speziell für einheitliche Arrays von Zahlen. Dies ist der Ort, an dem die Sortierroutinen von NumPy

kommen.

88
Schnelles Sortieren in NumPy: np.sort und np.argsort
Die Funktion np.sort entspricht der in Python eingebauten Funktion sorted und liefert effizient eine
effizient eine sortierte Kopie eines Arrays zurück:

In [4]: import numpy as np

x = np.array([2, 1, 4, 3, 5])
np.sort(x)
Out[4]: array([1, 2, 3, 4, 5])

Ähnlich wie bei der Sortiermethode von Python-Listen, können Sie ein Array auch direkt sortieren, indem Sie

die Array-Sortiermethode:

In [5]: x.sort()
print(x)
Out[5]: [1 2 3 4 5]

Eine verwandte Funktion ist argsort, die stattdessen die Indizes der sortierten Elemente zurückgibt.

ments:

In [6]: x = np.array([2, 1, 4, 3, 5])
i = np.argsort(x)
print(i)
Out[6]: [1 0 3 2 4]

Das erste Element dieses Ergebnisses gibt den Index des kleinsten Elements an, das zweite

Wert ergibt den Index des zweitkleinsten Wertes und so weiter. Diese Indizes können dann
verwendet werden (über eine ausgefallene Indizierung), um das sortierte Array zu konstruieren, falls gewünscht:

In [7]: x[i]
Out[7]: array([1, 2, 3, 4, 5])

Sie werden später in diesem Kapitel eine Anwendung von argsort sehen.

Sortieren entlang von Zeilen oder Spalten
Eine nützliche Funktion der Sortieralgorithmen von NumPy ist die Möglichkeit, entlang bestimmter

Zeilen oder Spalten eines mehrdimensionalen Arrays mit Hilfe des Achsenarguments. Zum Beispiel:

In [8]: rng = np.random.default_rng(seed=42)
X = rng.integers(0, 10, (4, 6))
print(X)
Out[8]: [[0 7 6 4 4 8]
[0 6 2 0 5 9]
[7 7 7 7 5 1]
[8 4 5 3 1 9]]

In [9]: # jede Spalte von X sortieren
np.sort(X, axis=0)
Out[9]: array([[0, 4, 2, 0, 1, 1],
[0, 6, 5, 3, 4, 8],

Schnelles Sortieren in NumPy: np.sort und np.argsort | 89
[7, 7, 6, 4, 5, 9],
[8, 7, 7, 7, 5, 9]])
In [10]: # jede Zeile von X sortieren
np.sort(X, Achse=1)
Out[10]: array([[0, 4, 4, 6, 7, 8],
[0, 0, 2, 5, 6, 9],
[1, 5, 7, 7, 7, 7],
[1, 3, 4, 5, 8, 9]])

Beachten Sie, dass dabei jede Zeile oder Spalte als unabhängiges Array behandelt wird und alle
Beziehungen zwischen den Zeilen- oder Spaltenwerten verloren gehen!

Partielle Sortierungen: Partitionierung
Manchmal sind wir nicht daran interessiert, das gesamte Array zu sortieren, sondern wollen nur die

k kleinste Werte im Array. NumPy ermöglicht dies mit der Funktion np.partition.

np.partition nimmt ein Array und eine Zahl k; das Ergebnis ist ein neues Array mit den kleinen
Das Ergebnis ist ein neues Array mit den kleinsten k Werten auf der linken Seite der Partition und den restlichen Werten auf der rechten Seite:

In [11]: x = np.array([7, 2, 3, 1, 6, 5, 4])
np.partition(x, 3)
Out[11]: array([2, 1, 3, 4, 6, 5, 7])

Beachten Sie, dass die ersten drei Werte in der resultierenden Matrix die drei kleinsten in der
Array sind, und die restlichen Array-Positionen die restlichen Werte enthalten. Innerhalb der

zwei Partitionen, die Elemente haben eine beliebige Reihenfolge.

Ähnlich wie beim Sortieren können wir entlang einer beliebigen Achse eines mehrdimensionalen Systems partitionieren

Array:

In [12]: np.partition(X, 2, axis=1)
Out[12]: array([[0, 4, 4, 7, 6, 8],
[0, 0, 2, 6, 5, 9],
[1, 5, 7, 7, 7, 7],
[1, 3, 4, 5, 8, 9]])

Das Ergebnis ist ein Array, bei dem die ersten beiden Slots in jeder Zeile die kleinsten Werte
aus dieser Zeile enthalten, während die übrigen Werte die restlichen Slots füllen.

Schließlich gibt es auch eine np.argsort-Funktion, die Indizes der Sortierung berechnet,

gibt es eine Funktion np.argpartition, die die Indizes der Partition berechnet. Wir werden
beide Funktionen im folgenden Abschnitt in Aktion sehen.

Beispiel: k-Nächste Nachbarn
Schauen wir uns kurz an, wie wir die Funktion argsort entlang mehrerer Achsen verwenden können, um
die nächsten Nachbarn eines jeden Punktes in einer Menge zu finden. Wir beginnen mit der Erstellung einer zufälligen Menge von 10

90 | Kapitel 11: Arrays sortieren

Punkte in einer zweidimensionalen Ebene. Unter Verwendung der Standardkonvention ordnen wir Folgendes an

diese in einer 10 × 2 Anordnung:

In [13]: X = rng.random((10, 2))

Um eine Vorstellung davon zu bekommen, wie diese Punkte aussehen, erstellen wir ein schnelles Streudiagramm (siehe
Abbildung 11-1).

In [14]: % matplotlib inline
import matplotlib.pyplot as plt
plt.style.use('seaborn-whitegrid')
plt.scatter(X[:, 0], X[:, 1], s=100);

Abbildung 11-1. Visualisierung der Punkte im Beispiel der k-Nachbarn

Nun berechnen wir den Abstand zwischen den einzelnen Punktpaaren. Erinnern Sie sich daran, dass die quadrierte

Der Abstand zwischen zwei Punkten ist die Summe der quadrierten Differenzen in jeder Dimension;
Verwendung der effizienten Routinen für die Übertragung (Kapitel 8) und die Aggregation (Kapitel 7)

von NumPy können wir die Matrix der quadratischen Abstände in einer einzigen Zeile von

Code:

In [15]: dist_sq = np.sum((X[:, np.newaxis] - X[np.newaxis, :]) ** 2, axis=-1)

Dieser Vorgang ist sehr umfangreich, und er könnte etwas verwirrend sein, wenn Sie nicht...

miliar mit den Übertragungsregeln von NumPy. Wenn Sie auf Code wie diesen stoßen, kann er

nützlich sein, sie in ihre einzelnen Schritte zu zerlegen:

In [16]: # für jedes Punktpaar die Differenzen ihrer Koordinaten berechnen
differences = X[:, np.newaxis] - X[np.newaxis, :]
differences.shape
Out[16]: (10, 10, 2)

Beispiel: k-Nächste Nachbarn | 91
In [17]: # Quadrieren der Koordinatendifferenzen
sq_differences = differences ** 2
sq_differences.shape
Out[17]: (10, 10, 2)

In [18]: # Summe der Koordinatendifferenzen, um den quadratischen Abstand zu erhalten
dist_sq = sq_differences.sum(-1)
dist_sq.shape
Out[18]: (10, 10)

Um unsere Logik zu überprüfen, sollten wir sehen, dass die Diagonale dieser Matrix (d. h. die
Menge der Entfernungen zwischen jedem Punkt und sich selbst) alle Nullen sind:

In [19]: dist_sq.diagonal()
Out[19]: array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])

Mit den umgewandelten paarweisen quadratischen Abständen können wir nun np.argsort verwenden, um zu sortieren

entlang jeder Zeile. Die Spalten ganz links enthalten dann die Indizes der nächstgelegenen
Nachbarn:

In [20]: nearest = np.argsort(dist_sq, axis=1)
print(nearest)
Out[20]: [[0 9 3 5 4 8 1 6 2 7]
[1 7 2 6 4 8 3 0 9 5]
[2 7 1 6 4 3 8 0 9 5]
[3 0 4 5 9 6 1 2 8 7]
[4 6 3 1 2 7 0 5 9 8]
[5 9 3 0 4 6 8 1 2 7]
[6 4 2 1 7 3 0 5 9 8]
[7 2 1 6 4 3 8 0 9 5]
[8 0 1 9 3 4 7 2 6 5]
[9 0 5 3 4 8 6 1 2 7]]

Beachten Sie, dass in der ersten Spalte die Zahlen 0 bis 9 in der Reihenfolge angegeben sind: Dies ist auf die

die Tatsache, dass der nächste Nachbar eines jeden Punktes erwartungsgemäß er selbst ist.

Indem wir hier eine vollständige Sortierung verwenden, haben wir in diesem Fall mehr Arbeit als nötig geleistet.

Wenn wir einfach nur an den nächsten k Nachbarn interessiert sind, müssen wir nur eine Partitionierung vornehmen

jede Zeile so, dass die kleinsten k+ 1 quadrierten Abstände zuerst kommen, mit größeren Abständen

die verbleibenden Positionen des Arrays zu füllen. Wir können dies mit der Funktion np.argpartition tun
Funktion:

In [21]: K = 2
nearest_partition = np.argpartition(dist_sq, K + 1, axis=1)

Um dieses Netzwerk von Nachbarn zu visualisieren, zeichnen wir die Punkte zusammen mit

Linien, die die Verbindungen von jedem Punkt zu seinen beiden nächsten Nachbarn darstellen (siehe

Abbildung 11-2).

In [22]: plt.scatter(X[:, 0], X[:, 1], s=100)

# Zeichne Linien von jedem Punkt zu seinen beiden nächsten Nachbarn
K = 2

92 | Kapitel 11: Arrays sortieren

for i in range(X.shape[0]):
for j in nearest_partition[i, :K+1]:
_# eine Linie von X[i] nach X[j] zeichnen

Verwenden Sie ein wenig Zip-Magie, um dies zu erreichen:_
plt.plot(*zip(X[j], X[i]), color='black')

Abbildung 11-2. Visualisierung der nächstgelegenen Nachbarn jedes Punktes

Für jeden Punkt in der Grafik sind Linien zu den beiden nächstgelegenen Nachbarn eingezeichnet. Auf den ersten Blick scheint es

Es mag seltsam erscheinen, dass einige der Punkte mehr als zwei Linien haben, die von ihnen ausgehen
Das liegt daran, dass, wenn Punkt A einer der beiden nächsten Nachbarn von Punkt

B ist, bedeutet dies nicht zwangsläufig, dass Punkt B einer der beiden nächstgelegenen Nachbarn von
Punkt A ist.

Obwohl die Übertragung und zeilenweise Sortierung bei diesem Ansatz weniger

als eine Schleife zu schreiben, erweist sich als sehr effizienter Weg, diese Daten in
mit diesen Daten in Python zu arbeiten. Sie könnten versucht sein, die gleiche Art von Operation mit

manuell durch die Daten zu gehen und jeden Satz von Nachbarn einzeln zu sortieren, aber
Dies würde mit Sicherheit zu einem langsameren Algorithmus führen als die vektorisierte Version, die wir

verwendet. Das Schöne an diesem Ansatz ist, dass er so geschrieben ist, dass er unabhängig von der Größe ist.

der Eingabedaten: Wir könnten genauso gut die Nachbarn von 100 oder
1.000.000 Punkten in einer beliebigen Anzahl von Dimensionen berechnen, und der Code würde gleich aussehen.

Abschließend möchte ich anmerken, dass es bei sehr großen Suchen nach nächsten Nachbarn baumbasierte
basierte und/oder approximative Algorithmen gibt, die mit 𝐀NlogN oder besser skalieren können.

als die 𝐀 N^2 des Brute-Force-Algorithmus. Ein Beispiel hierfür ist der KD-Baum,

in Scikit-Learn implementiert.

Beispiel: k-Nächste Nachbarn | 93
KAPITEL 12

Strukturierte Daten: NumPy's
Strukturierte Arrays
Während unsere Daten oft gut durch ein homogenes Array von Werten dargestellt werden können,
ist dies manchmal nicht der Fall. Dieses Kapitel demonstriert die Verwendung von NumPy's struk-

strukturierte Arrays und Record-Arrays, die eine effiziente Speicherung von zusammengesetzten, hetero-

genen Daten. Die hier gezeigten Muster sind zwar für einfache Operationen nützlich,

Szenarien wie diese eignen sich oft für die Verwendung von Pandas DataFrames, die wir in Teil
in Teil III erkunden werden.

In [1]: import numpy as np

Stellen Sie sich vor, wir haben mehrere Kategorien von Daten über eine Reihe von Personen (z. B. Name,

Alter und Gewicht), und wir möchten diese Werte zur Verwendung in einem Python-Programm speichern. Es
wäre es möglich, diese Werte in drei separaten Arrays zu speichern:

In [2]: name = ['Alice', 'Bob', 'Cathy', 'Doug']
Alter = [25, 45, 37, 19]
gewicht = [55.0, 85.5, 68.0, 61.5]

Aber das ist ein bisschen ungeschickt. Es gibt hier nichts, was uns sagt, dass die drei Arrays
verwandt sind; die strukturierten Arrays von NumPy erlauben es uns, dies natürlicher zu tun, indem wir eine einzige

Struktur, um all diese Daten zu speichern.

Erinnern Sie sich, dass wir zuvor ein einfaches Array mit einem Ausdruck wie diesem erstellt haben:

In [3]: x = np.zeros(4, dtype=int)

Auf ähnliche Weise können wir ein strukturiertes Array mit einer zusammengesetzten Datentypspezifikation erstellen:

In [4]: # Einen zusammengesetzten Datentyp für strukturierte Arrays verwenden
data = np.zeros(4, dtype={'names':('name', 'age', 'weight'),
'Formate':('U10', 'i4', 'f8')})

94
print(daten.dtype)
Out[4]: [('Name', '<U10'), ('Alter', '<i4'), ('Gewicht', '<f8')]

Hier bedeutet "U10" "Unicode-String mit maximaler Länge 10", "i4" bedeutet

"4-Byte (d.h. 32-Bit) Ganzzahl" und "f8" bedeutet "8-Byte (d.h. 64-Bit) Float". Wir werden

Weitere Optionen für diese Typencodes werden im folgenden Abschnitt erörtert.

Nachdem wir nun ein leeres Container-Array erstellt haben, können wir das Array mit unseren Listen von
Werten füllen:

In [5]: data['name'] = name
daten['alter'] = Alter
daten['gewicht'] = gewicht
print(daten)
Out[5]: [('Alice', 25, 55. ) ('Bob', 45, 85.5) ('Cathy', 37, 68. )
('Doug', 19, 61.5)]

Wie wir gehofft hatten, sind die Daten nun bequem in einem strukturierten Array angeordnet.

Das Praktische an strukturierten Arrays ist, dass wir uns nun auf Werte entweder durch
Index oder über den Namen:

In [6]: # Alle Namen abrufen
data['name']
Out[6]: array(['Alice', 'Bob', 'Cathy', 'Doug'], dtype='<U10')

In [7]: # Erste Zeile der Daten abrufen
data[0]
Out[7]: ('Alice', 25, 55.)

In [8]: # Holt den Namen aus der letzten Zeile
data[-1]['name']
Out[8]: 'Doug'

Mit Hilfe der booleschen Maskierung können wir sogar einige anspruchsvollere Operationen durchführen, wie z. B.
Filtern nach Alter:

In [9]: # Namen abrufen, bei denen das Alter unter 30 ist
data[data['Alter'] < 30]['Name']
Out[9]: array(['Alice', 'Doug'], dtype='<U10')

Wenn Sie kompliziertere Operationen als diese durchführen möchten, müssen Sie

sollten vielleicht das Pandas-Paket in Betracht ziehen, das in Teil IV behandelt wird. Wie Sie sehen werden, ist Pan-

das bietet ein DataFrame-Objekt, eine Struktur, die auf NumPy-Arrays aufgebaut ist, die

bietet eine Vielzahl von nützlichen Funktionen zur Datenmanipulation, ähnlich denen, die Sie hier gesehen haben
gesehen haben, und noch viel, viel mehr.

Strukturierte Daten: Die strukturierten Arrays von NumPy | 95
Erforschung der Erstellung strukturierter Arrays
Strukturierte Array-Datentypen können auf verschiedene Weise spezifiziert werden. Zuvor sahen wir die

Wörterbuchmethode:

In [10]: np.dtype({'names':('name', 'age', 'weight'),
'Formate':('U10', 'i4', 'f8')})
Out[10]: dtype([('Name', '<U10'), ('Alter', '<i4'), ('Gewicht', '<f8')])

Der Übersichtlichkeit halber können numerische Typen mit Python-Typen oder NumPy dtypes angegeben werden

stattdessen:

In [11]: np.dtype({'names':('name', 'age', 'weight'),
'Formate':((np.str_, 10), int, np.float32)})
Out[11]: dtype([('Name', '<U10'), ('Alter', '<i8'), ('Gewicht', '<f4')])

Ein zusammengesetzter Typ kann auch als eine Liste von Tupeln angegeben werden:

In [12]: np.dtype([('Name', 'S10'), ('Alter', 'i4'), ('Gewicht', 'f8')])
Out[12]: dtype([('Name', 'S10'), ('Alter', '<i4'), ('Gewicht', '<f8')])

Wenn die Namen der Typen für Sie nicht wichtig sind, können Sie die Typen allein in einer

Komma-getrennte Zeichenfolge:

In [13]: np.dtype('S10,i4,f8')
Out[13]: dtype([('f0', 'S10'), ('f1', '<i4'), ('f2', '<f8')])

Die verkürzten String-Formatcodes sind vielleicht nicht sofort intuitiv, aber sie sind

auf einfachen Prinzipien aufgebaut. Das erste (optionale) Zeichen < oder > bedeutet "Little Endian"
bzw. "Big Endian" und gibt die Ordnungskonvention für signifikante

Bits. Das nächste Zeichen gibt den Datentyp an: Zeichen, Bytes, Ints, Floating

Punkte und so weiter (siehe Tabelle 12-1). Das letzte Zeichen oder die letzten Zeichen geben die Größe
des Objekts in Bytes an.

Tabelle 12-1. NumPy-Datentypen

Zeichenbeschreibung Beispiel
'b' Byte np.dtype('b')
'i' Ganze Zahl mit Vorzeichen np.dtype('i4') == np.int32
'u' Ganzzahl ohne Vorzeichen np.dtype('u1') == np.uint8
f' Fließkomma np.dtype('f8') == np.int64
c' Komplexes Fließkomma np.dtype('c16') == np.complex128
'S', 'a' Zeichenkette np.dtype('S5')
U' Unicode Zeichenkette np.dtype('U') == np.str_
'V' Rohdaten (void) np.dtype('V') == np.void
96 | Kapitel 12: Strukturierte Daten: NumPy's strukturierte Arrays

Erweiterte Verbindungstypen
Es ist möglich, noch weitergehende Verbundtypen zu definieren. Zum Beispiel können Sie

einen Typ erstellen, bei dem jedes Element ein Array oder eine Matrix von Werten enthält. Hier werden wir

einen Datentyp mit einer Matte-Komponente erstellen, die aus einer 3 × 3-Gleitkomma-Matrix besteht:

In [14]: tp = np.dtype([('id', 'i8'), ('mat', 'f8', (3, 3))])
X = np.zeros(1, dtype=tp)
print(X[0])
print(X['mat'][0])
Out[14]: (0, [[0., 0., 0.], [0., 0., 0.], [0., 0., 0.]])
[[0. 0. 0.]
[0. 0. 0.]
[0. 0. 0.]]

Nun besteht jedes Element in der X-Matrix aus einer ID und einer 3 × 3-Matrix. Warum würden Sie

dies anstelle eines einfachen mehrdimensionalen Arrays oder vielleicht eines Python-Wörterbuchs verwenden?

Ein Grund dafür ist, dass dieser NumPy-Dtype direkt auf eine C-Strukturdefinition abgebildet wird, so dass

kann der Puffer mit dem Array-Inhalt direkt in einem entsprechend geschriebenen C-Programm zugegriffen werden.
entsprechend geschriebenen C-Programm zugegriffen werden. Wenn Sie eine Python-Schnittstelle für ein altes C-Programm schreiben

oder Fortran-Bibliothek, die strukturierte Daten verarbeitet, können strukturierte Arrays eine

leistungsfähige Schnittstelle.

Datensatz-Arrays: Strukturierte Arrays mit einem Twist
NumPy bietet auch Datensatz-Arrays (Instanzen der Klasse np.recarray), die

fast identisch mit den soeben beschriebenen strukturierten Arrays, jedoch mit einem zusätzlichen Merkmal
tung: Auf Felder kann als Attribute und nicht als Wörterbuchschlüssel zugegriffen werden. Wir erinnern uns, dass wir

zuvor auf die Altersangaben in unserem Beispieldatensatz zugreifen, indem Sie schreiben:

In [15]: data['age']
Out[15]: array([25, 45, 37, 19], dtype=int32)

Wenn wir unsere Daten stattdessen als Datensatz-Array betrachten, können wir auf diese mit etwas weniger

Tastenanschläge:

In [16]: data_rec = data.view(np.recarray)
data_rec.age
Out[16]: array([25, 45, 37, 19], dtype=int32)

Der Nachteil ist, dass bei Datensatzarrays ein gewisser zusätzlicher Aufwand für den
auf die Felder zuzugreifen, selbst wenn dieselbe Syntax verwendet wird:

In [17]: % timeit data['age']
% timeit data_rec['age']
% timeit data_rec.age
Out[17]: 121 ns ± 1,4 ns pro Schleife (Mittelwert ± std. Abweichung von 7 Läufen, je 1000000 Schleifen)
2,41 μs ± 15,7 ns pro Schleife (Mittelwert ± Standardabweichung von 7 Läufen mit je 100000 Schleifen)
3,98 μs ± 20,5 ns pro Schleife (Mittelwert ± Standardabweichung von 7 Läufen mit je 100000 Schleifen)

Erweiterte zusammengesetzte Typen | 97
Ob die bequemere Notation den (geringen) Mehraufwand wert ist, hängt ab von

Ihre eigene Anwendung.

Weiter zu Pandas
Dieses Kapitel über strukturierte Arrays und Datensätze steht absichtlich am Ende dieses
Teils des Buches, weil es so gut zum nächsten Paket führt, das wir behandeln werden: Pandas.

Strukturierte Arrays können in bestimmten Situationen sehr nützlich sein, z. B. wenn Sie mit

NumPy-Arrays auf binäre Datenformate in C, Fortran oder einer anderen Sprache abbilden.
Für die alltägliche Verwendung strukturierter Daten ist das Pandas-Paket jedoch eine viel bessere Wahl;

In den folgenden Kapiteln werden wir uns eingehend damit befassen.

98 | Kapitel 12: Strukturierte Daten: NumPy's strukturierte Arrays

TEIL III
Datenmanipulation mit Pandas

In Teil II haben wir uns ausführlich mit NumPy und seinem ndarray-Objekt befasst, das eine effi-

cientes Speichern und Manipulieren von dichten typisierten Arrays in Python. Hier werden wir auf
dieses Wissen auf, indem wir uns die Datenstrukturen von Pandas genauer ansehen

Bibliothek. Pandas ist ein neueres Paket, das auf NumPy aufbaut und eine effiziente

Implementierung eines DataFrame. DataFrames sind im Wesentlichen mehrdimensionale Arrays

mit angehängten Zeilen- und Spaltenbeschriftungen, oft mit heterogenen Typen und/oder fehlenden
Daten. Pandas bietet nicht nur eine bequeme Speicherschnittstelle für beschriftete Daten, sondern implementiert-

eine Reihe leistungsfähiger Datenoperationen, die den Benutzern beider Datenbankrahmen vertraut sind.

Werke und Tabellenkalkulationsprogramme.

Wie wir gesehen haben, bietet die ndarray-Datenstruktur von NumPy wesentliche Funktionen für die Art
von sauberen, gut organisierten Daten, wie sie typischerweise in numerischen Berechnungsaufgaben vorkommen. Während es

diesen Zweck sehr gut erfüllt, werden seine Grenzen deutlich, wenn wir mehr Flexibilität benötigen.

(z. B. das Anbringen von Etiketten an Daten, die Arbeit mit fehlenden Daten usw.) und bei
bei Operationen, die sich nicht gut auf die elementweise Übertragung übertragen lassen (z. B.,

Gruppierungen, Pivots usw.), die alle einen wichtigen Beitrag zur Analyse der weniger
strukturierten Daten, die in der Welt um uns herum in vielen Formen vorhanden sind. Pandas, und in par-

insbesondere seine Series- und DataFrame-Objekte, baut auf der NumPy-Array-Struktur auf und
bietet einen effizienten Zugriff auf diese Art von "Datenvermischungs"-Aufgaben, die einen Großteil eines

die Zeit des Datenwissenschaftlers.

In diesem Teil des Buches werden wir uns auf die Mechanik der Verwendung von Series, DataFrame,

und verwandte Strukturen effektiv zu nutzen. Wir werden Beispiele aus realen Datensätzen verwenden
verwenden, aber diese Beispiele sind nicht unbedingt der Schwerpunkt.

Die Installation von Pandas auf Ihrem System erfordert NumPy, und wenn Sie
Wenn Sie die Bibliothek aus dem Quellcode erstellen, benötigen Sie die entsprechenden
Werkzeuge, um die C- und Cython-Quellen zu kompilieren, auf denen Pandas
gebaut wird. Details zum Installationsprozess finden Sie in der Pandas
Dokumentation. Wenn Sie den Ratschlag aus dem Vorwort befolgt haben
und den Anaconda-Stack verwendet haben, ist Pandas bereits installiert.
Sobald Pandas installiert ist, können Sie es importieren und die Version überprüfen;
Hier ist die Version, die in diesem Buch verwendet wird:
In [1]: import pandas
pandas.__version__
Out[1]: '1.3.5'
So wie wir im Allgemeinen NumPy unter dem Alias np importieren, werden wir
importieren wir Pandas unter dem Alias pd:
In [2]: import pandas as pd
Diese Importkonvention wird im weiteren Verlauf des Buches verwendet.
dieses Buches verwendet.
Erinnerung an die eingebaute Dokumentation

Wenn Sie diesen Teil des Buches durchlesen, vergessen Sie nicht, dass IPython Ihnen die Möglichkeit gibt
die Möglichkeit gibt, den Inhalt eines Pakets schnell zu erforschen (indem Sie die Tabulatorvervollständigung

ture) sowie die Dokumentation der verschiedenen Funktionen (mit dem Zeichen?). Siehe
zurück zu Kapitel 1, wenn Sie eine Auffrischung benötigen.

Um zum Beispiel den gesamten Inhalt des Pandas-Namensraums anzuzeigen, können Sie Folgendes eingeben:

In [3]: pd.

Und um die eingebaute Pandas-Dokumentation anzuzeigen, können Sie dies verwenden:

In [4]: pd?

Auf der Pandas-Website finden Sie eine ausführlichere Dokumentation sowie Tutorials und
andere Ressourcen.

KAPITEL 13

Einführung in Pandas-Objekte
Auf einer sehr grundlegenden Ebene kann man sich Pandas-Objekte als erweiterte Versionen von
NumPy-strukturierten Arrays, in denen die Zeilen und Spalten mit Beschriftungen versehen sind

als einfache Ganzzahl-Indizes. Wie wir im Laufe dieses Kapitels sehen werden,

Pandas bietet eine Vielzahl von nützlichen Werkzeugen, Methoden und Funktionen zusätzlich zu den grundlegenden
Datenstrukturen, aber fast alles, was folgt, erfordert ein Verständnis von

was diese Strukturen sind. Bevor wir also weitermachen, sollten wir einen Blick auf diese Strukturen werfen

drei grundlegende Pandas-Datenstrukturen: Series, DataFrame und Index.

Wir werden unsere Codesitzungen mit den Standardimporten von NumPy und Pandas beginnen:

In [1]: importiere numpy als np
import pandas as pd

Das Pandas-Serien-Objekt
Eine Pandas-Serie ist ein eindimensionales Array mit indizierten Daten. Es kann aus einer
Liste oder Array wie folgt erstellt werden:

In [2]: Daten = pd.Reihe([0,25, 0,5, 0,75, 1,0])
Daten
Out[2]: 0 0.25
1 0.50
2 0.75
3 1.00
dtype: float64

Die Reihe kombiniert eine Folge von Werten mit einer expliziten Folge von Indizes,

auf die wir mit den Attributen values und index zugreifen können. Die Werte sind einfach ein

vertrautes NumPy-Array:

In [3]: data.values
Out[3]: array([0.25, 0.5 , 0.75, 1. ])

101
Der Index ist ein Array-ähnliches Objekt vom Typ pd.Index, auf das wir gleich noch näher eingehen werden
besprechen werden:

In [4]: Daten.index
Out[4]: RangeIndex(start=0, stop=4, step=1)

Wie bei einem NumPy-Array kann auf die Daten über den zugehörigen Index mit der bekannten

Python-Notation mit eckigen Klammern:

In [5]: Daten[1]
Out[5]: 0.5

Eingang [6]: Daten[1:3]
Out[6]: 1 0.50
2 0.75
dtype: float64

Wie wir jedoch sehen werden, ist die Pandas-Serie viel allgemeiner und flexibler als die

eindimensionales NumPy-Array, das es emuliert.

Reihen als verallgemeinertes NumPy-Array
Nach dem, was wir bisher gesehen haben, scheint das Series-Objekt grundsätzlich inter-

mit einem eindimensionalen NumPy-Array geändert werden. Der wesentliche Unterschied besteht darin, dass
während das NumPy-Array einen implizit definierten Integer-Index hat, der für den Zugriff auf die Wert-

Die Pandas-Serie hat einen explizit definierten Index, der mit den Werten verbunden ist.

Diese explizite Indexdefinition verleiht dem Series-Objekt zusätzliche Fähigkeiten. Für
muss der Index beispielsweise keine ganze Zahl sein, sondern kann aus Werten beliebiger Art bestehen

Typ. Wenn wir wollen, können wir also Zeichenketten als Index verwenden:

In [7]: data = pd.Series([0.25, 0.5, 0.75, 1.0],
index=['a', 'b', 'c', 'd'])
Daten
Out[7]: a 0,25
b 0.50
c 0.75
d 1.00
dTyp: float64

Und der Zugriff auf den Artikel funktioniert wie erwartet:

In [8]: data['b']
Out[8]: 0.5

Wir können sogar nicht zusammenhängende oder nicht sequentielle Indizes verwenden:

In [9]: data = pd.Series([0.25, 0.5, 0.75, 1.0],
index=[2, 5, 3, 7])
Daten
Out[9]: 2 0,25
5 0.50
3 0.75

102 | Kapitel 13: Einführung in die Pandas-Objekte

7 1.00
dTyp: float64

In [10]: Daten[5]
Out[10]: 0.5

Serien als spezialisiertes Wörterbuch
Auf diese Weise können Sie sich eine Pandas-Serie ein wenig wie eine Spezialisierung eines Python

Wörterbuch. Ein Wörterbuch ist eine Struktur, die beliebige Schlüssel auf eine Menge beliebiger

Werte, und eine Serie ist eine Struktur, die eingegebene Tasten auf eine Reihe von eingegebenen Werten abbildet. Diese

Typisierung ist wichtig: So wie der typspezifische kompilierte Code hinter einem NumPy-Array
bei bestimmten Operationen effizienter ist als eine Python-Liste, ist die Typinformation

einer Pandas-Serie macht sie für bestimmte Operationen effizienter als Python-Wörterbücher
Operationen.

Die Analogie zwischen Serie und Wörterbuch kann noch deutlicher gemacht werden, indem man eine

Serienobjekt direkt aus einem Python-Wörterbuch, hier die fünf bevölkerungsreichsten US
Staaten nach der Volkszählung 2020:

In [11]: population_dict = {'California': 39538223, 'Texas': 29145505,
'Florida': 21538187, 'New York': 20201249,
'Pennsylvania': 13002700}
Bevölkerung = pd.Series(population_dict)
Bevölkerung
Out[11]: Kalifornien 39538223
Texas 29145505
Florida 21538187
New York 20201249
Pennsylvania 13002700
dtTyp: int64

Von hier aus kann ein typischer Zugriff auf Elemente im Stil eines Wörterbuchs erfolgen:

In [12]: Bevölkerung['Kalifornien']
Out[12]: 39538223

Im Gegensatz zu einem Wörterbuch unterstützt die Serie jedoch auch Array-ähnliche Operationen wie
Slicing:

In [13]: Bevölkerung['Kalifornien':'Florida']
Out[13]: Kalifornien 39538223
Texas 29145505
Florida 21538187
dtTyp: int64

Wir werden einige der Eigenheiten von Pandas Indexierung und Slicing in Kapitel 14 besprechen.

Das Pandas-Serienobjekt | 103
Konstruktion von Reihenobjekten
Wir haben bereits einige Möglichkeiten gesehen, eine Pandas-Serie von Grund auf zu konstruieren. Alle
sind eine Version der folgenden:

pd.Reihe(Daten, index=index)
wobei index ein optionales Argument ist und die Daten eine von vielen Entitäten sein können.

Daten können zum Beispiel eine Liste oder ein NumPy-Array sein, in diesem Fall ist index standardmäßig ein

ganzzahlige Folge:

In [14]: pd.Serie([2, 4, 6])
Out[14]: 0 2
1 4
2 6
dtTyp: int64

Die Daten können auch ein Skalar sein, der wiederholt wird, um den angegebenen Index zu füllen:

In [15]: pd.Series(5, index=[100, 200, 300])
Out[15]: 100 5
200 5
300 5
dtTyp: int64

Es kann sich auch um ein Wörterbuch handeln; in diesem Fall entspricht index standardmäßig den Schlüsseln des Wörterbuchs:

In [16]: pd.Series({2:'a', 1:'b', 3:'c'})
Out[16]: 2 a
1 b
3 c
dtype: Objekt

In jedem Fall kann der Index explizit gesetzt werden, um die Reihenfolge oder die Teilmenge der Schlüssel zu steuern
zu steuern:

In [17]: pd.Series({2:'a', 1:'b', 3:'c'}, index=[1, 2])
Out[17]: 1 b
2 a
dtype: Objekt

Das Pandas DataFrame-Objekt
Die nächste grundlegende Struktur in Pandas ist der DataFrame. Wie das Series-Objekt

Wie im vorigen Abschnitt beschrieben, kann der DataFrame entweder als eine Genera-
alisierung eines NumPy-Arrays oder als eine Spezialisierung eines Python-Wörterbuchs betrachtet werden. Wir werden nun

werfen wir einen Blick auf jede dieser Perspektiven.

104 | Kapitel 13: Einführung in die Pandas-Objekte

DataFrame als verallgemeinertes NumPy-Array
Wenn eine Reihe ein Analogon eines eindimensionalen Arrays mit expliziten Indizes ist, ist ein DataFrame
ein Analogon zu einem zweidimensionalen Array mit expliziten Zeilen- und Spaltenindizes. Genauso wie

Ein zweidimensionales Array kann man sich als eine geordnete Folge von aneinandergereihten ein-

dimensionalen Spalten können Sie sich einen DataFrame als eine Folge von ausgerichteten Reihen vorstellen

Objekte. Mit "ausgerichtet" ist hier gemeint, dass sie den gleichen Index haben.

Um dies zu demonstrieren, erstellen wir zunächst eine neue Reihe, in der die Flächen der einzelnen

fünf Staaten, die im vorherigen Abschnitt behandelt wurden (in Quadratkilometern):

In [18]: area_dict = {'California': 423967, 'Texas': 695662, 'Florida': 170312,
'New York': 141297, 'Pennsylvania': 119280}
area = pd.Series(area_dict)
area
Out[18]: Kalifornien 423967
Texas 695662
Florida 170312
New York 141297
Pennsylvania 119280
dtTyp: int64

Nun, da wir dies zusammen mit der Bevölkerungsreihe von vorher haben, können wir ein
Wörterbuch verwenden, um ein einziges zweidimensionales Objekt zu erstellen, das diese Informationen enthält:

In [19]: states = pd.DataFrame({'population': population,
'Fläche': Fläche})
Staaten
Out[19]: Bevölkerung Fläche
Kalifornien 39538223 423967
Texas 29145505 695662
Florida 21538187 170312
New York 20201249 141297
Pennsylvania 13002700 119280

Wie das Series-Objekt hat auch der DataFrame ein Index-Attribut, das den Zugriff auf die

Index-Etiketten:

In [20]: states.index
Out[20]: Index(['California', 'Texas', 'Florida', 'New York', 'Pennsylvania'],

dtype='object')

Zusätzlich hat der DataFrame ein Attribut columns, das ein Index-Objekt ist, das
die Spaltenbezeichnungen enthält:

In [21]: states.columns
Out[21]: Index(['Bevölkerung', 'Fläche'], dtype='object')

Der DataFrame kann also als eine Verallgemeinerung eines zweidimensionalen

NumPy-Array, bei dem sowohl die Zeilen als auch die Spalten einen generalisierten Index für den Zugriff auf die Daten haben.
Zugriff auf die Daten haben.

Das Pandas DataFrame-Objekt | 105
DataFrame als spezialisiertes Wörterbuch
In ähnlicher Weise können wir uns einen DataFrame auch als eine Spezialisierung eines Wörterbuchs vorstellen. Wobei

ein Wörterbuch ordnet einen Schlüssel einem Wert zu, ein DataFrame ordnet einen Spaltennamen einer Reihe von

Spaltendaten. Die Abfrage des Attributs "area" gibt zum Beispiel das Objekt "Series" zurück

mit den zuvor gesehenen Bereichen:

In [22]: states['area']
Out[22]: Kalifornien 423967
Texas 695662
Florida 170312
New York 141297
Pennsylvania 119280
Name: Gebiet, dtype: int64

Beachten Sie den potenziellen Punkt der Verwirrung hier: in einem zweidimensionalen NumPy-Array,

data[0] gibt die erste Zeile zurück. Bei einem DataFrame gibt data['col0'] die erste Zeile zurück.

Spalte. Aus diesem Grund ist es wahrscheinlich besser, sich DataFrames als verallgemeinerte

Wörterbüchern statt verallgemeinerten Arrays, obwohl beide Betrachtungsweisen der Situation

tion nützlich sein. Wir werden flexiblere Mittel zur Indizierung von DataFrames in

Kapitel 14.

Konstruktion von DataFrame-Objekten
Ein Pandas DataFrame kann auf verschiedene Arten konstruiert werden. Hier werden wir sieben
eral Beispiele.

Von einem einzelnen Serienobjekt

Ein DataFrame ist eine Sammlung von Series-Objekten, und ein einspaltiger DataFrame kann sein

aus einer einzigen Serie aufgebaut:

In [23]: pd.DataFrame(population, columns=['population'])
Out[23]: Bevölkerung
Kalifornien 39538223
Texas 29145505
Florida 21538187
New York 20201249
Pennsylvania 13002700

Aus einer Liste von Dikten

Jede Liste von Wörterbüchern kann in einen DataFrame umgewandelt werden. Wir werden eine einfache Liste verwenden, die...

um einige Daten zu erstellen:

In [24]: Daten = [{'a': i, 'b': 2 * i}
for i in range(3)]
pd.DataFrame(data)
Out[24]: a b
0 0 0

106 | Kapitel 13: Einführung in die Pandas-Objekte

1 1 2
2 2 4
Auch wenn einige Schlüssel im Wörterbuch fehlen, füllt Pandas sie mit NaN auf

Werte (d. h. "Keine Zahl"; siehe Kapitel 16):

In [25]: pd.DataFrame([{'a': 1, 'b': 2}, {'b': 3, 'c': 4}])
Out[25]: a b c
0 1.0 2 NaN
1 NaN 3 4.0

Aus einem Wörterbuch der Serienobjekte

Wie wir bereits gesehen haben, kann ein DataFrame aus einem Wörterbuch von Serien konstruiert werden

auch Objekte:

In [26]: pd.DataFrame({'Bevölkerung': Bevölkerung,
'Fläche': Fläche})
Out[26]: Bevölkerung Gebiet
Kalifornien 39538223 423967
Texas 29145505 695662
Florida 21538187 170312
New York 20201249 141297
Pennsylvania 13002700 119280

Aus einem zweidimensionalen NumPy-Array

Ausgehend von einem zweidimensionalen Array von Daten können wir einen DataFrame mit allen angegebenen
Spalten- und Indexnamen erstellen. Wird nichts angegeben, wird für jede Spalte ein Integer-Index verwendet:

In [27]: pd.DataFrame(np.random.rand(3, 2),
columns=['foo', 'bar'],
index=['a', 'b', 'c'])
Out[27]: foo bar
a 0.471098 0.317396
b 0.614766 0.305971
c 0.533596 0.512377

Aus einem NumPy-strukturierten Array

Wir haben strukturierte Arrays in Kapitel 12 behandelt. Ein Pandas DataFrame funktioniert ähnlich wie

ein strukturiertes Array und kann direkt aus einem solchen erstellt werden:

In [28]: A = np.zeros(3, dtype=[('A', 'i8'), ('B', 'f8')])
A
Out[28]: array([(0, 0.), (0, 0.), (0, 0.)], dtype=[('A', '<i8'), ('B', '<f8')])

In [29]: pd.DataFrame(A)
Out[29]: A B
0 0 0.0
1 0 0.0
2 0 0.0

Das Pandas DataFrame-Objekt | 107
Das Pandas Index-Objekt
Wie Sie gesehen haben, enthalten die Objekte Series und DataFrame beide einen expliziten Index, der

können Sie Daten referenzieren und ändern. Dieses Index-Objekt ist an sich eine interessante Struktur
Struktur und kann entweder als unveränderliches Array oder als geordnete Menge betrachtet werden (tech-

nisch ein Multiset, da Indexobjekte wiederholte Werte enthalten können). Diese Ansichten haben

einige interessante Konsequenzen in Bezug auf die für Indexobjekte verfügbaren Operationen.

Als einfaches Beispiel wollen wir einen Index aus einer Liste von Ganzzahlen erstellen:

In [30]: ind = pd.Index([2, 3, 5, 7, 11])
ind
Out[30]: Int64Index([2, 3, 5, 7, 11], dtype='int64')

Index als unveränderliches Array
Der Index funktioniert in vielerlei Hinsicht wie ein Array. Zum Beispiel können wir den Standard

Python-Indizierungsnotation zum Abrufen von Werten oder Slices:

In [31]: ind[1]
Out[31]: 3

In [32]: ind[::2]
Out[32]: Int64Index([2, 5, 11], dtype='int64')

Index-Objekte haben auch viele der Attribute, die von NumPy-Arrays bekannt sind:

In [33]: print(ind.size, ind.shape, ind.ndim, ind.dtype)
Out[33]: 5 (5,) 1 int64

Ein Unterschied zwischen Index-Objekten und NumPy-Arrays ist, dass die Indizes
unveränderlich sind, d.h. sie können nicht mit den üblichen Mitteln geändert werden:

In [34]: ind[1] = 0
TypeError : Index unterstützt keine veränderbaren Operationen

Diese Unveränderlichkeit macht es sicherer, Indizes zwischen mehreren DataFrames und
Arrays gemeinsam zu nutzen, ohne dass die Möglichkeit von Seiteneffekten durch versehentliche Indexänderungen besteht.

Index als geordnete Menge
Pandas-Objekte wurden entwickelt, um Operationen wie Joins über Datensätze hinweg zu erleichtern,

die von vielen Aspekten der Mengenarithmetik abhängen. Das Index-Objekt folgt vielen der

die Konventionen der in Python eingebauten Set-Datenstruktur, so dass Unions, Intersec-

tionen, Differenzen und andere Kombinationen können auf bekannte Weise berechnet werden:

108 | Kapitel 13: Einführung in die Pandas-Objekte

In [35]: indA = pd.Index([1, 3, 5, 7, 9])
indB = pd.Index([2, 3, 5, 7, 11])

In [36]: indA.intersection(indB)
Out[36]: Int64Index([3, 5, 7], dtype='int64')

In [37]: indA.union(indB)
Out[37]: Int64Index([1, 2, 3, 5, 7, 9, 11], dtype='int64')

In [38]: indA.symmetrische_Abweichung(indB)
Out[38]: Int64Index([1, 2, 9, 11], dtype='int64')

Das Pandas-Index-Objekt | 109
KAPITEL 14

Indizierung und Auswahl von Daten
In Teil II haben wir uns ausführlich mit Methoden und Werkzeugen für den Zugriff, das Setzen und Ändern von Werten befasst

in NumPy-Arrays. Dazu gehören die Indizierung (z. B. arr[2, 1]), die Aufteilung (z. B. arr[:,

1:5]), Maskierung (z. B. arr[arr > 0]), ausgefallene Indexierung (z. B. arr[0, [1, 5]]) und

Kombinationen davon (z. B. arr[:, [1, 5]]). Hier werden wir uns ähnliche Mittel ansehen, um

Zugriff auf und Änderung von Werten in Pandas Series- und DataFrame-Objekten. Wenn Sie bereits
die NumPy-Muster verwendet haben, werden Ihnen die entsprechenden Muster in Pandas sehr vertraut vorkommen.

iar, allerdings gibt es einige Besonderheiten zu beachten.

Wir beginnen mit dem einfachen Fall eines eindimensionalen Reihenobjekts und gehen dann weiter

auf das kompliziertere zweidimensionale DataFrame-Objekt.

Datenauswahl in Reihen
Wie Sie im vorigen Kapitel gesehen haben, verhält sich ein Series-Objekt in vielerlei Hinsicht wie ein eindimensionales
dimensionales NumPy-Array und in vielerlei Hinsicht wie ein Standard-Python-Wörterbuch. Wenn

Wenn Sie sich diese beiden sich überschneidenden Analogien vor Augen halten, wird es Ihnen helfen, die

Muster der Datenindizierung und -auswahl in diesen Feldern.

Reihen als Wörterbuch
Wie ein Wörterbuch bietet das Series-Objekt eine Zuordnung von einer Sammlung von Schlüsseln zu einer
Sammlung von Werten:

In [1]: import pandas as pd
data = pd.Series([0.25, 0.5, 0.75, 1.0],
index=['a', 'b', 'c', 'd'])
Daten
Out[1]: a 0,25
b 0.50
c 0.75

110
d 1.00
dTyp: float64

In [2]: data['b']
Out[2]: 0.5

Wir können auch wörterbuchartige Python-Ausdrücke und -Methoden verwenden, um die

Schlüssel/Indizes und Werte:

In [3]: 'a' in Daten
Out[3]: True

In [4]: data.keys()
Out[4]: Index(['a', 'b', 'c', 'd'], dtype='object')

In [5]: list(data.items())
Out[5]: [('a', 0.25), ('b', 0.5), ('c', 0.75), ('d', 1.0)]

Serienobjekte können auch mit einer wörterbuchähnlichen Syntax geändert werden. Genauso wie Sie

ein Wörterbuch durch Zuweisung eines neuen Schlüssels erweitern, können Sie eine Serie durch Zuweisung von

auf einen neuen Indexwert:

In [6]: data['e'] = 1,25
Daten
Out[6]: a 0,25
b 0.50
c 0.75
d 1.00
e 1.25
dTyp: float64

Diese einfache Veränderbarkeit der Objekte ist eine praktische Funktion: Unter der Haube ist Pandas

Entscheidungen über das Speicherlayout und das Kopieren von Daten zu treffen, die möglicherweise
und der Benutzer muss sich im Allgemeinen nicht um diese Fragen kümmern.

Serien als eindimensionales Array
Eine Serie baut auf dieser wörterbuchähnlichen Schnittstelle auf und bietet die Auswahl von Elementen im Array-Stil.

tion über die gleichen grundlegenden Mechanismen wie NumPy-Arrays, d. h. Slices, Maskierung und
ausgefallene Indizierung. Beispiele hierfür sind die folgenden:

In [7]: # Aufteilung nach explizitem Index
data['a':'c']
Out[7]: a 0.25
b 0.50
c 0.75
dtype: float64

In [8]: # Aufteilung nach implizitem ganzzahligem Index
data[0:2]
Out[8]: a 0,25
b 0.50
dtype: float64

Datenauswahl in Serien | 111
In [9]: # Maskierung
Daten[(Daten > 0.3) & (Daten < 0.8)]
Out[9]: b 0.50
c 0.75
dtype: float64

In [10]: # fancy indexing
data[['a', 'e']]
Out[10]: a 0.25
e 1.25
dtype: float64

Von diesen ist das Slicing wohl die Quelle der größten Verwirrung. Beachten Sie, dass beim Slicing

mit einem expliziten Index (z. B. data['a':'c']), wird der endgültige Index in das Slice aufgenommen,

während beim Slicing mit einem impliziten Index (z. B. data[0:2]) der endgültige Index nicht
aus dem Slice ausgeschlossen.

Indizierer: loc und iloc
Wenn Ihre Serie einen expliziten Integer-Index hat, kann eine Indexierungsoperation wie data[1]

werden die expliziten Indizes verwendet, während eine Slicing-Operation wie data[1:3] die

implizite Indizes im Python-Stil:

In [11]: Daten = pd.Reihe(['a', 'b', 'c'], index=[1, 3, 5])
Daten
Out[11]: 1 a
3 b
5 c
dTyp: Objekt

In [12]: # expliziter Index bei Indizierung
data[1]
Out[12]: 'a'

In [13]: # impliziter Index beim Slicing
data[1:3]
Out[13]: 3 b
5 c
dTyp: Objekt

Wegen dieser möglichen Verwirrung im Fall von Integer-Indizes bietet Pandas

einige spezielle Indexer-Attribute, die bestimmte Indexierungsschemata explizit offenlegen. Diese

sind keine funktionalen Methoden, sondern Attribute, die eine bestimmte Schnittstelle für das Slicing offenlegen

die Daten in der Reihe.

Erstens ermöglicht das loc-Attribut eine Indizierung und Aufteilung, die immer auf die explizite

Index:

In [14]: data.loc[1]
Out[14]: 'a'

112 | Kapitel 14: Datenindizierung und -auswahl

In [15]: data.loc[1:3]
Out[15]: 1 a
3 b
dtype: Objekt

Das iloc-Attribut erlaubt Indexierung und Slicing, die immer auf den impliziten
Index im Python-Stil verweist:

In [16]: data.iloc[1]
Out[16]: 'b'

In [17]: data.iloc[1:3]
Out[17]: 3 b
5 c
dTyp: Objekt

Ein Leitprinzip des Python-Codes lautet: "Explizit ist besser als implizit". Die

explizite Natur von loc und iloc macht sie hilfreich bei der Aufrechterhaltung sauberer und lesbarer

Code; insbesondere im Fall von Integer-Indizes kann deren konsistente Verwendung
subtile Fehler aufgrund der gemischten Indexierungs-/Slicing-Konvention verhindern.

Datenauswahl in DataFrames
Ein DataFrame verhält sich in vielerlei Hinsicht wie ein zweidimensionales oder strukturiertes Array,

und auf andere Weise wie ein Wörterbuch von Reihenstrukturen, die denselben Index haben.

Diese Analogien können hilfreich sein, wenn wir die Datenauswahl innerhalb dieser Struktur
dieser Struktur.

DataFrame als Wörterbuch
Die erste Analogie, die wir betrachten werden, ist der DataFrame als ein Wörterbuch verwandter Reihen

Objekte. Kehren wir zu unserem Beispiel der Gebiete und der Bevölkerung von Staaten zurück:

In [18]: area = pd.Series({'California': 423967, 'Texas': 695662,
'Florida': 170312, 'New York': 141297,
'Pennsylvania': 119280})
pop = pd.Series({'California': 39538223, 'Texas': 29145505,
'Florida': 21538187, 'New York': 20201249,
'Pennsylvania': 13002700})
data = pd.DataFrame({'area':area, 'pop':pop})
data
Out[18]: Gebiet pop
Kalifornien 423967 39538223
Texas 695662 29145505
Florida 170312 21538187
New York 141297 20201249
Pennsylvania 119280 13002700

Datenauswahl in DataFrames | 113
Auf die einzelnen Reihen, aus denen die Spalten des DataFrame bestehen, kann zugegriffen werden
über eine wörterbuchähnliche Indizierung des Spaltennamens:

In [19]: data['area']
Out[19]: Kalifornien 423967
Texas 695662
Florida 170312
New York 141297
Pennsylvania 119280
Name: Gebiet, dtype: int64

Gleichwertig kann der Zugriff über Attribute mit Spaltennamen erfolgen, die Strings sind:

In [20]: data.area
Out[20]: Kalifornien 423967
Texas 695662
Florida 170312
New York 141297
Pennsylvania 119280
Name: Gebiet, dtype: int64

Dies ist zwar eine nützliche Abkürzung, aber sie funktioniert nicht in allen Fällen!

Zum Beispiel, wenn die Spaltennamen keine Zeichenketten sind, oder wenn die Spaltennamen in Konflikt stehen

mit Methoden des DataFrame ist dieser attributähnliche Zugriff nicht möglich. Zum Beispiel-

ple hat der DataFrame eine pop-Methode, so dass data.pop auf diese Methode verweist und nicht auf die

Pop-Kolumne:

In [21]: data.pop ist data["pop"]
Out[21]: False

Insbesondere sollten Sie der Versuchung widerstehen, die Spaltenzuordnung über

Attribute (d. h. verwenden Sie data['pop'] = z statt data.pop = z).

Wie bei den zuvor besprochenen Reihenobjekten kann diese wörterbuchartige Syntax auch

verwendet, um das Objekt zu ändern, in diesem Fall das Hinzufügen einer neuen Spalte:

In [22]: data['density'] = data['pop'] / data['area']
Daten
Out[22]: Gebiet Bevölkerungsdichte
Kalifornien 423967 39538223 93,257784
Texas 695662 29145505 41,896072
Florida 170312 21538187 126,463121
New York 141297 20201249 142,970120
Pennsylvania 119280 13002700 109,009893

Dies zeigt eine Vorschau auf die einfache Syntax der Element-für-Element-Arithmetik

zwischen Serienobjekten; wir werden dies in Kapitel 15 näher erläutern.

114 | Kapitel 14: Datenindizierung und -auswahl

DataFrame als zweidimensionales Array
Wie bereits erwähnt, können wir den DataFrame auch als erweitertes zweidimensionales Array betrachten.

dimensionalen Array. Wir können das zugrunde liegende Rohdatenfeld anhand der Werte untersuchen

Attribut:

In [23]: data.values
Out[23]: array([[4.23967000e+05, 3.95382230e+07, 9.32577842e+01],
[6.95662000e+05, 2.91455050e+07, 4.18960717e+01],
[1.70312000e+05, 2.15381870e+07, 1.26463121e+02],
[1.41297000e+05, 2.02012490e+07, 1.42970120e+02],
[1.19280000e+05, 1.30027000e+07, 1.09009893e+02]])

Mit diesem Bild im Hinterkopf können viele vertraute Array-ähnliche Operationen mit der

DataFrame selbst. Zum Beispiel können wir den vollständigen DataFrame transponieren, um Zeilen und Spalten zu vertauschen
Spalten:

Eingang [24]: Daten.T
Out[24]: Kalifornien Texas Florida New York Pennsylvania
Fläche 4.239670e+05 6.956620e+05 1.703120e+05 1.412970e+05 1.192800e+05
Bevölkerung 3.953822e+07 2.914550e+07 2.153819e+07 2.020125e+07 1.300270e+07
Dichte 9.325778e+01 4.189607e+01 1.264631e+02 1.429701e+02 1.090099e+02

Wenn es um die Indizierung eines DataFrame-Objekts geht, ist es jedoch klar, dass die

Die wörterbuchartige Indizierung von Spalten verhindert, dass wir sie einfach als eine

NumPy-Array. Insbesondere greift die Übergabe eines einzelnen Index an ein Array auf eine Zeile zu:

In [25]: data.values[0]
Out[25]: array([4.23967000e+05, 3.95382230e+07, 9.32577842e+01])

und die Übergabe eines einzelnen "Index" an einen DataFrame greift auf eine Spalte zu:

In [26]: data['area']
Out[26]: Kalifornien 423967
Texas 695662
Florida 170312
New York 141297
Pennsylvania 119280
Name: Gebiet, dtype: int64

Für die Indizierung im Array-Stil benötigen wir also eine andere Konvention. Auch hier verwendet Pandas

die bereits erwähnten Indexer loc und iloc. Mit dem iloc-Indexer können wir indizieren

das zugrundeliegende Array, als wäre es ein einfaches NumPy-Array (unter Verwendung der impliziten Python-

style index), aber der DataFrame-Index und die Spaltenbezeichnungen werden im Ergebnis beibehalten:

In [27]: data.iloc[:3, :2]
Out[27]: Gebiet pop
Kalifornien 423967 39538223
Texas 695662 29145505
Florida 170312 21538187

Datenauswahl in DataFrames | 115
In ähnlicher Weise können wir mit dem loc-Indexer die zugrundeliegenden Daten in einem Array-ähnlichen Stil indizieren
Array-ähnlichen Stil indizieren, jedoch unter Verwendung der expliziten Index- und Spaltennamen:

In [28]: data.loc[:'Florida', :'pop']
Out[28]: Gebiet pop
Kalifornien 423967 39538223
Texas 695662 29145505
Florida 170312 21538187

Jedes der bekannten NumPy-ähnlichen Datenzugriffsmuster kann innerhalb dieser Indexe verwendet werden.

ers. Zum Beispiel können wir im loc-Indexer Maskierung und Fancy-Indexierung kombinieren als

folgt:

In [29]: data.loc[data.density > 120, ['pop', 'density']]
Out[29]: Bevölkerungsdichte
Florida 21538187 126.463121
New York 20201249 142,970120

Jede dieser Indexierungskonventionen kann auch verwendet werden, um Werte zu setzen oder zu ändern; dies ist

auf die übliche Art und Weise, wie Sie es vielleicht von der Arbeit mit
NumPy gewohnt sind:

In [30]: data.iloc[0, 2] = 90
Daten
Out[30]: Gebiet Bevölkerungsdichte
Kalifornien 423967 39538223 90.000000
Texas 695662 29145505 41.896072
Florida 170312 21538187 126.463121
New York 141297 20201249 142,970120
Pennsylvania 119280 13002700 109,009893

Um Ihre Kenntnisse in der Datenmanipulation mit Pandas zu verbessern, schlage ich vor, einige Zeit damit zu verbringen

mit einem einfachen DataFrame und der Erkundung der Arten von Indizierung, Slicing, Maskierung und
Phantasie-Indizierung, die durch diese verschiedenen Indizierungsansätze möglich sind.

Zusätzliche Indizierungskonventionen
Es gibt einige zusätzliche Indizierungskonventionen, die vielleicht im Widerspruch zu der
im Widerspruch zu stehen scheinen, aber dennoch in der Praxis nützlich sein können. Erstens, während der Indizierung

bezieht sich auf Spalten, Slicing bezieht sich auf Zeilen:

In [31]: Daten['Florida':'New York']
Out[31]: Gebiet Bevölkerungsdichte
Florida 170312 21538187 126.463121
New York 141297 20201249 142,970120

Solche Slices können sich auch auf Zeilen nach Nummer statt nach Index beziehen:

In [32]: Daten[1:3]
Out[32]: Gebiet Bevölkerungsdichte
Texas 695662 29145505 41.896072
Florida 170312 21538187 126.463121

116 | Kapitel 14: Datenindizierung und -auswahl

In ähnlicher Weise werden direkte Maskierungsoperationen zeilenweise und nicht spaltenweise interpretiert.

weise:

In [33]: data[data.density > 120]
Out[33]: Gebiet Bevölkerungsdichte
Florida 170312 21538187 126.463121
New York 141297 20201249 142,970120

Diese beiden Konventionen sind syntaktisch ähnlich zu denen eines NumPy-Arrays, und während

sie nicht genau in die Form der Pandas-Konventionen passen, sind sie aufgrund ihres
aufgrund ihres praktischen Nutzens.

Datenauswahl in DataFrames | 117
KAPITEL 15

Operieren mit Daten in Pandas
Eine der Stärken von NumPy ist, dass es uns erlaubt, schnelle elementweise
Operationen durchzuführen, sowohl mit Grundrechenarten (Addition, Subtraktion, Multiplikation, etc.) als auch

mit komplizierteren Operationen (trigonometrische Funktionen, Exponential- und Logarithmusfunktionen)

rithmische Funktionen, usw.). Pandas erbt einen Großteil dieser Funktionalität von NumPy, und
die in Kapitel 6 vorgestellten ufuncs sind der Schlüssel dazu.

Pandas enthält jedoch einige nützliche Wendungen: für unäre Operationen wie Negation
und trigonometrische Funktionen behalten diese ufuncs Index- und Spaltenbeschriftungen in der

Ausgabe, und für binäre Operationen wie Addition und Multiplikation wird Pandas

automatisch die Indizes ausrichten, wenn die Objekte an die ufunc übergeben werden. Dies bedeutet, dass
den Kontext der Daten zu erhalten und Daten aus verschiedenen Quellen zu kombinieren - beides potenzielle

mit rohen NumPy-Arrays - werden mit Pandas im Wesentlichen narrensicher.
Pandas. Wir werden außerdem sehen, dass es wohldefinierte Operationen zwischen ein-

dimensionalen Reihenstrukturen und zweidimensionalen DataFrame-Strukturen.

Ufuncs: Index-Erhaltung
Da Pandas für die Zusammenarbeit mit NumPy entwickelt wurde, funktioniert jede NumPy-Ufunc auf

Pandas Serie und DataFrame Objekte. Beginnen wir mit der Definition einer einfachen Serie und

DataFrame, um dies zu demonstrieren:

In [1]: importiere pandas als pd
import numpy as np

In [2]: rng = np.random.default_rng(42)
ser = pd.Series(rng.integers(0, 10, 4))
ser
Out[2]: 0 0
1 7
2 6

118
3 4
dTyp: int64

In [3]: df = pd.DataFrame(rng.integers(0, 10, (3, 4)),
columns=['A', 'B', 'C', 'D'])
df
Out[3]: A B C D
0 4 8 0 6
1 2 0 5 9
2 7 7 7 7

Wenn wir eine NumPy ufunc auf eines dieser Objekte anwenden, ist das Ergebnis ein weiteres Pan-

das Objekt unter Beibehaltung der Indizes:

In [4]: np.exp(ser)
Out[4]: 0 1.000000
1 1096.633158
2 403.428793
3 54.598150
dtype: float64

Dies gilt auch für kompliziertere Vorgangsreihen:

In [5]: np.sin(df * np.pi / 4)
Out[5]: A B C D
0 1.224647e-16 -2.449294e-16 0.000000 -1.000000
1 1,000000e+00 0,000000e+00 -0,707107 0,707107
2 -7.071068e-01 -7.071068e-01 -0.707107 -0.707107

Jedes der in Kapitel 6 besprochenen ufuncs kann auf ähnliche Weise verwendet werden.

Ufuncs: Index-Ausrichtung
Bei binären Operationen auf zwei Series- oder DataFrame-Objekten richtet Pandas die Indizes
während der Durchführung der Operation. Dies ist sehr praktisch bei der Arbeit mit

mit unvollständigen Daten, wie wir in einigen der folgenden Beispiele sehen werden.

Indexabgleich in Serien
Nehmen wir an, wir kombinieren zwei verschiedene Datenquellen und möchten Folgendes finden

nur die drei flächenmäßig größten US-Bundesstaaten und die drei bevölkerungsmäßig größten US-Bundesstaaten:

In [6]: area = pd.Series({'Alaska': 1723337, 'Texas': 695662,
'California': 423967}, name='area')
Bevölkerung = pd.Series({'Kalifornien': 39538223, 'Texas': 29145505,
'Florida': 21538187}, name='Bevölkerung')

Schauen wir uns an, was passiert, wenn wir diese Zahlen teilen, um die Bevölkerungsdichte zu berechnen:

In [7]: Bevölkerung / Fläche
Aus[7]: Alaska NaN
Kalifornien 93.257784
Florida NaN

Ufuncs: Indexausrichtung | 119
Texas 41.896072
dtype: float64

Das resultierende Array enthält die Vereinigung der Indizes der beiden Eingabe-Arrays, die
direkt aus diesen Indizes ermittelt werden:

In [8]: area.index.union(population.index)
Out[8]: Index(['Alaska', 'California', 'Florida', 'Texas'], dtype='object')

Jedes Element, für das weder das eine noch das andere einen Eintrag hat, wird mit NaN markiert, oder
"Not a Number" markiert, so wie Pandas fehlende Daten markiert (siehe weitere Diskussion über

fehlende Daten in Kapitel 16). Dieser Indexabgleich wird auf diese Weise für jedes der folgenden Elemente durchgeführt

Pythons eingebaute arithmetische Ausdrücke; fehlende Werte werden durch NaN gekennzeichnet:

In [9]: A = pd.Reihe([2, 4, 6], index=[0, 1, 2])
B = pd.Reihe([1, 3, 5], index=[1, 2, 3])
A + B
Out[9]: 0 NaN
1 5.0
2 9.0
3 NaN
dTyp: float64

Wenn die Verwendung von NaN-Werten nicht das gewünschte Verhalten ist, kann der Füllwert mit

entsprechende Objektmethoden anstelle der Operatoren. Zum Beispiel würde der Aufruf von A.add(B)

ist gleichbedeutend mit dem Aufruf von A + B, erlaubt aber die optionale explizite Angabe des Füllwertes

für alle Elemente in A oder B, die möglicherweise fehlen:

In [10]: A.add(B, fill_value=0)
Out[10]: 0 2.0
1 5.0
2 9.0
3 5.0
dtype: float64

Indexausrichtung in DataFrames
Eine ähnliche Art der Ausrichtung findet sowohl für Spalten als auch für Indizes statt, wenn die-

Operationen auf DataFrame-Objekten:

In [11]: A = pd.DataFrame(rng.integers(0, 20, (2, 2)),
columns=['a', 'b'])
A
Out[11]: a b
0 10 2
1 16 9

In [12]: B = pd.DataFrame(rng.integers(0, 10, (3, 3)),
columns=['b', 'a', 'c'])
B
Out[12]: b a c
0 5 3 1

120 | Kapitel 15: Operieren auf Daten in Pandas

1 9 7 6
2 4 8 5
In [13]: A + B
Aus [12]: a b c
0 13.0 7.0 NaN
1 23,0 18,0 NaN
2 NaN NaN NaN NaN

Beachten Sie, dass die Indizes unabhängig von ihrer Reihenfolge in den beiden Objekten korrekt ausgerichtet sind,

und die Indizes im Ergebnis werden sortiert. Wie im Fall von Series können wir die asso-

arithmetischen Methoden des betreffenden Objekts und übergeben Sie einen beliebigen fill_value, der in

anstelle der fehlenden Einträge. Hier füllen wir mit dem Mittelwert aller Werte in A:

In [14]: A.add(B, fill_value=A.values.mean())
Out[14]: a b c
0 13.00 7.00 10.25
1 23.00 18.00 15.25
2 17.25 13.25 14.25

Tabelle 15-1 listet Python-Operatoren und ihre äquivalenten Pandas-Objektmethoden auf.

Tabelle 15-1. Mapping zwischen Python-Operatoren und Pandas-Methoden

Python-Operator Pandas-Methode(n)
+ addieren
sub, subtrahieren
mul, multiplizieren
/ truediv, div, divide
// floordiv
% mod
** pow
Ufuncs: Operationen zwischen DataFrames und Reihen
Bei der Durchführung von Operationen zwischen einem DataFrame und einer Serie werden der Index und die Spalten...

umn-Ausrichtung wird in ähnlicher Weise beibehalten, und das Ergebnis ist ähnlich wie bei Operationen
zwischen einem zweidimensionalen und einem eindimensionalen NumPy-Array. Betrachten Sie eine Kom-

mon-Operation, bei der die Differenz zwischen einem zweidimensionalen Feld und einer seiner Zeilen ermittelt wird
seiner Zeilen:

In [15]: A = rng.integers(10, size=(3, 4))
A
Out[15]: array([[4, 4, 2, 0],
[5, 8, 0, 8],
[8, 2, 6, 1]])

Ufuncs: Operationen zwischen DataFrames und Serien | 121
In [16]: A - A[0]
Out[16]: array([[ 0, 0, 0, 0],
[ 1, 4, -2, 8],
[ 4, -2, 4, 1]])

Nach den Übertragungsregeln von NumPy (siehe Kapitel 8) ist die Subtraktion zwischen einer zwei-

dimensionales Array und eine seiner Zeilen wird zeilenweise angewendet.

In Pandas funktioniert die Konvention standardmäßig ebenfalls zeilenweise:

In [17]: df = pd.DataFrame(A, columns=['Q', 'R', 'S', 'T'])
df - df.iloc[0]
Out[17]: Q R S T
0 0 0 0 0
1 1 4 -2 8
2 4 -2 4 1

Wenn Sie stattdessen spaltenweise arbeiten möchten, können Sie die Objektmethoden

bei der Angabe des Schlüsselworts axis:

In [18]: df.subtract(df['R'], axis=0)
Out[18]: Q R S T
0 0 0 -2 -4
1 -3 0 -8 0
2 6 0 4 -1

Beachten Sie, dass diese DataFrame/Series-Operationen, wie die zuvor besprochenen Operationen

automatisch die Indizes zwischen den beiden Elementen angleichen:

In [19]: halfrow = df.iloc[0, ::2]
halfrow
Out[19]: Q 4
S 2
Name: 0, dtype: int64

In [20]: df - halbe Reihe
Out[20]: Q R S T
0 0.0 NaN 0.0 NaN
1 1.0 NaN -2.0 NaN
2 4.0 NaN 4.0 NaN

Diese Erhaltung und Ausrichtung von Indizes und Spalten bedeutet, dass Operationen auf
Daten in Pandas immer den Datenkontext beibehalten, was die üblichen

Fehler, die bei der Arbeit mit heterogenen und/oder falsch ausgerichteten Daten in
rohen NumPy-Arrays auftreten können.

122 | Kapitel 15: Operieren auf Daten in Pandas

KAPITEL 16

Umgang mit fehlenden Daten
Der Unterschied zwischen den Daten in vielen Lehrbüchern und den Daten in der realen Welt besteht darin, dass
Daten in der realen Welt selten sauber und homogen sind. Insbesondere sind viele interessante

Datensätze einen gewissen Anteil an fehlenden Daten aufweisen. Um die Sache noch komplizierter zu machen

können verschiedene Datenquellen fehlende Daten auf unterschiedliche Weise anzeigen.

In diesem Kapitel werden wir einige allgemeine Überlegungen zu fehlenden Daten erörtern und uns mit

wie Pandas sie darstellt, und erkunden Sie einige eingebaute Pandas-Werkzeuge für den Umgang mit
Umgang mit fehlenden Daten in Python. Hier und im gesamten Buch werde ich mich auf fehlende

Daten im Allgemeinen als Null-, NaN- oder NA-Werte.

Kompromisse bei Konventionen für fehlende Daten
Es wurde eine Reihe von Ansätzen entwickelt, um das Vorhandensein von fehlenden Daten in

eine Tabelle oder einen DataFrame. Im Allgemeinen geht es dabei um eine von zwei Strategien: die Verwendung einer
Maske, die fehlende Werte global anzeigt, oder die Auswahl eines Sentinel-Wertes, der die

ein fehlender Eintrag.

Beim Maskierungsansatz kann die Maske ein völlig separates boolesches Array sein, oder sie
kann die Aneignung eines Bits in der Datendarstellung beinhalten, um lokal anzuzeigen

den Null-Status eines Wertes.

Beim Sentinel-Ansatz könnte der Sentinel-Wert eine datenspezifische Konvention sein,

z. B. einen fehlenden Integer-Wert mit -9999 oder einem seltenen Bit-Muster angeben, oder es

könnte eine globalere Konvention sein, z. B. die Angabe eines fehlenden Gleitkommawertes

mit NaN (Not a Number), einem speziellen Wert, der Teil der IEEE-Gleitkomma-Spezifikation ist
Spezifikation ist.

123
Keiner dieser Ansätze ist ohne Kompromisse. Verwendung eines separaten Maskenfeldes

erfordert die Zuweisung eines zusätzlichen booleschen Arrays, was sowohl bei der Speicherung als auch bei der
Speicher- und Berechnungsaufwand. Ein Sentinel-Wert reduziert den Bereich der gültigen Werte, die verwendet werden können.

dargestellt und kann zusätzliche (oft nicht optimierte) Logik in CPU und GPU erfordern

Arithmetik, da gängige Sonderwerte wie NaN nicht für alle Daten verfügbar sind

Typen.

Wie in den meisten Fällen, in denen es keine allgemeingültige optimale Wahl gibt, sind verschiedene Sprachen und

Systeme verwenden unterschiedliche Konventionen. Zum Beispiel verwendet die Sprache R reservierte Bit-Pat-

terns innerhalb jedes Datentyps als Sentinel-Werte, die fehlende Daten anzeigen, während das SciDB
System ein zusätzliches Byte verwendet, das an jede Zelle angehängt wird, um einen NA-Status anzuzeigen.

Fehlende Daten in Pandas
Die Art und Weise, wie Pandas mit fehlenden Werten umgeht, wird durch seine Abhängigkeit von der

NumPy-Paket, das keine eingebaute Vorstellung von NA-Werten für Nicht-Gleitkomma
Fließkomma-Datentypen hat.

Vielleicht hätte Pandas dem Beispiel von R folgen können, indem es Bit-Muster für jede einzelne Datei angibt.

Datentyp zu verwenden, um die Nichtigkeit anzuzeigen, aber dieser Ansatz erweist sich als ziemlich
unhandlich. Während R nur 4 Hauptdatentypen hat, unterstützt NumPy weit mehr als das: für

Während R beispielsweise nur einen einzigen Integer-Typ hat, unterstützt NumPy 14 grundlegende Integer-Typen
wenn man die verfügbaren Bitbreiten, die Vorzeichenbehaftetheit und die Endianness der Kodierung berücksichtigt.

ing. Die Reservierung eines bestimmten Bitmusters in allen verfügbaren NumPy-Typen würde zu einem

unhandliche Menge an Overhead bei der Spezialisierung verschiedener Operationen für verschiedene Typen,
was wahrscheinlich sogar einen neuen Fork des NumPy-Pakets erfordert. Außerdem ist für die kleineren Daten

Typen (z. B. 8-Bit-Ganzzahlen), würde der Verzicht auf ein Bit zur Verwendung als Maske den
den Bereich der darstellbaren Werte erheblich einschränken.

Aufgrund dieser Einschränkungen und Kompromisse hat Pandas zwei "Modi" zum Speichern und

Manipulation von Nullwerten:

Standardmäßig wird ein Sentinel-basiertes Schema für fehlende Daten verwendet, mit Sentinel
Werten NaN oder None, je nach Datentyp.
Alternativ können Sie die von Pandas bereitgestellten nullable data types (dtypes) verwenden.
Pandas bietet (wird später in diesem Kapitel besprochen), was zur Erstellung eines begleitenden
ein begleitendes Masken-Array, um fehlende Einträge zu verfolgen. Diese fehlenden Einträge werden dann
dem Benutzer als spezieller pd.NA-Wert präsentiert.
In beiden Fällen werden die von der Pandas-API bereitgestellten Datenoperationen und -manipulationen

wird diese fehlenden Einträge in vorhersehbarer Weise behandeln und weitergeben. Aber um
ein Gefühl dafür zu entwickeln, warum diese Entscheidungen getroffen werden, sollten wir uns kurz mit der

Kompromisse, die mit None, NaN und NA verbunden sind. Wie üblich beginnen wir mit dem Import von NumPy

und Pandas:

124 | Kapitel 16: Umgang mit fehlenden Daten

In [1]: importiere numpy als np
import pandas as pd

None als Sentinel-Wert
Für einige Datentypen verwendet Pandas None als Sentinel-Wert. None ist ein Python-Objekt,

was bedeutet, dass jedes Array, das None enthält, dtype=object haben muss, das heißt, es

muss eine Folge von Python-Objekten sein.

Beobachten Sie zum Beispiel, was passiert, wenn Sie None an ein NumPy-Array übergeben:

In [2]: vals1 = np.array([1, None , 2, 3])
vals1
Out[2]: array([1, None , 2, 3], dtype=object)

Dieses dtype=object bedeutet, dass die beste allgemeine Typdarstellung, die NumPy
für den Inhalt des Arrays ableiten kann, dass es sich um Python-Objekte handelt. Der Nachteil von

Die Verwendung von None auf diese Weise hat zur Folge, dass Operationen mit den Daten auf der Python-Ebene durchgeführt werden,
mit viel mehr Overhead als die typischerweise schnellen Operationen, die für Arrays mit

einheimische Arten:

In [3]: % timeit np.arange(1E6, dtype=int).sum()
Out[3]: 2.73 ms ± 288 μs pro Schleife (Mittelwert ± std. dev. von 7 Durchläufen, je 100 Schleifen)

In [4]: % timeit np.arange(1E6, dtype=object).sum()
Out[4]: 92.1 ms ± 3.42 ms pro Schleife (Mittelwert ± std. Abweichung von 7 Durchläufen, je 10 Schleifen)

Da Python außerdem keine arithmetischen Operationen mit None unterstützt, können Aggrega-

Begriffe wie sum oder min führen in der Regel zu einem Fehler:

In [5]: vals1.sum()
TypeError : nicht unterstützte(r) Operandentyp(en) für +: 'int' und 'NoneType'

Aus diesem Grund verwendet Pandas keine None als Sentinel in seinen numerischen Arrays.

NaN: Fehlende numerische Daten
Der andere Sentinel für fehlende Daten, NaN, ist anders; er ist ein spezieller Gleitkommawert
der von allen Systemen erkannt wird, die die standardmäßige IEEE-Gleitkommadarstellung verwenden:

In [6]: vals2 = np.array([1, np.nan, 3, 4])
vals2
Out[6]: array([ 1., nan, 3., 4.])

Beachten Sie, dass NumPy einen nativen Fließkomma-Typ für dieses Array gewählt hat: Das bedeutet, dass

Anders als das Objekt-Array von vorher, unterstützt dieses Array schnelle Operationen, die in

kompilierten Code. Denken Sie daran, dass NaN ein bisschen wie ein Datenvirus ist - es infiziert jeden anderen

Objekt, das es berührt.

Fehlende Daten in Pandas | 125
Unabhängig von der Operation wird das Ergebnis der Arithmetik mit NaN ein weiteres NaN sein:

In [7]: 1 + np.nan
Ausgang[7]: nan

In [8]: 0 * np.nan
Out[8]: nan

Dies bedeutet, dass die Aggregate über die Werte wohldefiniert sind (d. h. sie führen nicht zu

ein Fehler), aber nicht immer nützlich:

In [9]: vals2.sum(), vals2.min(), vals2.max()
Out[9]: (nan, nan, nan)

Abgesehen davon bietet NumPy NaN-fähige Versionen von Aggregationen an, die Folgendes ignorieren

diese fehlenden Werte:

In [10]: np.nansum(vals2), np.nanmin(vals2), np.nanmax(vals2)
Out[10]: (8.0, 1.0, 4.0)

Der größte Nachteil von NaN ist, dass es sich um einen Fließkommawert handelt; es gibt keine

äquivalenter NaN-Wert für Ganzzahlen, Zeichenketten oder andere Typen.

NaN und None in Pandas
NaN und None haben beide ihren Platz, und Pandas ist so aufgebaut, dass die beiden
fast austauschbar zu handhaben und konvertiert zwischen ihnen, wo es angebracht ist:

In [11]: pd.Series([1, np.nan, 2, None ])
Out[11]: 0 1.0
1 NaN
2 2.0
3 NaN
dTyp: float64

Für Typen, die keinen verfügbaren Sentinel-Wert haben, typisiert Pandas automatisch

wenn NA-Werte vorhanden sind. Wenn wir zum Beispiel einen Wert in einem Integer-Array auf

np.nan, wird sie automatisch in einen Gleitkommatyp umgewandelt, um die

NA:

In [12]: x = pd.Series(range(2), dtype=int)
x
Out[12]: 0 0
1 1
dTyp: int64

In [13]: x[0] = Keine
x
Out[13]: 0 NaN
1 1.0
dTyp: float64

126 | Kapitel 16: Umgang mit fehlenden Daten

Beachten Sie, dass Pandas nicht nur das Integer-Array in Fließkomma umwandelt, sondern auch automatisch

wandelt den Wert None in einen NaN-Wert um.

Diese Art von Magie mag sich im Vergleich zu den einheitlicheren

Ansatz für NA-Werte in domänenspezifischen Sprachen wie R funktioniert der Pandas-Sentinel/Cast-Ansatz in der Praxis recht gut und führt meiner Erfahrung nach nur selten zu
ing-Ansatz funktioniert in der Praxis recht gut und verursacht meiner Erfahrung nach nur selten

Probleme.

Tabelle 16-1 listet die Upcasting-Konventionen in Pandas auf, wenn NA-Werte eingeführt werden.

Tabelle 16-1. Pandas Behandlung von NAs nach Typ

Typklassenumwandlung beim Speichern von NAs NA-Sentinel-Wert
floating Keine Änderung np.nan
object Keine Änderung None oder np.nan
ganze Zahl Umwandlung in float64 np.nan
boolean Umwandlung in object Keine oder np.nan
Beachten Sie, dass String-Daten in Pandas immer mit einem Objekt-Dtype gespeichert werden.

Pandas Nullable Dtypes
In frühen Versionen von Pandas waren NaN und None als Sentinel-Werte die einzigen fehlenden

verfügbare Datendarstellungen. Die Hauptschwierigkeit, die dies mit sich brachte, war in Bezug auf
die implizite Typzuordnung: Es gab zum Beispiel keine Möglichkeit, eine echte ganze Zahl darzustellen

Array mit fehlenden Daten.

Um diese Schwierigkeit zu beheben, fügte Pandas später nullable dtypes hinzu, die unterschieden werden

von regulären dtypes durch Großschreibung ihrer Namen (z.B. pd.Int32 gegenüber np.int32).
Aus Gründen der Abwärtskompatibilität werden diese löschbaren dtypes nur verwendet, wenn sie speziell

angefordert.

Hier ist zum Beispiel eine Reihe ganzer Zahlen mit fehlenden Daten, die aus einer Liste mit

die alle drei verfügbaren Marker für fehlende Daten enthalten:

In [14]: pd.Series([1, np.nan, 2, None , pd.NA], dtype='Int32')
Out[14]: 0 1
1
2 2
3
4
dtype: Int32

Diese Darstellung kann austauschbar mit den anderen in allen Operationen verwendet werden
die im weiteren Verlauf dieses Kapitels untersucht werden.

Pandas nullbare D-Typen | 127
Operieren mit Nullwerten
Wie wir gesehen haben, behandelt Pandas None, NaN und NA als im Wesentlichen austauschbar für
fehlende oder ungültige Werte anzugeben. Um diese Konvention zu erleichtern, bietet Pandas sieben

Methoden zum Erkennen, Entfernen und Ersetzen von Nullwerten in Pandas-Datenstrukturen
strukturen. Sie sind:

isnull

Erzeugt eine boolesche Maske, die fehlende Werte anzeigt

notnull

Das Gegenteil von isnull

dropna
Gibt eine gefilterte Version der Daten zurück

fillna

Gibt eine Kopie der Daten mit aufgefüllten oder unterstellten fehlenden Werten zurück

Wir schließen dieses Kapitel mit einer kurzen Erkundung und Demonstration dieser
Routinen.

Erkennung von Nullwerten
Pandas-Datenstrukturen haben zwei nützliche Methoden zur Erkennung von Null-Daten: isnull und

notnull. In beiden Fällen wird eine boolesche Maske über die Daten zurückgegeben. Zum Beispiel:

In [15]: data = pd.Series([1, np.nan, 'hallo', None ])

In [16]: data.isnull()
Out[16]: 0 Falsch
1 Wahr
2 Falsch
3 Wahr
dtype: bool

Wie in Kapitel 14 erwähnt, können boolesche Masken direkt als Serie oder

DataFrame index:

In [17]: data[data.notnull()]
Out[17]: 0 1
2 hallo
dtype: Objekt

Die Methoden isnull() und notnull() liefern ähnliche boolesche Ergebnisse für

DataFrame-Objekte.

128 | Kapitel 16: Umgang mit fehlenden Daten

Nullwerte fallen lassen
Zusätzlich zu diesen Maskierungsmethoden gibt es noch die Komfortmethoden dropna

(entfernt die NA-Werte) und fillna (füllt die NA-Werte aus). Bei einer Reihe wird die

Das Ergebnis ist einfach:

In [18]: data.dropna()
Out[18]: 0 1
2 hallo
dtype: Objekt

Für einen DataFrame gibt es mehr Optionen. Betrachten Sie den folgenden DataFrame:

In [19]: df = pd.DataFrame([[1, np.nan, 2],
[2, 3, 5],
[np.nan, 4, 6]])
df
Out[19]: 0 1 2
0 1.0 NaN 2
1 2.0 3.0 5
2 NaN 4.0 6

Wir können keine einzelnen Werte aus einem DataFrame löschen; wir können nur ganze Zeilen oder Spalten löschen.

umns. Je nach Anwendung möchten Sie vielleicht das eine oder das andere, also lassen Sie

enthält eine Reihe von Optionen für einen DataFrame.

Standardmäßig lässt dropna alle Zeilen fallen, in denen ein Nullwert vorhanden ist:

In [20]: df.dropna()
Out[20]: 0 1 2
1 2.0 3.0 5

Alternativ können Sie die NA-Werte auch entlang einer anderen Achse ablegen. Mit Achse=1 oder

axis='columns' lässt alle Spalten aus, die einen Nullwert enthalten:

In [21]: df.dropna(axis='columns')
Out[21]: 2
0 2
1 5
2 6

Dabei gehen aber auch einige gute Daten verloren; Sie sind vielleicht eher daran interessiert
Zeilen oder Spalten mit allen NA-Werten oder einer Mehrheit von NA-Werten. Dies kann angegeben werden

über die Parameter how oder thresh, die eine Feinsteuerung der Anzahl der

Nullen durchzulassen.

Operieren mit Nullwerten | 129
Der Standardwert ist how='any', so dass jede Zeile oder Spalte, die einen Nullwert enthält, als

gelöscht. Sie können auch how='all' angeben, dann werden nur Zeilen/Spalten gelöscht, die

enthalten alle Nullwerte:

In [22]: df[3] = np.nan
df
Out[22]: 0 1 2 3
0 1.0 NaN 2 NaN
1 2.0 3.0 5 NaN
2 NaN 4.0 6 NaN

In [23]: df.dropna(axis='columns', how='all')
Out[23]: 0 1 2
0 1.0 NaN 2
1 2.0 3.0 5
2 NaN 4.0 6

Für eine feinere Steuerung können Sie mit dem thresh-Parameter eine Mindestanzahl von

von Nicht-Null-Werten für die aufzubewahrende Zeile/Spalte:

In [24]: df.dropna(axis='rows', thresh=3)
Out[24]: 0 1 2 3
1 2.0 3.0 5 NaN

Hier wurden die erste und die letzte Zeile gestrichen, da sie jeweils nur zwei
Nicht-Null-Werte enthalten.

Ausfüllen von Nullwerten
Anstatt NA-Werte zu löschen, möchten Sie sie manchmal durch einen gültigen
Wert ersetzen. Dieser Wert kann eine einzelne Zahl wie Null sein, oder eine Art von

Imputation oder Interpolation aus den guten Werten. Sie könnten dies an Ort und Stelle tun, indem Sie

die isnull-Methode als Maske, aber weil es sich um eine so häufige Operation handelt, hat Pandas

bietet die Methode fillna, die eine Kopie des Arrays mit den Nullwerten zurückgibt

ersetzt.

Betrachten Sie die folgende Serie:

In [25]: data = pd.Series([1, np.nan, 2, None , 3], index=list('abcde'),
dtype='Int32')
Daten
Out[25]: a 1
b
c 2
d
e 3
dtype: Int32

130 | Kapitel 16: Umgang mit fehlenden Daten

Wir können NA-Einträge mit einem einzigen Wert, z. B. Null, füllen:

In [26]: data.fillna(0)
Out[26]: a 1
b 0
c 2
d 0
e 3
dtype: Int32

Wir können eine Vorwärtsfüllung angeben, um den vorherigen Wert weiterzugeben:

In [27]: # forward fill
data.fillna(method='ffill')
Out[27]: a 1
b 1
c 2
d 2
e 3
dtype: Int32

Oder wir können eine Rückwärtsfüllung angeben, um die nächsten Werte rückwärts zu propagieren:

In [28]: # back fill
data.fillna(method='bfill')
Out[28]: a 1
b 2
c 2
d 3
e 3
dtype: Int32

Im Falle eines DataFrame sind die Optionen ähnlich, aber wir können auch eine Achse angeben

entlang derer die Auffüllungen erfolgen sollen:

Eingang [29]: df
Out[29]: 0 1 2 3
0 1.0 NaN 2 NaN
1 2.0 3.0 5 NaN
2 NaN 4.0 6 NaN

In [30]: df.fillna(method='ffill', axis=1)
Out[30]: 0 1 2 3
0 1.0 1.0 2.0 2.0
1 2.0 3.0 5.0 5.0
2 NaN 4.0 6.0 6.0

Beachten Sie, dass, wenn ein vorheriger Wert während einer Vorwärtsfüllung nicht verfügbar ist, der NA-Wert

bleibt.

Operieren mit Nullwerten | 131
KAPITEL 17

Hierarchische Indizierung
Bis zu diesem Punkt haben wir uns hauptsächlich auf ein- und zweidimensionale

dimensionalen Daten, die in Pandas Series- bzw. DataFrame-Objekten gespeichert sind. Häufig

ist es sinnvoll, darüber hinauszugehen und höherdimensionale Daten zu speichern, d. h. Daten, die mit einem Index versehen sind

durch mehr als eine oder zwei Tasten. Frühe Pandas-Versionen boten Panel und Panel4D

Objekte, die man als 3D- oder 4D-Analoga zum 2D-DataFrame betrachten könnte, aber sie

waren in der Praxis etwas umständlich zu handhaben. Ein weitaus üblicheres Muster für den Umgang mit
höherdimensionalen Daten ist die Verwendung einer hierarchischen Indizierung (auch bekannt als Multi

Indexierung), um mehrere Indexebenen in einen einzigen Index einzubeziehen. Auf diese Weise
können höherdimensionale Daten kompakt innerhalb der bekannten eindimensionalen Datenbank dargestellt werden.

dimensionalen Reihen und zweidimensionalen DataFrame-Objekten. (Wenn Sie interessiert sind an
echte N-dimensionale Arrays mit flexiblen Indizes im Stil von Pandas interessieren, können Sie sich die

ausgezeichnetes Xarray-Paket).

In diesem Kapitel werden wir uns mit der direkten Erstellung von MultiIndex-Objekten befassen; Überlegungen

bei der Indizierung, Aufteilung und Berechnung von Statistiken über mehrfach indizierte Daten; und
nützliche Routinen für die Konvertierung zwischen einfachen und hierarchisch indizierten Repräsenta-

der Daten.

Wir beginnen mit den Standardimporten:

In [1]: importiere pandas als pd
import numpy as np

Eine mehrfach indizierte Reihe
Beginnen wir mit der Überlegung, wie wir zweidimensionale Daten innerhalb einer

eindimensionale Reihen. Zur Konkretisierung betrachten wir eine Datenreihe, bei der

Jeder Punkt hat einen Buchstaben- und einen Zahlenschlüssel.

132
Der schlechte Weg
Angenommen, Sie möchten Daten über Staaten aus zwei verschiedenen Jahren verfolgen. Unter Verwendung der

Pandas-Tools, die wir bereits behandelt haben, könnten Sie versucht sein, einfach Python
Tupel als Schlüssel zu verwenden:

In [2]: index = [('California', 2010), ('California', 2020),
('New York', 2010), ('New York', 2020),
('Texas', 2010), ('Texas', 2020)]
Populationen = [37253956, 39538223,
19378102 , 20201249,
25145561 , 29145505]
pop = pd.Series(bevölkerungen, index=index)
pop
Out[2]: (Kalifornien, 2010) 37253956
(Kalifornien, 2020) 39538223
(New York, 2010) 19378102
(New York, 2020) 20201249
(Texas, 2010) 25145561
(Texas, 2020) 29145505
dTyp: int64

Mit diesem Indizierungsschema können Sie die Serie einfach indizieren oder aufteilen, basierend auf

auf diesen Tupel-Index:

In [3]: pop[('Kalifornien', 2020):('Texas', 2010)]
Out[3]: (Kalifornien, 2020) 39538223
(New York, 2010) 19378102
(New York, 2020) 20201249
(Texas, 2010) 25145561
dtype: int64

Aber der Komfort endet hier. Wenn Sie zum Beispiel alle Werte aus

2010 müssen Sie etwas unordentliches (und möglicherweise langwieriges) Misten durchführen, um es zu

passieren:

In [4]: pop[[i for i in pop.index if i[1] == 2010]]
Out[4]: (Kalifornien, 2010) 37253956
(New York, 2010) 19378102
(Texas, 2010) 25145561
dtype: int64

Dies führt zwar zum gewünschten Ergebnis, ist aber nicht so sauber (oder bei großen Datensätzen nicht so effizient)

wie die Slicing-Syntax, die wir in Pandas lieben gelernt haben.

Der bessere Weg: Der Pandas MultiIndex
Glücklicherweise bietet Pandas einen besseren Weg. Unsere tupelbasierte Indizierung ist im Wesentlichen ein

rudimentären Multi-Index, und der Pandas MultiIndex-Typ gibt uns die Arten von Operationen.

Informationen, die wir haben möchten. Wir können einen Multi-Index aus den Tupeln wie folgt erstellen:

In [5]: index = pd.MultiIndex.from_tuples(index)

Eine mehrfach indizierte Reihe | 133
Der MultiIndex repräsentiert mehrere Indexierungsebenen - in diesem Fall die Namen der Bundesländer
und die Jahre - sowie mehrere Beschriftungen für jeden Datenpunkt, die diese Daten kodieren

Ebenen.

Wenn wir unsere Reihen mit diesem MultiIndex neu indizieren, sehen wir die hierarchische Darstellung

der Daten:

In [6]: pop = pop.reindex(index)
pop
Out[6]: Kalifornien 2010 37253956
2020 39538223
New York 2010 19378102
2020 20201249
Texas 2010 25145561
2020 29145505
dTyp: int64

Hier zeigen die ersten beiden Spalten der Reihendarstellung die Mehrfachindexwerte.

ues, während die dritte Spalte die Daten enthält. Beachten Sie, dass einige Einträge in der
der ersten Spalte fehlen: In dieser Multi-Index-Darstellung bedeutet jeder leere Eintrag, dass die

den gleichen Wert wie die Zeile darüber.

Um nun auf alle Daten zuzugreifen, für die der zweite Index 2020 ist, können wir die Pandas-Slicing-
Notation verwenden:

In [7]: pop[:, 2020]
Out[7]: Kalifornien 39538223
New York 20201249
Texas 29145505
dtTyp: int64

Das Ergebnis ist eine einfach indizierte Reihe mit nur den Schlüsseln, an denen wir interessiert sind. Diese Syntax
ist viel bequemer (und die Operation ist viel effizienter!) als die Standard-

spun tuple-based multi-indexing solution, mit der wir begonnen haben. Wir werden jetzt weiter dis-

über diese Art von Indizierungsoperationen bei hierarchisch indizierten Daten.

MultiIndex als zusätzliche Dimension
Vielleicht fällt Ihnen hier noch etwas anderes auf: Wir hätten die gleichen Daten auch einfach speichern können

mit einem einfachen DataFrame mit Index und Spaltenbeschriftungen. Tatsächlich ist Pandas gebaut mit

diese Äquivalenz im Auge behalten. Die Entstapelungsmethode konvertiert schnell eine Multiplikation

indizierte Reihe in einen konventionell indizierten DataFrame:

In [8]: pop_df = pop.unstack()
pop_df
Out[8]: 2010 2020
Kalifornien 37253956 39538223
New York 19378102 20201249
Texas 25145561 29145505

134 | Kapitel 17: Hierarchische Indizierung

Natürlich bietet die Stack-Methode den umgekehrten Vorgang:

In [9]: pop_df.stack()
Out[9]: Kalifornien 2010 37253956
2020 39538223
New York 2010 19378102
2020 20201249
Texas 2010 25145561
2020 29145505
dTyp: int64

Angesichts dessen werden Sie sich vielleicht fragen, warum wir uns die Mühe machen sollten, einen hierarchischen Index zu erstellen.

ing überhaupt nicht. Der Grund dafür ist einfach: So wie wir die Mehrfachindizierung nutzen konnten, um die

späten zweidimensionalen Daten innerhalb einer eindimensionalen Reihe, können wir sie auch verwenden, um

Daten mit drei oder mehr Dimensionen in einer Serie oder einem DataFrame bearbeiten. Jede zusätzliche

Ebene in einem Multi-Index stellt eine zusätzliche Datendimension dar; die Ausnutzung dieser
Eigenschaft gibt uns viel mehr Flexibilität bei den Datentypen, die wir darstellen können. Kon-

Vielleicht sollten wir für jeden Staat eine weitere Spalte mit demografischen Daten hinzufügen, und zwar unter

jedes Jahr (z. B. Bevölkerung unter 18 Jahren); mit einem MultiIndex ist dies so einfach wie das Hinzufügen

eine weitere Spalte in den DataFrame:

In [10]: pop_df = pd.DataFrame({'total': pop,
'unter18': [9284094, 8898092,
4318033 , 4181528,
6879014 , 7432474]})
pop_df
Out[10]: gesamt unter18
Kalifornien 2010 37253956 9284094
2020 39538223 8898092
New York 2010 19378102 4318033
2020 20201249 4181528
Texas 2010 25145561 6879014
2020 29145505 7432474

Außerdem arbeiten alle ufuncs und andere Funktionen, die in Kapitel 15 besprochen werden, mit

auch hierarchische Indizes. Hier berechnen wir den Anteil der Personen unter 18 Jahren nach Jahren,
unter Berücksichtigung der oben genannten Daten:

In [11]: f_u18 = pop_df['unter18'] / pop_df['gesamt']
f_u18.unstack()
Out[11]: 2010 2020
Kalifornien 0,249211 0,225050
New York 0,222831 0,206994
Texas 0,273568 0,255013

So können wir selbst hochdimensionale Daten einfach und schnell bearbeiten und untersuchen.

Daten.

Eine mehrfach indizierte Reihe | 135
Methoden der MultiIndex-Erstellung
Der einfachste Weg, eine mehrfach indizierte Reihe oder einen DataFrame zu erstellen
zu konstruieren, besteht darin, dem Konstruktor einfach eine Liste von zwei oder mehr Index-Arrays zu übergeben. Zum Beispiel:

In [12]: df = pd.DataFrame(np.random.rand(4, 2),
index=[['a', 'a', 'b', 'b'], [1, 2, 1, 2]],
columns=['data1', 'data2'])
df
Out[12]: data1 data2
a 1 0.748464 0.561409
2 0.379199 0.622461
b 1 0.701679 0.687932
2 0.436200 0.950664

Die Erstellung des MultiIndex wird im Hintergrund durchgeführt.

Ähnlich verhält es sich, wenn Sie ein Dictionary mit entsprechenden Tupeln als Schlüssel übergeben, wird Pandas auto-

erkennen dies automatisch und verwenden standardmäßig einen MultiIndex:

In [13]: Daten = {('Kalifornien', 2010): 37253956,
('Kalifornien', 2020): 39538223,
('New York', 2010): 19378102,
('New York', 2020): 20201249,
('Texas', 2010): 25145561,
('Texas', 2020): 29145505}
pd.Series(data)
Out[13]: Kalifornien 2010 37253956
2020 39538223
New York 2010 19378102
2020 20201249
Texas 2010 25145561
2020 29145505
dTyp: int64

Dennoch ist es manchmal nützlich, einen MultiIndex explizit zu erstellen.
einige Methoden, die dies ermöglichen.

Explizite MultiIndex-Konstruktoren
Um flexibler zu sein, wie der Index konstruiert wird, können Sie stattdessen die Kon-

Struktormethoden, die in der Klasse pd.MultiIndex verfügbar sind. Zum Beispiel, wie wir es zuvor getan haben,

können Sie einen MultiIndex aus einer einfachen Liste von Arrays konstruieren, die die Indexwerte enthalten

innerhalb jeder Ebene:

In [14]: pd.MultiIndex.from_arrays([['a', 'a', 'b', 'b'], [1, 2, 1, 2]])
Out[14]: MultiIndex([('a', 1),
('a', 2),
('b', 1),
('b', 2)],
)

136 | Kapitel 17: Hierarchische Indizierung

Oder Sie können es aus einer Liste von Tupeln konstruieren, die die mehrfachen Indexwerte von jedem

Punkt:

In [15]: pd.MultiIndex.from_tuples([('a', 1), ('a', 2), ('b', 1), ('b', 2)])
Out[15]: MultiIndex([('a', 1),
('a', 2),
('b', 1),
('b', 2)],
)

Sie können es sogar aus einem kartesischen Produkt einzelner Indizes konstruieren:

In [16]: pd.MultiIndex.from_product([['a', 'b'], [1, 2]])
Out[16]: MultiIndex([('a', 1),
('a', 2),
('b', 1),
('b', 2)],
)

Auf ähnliche Weise können Sie einen MultiIndex direkt unter Verwendung seiner internen Kodierung konstruieren, indem Sie

Weitergabe von Stufen (eine Liste von Listen mit den verfügbaren Indexwerten für jede Stufe) und

Codes (eine Liste von Listen, die auf diese Etiketten verweisen):

In [17]: pd.MultiIndex(levels=[['a', 'b'], [1, 2]],
codes=[[0, 0, 1, 1], [0, 1, 0, 1]])
Out[17]: MultiIndex([('a', 1),
('a', 2),
('b', 1),
('b', 2)],
)

Jedes dieser Objekte kann als Index-Argument übergeben werden, wenn eine Serie oder

DataFrame, oder an die reindex-Methode einer vorhandenen Serie oder eines DataFrame übergeben werden.

MultiIndex-Ebenen-Namen
Manchmal ist es sinnvoll, die Ebenen des MultiIndex zu benennen. Dies kann mit

durch Übergabe des Arguments names an eine der zuvor besprochenen MultiIndex

Konstruktoren oder durch nachträgliches Setzen des Attributs names des Indexes:

In [18]: pop.index.names = ['Staat', 'Jahr']
pop
Out[18]: Staat Jahr
Kalifornien 2010 37253956
2020 39538223
New York 2010 19378102
2020 20201249
Texas 2010 25145561
2020 29145505
dTyp: int64

Methoden der MultiIndex-Erstellung | 137
Bei umfangreicheren Datensätzen kann dies eine nützliche Methode sein, um die Bedeutung von

verschiedene Indexwerte.

MultiIndex für Spalten
In einem DataFrame sind die Zeilen und Spalten vollständig symmetrisch, und so wie die Zeilen
mehrere Ebenen von Indizes haben können, können auch die Spalten mehrere Ebenen haben. Kon-

Betrachten Sie das folgende Beispiel, das einige (einigermaßen realistische) medizinische Daten wiedergibt:

In [19]: # hierarchische Indizes und Spalten
index = pd.MultiIndex.from_product([[2013, 2014], [1, 2]],
names=['Jahr', 'Besuch'])
columns = pd.MultiIndex.from_product([['Bob', 'Guido', 'Sue'],
['HR', 'Temp']],
names=['subject', 'type'])

# einige Daten simulieren
Daten = np.round(np.random.randn(4, 6), 1)
daten[:, ::2] *= 10
daten += 37

# Erstellen des DataFrame
health_data = pd.DataFrame(data, index=index, columns=columns)
health_data
Out[19]: Betreff Bob Guido Sue
Typ HR-Temp HR-Temp HR-Temp
Jahr Besuch
2013 1 30.0 38.0 56.0 38.3 45.0 35.8
2 47.0 37.1 27.0 36.0 37.0 36.4
2014 1 51.0 35.9 24.0 36.7 32.0 36.2
2 49.0 36.3 48.0 39.2 31.0 35.7

Es handelt sich im Grunde um vierdimensionale Daten, bei denen die Dimensionen das Subjekt sind,

die Art der Messung, das Jahr und die Besuchsnummer. Auf diese Weise können wir für

Beispiel die oberste Spalte durch den Namen der Person indizieren und einen vollständigen DataFrame erhalten
der nur die Informationen dieser Person enthält:

In [20]: health_data['Guido']
Out[20]: Typ HR Temp
Jahr Besuch
2013 1 56.0 38.3
2 27.0 36.0
2014 1 24.0 36.7
2 48.0 39.2

Indizierung und Slicing eines MultiIndex
Die Indizierung und das Slicing eines MultiIndex sind so konzipiert, dass sie intuitiv sind, und es ist hilfreich, wenn Sie

betrachten Sie die Indizes als zusätzliche Dimensionen. Zunächst werden wir die Indizierung mehrfach betrachten

indizierte Reihen und dann mehrfach indizierte DataFrame-Objekte.

138 | Kapitel 17: Hierarchische Indizierung

Vielfach indizierte Reihen
Betrachten wir die mehrfach indizierte Reihe der Bevölkerungszahlen der Staaten, die wir zuvor gesehen haben:

In [21]: pop
Out[21]: Staat Jahr
Kalifornien 2010 37253956
2020 39538223
New York 2010 19378102
2020 20201249
Texas 2010 25145561
2020 29145505
dTyp: int64

Wir können auf einzelne Elemente zugreifen, indem wir mit mehreren Begriffen indizieren:

In [22]: pop['California', 2010]
Out[22]: 37253956

Der MultiIndex unterstützt auch die partielle Indizierung, d.h. die Indizierung nur einer der Ebenen in

den Index. Das Ergebnis ist eine weitere Reihe, wobei die untergeordneten Indizes beibehalten werden:

In [23]: pop['Kalifornien']
Out[23]: Jahr
2010 37253956
2020 39538223
dtTyp: int64

Partielles Slicing ist ebenfalls möglich, solange der MultiIndex sortiert ist (siehe Diskus-

(siehe Abschnitt "Sortierte und unsortierte Indizes" auf Seite 141):

In [24]: poploc['california':'new york']
Out[24]: Staat Jahr
Kalifornien 2010 37253956
2020 39538223
new york 2010 19378102
2020 20201249
dtTyp: int64

mit sortierten Indizes kann eine Teilindexierung auf niedrigeren Ebenen durchgeführt werden, indem ein
leeren Slice im ersten Index:

In [25]: pop[:, 2010]
Out[25]: Staat
Kalifornien 37253956
new york 19378102
texas 25145561
dtTyp: int64

Andere Arten der Indizierung und Auswahl (die in Kapitel 14 besprochen werden) funktionieren ebenfalls; zum Beispiel

Beispiel: Auswahl auf der Grundlage von booleschen Masken:

In [26]: pop[pop > 22000000]
Out[26]: state Jahr

Indizierung und Aufteilung eines MultiIndex | 139
Kalifornien 2010 37253956
2020 39538223
Texas 2010 25145561
2020 29145505
dtTyp: int64

Die Auswahl auf der Grundlage einer ausgeklügelten Indizierung funktioniert ebenfalls:

In [27]: pop[[['Kalifornien', 'Texas']]
Out[27]: Staat Jahr
Kalifornien 2010 37253956
2020 39538223
Texas 2010 25145561
2020 29145505
dtTyp: int64

Vielfach indizierte DataFrames
Ein mehrfach indizierter DataFrame verhält sich auf ähnliche Weise. Betrachten wir unser Spielzeugmedium

cal DataFrame von vorher:

In [28]: health_data
Out[28]: Betreff Bob Guido Sue
Typ HR-Temp HR-Temp HR-Temp
Jahr Besuch
2013 1 30.0 38.0 56.0 38.3 45.0 35.8
2 47.0 37.1 27.0 36.0 37.0 36.4
2014 1 51.0 35.9 24.0 36.7 32.0 36.2
2 49.0 36.3 48.0 39.2 31.0 35.7

Denken Sie daran, dass Spalten in einem DataFrame primär sind, und dass die Syntax, die für Multi

ply indizierte Reihe gilt für die Spalten. Zum Beispiel können wir Guidos Herzfrequenz
Herzfrequenzdaten mit einer einfachen Operation wiederherstellen:

In [29]: health_data['Guido', 'HR']
Out[29]: Jahr Besuch
2013 1 56.0
2 27.0
2014 1 24.0
2 48.0
Name: (Guido, HR), dtype: float64

Außerdem können wir, wie im Fall des Einzelindexes, die Indexer loc, iloc und ix einführen.

die in Kapitel 14 beschrieben werden. Zum Beispiel:

In [30]: health_data.iloc[:2, :2]
Out[30]: Betreff Bob
Typ HR Temp
Jahr Besuch
2013 1 30.0 38.0
2 47.0 37.1

140 | Kapitel 17: Hierarchische Indizierung

Diese Indexer bieten eine Array-ähnliche Sicht auf die zugrunde liegenden zweidimensionalen Daten,

aber jeder einzelne Index in loc oder iloc kann als Tupel aus mehreren Indizes übergeben werden. Für

Beispiel:

In [31]: health_data.loc[:, ('Bob', 'HR')]
Out[31]: Jahr Besuch
2013 1 30.0
2 47.0
2014 1 51.0
2 49.0
Name: (Bob, HR), dtype: float64

Die Arbeit mit Slices innerhalb dieser Index-Tupel ist nicht besonders bequem; der Versuch, die

ein Slice innerhalb eines Tupels zu erstellen, führt zu einem Syntaxfehler:

In [32]: health_data.loc[(:, 1), (:, 'HR')]
SyntaxFehler: ungültige Syntax (3311942670.py, Zeile 1)

Sie können dies umgehen, indem Sie das gewünschte Slice explizit mit Python's built- erstellen.

in der Slice-Funktion, aber ein besserer Weg in diesem Zusammenhang ist es, ein IndexSlice-Objekt zu verwenden,
das Pandas für genau diese Situation bereitstellt. Zum Beispiel:

In [33]: idx = pd.IndexSlice
health_data.loc[idx[:, 1], idx[:, 'HR']]
Out[33]: Betreff Bob Guido Sue
Typ HR HR HR
Jahr Besuch
2013 1 30.0 56.0 45.0
2014 1 51.0 24.0 32.0

Wie Sie sehen können, gibt es viele Möglichkeiten, mit Daten in mehrfach indizierten Reihen zu interagieren

und DataFrames, und wie bei vielen Tools in diesem Buch ist der beste Weg, sich mit

ist es, sie auszuprobieren!

Neuordnung von Multi-Indizes
Einer der Schlüssel zur Arbeit mit mehrfach indizierten Daten ist das Wissen, wie man effektiv
die Daten umzuwandeln. Es gibt eine Reihe von Operationen, bei denen alle Informationen erhalten bleiben.

Informationen im Datensatz, sondern ordnen sie für die Zwecke verschiedener Berechnungen neu an. Wir

ein kurzes Beispiel dafür bei den Methoden stack und unstack gesehen, aber es gibt viele

mehr Möglichkeiten, die Neuanordnung von Daten zwischen hierarchischen Indizes und
und Spalten zu steuern, und wir werden sie hier erkunden.

Sortierte und unsortierte Indizes
Vorhin habe ich kurz einen Vorbehalt erwähnt, aber ich sollte ihn hier stärker betonen. Viele der

MultiIndex-Slicing-Operationen schlagen fehl, wenn der Index nicht sortiert ist. Schauen wir uns das mal genauer an.

Neuordnung von Multi-Indizes | 141
Wir beginnen mit der Erstellung einiger einfacher mehrfach indizierter Daten, bei denen die Indizes nicht

lexographisch sortiert:

In [34]: index = pd.MultiIndex.from_product([['a', 'c', 'b'], [1, 2]])
Daten = pd.Series(np.random.rand(6), index=index)
data.index.names = ['char', 'int']
data
Out[34]: char int
a 1 0.280341
2 0.097290
c 1 0.206217
2 0.431771
b 1 0.100183
2 0.015851
dTyp: float64

Wenn wir versuchen, ein Teilstück dieses Indexes zu nehmen, führt dies zu einem Fehler:

In [35]: try :
data['a':'b']
except KeyError as e:
print("KeyError", e)
KeyError 'Schlüssellänge (1) war größer als MultiIndex lexsort depth (0)'

Obwohl es aus der Fehlermeldung nicht ganz klar hervorgeht, ist dies das Ergebnis des Multi

Index wird nicht sortiert. Aus verschiedenen Gründen können partielle Slices und andere ähnliche Operationen

tionen erfordern, dass die Ebenen im MultiIndex in sortierter (d. h. lexographischer) Reihenfolge vorliegen.

Pandas bietet eine Reihe von Komfortroutinen, um diese Art der Sortierung durchzuführen,

wie z. B. die Methoden sort_index und sortlevel des DataFrame. Wir verwenden die sim-

plest, sort_index, hier:

In [36]: daten = daten.sort_index()
daten
Out[36]: char int
a 1 0.280341
2 0.097290
b 1 0.100183
2 0.015851
c 1 0.206217
2 0.431771
dTyp: float64

Wenn der Index auf diese Weise sortiert ist, funktioniert das Partial Slicing wie erwartet:

In [37]: data['a':'b']
Out[37]: char int
a 1 0.280341
2 0.097290
b 1 0.100183
2 0.015851
dtype: float64

142 | Kapitel 17: Hierarchische Indizierung

Stapeln und Entstapeln von Indizes
Wie wir bereits kurz gesehen haben, ist es möglich, einen Datensatz aus einem gestapelten Multi-Index zu konvertieren

in eine einfache zweidimensionale Darstellung umwandeln, wobei optional die zu verwendende Ebene angegeben werden kann:

In [38]: pop.unstack(level=0)
Out[38]: Jahr 2010 2020
Staat
Kalifornien 37253956 39538223
New York 19378102 20201249
Texas 25145561 29145505

In [39]: pop.unstack(level=1)
Out[39]: Staat Jahr
Kalifornien 2010 37253956
2020 39538223
New York 2010 19378102
2020 20201249
Texas 2010 25145561
2020 29145505
dTyp: int64

Das Gegenteil von unstack ist stack, das hier verwendet werden kann, um die ursprüngliche

Serie:

In [40]: pop.unstack().stack()
Out[40]: Staat Jahr
Kalifornien 2010 37253956
2020 39538223
New York 2010 19378102
2020 20201249
Texas 2010 25145561
2020 29145505
dTyp: int64

Index setzen und zurücksetzen
Eine weitere Möglichkeit, hierarchische Daten neu anzuordnen, besteht darin, die Indexbeschriftungen in Spalten umzuwandeln;

kann dies mit der Methode reset_index erreicht werden. Der Aufruf dieser Methode auf der popula-

Das Ergebnis ist ein DataFrame mit den Spalten Bundesland und Jahr, die die

Informationen, die zuvor im Index enthalten waren. Der Übersichtlichkeit halber können wir optional den
Namen der Daten für die Spaltendarstellung angeben:

In [41]: pop_flat = pop.reset_index(name='population')
pop_flat
Out[41]: Staat Jahr Bevölkerung
0 Kalifornien 2010 37253956
1 Kalifornien 2020 39538223
2 New York 2010 19378102
3 New York 2020 20201249
4 Texas 2010 25145561
5 Texas 2020 29145505

Neuordnung von Multi-Indizes | 143
Ein gängiges Muster ist es, einen MultiIndex aus den Spaltenwerten zu bilden. Dies kann

mit der set_index-Methode des DataFrame, die einen mehrfach indizierten

DataFrame:

In [42]: pop_flat.set_index(['state', 'year'])
Out[42]: Bevölkerung
Bundesland Jahr
Kalifornien 2010 37253956
2020 39538223
New York 2010 19378102
2020 20201249
Texas 2010 25145561
2020 29145505

In der Praxis ist diese Art der Neuindizierung eines der nützlichsten Muster bei der Untersuchung von
realen Datensätzen.

144 | Kapitel 17: Hierarchische Indizierung

KAPITEL 18

Kombinieren von Datensätzen: concat und append
Einige der interessantesten Studien über Daten entstehen durch die Kombination verschiedener Daten
Quellen. Diese Operationen können alles Mögliche beinhalten, von sehr einfachen Verkettungen bis hin zu

von zwei verschiedenen Datensätzen bis hin zu komplizierteren datenbankähnlichen Verknüpfungen und Zusammenführungen

die alle Überschneidungen zwischen den Datensätzen korrekt behandeln. Serien und DataFrames sind

Pandas enthält Funktionen und Methoden, die diese Art der Datenverarbeitung
die diese Art der Datenverarbeitung schnell und einfach machen.

Hier sehen wir uns die einfache Verkettung von Serien und DataFrames mit der

pd.concat-Funktion; später werden wir uns mit anspruchsvolleren In-Memory-Merges und
Joins, die in Pandas implementiert sind.

Wir beginnen mit den Standardimporten:

In [1]: importiere pandas als pd
import numpy as np

Der Einfachheit halber werden wir diese Funktion definieren, die einen DataFrame einer bestimmten
Form erstellt, die in den folgenden Beispielen nützlich sein wird:

In [2]: def make_df(cols, ind):
"""Schnell einen DataFrame erstellen"""
data = {c: [str(c) + str(i) for i in ind]
for c in cols}
return pd.DataFrame(data, ind)

# Beispiel DataFrame
make_df('ABC', Bereich(3))
Out[2]: A B C
0 A0 B0 C0
1 A1 B1 C1
2 A2 B2 C2

145
Außerdem werden wir eine schnelle Klasse erstellen, mit der wir mehrere DataFrames anzeigen können

Seite an Seite. Der Code macht von der speziellen Methode repr_html Gebrauch, die IPython/

Jupyter verwendet, um seine umfangreiche Objektanzeige zu implementieren:

In [3]: Klasse display (Objekt):
"""HTML-Darstellung mehrerer Objekte anzeigen"""
template = """

{0}{1} """ **def** __init__(self, *args): self.args = args

def repr_html(self):
return ' \n '.join(self.template.format(a, eval(a).repr_html())
for a in self.args)

def repr(self):
return ' \n\n '.join(a + ' \n ' + repr(eval(a))
for a in self.args)

Der Nutzen wird im weiteren Verlauf unserer Diskussion deutlicher werden

Abschnitt.

Rückruf: Verkettung von NumPy-Arrays
Die Verkettung von Series- und DataFrame-Objekten verhält sich ähnlich wie die Verkettung

von NumPy-Arrays, was mit der Funktion np.concatenate möglich ist, wie in
in Kapitel 5 beschrieben. Erinnern Sie sich, dass Sie damit den Inhalt von zwei oder mehr Arrays kombinieren können

in ein einziges Array:

In [4]: x = [1, 2, 3]
y = [4, 5, 6]
z = [7, 8, 9]
np.concatenate([x, y, z])
Out[4]: array([1, 2, 3, 4, 5, 6, 7, 8, 9])

Das erste Argument ist eine Liste oder ein Tupel von Arrays, die miteinander verknüpft werden sollen. Zusätzlich, im Fall

von mehrdimensionalen Arrays ein Achsenschlüsselwort, mit dem Sie die Achse angeben können
Achse angeben können, entlang derer das Ergebnis verkettet werden soll:

In [5]: x = [[1, 2],
[3, 4]]
np.concatenate([x, x], axis=1)
Out[5]: array([[1, 2, 1, 2],
[3, 4, 3, 4]])

146 | Kapitel 18: Kombinieren von Datensätzen: concat und append

Einfache Verkettung mit pd.concat
Die Funktion pd.concat bietet eine ähnliche Syntax wie np.concatenate, enthält aber eine
eine Reihe von Optionen, die wir gleich besprechen werden:

# Signatur in Pandas v1.3.5
pd.concat(objs, axis=0, join='outer', ignore_index= False , keys= None ,
levels= None , names= None , verify_integrity= False ,
sort= False , copy= True )
pd.concat kann für eine einfache Verkettung von Series- oder DataFrame-Objekten verwendet werden,

ebenso wie np.concatenate für einfache Verkettungen von Arrays verwendet werden kann:

In [6]: Serie1 = pd.Serie(['A', 'B', 'C'], index=[1, 2, 3])
Reihe2 = pd.Reihe(['D', 'E', 'F'], index=[4, 5, 6])
pd.concat([ser1, ser2])
Out[6]: 1 A
2 B
3 C
4 D
5 E
6 F
dTyp: Objekt

Sie funktioniert auch bei der Verkettung höherdimensionaler Objekte, wie z. B. DataFrames:

In [7]: df1 = make_df('AB', [1, 2])
df2 = make_df('AB', [3, 4])
display('df1', 'df2', 'pd.concat([df1, df2])')
Out[7]: df1 df2 pd.concat([df1, df2])
A B A B A B
1 A1 B1 3 A3 B3 1 A1 B1
2 A2 B2 4 A4 B4 2 A2 B2
3 A3 B3
4 A4 B4

Das Standardverhalten ist die zeilenweise Verkettung innerhalb des DataFrame (d.h. Achse=0).

Wie np.concatenate erlaubt auch pd.concat die Angabe einer Achse, entlang derer die Verkettung
Verknüpfung stattfinden soll. Betrachten Sie das folgende Beispiel:

In [8]: df3 = make_df('AB', [0, 1])
df4 = make_df('CD', [0, 1])
display('df3', 'df4', "pd.concat([df3, df4], axis='columns')")
Out[8]: df3 df4 pd.concat([df3, df4], axis='columns')
A B C D A B C D
0 A0 B0 0 C0 D0 0 A0 B0 C0 D0
1 A1 B1 1 C1 D1 1 A1 B1 C1 D1

Wir hätten auch Achse=1 angeben können; hier haben wir die intuitivere

axis='columns'.

Einfache Verkettung mit pd.concat | 147
Doppelte Indizes
Ein wichtiger Unterschied zwischen np.concatenate und pd.concat ist, dass Pandas
Verkettung Indizes beibehält, auch wenn das Ergebnis doppelte Indizes hat! Betrachten Sie .

dieses kurze Beispiel:

In [9]: x = make_df('AB', [0, 1])
y = make_df('AB', [2, 3])
y.index = x.index # Indizes übereinstimmen lassen
display('x', 'y', 'pd.concat([x, y])')
Out[9]: x y pd.concat([x, y])
A B A B A B
0 A0 B0 0 A2 B2 0 A0 B0
1 A1 B1 1 A3 B3 1 A1 B1
0 A2 B2
1 A3 B3

Beachten Sie die wiederholten Indizes im Ergebnis. Während dies innerhalb von DataFrames gültig ist, ist die

Ergebnis ist oft unerwünscht. pd.concat gibt uns einige Möglichkeiten, damit umzugehen.

Behandlung von wiederholten Indizes als Fehler

Wenn Sie einfach überprüfen möchten, dass sich die Indizes im Ergebnis von pd.concat nicht überschneiden,

können Sie das Flag verify_integrity einfügen. Wenn dieses auf True gesetzt ist, wird die Verkettung

wird eine Ausnahme auslösen, wenn es doppelte Indizes gibt. Hier ist ein Beispiel, bei dem für

Klarheit werden wir die Fehlermeldung abfangen und ausgeben:

In [10]: try :
pd.concat([x, y], verify_integrity= True )
except ValueError as e:
print("WertFehler:", e)
ValueError : Indizes haben überlappende Werte: Int64Index([0, 1], dtype='int64')

Ignorieren des Indexes

Manchmal spielt der Index selbst keine Rolle, und Sie möchten ihn einfach als

ignoriert. Diese Option kann mit dem Flag ignore_index angegeben werden. Wenn dieses auf

True, wird bei der Verkettung ein neuer Ganzzahlindex für den resultierenden DataFrame erstellt:

In [11]: display('x', 'y', 'pd.concat([x, y], ignore_index=True)')
Out[11]: x y pd.concat([x, y], ignore_index= True )
A B A B A B
0 A0 B0 0 A2 B2 0 A0 B0
1 A1 B1 1 A3 B3 1 A1 B1
2 A2 B2
3 A3 B3

148 | Kapitel 18: Kombinieren von Datensätzen: concat und append

Hinzufügen von MultiIndex-Schlüsseln

Eine andere Möglichkeit besteht darin, die Option keys zu verwenden, um eine Bezeichnung für die Datenquellen anzugeben; das
Ergebnis ist eine hierarchisch indizierte Reihe, die die Daten enthält:

In [12]: display('x', 'y', "pd.concat([x, y], keys=['x', 'y'])")
Out[12]: x y pd.concat([x, y], keys=['x', 'y'])
A B A B A B
0 A0 B0 0 A2 B2 x 0 A0 B0
1 A1 B1 1 A3 B3 1 A1 B1
y 0 A2 B2
1 A3 B3

Wir können die in Kapitel 17 besprochenen Werkzeuge verwenden, um diese mehrfach indizierte

DataFrame in die gewünschte Darstellung umzuwandeln.

Verkettung mit Joins
In den kurzen Beispielen, die wir uns gerade angesehen haben, haben wir hauptsächlich DataFrames verkettet

mit gemeinsamen Spaltennamen. In der Praxis können Daten aus verschiedenen Quellen unterschiedliche Namen haben.

und pd.concat bietet in diesem Fall mehrere Optionen. Betrachten Sie

die Verkettung der folgenden zwei DataFrames, die einige (aber nicht alle!)

Spalten gemeinsam:

In [13]: df5 = make_df('ABC', [1, 2])
df6 = make_df('BCD', [3, 4])
display('df5', 'df6', 'pd.concat([df5, df6])')
Out[13]: df5 df6 pd.concat([df5, df6])
A B C B C D A B C D
1 A1 B1 C1 3 B3 C3 D3 1 A1 B1 C1 NaN
2 A2 B2 C2 4 B4 C4 D4 2 A2 B2 C2 NaN
3 NaN B3 C3 D3
4 NaN B4 C4 D4

Standardmäßig werden Einträge, für die keine Daten verfügbar sind, mit NA-Werten gefüllt. An

ändern, können wir den join-Parameter der concat-Funktion anpassen. Standardmäßig ist der

Die Verknüpfung ist eine Vereinigung der Eingabespalten (join='outer'), aber wir können dies ändern in eine

Schnittpunkt der Spalten mit join='inner':

In [14]: display('df5', 'df6',
"pd.concat([df5, df6], join='inner')")
Out[14]: df5 df6
A B C B C D
1 A1 B1 C1 3 B3 C3 D3
2 A2 B2 C2 4 B4 C4 D4

pd.concat([df5, df6], join='inner')
B C
1 B1 C1
2 B2 C2

Einfache Verkettung mit pd.concat | 149
3 B3 C3
4 B4 C4
Ein weiteres nützliches Muster ist die Verwendung der Re-Index-Methode vor der Verkettung für feinere

Kontrolle darüber, welche Spalten weggelassen werden:

In [15]: pd.concat([df5, df6.reindex(df5.columns, axis=1)])
Out[15]: A B C
1 A1 B1 C1
2 A2 B2 C2
3 NaN B3 C3
4 NaN B4 C4

Die append-Methode
Da die direkte Array-Verkettung so üblich ist, können Series- und DataFrame-Objekte

haben eine Append-Methode, die dasselbe mit weniger Tastenanschlägen erreichen kann. Für

Beispiel: Anstelle von pd.concat([df1, df2]) können Sie df1.append(df2) verwenden:

In [16]: display('df1', 'df2', 'df1.append(df2)')
Out[16]: df1 df2 df1.append(df2)
A B A B A B
1 A1 B1 3 A3 B3 1 A1 B1
2 A2 B2 4 A4 B4 2 A2 B2
3 A3 B3
4 A4 B4

Beachten Sie, dass im Gegensatz zu den append- und extend-Methoden von Python-Listen, die append

Methode in Pandas verändert das ursprüngliche Objekt nicht, sondern erzeugt ein neues Objekt
mit den kombinierten Daten. Es ist auch keine sehr effiziente Methode, da sie die Erstellung von

eines neuen Index und Datenpuffers. Wenn Sie also vorhaben, mehrere Anhängeoperationen durchzuführen

ist es in der Regel besser, eine Liste von DataFrame-Objekten zu erstellen und sie alle auf einmal zu übergeben.

einmal an die Funktion concat.

Im nächsten Kapitel werden wir uns mit einem leistungsfähigeren Ansatz zur Kombination von Daten aus

mehrere Quellen: die in pd.merge implementierten datenbankähnlichen Zusammenführungen/Verbindungen. Für

Weitere Informationen zu concat, append und verwandten Funktionen finden Sie unter "Merge, Join,
Verknüpfen und Vergleichen" in der Pandas-Dokumentation.

150 | Kapitel 18: Kombinieren von Datensätzen: concat und append

KAPITEL 19

Kombinieren von Datensätzen: Merge und Join
Eine wichtige Funktion von Pandas ist die hochperformante, speicherinterne Verknüpfung
und Merge-Operationen, mit denen Sie vielleicht vertraut sind, wenn Sie schon einmal mit

Datenbanken. Die Hauptschnittstelle dafür ist die Funktion pd.merge, und wir werden ein paar
Beispiele sehen, wie dies in der Praxis funktionieren kann.

Der Einfachheit halber definieren wir wieder die Anzeigefunktion aus dem vorherigen Kapitel

nach den üblichen Einfuhren:

In [1]: importiere pandas als pd
import numpy as np

Klasse display (Objekt):
"""Anzeige der HTML-Darstellung mehrerer Objekte"""
template = """

{0}{1} """ **def** __init__(self, *args): self.args = args

def repr_html(self):
return ' \n '.join(self.template.format(a, eval(a).repr_html())
for a in self.args)

def repr(self):
return ' \n\n '.join(a + ' \n ' + repr(eval(a))
for a in self.args)

Relationale Algebra
Das in pd.merge implementierte Verhalten ist eine Teilmenge dessen, was man als relationale

Algebra, einem formalen Satz von Regeln für die Bearbeitung relationaler Daten, der die konzeptionelle Grundlage
konzeptionelle Grundlage der in den meisten Datenbanken verfügbaren Operationen. Die Stärke der

151
Ansatz der relationalen Algebra ist, dass er mehrere grundlegende Operationen vorschlägt, die

werden zu Bausteinen für kompliziertere Operationen auf jedem Datensatz. Mit diesem
Lexikon der grundlegenden Operationen, die effizient in einer Datenbank oder einem anderen Pro-

gram kann eine breite Palette recht komplizierter Verbundoperationen durchgeführt werden.

Pandas implementiert mehrere dieser grundlegenden Bausteine in der pd.merge

Funktion und die zugehörige Join-Methode von Series- und DataFrame-Objekten. Wie Sie feststellen werden

Diese ermöglichen es Ihnen, Daten aus verschiedenen Quellen effizient zu verknüpfen.

Kategorien von Joins
Die Funktion pd.merge implementiert verschiedene Arten von Joins: eins-zu-eins, viele-zu
one und many-to-many. Auf alle drei Arten von Verknüpfungen wird über einen identischen Aufruf von

die Schnittstelle pd.merge; die Art der Verknüpfung hängt von der Form der Eingabe ab

Daten. Wir beginnen mit einigen einfachen Beispielen für die drei Arten von Zusammenführungen und diskutieren

detaillierte Optionen etwas später.

Eins-zu-Eins-Zusammenführungen
Die vielleicht einfachste Art der Zusammenführung ist die Eins-zu-Eins-Verknüpfung, die in vielerlei Hinsicht einfach ist.

ähnlich wie die spaltenweise Verkettung, die Sie in Kapitel 18 kennen gelernt haben. Hier ein konkretes Beispiel,

Betrachten wir die folgenden zwei DataFrame-Objekte, die Informationen über mehrere
Mitarbeiter eines Unternehmens enthalten:

In [2]: df1 = pd.DataFrame({'employee': ['Bob', 'Jake', 'Lisa', 'Sue'],
'group': ['Buchhaltung', 'Technik',
'Technik', 'HR']})
df2 = pd.DataFrame({'employee': ['Lisa', 'Bob', 'Jake', 'Sue'],
hire_date': [2004, 2008, 2012, 2014]})
display('df1', 'df2')
Out[2]: df1 df2
Mitarbeiter Gruppe Mitarbeiter Einstellungsdatum
0 Bob Buchhaltung 0 Lisa 2004
1 Jake Ingenieurwesen 1 Bob 2008
2 Lisa Technik 2 Jake 2012
3 Sue Personalwesen 3 Sue 2014

Um diese Informationen in einem einzigen DataFrame zu kombinieren, können wir die Funktion pd.merge

Funktion:

In [3]: df3 = pd.merge(df1, df2)
df3
Out[3]: Arbeitnehmergruppe hire_date
0 Bob Buchhaltung 2008
1 Jake Ingenieurwesen 2012
2 Lisa Ingenieurwesen 2004
3 Sue HR 2014

152 | Kapitel 19: Kombinieren von Datensätzen: Zusammenführen und Verbinden

Die Funktion pd.merge erkennt, dass jeder DataFrame eine Mitarbeiterspalte hat, und

automatisch mit dieser Spalte als Schlüssel zusammenführt. Das Ergebnis der Zusammenführung ist eine neue Data

Frame, der die Informationen aus den beiden Eingängen kombiniert. Beachten Sie, dass die Reihenfolge der

Einträge in jeder Spalte wird nicht unbedingt beibehalten: In diesem Fall wird die Reihenfolge der

Mitarbeiter-Spalte unterscheidet sich zwischen df1 und df2, und die Funktion pd.merge ist korrekt

berücksichtigt dies. Außerdem ist zu beachten, dass die Zusammenführung im Allgemeinen die

index, außer im Sonderfall der Zusammenführung nach Index (siehe left_index und

right_index-Schlüsselwörter, auf die wir gleich eingehen werden).

Viele-zu-Eins-Joins
Many-to-One-Joins sind Joins, bei denen eine der beiden Schlüsselspalten eine doppelte

Einträge. Im Fall von vielen Einträgen bleiben die doppelten Einträge im resultierenden DataFrame erhalten.
doppelte Einträge erhalten. Betrachten Sie das folgende Beispiel für eine Viele-zu-Eins-Verknüpfung:

In [4]: df4 = pd.DataFrame({'group': ['Buchhaltung', 'Technik', 'HR'],
'supervisor': ['Carly', 'Guido', 'Steve']})
display('df3', 'df4', 'pd.merge(df3, df4)')
Out[4]: df3 df4
Mitarbeiter Gruppe Einstellungsdatum Gruppe Vorgesetzter
0 Bob Buchhaltung 2008 0 Buchhaltung Carly
1 Jake Technik 2012 1 Technik Guido
2 Lisa Technik 2004 2 HR Steve
3 Sue HR 2014

pd.merge(df3, df4)
Mitarbeitergruppe Einstellungsdatum Vorgesetzter
0 Bob Buchhaltung 2008 Carly
1 Jake Ingenieurwesen 2012 Guido
2 Lisa Ingenieurwesen 2004 Guido
3 Sue Personalwesen 2014 Steve

Der resultierende DataFrame hat eine zusätzliche Spalte mit den "Supervisor"-Informationen,
in der die Informationen an einer oder mehreren Stellen wiederholt werden, wie es die Eingaben erfordern.

Many-to-Many-Joins
Many-to-many-Joins sind zwar begrifflich etwas verwirrend, aber dennoch gut
definiert. Wenn die Schlüsselspalte sowohl im linken als auch im rechten Array Duplikate enthält, dann

das Ergebnis ist eine Zusammenführung von vielen zu vielen. Am deutlichsten wird dies vielleicht an einem konkreten

Beispiel. Betrachten wir das folgende Beispiel, in dem ein DataFrame mit einem oder mehreren

Fähigkeiten, die mit einer bestimmten Gruppe verbunden sind.

Durch die Durchführung einer Many-to-many-Verknüpfung können wir die Fähigkeiten wiederherstellen, die mit jeder

einzelne Person:

In [5]: df5 = pd.DataFrame({'group': ['Buchhaltung', 'Buchhaltung',
'Technik', 'Technik', 'HR', 'HR'],

Kategorien von Joins | 153
'Fähigkeiten': ['Mathe', 'Tabellenkalkulation', 'Software', 'Mathe',
'Tabellenkalkulation', 'Organisation']})
display('df1', 'df5', "pd.merge(df1, df5)")
Out[5]: df1 df5
Mitarbeiter Gruppe Gruppe Fähigkeiten
0 Bob Buchhaltung 0 Buchhaltungsmathematik
1 Jake Technik 1 Buchhaltung Tabellenkalkulation
2 Lisa Technik 2 Technische Software
3 Sue HR 3 Technisches Rechnen
4 HR Tabellenkalkulationen
5 HR-Organisation

pd.merge(df1, df5)
Fähigkeiten der Mitarbeitergruppe
0 Bob Buchhaltung Mathe
1 Bob Buchhaltung Tabellenkalkulationen
2 Jake Technische Software
3 Jake Ingenieurwesen Mathematik
4 Lisa Technische Software
5 Lisa Ingenieurwesen Mathe
6 Sue HR Tabellenkalkulationen
7 Sue HR-Organisation

Diese drei Arten von Joins können mit anderen Pandas-Tools verwendet werden, um eine breite
eine Vielzahl von Funktionen zu implementieren. In der Praxis sind die Datensätze jedoch selten so sauber wie der, den wir

mit denen wir hier arbeiten. Im folgenden Abschnitt werden wir einige der Optionen betrachten, die pro-

die von pd.merge zur Verfügung gestellt werden und die es Ihnen ermöglichen, die Arbeitsweise der Join-Operationen zu optimieren.

Spezifikation des Zusammenführungsschlüssels
Wir haben bereits das Standardverhalten von pd.merge gesehen: Es sucht nach einem oder mehreren übereinstimmenden
übereinstimmenden Spaltennamen zwischen den beiden Eingaben und verwendet diese als Schlüssel. Oft wird jedoch

werden die Spaltennamen nicht so gut übereinstimmen, und pd.merge bietet eine Vielzahl von

Optionen für den Umgang damit.

Das on-Schlüsselwort
Am einfachsten ist es, wenn Sie den Namen der Schlüsselspalte explizit mit dem on-Schlüsselwort angeben.

Wort, das einen Spaltennamen oder eine Liste von Spaltennamen enthält:

In [6]: display('df1', 'df2', "pd.merge(df1, df2, on='employee')")
Out[6]: df1 df2
mitarbeiter gruppe mitarbeiter anstellung_datum
0 Bob Buchhaltung 0 Lisa 2004
1 Jake Ingenieurwesen 1 Bob 2008
2 Lisa Technik 2 Jake 2012
3 Sue Personalwesen 3 Sue 2014

pd.merge(df1, df2, on='employee')
Arbeitnehmergruppe hire_date

154 | Kapitel 19: Kombinieren von Datensätzen: Zusammenführen und Verbinden

0 Bob Buchhaltung 2008
1 Jake Ingenieurwesen 2012
2 Lisa Ingenieurwesen 2004
3 Sue HR 2014

Diese Option funktioniert nur, wenn sowohl der linke als auch der rechte DataFrames den angegebenen
Spaltennamen haben.

Die Schlüsselwörter left_on und right_on
Es kann vorkommen, dass Sie zwei Datensätze mit unterschiedlichen Spaltennamen zusammenführen möchten; zum Beispiel
Beispiel: Sie haben einen Datensatz, in dem der Name des Mitarbeiters als "Name" bezeichnet wird und nicht als

als "Mitarbeiter". In diesem Fall können wir die Schlüsselwörter left_on und right_on verwenden, um

geben Sie die beiden Spaltennamen an:

In [7]: df3 = pd.DataFrame({'Name': ['Bob', 'Jake', 'Lisa', 'Sue'],
'Gehalt': [70000, 80000, 120000, 90000]})
display('df1', 'df3', 'pd.merge(df1, df3, left_on="employee",
right_on="name")')
Out[7]: df1 df3
Mitarbeiter Gruppe Name Gehalt
0 Bob Buchhaltung 0 Bob 70000
1 Jake Ingenieurwesen 1 Jake 80000
2 Lisa Ingenieurwesen 2 Lisa 120000
3 Sue Personalwesen 3 Sue 90000

pd.merge(df1, df3, left_on="Mitarbeiter", right_on="Name")
Mitarbeiter Gruppe Name Gehalt
0 Bob Buchhaltung Bob 70000
1 Jake Ingenieurwesen Jake 80000
2 Lisa Ingenieurwesen Lisa 120000
3 Sue Personalwesen Sue 90000

Das Ergebnis enthält eine überflüssige Spalte, die wir bei Bedarf weglassen können - zum Beispiel, indem wir

mit der Methode DataFrame.drop():

In [8]: pd.merge(df1, df3, left_on="employee", right_on="name").drop('name', axis=1)
Out[8]: Angestelltengruppe Gehalt
0 Bob Buchhaltung 70000
1 Jake Ingenieurwesen 80000
2 Lisa Ingenieurwesen 120000
3 Sue HR 90000

Die Schlüsselwörter left_index und right_index
Manchmal möchte man nicht auf einer Spalte zusammenführen, sondern auf einem
Index zusammenführen. Ihre Daten könnten zum Beispiel wie folgt aussehen:

In [9]: df1a = df1.set_index('Arbeitnehmer')
df2a = df2.set_index('Angestellter')
display('df1a', 'df2a')

Spezifikation des Zusammenführungsschlüssels | 155
Out[9]: df1a df2a
Gruppe hire_date
Mitarbeiter Mitarbeiter
Bob Buchhaltung Lisa 2004
Jake Technik Bob 2008
Lisa Technik Jake 2012
Sue HR Sue 2014

Sie können den Index als Schlüssel für die Zusammenführung verwenden, indem Sie den left_index und/oder

right_index-Flags in pd.merge():

In [10]: display('df1a', 'df2a',
"pd.merge(df1a, df2a, left_index=True, right_index=True)")
Out[10]: df1a df2a
gruppe mieten_datum
Mitarbeiter Mitarbeiter
Bob Buchhaltung Lisa 2004
Jake Technik Bob 2008
Lisa Technik Jake 2012
Sue HR Sue 2014

pd.merge(df1a, df2a, left_index= True , right_index= True )
gruppe anstellung_datum
Mitarbeiter
Bob Buchhaltung 2008
Jake Ingenieurwesen 2012
Lisa Ingenieurwesen 2004
Sue HR 2014

Zur Vereinfachung enthält Pandas die Methode DataFrame.join(), die eine

indexbasierte Zusammenführung ohne zusätzliche Schlüsselwörter:

In [11]: df1a.join(df2a)
Out[11]: Gruppe hire_date
Mitarbeiter
Bob Buchhaltung 2008
Jake Ingenieurwesen 2012
Lisa Ingenieurwesen 2004
Sue HR 2014

Wenn Sie Indizes und Spalten mischen möchten, können Sie left_index mit right_on kombinieren

oder left_on mit right_index, um das gewünschte Verhalten zu erreichen:

In [12]: display('df1a', 'df3', "pd.merge(df1a, df3, left_index=True,
right_on='name')")
Out[12]: df1a df3
gruppe name gehalt
Mitarbeiter 0 Bob 70000
Bob Buchhaltung 1 Jake 80000
Jake Ingenieurwesen 2 Lisa 120000
Lisa Ingenieurwesen 3 Sue 90000
Sue HR

156 | Kapitel 19: Kombinieren von Datensätzen: Zusammenführen und Verbinden

pd.merge(df1a, df3, left_index= True , right_on='name')
Gruppe Name Gehalt
0 Buchhaltung Bob 70000
1 Ingenieurwesen Jake 80000
2 Ingenieurwesen Lisa 120000
3 Personalwesen Sue 90000

Alle diese Optionen funktionieren auch mit mehreren Indizes und/oder mehreren Spalten; die

Schnittstelle für dieses Verhalten ist sehr intuitiv. Weitere Informationen hierzu finden Sie in der

Abschnitt "Merge, Join, and Concatenate" der Pandas-Dokumentation.

Festlegen der Mengenarithmetik für Joins
In allen vorangegangenen Beispielen haben wir eine wichtige Überlegung bei der
einer Verknüpfung übergangen: die Art der Mengenarithmetik, die in der Verknüpfung verwendet wird. Dies tritt auf, wenn eine

Wert in einer Schlüsselspalte erscheint, in der anderen aber nicht. Betrachten Sie dieses Beispiel:

In [13]: df6 = pd.DataFrame({'name': ['Peter', 'Paul', 'Mary'],
'Essen': ['Fisch', 'Bohnen', 'Brot']},
columns=['name', 'food'])
df7 = pd.DataFrame({'Name': ['Maria', 'Josef'],
'Getränk': ['Wein', 'Bier']},
columns=['Name', 'Getränk'])
display('df6', 'df7', 'pd.merge(df6, df7)')
Out[13]: df6 df7
Name Lebensmittel Name Getränk
0 Peter Fisch 0 Maria Wein
1 Paul Bohnen 1 Joseph Bier
2 Maria Brot

pd.merge(df6, df7)
Name Lebensmittel Getränk
0 Maria Brot Wein

Hier haben wir zwei Datensätze zusammengeführt, die nur einen einzigen "Namens"-Eintrag gemeinsam haben:
Maria. Standardmäßig enthält das Ergebnis die Schnittmenge der beiden Eingabedatensätze; dies ist

was als innerer Join bezeichnet wird. Wir können dies explizit mit dem Schlüsselwort how angeben,

die standardmäßig auf "inner" eingestellt ist:

In [14]: pd.merge(df6, df7, how='inner')
Out[14]: Name Nahrung Getränk
0 Maria Brot Wein

Andere Optionen für das Schlüsselwort how sind "outer", "left" und "right". Eine äußere Verknüpfung

gibt eine Verknüpfung über die Vereinigung der Eingabespalten zurück und füllt fehlende Werte mit

NAs:

In [15]: display('df6', 'df7', "pd.merge(df6, df7, how='outer')")
Out[15]: df6 df7
Name Lebensmittel Name Getränk
0 Peter Fisch 0 Maria Wein

Festlegen der Mengenarithmetik für Joins | 157
1 Paul Bohnen 1 Joseph Bier
2 Maria Brot

pd.merge(df6, df7, how='outer')
Name Lebensmittel Getränk
0 Peter Fisch NaN
1 Paul Bohnen NaN
2 Maria Brot Wein
3 Josef NaN Bier

Der linke Join und der rechte Join geben Joins über die linken bzw. rechten Einträge zurück.

tiv. Zum Beispiel:

In [16]: display('df6', 'df7', "pd.merge(df6, df7, how='left')")
Out[16]: df6 df7
Name Lebensmittel Name Getränk
0 Peter Fisch 0 Maria Wein
1 Paul Bohnen 1 Joseph Bier
2 Maria Brot

pd.merge(df6, df7, how='left')
Name Lebensmittel Getränk
0 Peter Fisch NaN
1 Paul Bohnen NaN
2 Maria Brot Wein

Die Ausgabezeilen entsprechen nun den Einträgen in der linken Eingabe. Verwendung von how='right'

funktioniert auf ähnliche Weise.

Alle diese Optionen können ohne weiteres auf jede der vorangegangenen Verbindungen angewendet werden

Typen.

Überlappende Spaltennamen: Das Schlüsselwort suffixes
Schließlich kann es vorkommen, dass Ihre beiden Eingabe-DataFrames widersprüchliche Spaltennamen haben.
Spaltennamen haben. Betrachten Sie dieses Beispiel:

In [17]: df8 = pd.DataFrame({'name': ['Bob', 'Jake', 'Lisa', 'Sue'],
'rank': [1, 2, 3, 4]})
df9 = pd.DataFrame({'name': ['Bob', 'Jake', 'Lisa', 'Sue'],
'rank': [3, 1, 4, 2]})
display('df8', 'df9', 'pd.merge(df8, df9, on="name")')
Out[17]: df8 df9
Name Rang Name Rang
0 Bob 1 0 Bob 3
1 Jake 2 1 Jake 1
2 Lisa 3 2 Lisa 4
3 Sue 4 3 Sue 2

pd.merge(df8, df9, on="name")
name rang_x rang_y
0 Bob 1 3

158 | Kapitel 19: Kombinieren von Datensätzen: Zusammenführen und Verbinden

1 Jake 2 1
2 Lisa 3 4
3 Sue 4 2

Da die Ausgabe zwei sich widersprechende Spaltennamen enthalten würde, wird die Merge-Funktion

hängt automatisch die Suffixe _x und _y an, um die Ausgabespalten eindeutig zu machen. Wenn

diese Standardwerte ungeeignet sind, ist es möglich, ein benutzerdefiniertes Suffix mit der Option suf

fixiert das Schlüsselwort:

In [18]: pd.merge(df8, df9, on="name", suffixes=["_L", "_R"])
Out[18]: name rang_L rang_R
0 Bob 1 3
1 Jake 2 1
2 Lisa 3 4
3 Sue 4 2

Diese Suffixe funktionieren in jedem der möglichen Join-Muster und auch dann, wenn es
mehrere sich überschneidende Spalten gibt.

In Kapitel 20 werden wir ein wenig tiefer in die relationale Algebra eintauchen. Für weitere Diskussionen,

siehe "Merge, Join, Concatenate and Compare" in der Pandas-Dokumentation.

Beispiel: Daten der US-Bundesstaaten
Zusammenführungs- und Verknüpfungsoperationen kommen am häufigsten vor, wenn Daten aus verschiedenen
Quellen. Hier betrachten wir ein Beispiel für einige Daten über US-Bundesstaaten und ihre

Populationen:

In [19]: _# Nachfolgend die Befehle zum Herunterladen der Daten

repo = "https://raw.githubusercontent.com/jakevdp/data-USstates/master"
!cd data && curl -O {repo}/state-population.csv
!cd data && curl -O {repo}/state-areas.csv
!cd data && curl -O {repo}/state-abbrevs.csv_
Werfen wir einen Blick auf die drei Datensätze, indem wir die Pandas-Funktion read_csv verwenden:

In [20]: bevölkerung = pd.read_csv('daten/staat-bevoelkerung.csv')
Gebiete = pd.read_csv('Daten/Bundesland-Gebiete.csv')
Abkürzungen = pd.read_csv('daten/staat-abkürzungen.csv')

display('pop.head()', 'areas.head()', 'abbrevs.head()')
Out[20]: pop.head()
Staat/Region Alter Jahr Bevölkerung
0 AL unter 18 Jahren 2012 1117489.0
1 AL gesamt 2012 4817528.0
2 AL unter 18 Jahren 2010 1130966.0
3 AL gesamt 2010 4785570.0
4 AL unter 18 Jahren 2011 1125763,0

Gebiete.Kopf()
Fläche des Staates (qkm)

Beispiel: US-Staaten Daten | 159
0 Alabama 52423
1 Alaska 656425
2 Arizona 114006
3 Arkansas 53182
4 Kalifornien 163707

abkürzungen.kopf()
Abkürzung des Staates
0 Alabama AL
1 Alaska AK
2 Arizona AZ
3 Arkansas AR
4 Kalifornien CA

Angenommen, wir wollen ein relativ einfaches Ergebnis berechnen:
eine Rangfolge der US-Bundesstaaten und -Territorien nach ihrer Bevölkerungsdichte im Jahr 2010. Wir haben eindeutig die

um dieses Ergebnis zu finden, aber dazu müssen wir die Datensätze kombinieren.

Wir beginnen mit einer Viele-zu-Eins-Zusammenführung, die uns die vollständigen Zustandsnamen innerhalb der

Bevölkerung DataFrame. Wir wollen auf der Grundlage der Spalte Bundesland/Region der Bevölkerung zusammenführen

und die Abkürzungsspalte von abbrevs. Wir verwenden how='outer', um sicherzustellen, dass keine

Daten aufgrund nicht übereinstimmender Bezeichnungen weggeworfen werden:

In [21]: merged = pd.merge(pop, abbrevs, how='outer',
left_on='state/region', right_on='abbreviation')
merged = merged.drop('abbreviation', axis=1) # doppelte Informationen fallen lassen
merged.head()
Out[21]: Staat/Region Alter Jahr Bevölkerung Staat
0 AL unter 18 Jahren 2012 1117489.0 Alabama
1 AL gesamt 2012 4817528.0 Alabama
2 AL unter 18 Jahren 2010 1130966.0 Alabama
3 AL gesamt 2010 4785570.0 Alabama
4 AL unter 18 Jahren 2011 1125763,0 Alabama

Überprüfen wir noch einmal, ob es hier irgendwelche Unstimmigkeiten gab, was wir durch

Suche nach Zeilen mit Nullen:

In [22]: zusammengeführt.isnull().any()
Out[22]: Zustand/Region False
Alter Falsch
Jahr Falsch
Bevölkerung Wahr
Staat Wahr
dtype: bool

Einige Werte der Grundgesamtheit sind Null; finden wir heraus, welche das sind!

In [23]: merged[merged['Bevölkerung'].isnull()].head()
Out[23]: Staat/Region Alter Jahr Bevölkerung Staat
2448 PR unter 18 Jahren 1990 NaN NaN
2449 PR gesamt 1990 NaN NaN
2450 PR gesamt 1991 NaN NaN

160 | Kapitel 19: Kombinieren von Datensätzen: Zusammenführen und Verbinden

2451 PR unter 18 Jahren 1991 NaN NaN
2452 PR insgesamt 1993 NaN NaN

Es scheint, dass alle Null-Bevölkerungswerte aus Puerto Rico aus der Zeit vor dem Jahr
2000 stammen; dies ist wahrscheinlich darauf zurückzuführen, dass diese Daten in der ursprünglichen Quelle nicht verfügbar waren.

Noch wichtiger ist, dass einige der neuen Zustandseinträge ebenfalls null sind, was

bedeutet, dass es keinen entsprechenden Eintrag im Abkürzungsschlüssel gab! Lassen Sie uns herausfinden
in welchen Regionen diese Übereinstimmung fehlt:

In [24]: merged.loc[merged['state'].isnull(), 'state/region'].unique()
Out[24]: array(['PR', 'USA'], dtype=object)

Wir können schnell auf das Problem schließen: Unsere Bevölkerungsdaten enthalten Einträge für Puerto Rico

(PR) und die Vereinigten Staaten als Ganzes (USA), während diese Einträge nicht in der
Abkürzungsschlüssel des Staates. Wir können diese Probleme schnell beheben, indem wir die entsprechenden Einträge vornehmen:

In [25]: merged.loc[merged['Staat/Region'] == 'PR', 'Staat'] = 'Puerto Rico'
merged.loc[merged['Bundesland/Region'] == 'USA', 'Bundesland'] = 'Vereinigte Staaten'
zusammengefügt.isnull().any()
Out[25]: Staat/Region Falsch
Alter Falsch
Jahr Falsch
Bevölkerung Wahr
Staat Falsch
dtype: bool

Keine Nullen mehr in der Statusspalte: Wir sind bereit!

Nun können wir das Ergebnis mit den Flächendaten nach einem ähnlichen Verfahren zusammenführen. Untersuchen Sie

Unsere Ergebnisse werden wir mit der Spalte state in beiden verknüpfen wollen:

In [26]: final = pd.merge(merged, areas, on='state', how='left')
final.head()
Out[26]: Staat/Region Alter Jahr Bevölkerung Staat Fläche (qkm)
0 AL unter 18 Jahren 2012 1117489.0 Alabama 52423.0
1 AL gesamt 2012 4817528.0 Alabama 52423.0
2 AL unter 18 Jahren 2010 1130966.0 Alabama 52423.0
3 AL gesamt 2010 4785570.0 Alabama 52423.0
4 AL unter 18 Jahren 2011 1125763,0 Alabama 52423,0

Prüfen wir noch einmal auf Nullen, um zu sehen, ob es irgendwelche Unstimmigkeiten gab:

In [27]: final.isnull().any()
Out[27]: Zustand/Region False
Alter Falsch
Jahr Falsch
Bevölkerung Wahr
Bundesland Falsch
Fläche (qkm) Wahr
dtype: bool

Beispiel: US-Staaten Daten | 161
Es gibt Nullen in der Gebietsspalte; wir können uns ansehen, welche Regionen
hier ignoriert wurden:

In [28]: final['state'][final['area (sq. mi)'].isnull()].unique()
Out[28]: array(['Vereinigte Staaten'], dtype=object)

Wir sehen, dass unser DataFrame Gebiete nicht das Gebiet der Vereinigten Staaten als

ganz. Wir könnten den entsprechenden Wert einfügen (z. B. die Summe aller staatlichen Gebiete).
), aber in diesem Fall lassen wir einfach die Nullwerte weg, weil die Bevölkerungszahl...

der gesamten Vereinigten Staaten ist für unsere aktuelle Diskussion nicht relevant:

In [29]: final.dropna(inplace= True )
final.head()
Out[29]: Staat/Region Alter Jahr Bevölkerung Staat Fläche (qkm)
0 AL unter 18 Jahren 2012 1117489.0 Alabama 52423.0
1 AL gesamt 2012 4817528.0 Alabama 52423.0
2 AL unter 18 Jahren 2010 1130966.0 Alabama 52423.0
3 AL gesamt 2010 4785570.0 Alabama 52423.0
4 AL unter 18 Jahren 2011 1125763,0 Alabama 52423,0

Jetzt haben wir alle Daten, die wir brauchen. Um die Frage von Interesse zu beantworten, wählen wir zunächst

den Teil der Daten, der dem Jahr 2010 entspricht, und die Gesamtbevölkerung.

Wir werden die Abfragefunktion verwenden, um dies schnell zu tun (dazu muss das Paket NumExpr
installiert sein; siehe Kapitel 24):

In [30]: data2010 = final.query("year == 2010 & ages == 'total'")
data2010.head()
Out[30]: Staat/Region Alter Jahr Bevölkerung Staat Fläche (qkm)
3 AL gesamt 2010 4785570.0 Alabama 52423.0
91 AK gesamt 2010 713868.0 Alaska 656425.0
101 AZ gesamt 2010 6408790.0 Arizona 114006.0
189 AR gesamt 2010 2922280.0 Arkansas 53182.0
197 CA insgesamt 2010 37333601.0 Kalifornien 163707.0

Berechnen wir nun die Bevölkerungsdichte und stellen sie der Reihe nach dar. Wir beginnen mit der Neu
Indexierung unserer Daten für den Staat und berechnen dann das Ergebnis:

In [31]: data2010.set_index('state', inplace= True )
dichte = data2010['bevölkerung'] / data2010['fläche (sq. mi)']

In [32]: density.sort_values(ascending= False , inplace= True )
density.head()
Out[32]: state
District of Columbia 8898.897059
Puerto Rico 1058.665149
New Jersey 1009,253268
Rhode Island 681,339159
Connecticut 645,600649
dtype: float64

162 | Kapitel 19: Kombinieren von Datensätzen: Zusammenführen und Verbinden

Das Ergebnis ist eine Rangliste der US-Bundesstaaten plus Washington, DC, und Puerto Rico, in der Reihenfolge

ihrer Bevölkerungsdichte im Jahr 2010, ausgedrückt in Einwohnern pro Quadratmeile. Wir können sehen, dass die bei weitem
die dichteste Region in diesem Datensatz Washington, DC (d. h. der District of Columbia) ist;

Unter den Bundesstaaten ist New Jersey am dichtesten besiedelt.

Wir können auch das Ende der Liste überprüfen:

In [33]: density.tail()
Out[33]: Staat
South Dakota 10.583512
North Dakota 9,537565
Montana 6,736171
Wyoming 5,768079
Alaska 1,087509
dtype: float64

Der bei weitem am wenigsten dichte Staat ist Alaska mit durchschnittlich etwas mehr als einem Einwohner
pro Quadratmeile.

Diese Art der Datenzusammenführung ist eine häufige Aufgabe, wenn man versucht, Fragen zu beantworten, die

Datenquellen der realen Welt. Ich hoffe, dass dieses Beispiel Ihnen einen Eindruck davon vermittelt hat, wie Sie
wie Sie die hier vorgestellten Werkzeuge kombinieren können, um Einblicke in Ihre

Daten!

Beispiel: Daten der US-Bundesstaaten | 163
KAPITEL 20

Aggregation und Gruppierung
Ein grundlegender Bestandteil vieler Datenanalyseaufgaben ist die effiziente Zusammenfassung: comput-

Aggregationen wie Summe, Mittelwert, Median, Min und Max, bei denen eine einzelne Zahl summiert wird.

Aspekte eines potenziell großen Datensatzes zu optimieren. In diesem Kapitel werden wir uns mit
Aggregationen in Pandas erforschen, von einfachen Operationen, ähnlich denen, die wir in NumPy gesehen haben

Arrays zu anspruchsvolleren Operationen, die auf dem Konzept des Groupby basieren.

Der Einfachheit halber verwenden wir dieselbe Display-Magic-Funktion, die wir in den vorherigen Kapiteln verwendet haben.
vorherigen Kapiteln verwendet haben:

In [1]: importiere numpy als np
import pandas as pd

Klasse display (Objekt):
"""Anzeige der HTML-Darstellung mehrerer Objekte"""
template = """

{0}{1} """ **def** __init__(self, *args): self.args = args

def repr_html(self):
return ' \n '.join(self.template.format(a, eval(a).repr_html())
for a in self.args)

def repr(self):
return ' \n\n '.join(a + ' \n ' + repr(eval(a))
for a in self.args)

164
Planets-Daten
Hier verwenden wir den Planets-Datensatz, der über das Seaborn-Paket verfügbar ist (siehe Kap.

ter 36). Sie enthält Informationen über Planeten, die Astronomen um andere Sterne herum entdeckt haben
anderen Sternen entdeckt haben (so genannte extrasolare Planeten oder kurz Exoplaneten). Es kann heruntergeladen werden.

ded mit einem einfachen Seaborn-Befehl:

In [2]: import seaborn as sns
planets = sns.load_dataset('planets')
planets.shape
Out[2]: (1035, 6)

In [3]: planets.head()
Out[3]: methode nummer orbital_periode masse entfernung jahr
0 Radialgeschwindigkeit 1 269.300 7.10 77.40 2006
1 Radialgeschwindigkeit 1 874,774 2,21 56,95 2008
2 Radialgeschwindigkeit 1 763,000 2,60 19,84 2011
3 Radialgeschwindigkeit 1 326,030 19,40 110,62 2007
4 Radialgeschwindigkeit 1 516,220 10,50 119,47 2009

Hier finden Sie einige Details zu den mehr als eintausend entdeckten extrasolaren Planeten
bis zum Jahr 2014.

Einfache Aggregation in Pandas
In Kapitel 7 haben wir einige der für NumPy-Arrays verfügbaren Datenaggregationen erkundet.

Wie bei einem eindimensionalen NumPy-Array geben die Aggregate für eine Pandas-Serie eine

Einzelwert:

In [4]: rng = np.random.RandomState(42)
ser = pd.Series(rng.rand(5))
ser
Out[4]: 0 0.374540
1 0.950714
2 0.731994
3 0.598658
4 0.156019
dTyp: float64

In [5]: ser.sum()
Out[5]: 2.811925491708157

In [6]: ser.mean()
Out[6]: 0.5623850983416314

Planeten Daten | 165
Bei einem DataFrame geben die Aggregate standardmäßig Ergebnisse innerhalb jeder Spalte zurück:

In [7]: df = pd.DataFrame({'A': rng.rand(5),
'B': rng.rand(5)})
df
Out[7]: A B
0 0.155995 0.020584
1 0.058084 0.969910
2 0.866176 0.832443
3 0.601115 0.212339
4 0.708073 0.181825

In [8]: df.mean()
Out[8]: A 0.477888
B 0.443420
dtype: float64

Wenn Sie das Achsenargument angeben, können Sie stattdessen innerhalb jeder Zeile aggregieren:

In [9]: df.mean(axis='columns')
Out[9]: 0 0.088290
1 0.513997
2 0.849309
3 0.406727
4 0.444949
dTyp: float64

Pandas Series- und DataFrame-Objekte enthalten alle gängigen Aggregate.

in Kapitel 7 beschrieben; darüber hinaus gibt es eine Komfortmethode, describe, die

berechnet mehrere gemeinsame Aggregate für jede Spalte und gibt das Ergebnis zurück. Verwenden wir

dies für die Planetendaten, wobei Zeilen mit fehlenden Werten vorerst ausgelassen werden:

In [10]: planets.dropna().describe()
Out[10]: anzahl orbital_period masse entfernung jahr
Anzahl 498.00000 498.000000 498.000000 498.000000 498.000000 498.000000
Mittelwert 1.73494 835.778671 2.509320 52.068213 2007.377510
std 1,17572 1469,128259 3,636274 46,596041 4,167284
min 1,00000 1,328300 0,003600 1,350000 1989,000000
25% 1.00000 38.272250 0.212500 24.497500 2005.000000
50% 1.00000 357.000000 1.245000 39.940000 2009.000000
75% 2.00000 999.600000 2.867500 59.332500 2011.000000
max 6,00000 17337,500000 25,000000 354,000000 2014,000000

Diese Methode hilft uns, die allgemeinen Eigenschaften eines Datensatzes zu verstehen. Wir können zum Beispiel

In der Spalte "Jahr" sehen Sie, dass Exoplaneten zwar schon 1989 entdeckt wurden,

Die Hälfte aller Planeten im Datensatz wurde erst 2010 oder später entdeckt. Dies ist weitgehend

dank der Kepler-Mission, deren Ziel es war, mit einem speziellen Weltraumteleskop verfinsterte Planeten um andere
mit einem speziell entwickelten Weltraumteleskop.

Tabelle 20-1 fasst einige weitere eingebaute Pandas-Aggregationen zusammen.

166 | Kapitel 20: Aggregation und Gruppierung

Tabelle 20-1. Auflistung der Pandas-Aggregationsmethoden

Aggregation Rückgaben
count Gesamtzahl der Elemente
first, last Erstes und letztes Element
mean, median Mittelwert und Median
min, max Minimum und Maximum
std, var Standardabweichung und Varianz
mad Mittlere absolute Abweichung
prod Produkt aller Items
sum Summe aller Elemente
Dies sind alle Methoden der Objekte DataFrame und Series.

Um tiefer in die Daten einzudringen, reichen einfache Aggregate jedoch oft nicht aus. Die

Die nächste Stufe der Datenverdichtung ist die Operation groupby, mit der Sie
schnell und effizient Aggregate für Teilmengen von Daten zu berechnen.

Gruppieren: Teilen, Anwenden, Kombinieren
Einfache Aggregationen können Ihnen einen Eindruck von Ihrem Datensatz vermitteln, aber oft würden wir lieber

um abhängig von einem Label oder Index zu aggregieren: Dies ist in der so-

Groupby-Operation genannt. Der Name "group by" stammt von einem Befehl in der SQL

Datenbanksprache, aber es ist vielleicht aufschlussreicher, es mit den Begriffen zu betrachten, die zuerst von Hadley
Hadley Wickham, bekannt durch Rstats, geprägt hat: teilen, anwenden, kombinieren.

Teilen, Anwenden, Kombinieren
Ein kanonisches Beispiel für diese Split-Apply-Combine-Operation, bei der das "apply" eine
Summenaggregation ist, ist in Abbildung 20-1 dargestellt.

Abbildung 20-1 zeigt, was die Operation groupby bewirkt:

Der Schritt des Aufteilens beinhaltet das Aufteilen und Gruppieren eines DataFrame in Abhängigkeit von dem
Wert des angegebenen Schlüssels.
Der Schritt apply beinhaltet die Berechnung einer Funktion, in der Regel ein Aggregat, eine Transfor-
mation oder Filterung, innerhalb der einzelnen Gruppen.
Der Schritt combine fasst die Ergebnisse dieser Operationen in einem Ausgabe-Array zusammen.
groupby: Aufteilen, Anwenden, Kombinieren | 167
1 Der Code zur Erstellung dieser Abbildung ist im Online-Anhang zu finden.
Abbildung 20-1. Eine visuelle Darstellung einer Groupby-Operation^1

Dies könnte zwar auch manuell mit einer Kombination aus den Befehlen Maskierung,
Aggregations- und Merge-Befehlen, die weiter oben behandelt wurden, ist es wichtig zu erkennen, dass

müssen die Zwischensplits nicht explizit instanziiert werden. Vielmehr kann das Groupby
dies (oft) in einem einzigen Durchgang über die Daten tun, indem es die Summe, den Mittelwert, die Anzahl, das Minimum oder die

andere Aggregate für jede Gruppe auf dem Weg dorthin. Die Stärke des Groupby ist, dass es

abstrahiert diese Schritte: der Benutzer muss nicht darüber nachdenken, wie die Berechnung erfolgt

Sie müssen nicht unter der Motorhaube arbeiten, sondern können den Vorgang als Ganzes betrachten.

Als konkretes Beispiel wollen wir uns die Verwendung von Pandas für die in der folgenden Abbildung gezeigte Berechnung ansehen

die folgende Tabelle. Wir beginnen mit der Erstellung des Eingabe-DataFrames:

In [11]: df = pd.DataFrame({'Schlüssel': ['A', 'B', 'C', 'A', 'B', 'C'],
'data': range(6)}, columns=['key', 'data'])
df
Out[11]: Schlüssel Daten
0 A 0
1 B 1
2 C 2
3 A 3
4 B 4
5 C 5

168 | Kapitel 20: Aggregation und Gruppierung

Die grundlegendste Operation des Aufteilens, Anwendens und Kombinierens kann mit dem groupby

Methode des DataFrame und übergibt den Namen der gewünschten Schlüsselspalte:

In [12]: df.groupby('Schlüssel')
Out[12]: <pandas.core.groupby.generic.DataFrameGroupBy Objekt bei 0x11d241e20>

Beachten Sie, dass ein DataFrameGroupBy-Objekt zurückgegeben wird, nicht eine Gruppe von DataFrame
Objekten. In diesem Objekt liegt die Magie: Sie können es sich als eine spezielle Ansicht des

DataFrame, das bereit ist, die Gruppen zu untersuchen, aber keine eigentlichen Berechnungen durchführt

bis die Aggregation durchgeführt wird. Dieser Ansatz der "lazy evaluation" bedeutet, dass gemeinsame

Aggregate können effizient und für den Nutzer nahezu transparent implementiert werden.
Benutzer.

Um ein Ergebnis zu erhalten, können wir ein Aggregat auf dieses DataFrameGroupBy-Objekt anwenden,

die die entsprechenden Anwenden/Kombinieren-Schritte ausführt, um das gewünschte Ergebnis zu erzielen

Ergebnis:

In [13]: df.groupby('Schlüssel').sum()
Out[13]: Daten
Schlüssel
A 3
B 5
C 7

Die Summenmethode ist hier nur eine Möglichkeit; Sie können die meisten Pandas oder NumPy

Aggregationsfunktionen, sowie die meisten DataFrame-Operationen, wie Sie in der

folgende Diskussion.

Das GroupBy-Objekt
Das GroupBy-Objekt ist eine flexible Abstraktion: In vielerlei Hinsicht kann es einfach als

eine Sammlung von DataFrames, auch wenn sie unter der Bezeichnung "DataFrames" anspruchsvollere Dinge tut.

Kapuze. Sehen wir uns einige Beispiele anhand der Planets-Daten an.

Die vielleicht wichtigsten Operationen, die von einem GroupBy zur Verfügung gestellt werden, sind aggregieren,
filtern, transformieren und anwenden. Wir werden jede dieser Operationen im nächsten Abschnitt ausführlicher besprechen,

Doch zuvor wollen wir uns einige der anderen Funktionen ansehen, die genutzt werden können

mit der grundlegenden GroupBy-Operation.

Indizierung von Spalten

Das GroupBy-Objekt unterstützt die Spaltenindizierung auf die gleiche Weise wie das DataFrame-Objekt, und

gibt ein geändertes GroupBy-Objekt zurück. Zum Beispiel:

In [14]: planets.groupby('methode')
Out[14]: <pandas.core.groupby.generic.DataFrameGroupBy Objekt bei 0x11d1bc820>

In [15]: planets.groupby('method')['orbital_period']
Out[15]: <pandas.core.groupby.generic.SeriesGroupBy Objekt bei 0x11d1bcd60>

gruppieren: Teilen, Anwenden, Kombinieren | 169
Hier haben wir eine bestimmte Seriengruppe aus der ursprünglichen DataFrame-Gruppe ausgewählt, indem wir

Verweis auf seinen Spaltennamen. Wie beim GroupBy-Objekt werden keine Berechnungen durchgeführt

bis wir ein Aggregat für das Objekt aufrufen:

In [16]: planets.groupby('method')['orbital_period'].median()
Out[16]: methode
Astrometrie 631.180000
Sonnenfinsternis Zeitliche Schwankungen 4343.500000
Bildgebung 27500.000000
Mikrolensing 3300.000000
Modulation der Orbitalhelligkeit 0.342887
Pulsar-Zeitmessung 66,541900
Variationen der Pulsationszeit 1170,000000
Radialgeschwindigkeit 360.200000
Transit 5.714932
Transitzeitpunktvariationen 57.011000
Name: orbital_period, dtype: float64

Dies vermittelt einen Eindruck von der allgemeinen Skala der Umlaufzeiten (in Tagen), auf die jede Methode
empfindlich ist.

Iteration über Gruppen

Das GroupBy-Objekt unterstützt die direkte Iteration über die Gruppen und gibt jede Gruppe als

eine Serie oder einen DataFrame:

In [17]: for (method, group) in planets.groupby('method'):
print("{0:30s} shape={1}".format(methode, group.shape))
Out[17]: Astrometrie shape=(2, 6)
Finsternis-Zeitschwankungen shape=(9, 6)
Bildgebung shape=(38, 6)
Mikrolinsenbildung shape=(23, 6)
Modulation der Orbitalhelligkeit shape=(3, 6)
Pulsar-Zeitmessung shape=(5, 6)
Pulsationszeitpunktschwankungen shape=(1, 6)
Radialgeschwindigkeit shape=(553, 6)
Durchgang shape=(397, 6)
Transitzeitpunktvariationen shape=(4, 6)

Dies kann für die manuelle Überprüfung von Gruppen zum Zwecke der Fehlersuche nützlich sein, aber es ist

oft viel schneller, wenn Sie die eingebaute Anwendungsfunktion verwenden, die wir
besprechen werden.

Methoden der Versendung

Durch eine Art Python-Klassenzauber wird jede Methode, die nicht explizit von der

GroupBy-Objekt wird durchgereicht und für die Gruppen aufgerufen, unabhängig davon, ob es sich um

DataFrame- oder Series-Objekte. Die Verwendung der Methode describe entspricht zum Beispiel

zum Aufruf von describe auf dem DataFrame, der jede Gruppe repräsentiert:

170 | Kapitel 20: Aggregation und Gruppierung

In [18]: planets.groupby('methode')['jahr'].describe().unstack()
Out[18]: methode
count Astrometrie 2.0
Finsterniszeitpunkt-Variationen 9.0
Bildgebung 38.0
Microlensing 23.0
Modulation der Orbitalhelligkeit 3.0

max Pulsar Timing 2011.0
Pulsationszeitpunkt Variationen 2007.0
Radialgeschwindigkeit 2014.0
Durchgang 2014.0
Transitzeitpunktvariationen 2014.0
Länge: 80, dtype: float64

Ein Blick auf diese Tabelle hilft uns, die Daten besser zu verstehen: Zum Beispiel ist der Großteil der

Die meisten Planeten wurden bis 2014 mit der Radialgeschwindigkeits- und der Transitmethode entdeckt.
entdeckt, wobei die letztere Methode erst in jüngerer Zeit üblich wurde. Die neueste

Methoden scheinen Transit Timing Variation und Orbital Brightness Modulation zu sein,
die bis 2011 nicht zur Entdeckung eines neuen Planeten verwendet wurden.

Beachten Sie, dass diese Abfertigungsmethoden auf jede einzelne Gruppe angewendet werden, und die

Die Ergebnisse werden dann in GroupBy kombiniert und zurückgegeben. Auch hier gilt, dass jedes gültige DataFrame/

Die Methode Series kann in ähnlicher Weise für die entsprechende GroupBy aufgerufen werden

Objekt.

Aggregieren, Filtern, Transformieren, Anwenden
Die vorangegangene Diskussion konzentrierte sich auf die Aggregation für die Kombinationsoperation, aber

sind mehr Optionen verfügbar. Insbesondere verfügen GroupBy-Objekte über Aggregat-, Fil

Methoden, die eine Vielzahl von nützlichen Operationen effizient implementieren, bevor
Operationen, bevor die gruppierten Daten kombiniert werden.

Für die Zwecke der folgenden Unterabschnitte wird dieser DataFrame verwendet:

In [19]: rng = np.random.RandomState(0)
df = pd.DataFrame({'key': ['A', 'B', 'C', 'A', 'B', 'C'],
'data1': range(6),
'data2': rng.randint(0, 10, 6)},
columns = ['key', 'data1', 'data2'])
df
Out[19]: Schlüssel Daten1 Daten2
0 A 0 5
1 B 1 0
2 C 2 3
3 A 3 3
4 B 4 7
5 C 5 9

gruppieren: Teilen, Anwenden, Kombinieren | 171
Aggregation

Sie sind jetzt mit GroupBy-Aggregationen mit Summe, Median und dergleichen vertraut, aber die

Aggregat-Methode ermöglicht eine noch größere Flexibilität. Sie kann eine Zeichenkette, eine Funktion oder
eine Liste davon, und alle Aggregate auf einmal berechnen. Hier ist ein kurzes Beispiel...

all dies zu kombinieren:

In [20]: df.groupby('Schlüssel').aggregate(['min', np.median, max])
Out[20]: data1 data2
min median max min median max
Schlüssel
A 0 1.5 3 3 4.0 5
B 1 2.5 4 0 3.5 7
C 2 3.5 5 3 6.0 9

Ein weiteres gängiges Muster ist die Übergabe eines Wörterbuchs, das die Spaltennamen den Operatoren zuordnet.

die auf diese Spalte angewendet werden sollen:

In [21]: df.groupby('Schlüssel').aggregate({'Daten1': 'min',
'daten2': 'max'})
Out[21]: daten1 daten2
Schlüssel
A 0 5
B 1 7
C 2 9

Filtern

Ein Filtervorgang ermöglicht es Ihnen, Daten auf der Grundlage der Gruppeneigenschaften zu löschen. Für

Zum Beispiel könnten wir alle Gruppen behalten wollen, in denen die Standardabweichung größer ist als ein
als ein kritischer Wert ist:

In [22]: def filter_func(x):
return x['data2'].std() > 4

display('df', "df.groupby('Schlüssel').std()",
"df.groupby('schlüssel').filter(filter_func)")
Out[22]: df df.groupby('Schlüssel').std()
Schlüssel Daten1 Daten2 Daten1 Daten2
0 A 0 5 Schlüssel
1 B 1 0 A 2.12132 1.414214
2 C 2 3 B 2,12132 4,949747
3 A 3 3 C 2,12132 4,242641
4 B 4 7
5 C 5 9

df.groupby('Schlüssel').filter(filter_func)
Schlüssel data1 data2
1 B 1 0
2 C 2 3
4 B 4 7
5 C 5 9

172 | Kapitel 20: Aggregation und Gruppierung

Die Filterfunktion sollte einen booleschen Wert zurückgeben, der angibt, ob die Gruppe die

die Filterung. Da in diesem Fall die Gruppe A keine Standardabweichung von mehr als
4 hat, wird sie aus dem Ergebnis herausgenommen.

Umwandlung

Während die Aggregation eine reduzierte Version der Daten liefern muss, kann die Transformation
eine transformierte Version der vollständigen Daten zurückgeben, die neu kombiniert werden kann. Für eine solche Transfor-

Die Ausgabe hat die gleiche Form wie die Eingabe. Ein gängiges Beispiel ist die Zentrierung der
Daten durch Subtraktion des gruppenweisen Mittelwerts:

In [23]: def center(x):
return x - x.mean()
df.groupby('Schlüssel').transform(center)
Out[23]: data1 data2
0 -1.5 1.0
1 -1.5 -3.5
2 -1.5 -3.0
3 1.5 -1.0
4 1.5 3.5
5 1.5 3.0

Die Methode apply

Mit der Methode apply können Sie eine beliebige Funktion auf die Gruppenergebnisse anwenden. Die

Funktion sollte einen DataFrame nehmen und entweder ein Pandas-Objekt zurückgeben (z.B. DataFrame,

Serie) oder ein Skalar; das Verhalten des Kombinationsschritts wird auf den Typ der
zurückgegebenen Ausgabe angepasst.

Hier ist zum Beispiel eine Anwendungsoperation, die die erste Spalte durch die Summe der zweiten normalisiert
der zweiten normalisiert:

In [24]: def norm_by_data2(x):
# x ist ein DataFrame von Gruppenwerten
x['Daten1'] /= x['Daten2'].sum()
return x

df.groupby('Schlüssel').apply(norm_by_data2)
Out[24]: Schlüssel data1 data2
0 A 0.000000 5
1 B 0.142857 0
2 C 0.166667 3
3 A 0.375000 3
4 B 0.571429 7
5 C 0.416667 9

innerhalb eines GroupBy anzuwenden, ist flexibel: Das einzige Kriterium ist, dass die Funktion eine

DataFrame und gibt ein Pandas-Objekt oder einen Skalar zurück. Was Sie dazwischen tun, bleibt Ihnen überlassen

Sie!

gruppieren: Teilen, Anwenden, Kombinieren | 173
Den Aufteilungsschlüssel angeben
In den zuvor vorgestellten einfachen Beispielen haben wir den DataFrame nach einer einzigen Spalte aufgeteilt
Name. Dies ist nur eine von vielen Optionen, mit denen die Gruppen definiert werden können, und wir werden

gehen Sie hier einige andere Optionen für die Gruppenspezifikation durch.

Eine Liste, ein Array, eine Reihe oder ein Index mit den Gruppierungsschlüsseln

Der Schlüssel kann eine beliebige Reihe oder Liste mit einer Länge sein, die der des DataFrame entspricht. Für
Beispiel:

In [25]: L = [0, 1, 0, 1, 2, 0]
df.groupby(L).sum()
Out[25]: data1 data2
0 7 17
1 4 3
2 4 7

Das bedeutet natürlich, dass es einen anderen, ausführlicheren Weg gibt, um das Ziel zu erreichen

df.groupby('Schlüssel') from before:

In [26]: df.groupby(df['Schlüssel']).sum()
Out[26]: data1 data2
Schlüssel
A 3 8
B 5 7
C 7 12

Ein Wörterbuch oder eine Reihe, die einen Index einer Gruppe zuordnet

Eine andere Methode ist die Bereitstellung eines Wörterbuchs, das Indexwerte den Gruppenschlüsseln zuordnet:

In [27]: df2 = df.set_index('Schlüssel')
mapping = {'A': 'Vokal', 'B': 'Konsonant', 'C': 'Konsonant'}
display('df2', 'df2.groupby(mapping).sum()')
Out[27]: df2 df2.groupby(kartierung).summe()
Daten1 Daten2 Daten1 Daten2
Schlüssel Schlüssel
A 0 5 Konsonant 12 19
B 1 0 Vokal 3 8
C 2 3
A 3 3
B 4 7
C 5 9

Jede Python-Funktion

Ähnlich wie beim Mapping können Sie eine beliebige Python-Funktion übergeben, die den Indexwert eingibt
eingibt und die Gruppe ausgibt:

174 | Kapitel 20: Aggregation und Gruppierung

In [28]: df2.groupby(str.lower).mean()
Out[28]: data1 data2
Schlüssel
a 1.5 4.0
b 2.5 3.5
c 3.5 6.0

Eine Liste der gültigen Schlüssel

Darüber hinaus kann jede der vorstehenden Schlüsselwahlen kombiniert werden, um sie in einem Multi-Index zusammenzufassen:

In [29]: df2.groupby([str.lower, mapping]).mean()
Out[29]: data1 data2
Schlüssel Schlüssel
a Vokal 1.5 4.0
b Konsonant 2,5 3,5
c Konsonant 3,5 6,0

Beispiel für die Gruppierung
Als Beispiel dafür können wir in ein paar Zeilen Python-Code all dies zusammenfassen und
entdeckte Planeten nach Methode und Jahrzehnt zählen:

In [30]: dekade = 10 * (planets['year'] // 10)
jahrzehnt = jahrzehnt.astype(str) + 's'
dekade.name = 'dekade'
planets.groupby(['method', decade])['number'].sum().unstack().fillna(0)
Out[30]: Dekade 1980er 1990er 2000er 2010er
Methode
Astrometrie 0.0 0.0 0.0 2.0
Sonnenfinsternis Zeitliche Schwankungen 0.0 0.0 5.0 10.0
Bildgebung 0,0 0,0 29,0 21,0
Mikrolensing 0,0 0,0 12,0 15,0
Modulation der Helligkeit in der Umlaufbahn 0,0 0,0 0,0 5,0
Pulsar-Taktung 0,0 9,0 1,0 1,0
Pulsationszeitpunktschwankungen 0,0 0,0 1,0 0,0
Radiale Geschwindigkeit 1.0 52.0 475.0 424.0
Durchgang 0.0 0.0 64.0 712.0
Transitzeitpunktvariationen 0,0 0,0 0,0 9,0

Dies zeigt, wie leistungsfähig die Kombination vieler der bisher besprochenen Operationen ist

Punkt bei der Betrachtung realistischer Datensätze: Wir gewinnen schnell ein grobes Verständnis dafür
wann und wie extrasolare Planeten in den Jahren nach der ersten Entdeckung entdeckt wurden.

Ich würde vorschlagen, diese wenigen Codezeilen genauer zu untersuchen und die einzelnen
Schritte auszuwerten, um sicherzustellen, dass Sie genau verstehen, was sie für das Ergebnis tun. Es ist sicher...

Das ist zwar ein etwas kompliziertes Beispiel, aber wenn Sie diese Teile verstehen, werden Sie

die Möglichkeit, Ihre eigenen Daten auf ähnliche Weise zu untersuchen.

gruppieren: Teilen, Anwenden, Kombinieren | 175
KAPITEL 21

Pivot-Tabellen
Wir haben gesehen, wie die Groupby-Abstraktion es uns ermöglicht, Beziehungen innerhalb einer Datenstruktur zu untersuchen.

gesetzt. Eine Pivot-Tabelle ist ein ähnlicher Vorgang, der häufig in Tabellenkalkulationen vorkommt und

andere Programme, die mit Tabellendaten arbeiten. Die Pivot-Tabelle nimmt einfache spalten
Daten als Eingabe und gruppiert die Einträge in einer zweidimensionalen Tabelle, die

eine mehrdimensionale Verdichtung der Daten. Der Unterschied zwischen Pivot-Tabellen

und groupby können manchmal Verwirrung stiften; mir hilft es, mir Pivot-Tabellen als

im Wesentlichen eine mehrdimensionale Version der Groupby-Aggregation. Das heißt, Sie teilen-

apply-combine, aber sowohl die Aufteilung als auch die Kombination erfolgen nicht über einen eindimensionalen
dimensionalen Index, sondern über ein zweidimensionales Gitter.

Motivierende Pivot-Tabellen
Für die Beispiele in diesem Abschnitt verwenden wir die Datenbank der Passagiere der Titanic,

die über die Seaborn-Bibliothek erhältlich sind (siehe Kapitel 36):

In [1]: importiere numpy als np
importiere pandas als pd
import seaborn as sns
titanic = sns.load_dataset('titanic')

In [2]: titanic.head()
Out[2]: überlebt pclass Geschlecht Alter sibsp parch fare embarked class
0 0 3 männlich 22.0 1 0 7.2500 S Dritte
1 1 1 weiblich 38,0 1 0 71,2833 C Erste
2 1 3 weiblich 26,0 0 0 7,9250 S Dritte
3 1 1 weiblich 35,0 1 0 53,1000 S Erste
4 0 3 männlich 35,0 0 0 8,0500 S Dritte

176
wer adult_male deck embark_town lebendig allein
0 Mann Wahr NaN Southampton nein Falsch
1 Frau Falsch C Cherbourg ja Falsch
2 Frau Falsch NaN Southampton ja Wahr
3 Frau Falsch C Southampton ja Falsch
4 Mann Wahr NaN Southampton nein Wahr

Wie die Ausgabe zeigt, enthält dies eine Reihe von Datenpunkten zu jedem Passagier auf dieser

Geschlecht, Alter, Klasse, gezahlter Fahrpreis und vieles mehr.

Pivot-Tabellen von Hand
Um mehr über diese Daten zu erfahren, könnten wir mit der Gruppierung nach Geschlecht beginnen,

Überlebensstatus oder eine Kombination davon. Wenn Sie das vorherige Kapitel gelesen haben, wissen Sie

könnte man versucht sein, eine Gruppierung nach Geschlecht vorzunehmen - betrachten wir beispielsweise die Überlebensrate
Rate nach Geschlecht:

In [3]: titanic.groupby('Geschlecht')[['überlebt']].mean()
Out[3]: überlebt
Geschlecht
weiblich 0.742038
männlich 0.188908

Daraus ergibt sich ein erster Einblick: Insgesamt überleben drei von vier Frauen an Bord.

überlebten, während nur einer von fünf Männern überlebte!

Dies ist nützlich, aber wir möchten vielleicht noch einen Schritt weiter gehen und die Überlebensraten nach

sowohl das Geschlecht als auch, sagen wir, die Klasse. Unter Verwendung des Vokabulars von groupby könnten wir mit einer

Wir gruppieren zunächst nach Klasse und Geschlecht, wählen dann das Überleben aus, wenden einen Mittelwert

aggregieren, die resultierenden Gruppen kombinieren und schließlich den hierarchischen Index entstapeln, um
die versteckte Mehrdimensionalität aufzudecken. Im Code:

In [4]: titanic.groupby(['sex', 'class'])['survived'].aggregate('mean').unstack()
Out[4]: Klasse Erste Zweite Dritte
Geschlecht
weiblich 0.968085 0.921053 0.500000
männlich 0.368852 0.157407 0.135447

Dies gibt uns eine bessere Vorstellung davon, wie sich sowohl das Geschlecht als auch die Klasse auf das Überleben auswirken, aber der Code ist
ein wenig verworren aus. Während jeder Schritt dieser Pipeline im Lichte der folgenden Punkte Sinn macht

den zuvor besprochenen Tools ist die lange Codekette nicht besonders einfach zu

lesen oder verwenden. Dieses zweidimensionale Groupby ist so verbreitet, dass Pandas

eine Komfortroutine, pivot_table, die diese Art von Mul- ten kurz und bündig behandelt.

tidimensionale Aggregation.

Pivot-Tabellen von Hand | 177
Pivot-Tabellen-Syntax
Hier ist das Äquivalent zur vorangegangenen Operation unter Verwendung der DataFrame.pivot_table
Methode:

In [5]: titanic.pivot_table('survived', index='sex', columns='class', aggfunc='mean')
Out[5]: Klasse Erste Zweite Dritte
Geschlecht
weiblich 0.968085 0.921053 0.500000
männlich 0.368852 0.157407 0.135447

Diese Methode ist wesentlich lesbarer als die manuelle Gruppierung und führt zu

das gleiche Ergebnis. Wie man es von einer Transatlantikfahrt im frühen 20.
Überlebensgradient sowohl die höheren Klassen als auch die als weiblich registrierten Personen in den

Daten. Erstklassige Weibchen überlebten mit an Sicherheit grenzender Wahrscheinlichkeit (Hallo, Rose!), während nur eines von

Etwa acht Männer der dritten Klasse haben überlebt (Entschuldigung, Jack!).

Mehrstufige Pivot-Tabellen
Genau wie bei einem Groupby kann die Gruppierung in Pivot-Tabellen mit mehreren Ebenen
und über eine Reihe von Optionen festgelegt werden. Zum Beispiel könnten wir uns für die Betrachtung des Alters als

eine dritte Dimension. Wir binden das Alter mit der Funktion pd.cut:

In [6]: age = pd.cut(titanic['age'], [0, 18, 80])
titanic.pivot_table('survived', ['sex', age], 'class')
Out[6]: Klasse Erste Zweite Dritte
Geschlecht Alter
weiblich (0, 18] 0.909091 1.000000 0.511628
(18, 80] 0.972973 0.900000 0.423729
männlich (0, 18] 0.800000 0.600000 0.215686
(18, 80] 0.375000 0.071429 0.133663

Wir können die gleiche Strategie auch bei der Arbeit mit den Spalten anwenden; fügen wir also Informationen hinzu

über den gezahlten Fahrpreis, wobei pd.qcut zur automatischen Berechnung der Quantile verwendet wird:

In [7]: tarif = pd.qcut(titanic['tarif'], 2)
titanic.pivot_table('survived', ['sex', age], [fare, 'class'])
Out[7]: Fahrpreis (-0.001, 14.454] (14.454, 512.329]
Klasse Erste Zweite Dritte Erste
Geschlecht Alter
weiblich (0, 18] NaN 1.000000 0.714286 0.909091
(18, 80] NaN 0,880000 0,444444 0,972973
männlich (0, 18] NaN 0,000000 0,260870 0,800000
(18, 80] 0.0 0.098039 0.125000 0.391304

Tarif
Klasse Zweite Dritte
Geschlecht Alter
weiblich (0, 18) 1.000000 0.318182
(18, 80] 0.914286 0.391304

178 | Kapitel 21: Pivot-Tabellen

männlich (0, 18] 0.818182 0.178571
(18, 80] 0.030303 0.192308

Das Ergebnis ist eine vierdimensionale Aggregation mit hierarchischen Indizes (siehe Kap.
ter 17), die in einem Raster dargestellt werden, das die Beziehung zwischen den Werten verdeutlicht.

Zusätzliche Pivot-Tabellen-Optionen
Die vollständige Aufrufsignatur der Methode DataFrame.pivot_table lautet wie folgt:

# Aufrufsignatur ab Pandas 1.3.5
DataFrame.pivot_table(data, values= None , index= None , columns= None ,
aggfunc='mean', fill_value= None , margins= False ,
dropna= True , margins_name='All', observed= False ,
sort= True )
Wir haben bereits Beispiele für die ersten drei Argumente gesehen; hier sehen wir uns einige der folgenden Argumente an

die übrigen Optionen. Zwei der Optionen, fill_value und dropna, haben zu tun mit

fehlende Daten und sind recht einfach; ich werde hier keine Beispiele dafür zeigen.

Das Schlüsselwort aggfunc steuert, welche Art von Aggregation angewendet wird, nämlich ein Mittelwert

standardmäßig. Wie bei groupby kann die Aggregationsangabe eine Zeichenkette sein, die

eine von mehreren gebräuchlichen Auswahlmöglichkeiten ("Summe", "Mittelwert", "Anzahl", "Min", "Max" usw.) oder eine

Funktion, die eine Aggregation implementiert (z.B. np.sum(), min(), sum(), usw.). Addi-

tionell kann sie als Wörterbuch angegeben werden, das eine Spalte auf eine beliebige der gewünschten

Optionen:

In [8]: titanic.pivot_table(index='sex', columns='class',
aggfunc={'survived':sum, 'fare':'mean'})
Out[8]: Fahrpreis überlebt
Klasse Erste Zweite Dritte Erste Zweite Dritte
Geschlecht
weiblich 106.125798 21.970121 16.118810 91 70 72
männlich 67.226127 19.741782 12.661633 45 17 47

Beachten Sie auch hier, dass wir das Schlüsselwort "values" weggelassen haben; bei der Angabe eines Mappings

für aggfunc wird dies automatisch ermittelt.

Manchmal ist es nützlich, Summen für jede Gruppierung zu berechnen. Dies kann über die Funktion

Stichwort Ränder:

In [9]: titanic.pivot_table('survived', index='sex', columns='class', margins= True )
Out[9]: Klasse Erste Zweite Dritte Alle
Geschlecht
weiblich 0.968085 0.921053 0.500000 0.742038
männlich 0,368852 0,157407 0,135447 0,188908
Alle 0,629630 0,472826 0,242363 0,383838

Dies gibt automatisch Aufschluss über die klassenagnostische Überlebensrate durch

Geschlecht, die geschlechtsspezifische Überlebensrate nach Klasse und die Gesamtüberlebensrate von 38 %. Die

margin label kann mit dem Schlüsselwort margins_name angegeben werden; der Standardwert ist "All".

Pivot-Tabelle Syntax | 179
1 Der in diesem Abschnitt verwendete CDC-Datensatz verwendet das bei der Geburt zugewiesene Geschlecht, das als "Gender" bezeichnet wird, und beschränkt die Daten auf
auf männlich und weiblich. Obwohl das Geschlecht ein von der Biologie unabhängiges Spektrum ist, werde ich bei der Erörterung dieses Datensatzes die gleiche Terminologie verwenden
bei der Erörterung dieses Datensatzes aus Gründen der Konsistenz und Klarheit.
Beispiel: Daten zur Geburtenrate
Ein weiteres Beispiel sind die frei verfügbaren Daten über Geburten in den USA,

zur Verfügung gestellt von den Centers for Disease Control (CDC). (Dieser Datensatz wurde von Andrew Gelman
Andrew Gelman und seine Gruppe haben diesen Datensatz ausführlich analysiert; siehe z. B. den Blogbeitrag

zur Signalverarbeitung mit Gaußschen Prozessen):^1

In [10]: _# Shell-Befehl zum Herunterladen der Daten:

!cd data && curl -O \
https://raw.githubusercontent.com/jakevdp/data-CDCbirths/master/births.csv_
In [11]: geburten = pd.read_csv('daten/geburten.csv')

Wenn wir uns die Daten ansehen, sehen wir, dass sie relativ einfach sind - sie enthalten die Anzahl der

Geburten gruppiert nach Datum und Geschlecht:

In [12]: geburten.head()
Out[12]: Jahr Monat Tag Geschlecht Geburten
0 1969 1 1.0 F 4046
1 1969 1 1.0 M 4440
2 1969 1 2.0 F 4454
3 1969 1 2.0 M 4548
4 1969 1 3.0 F 4548

Mit Hilfe einer Pivot-Tabelle können wir diese Daten ein wenig besser verstehen. Fügen wir eine

Dekade und betrachten Sie die Geburten von Männern und Frauen in Abhängigkeit von der Dekade:

In [13]: geburten['jahrzehnt'] = 10 * (geburten['jahr'] // 10)
geburten.pivot_table('geburten', index='jahrzehnt', spalten='geschlecht',
aggfunc='sum')
Out[13]: Geschlecht F M
Jahrzehnt
1960 1753634 1846572
1970 16263075 17121550
1980 18310351 19243452
1990 19479454 20420553
2000 18229309 19106428

Wir sehen, dass in jedem Jahrzehnt mehr Männer als Frauen geboren werden. Um diesen Trend etwas
deutlicher zu sehen, können wir die in Pandas eingebauten Plotting-Tools verwenden, um die Gesamt

Anzahl der Geburten nach Jahr, wie in Abbildung 21-1 dargestellt (siehe Teil IV für eine Diskussion über
Plotten mit Matplotlib):

In [14]: % matplotlib inline
import matplotlib.pyplot as plt
plt.style.use('seaborn-whitegrid')
geburten.pivot_table(

180 | Kapitel 21: Pivot-Tabellen

2 Eine vollfarbige Version dieser Abbildung ist auf GitHub zu finden.
'Geburten', index='Jahr', columns='Geschlecht', aggfunc='Summe').plot()
plt.ylabel('Geburten insgesamt pro Jahr');

Abbildung 21-1. Gesamtzahl der US-Geburten nach Jahr und Geschlecht^2

Mit einer einfachen Pivot-Tabelle und der Plot-Methode können wir sofort den jährlichen
Trend bei den Geburten nach Geschlecht. Auf den ersten Blick sieht es so aus, dass in den letzten 50 Jahren die Geburten der Männer

die Zahl der weiblichen Geburten um etwa 5 % übersteigt.

Obwohl dies nicht notwendigerweise mit der Pivot-Tabelle zusammenhängt, gibt es ein paar weitere interessante...

Merkmale, die wir mit den bisher beschriebenen Pandas-Tools aus diesem Datensatz herausziehen können

Punkt. Zunächst müssen wir die Daten ein wenig bereinigen und Ausreißer entfernen, die durch falsch eingegebene
Daten (z. B. 31. Juni) oder fehlende Werte (z. B. 99. Juni) verursacht wurden. Eine einfache Möglichkeit zum Entfernen

um Ausreißer abzuschneiden; dies geschieht durch eine robuste Sigma-Clipping-Operation:

In [15]: quartiles = np.percentile(geburten['geburten'], [25, 50, 75])
mu = quartiles[1]
sig = 0,74 * (quartiles[2] - quartiles[0])

Diese letzte Linie ist eine robuste Schätzung der Standardabweichung der Stichprobe, bei der die 0,74

stammt aus dem Interquartilsbereich einer Gauß-Verteilung (mehr über
über Sigma-Clipping-Operationen in einem Buch, das ich zusammen mit Željko Ivezić, Andrew J.

Connolly, und Alexander Gray Statistik, Data Mining und maschinelles Lernen in der
Astronomie (Princeton University Press)).

Beispiel: Geburtendaten | 181
Damit können wir die Abfragemethode (die in Kapitel 24 näher erläutert wird) verwenden, um Zeilen mit Geburten außerhalb dieser Werte herauszufiltern
Zeilen mit Geburten außerhalb dieser Werte herausfiltern:

In [16]: births = births.query('(births > @mu - 5 * @sig) &
(Geburten < @mu + 5 * @sig)')

Als Nächstes setzen wir die Tagesspalte auf Ganzzahlen; vorher war es eine Stringspalte

weil einige Spalten des Datensatzes den Wert "null" enthielten:

In [17]: # Spalte 'Tag' auf Integer setzen; ursprünglich war sie wegen der Nullen ein String
births['day'] = births['day'].astype(int)

Schließlich können wir den Tag, den Monat und das Jahr kombinieren, um einen Datumsindex zu erstellen (siehe Kap.

ter 23). Auf diese Weise lässt sich der Wochentag für jede Zeile schnell berechnen:

In [18]: # einen Datumsindex aus Jahr, Monat, Tag erstellen
geburten.index = pd.to_datetime(10000 * geburten.jahr +
100 * geburten.monat +
geburten.tag, format='%Y%m%d')

births['Wochentag'] = births.index.Wochentag

Auf diese Weise können wir die Geburten nach Wochentagen für mehrere Jahrzehnte darstellen (siehe Abbildung 21-2).

In [19]: import matplotlib.pyplot as plt
import matplotlib as mpl

geburten.pivot_table('geburten', index='tagderwoche',
columns='decade', aggfunc='mean').plot()
plt.gca().set(xticks=range(7),
xticklabels=['Mon', 'Tues', 'Wed', 'Thurs',
'Fri', 'Sat', 'Sun'])
plt.ylabel('mittlere Geburten nach Tag');

Offenbar sind Geburten an Wochenenden etwas seltener als an Wochentagen! Beachten Sie, dass

den 1990er und 2000er Jahren fehlen, weil die CDC-Daten ab 1989 nur noch den
den Monat der Geburt enthalten.

Eine weitere interessante Ansicht ist die Darstellung der durchschnittlichen Zahl der Geburten nach dem Tag des Jahres.
Gruppieren wir zunächst die Daten nach Monat und Tag getrennt:

In [20]: geburten_nach_datum = geburten.pivot_table('geburten',
[geburten.index.monat, geburten.index.tag])
geburten_nach_datum.head()
Out[20]: Geburten
1 1 4009.225
2 4247.400
3 4500.900
4 4571.350
5 4603.625

182 | Kapitel 21: Pivot-Tabellen

3 Eine vollfarbige Version dieser Abbildung ist auf GitHub zu finden.
Abbildung 21-2. Durchschnittliche tägliche Geburten nach Wochentag und Jahrzehnt^3

Das Ergebnis ist ein Multi-Index über Monate und Tage. Um dies zu visualisieren, drehen wir

diese Monate und Tage in Daten umwandeln, indem Sie sie mit einer Dummy-Jahresvariable verknüpfen
zugeordnet werden (achten Sie darauf, dass Sie ein Schaltjahr wählen, damit der 29. Februar korrekt behandelt wird!)

In [21]: from datetime import datetime
geburten_nach_datum.index = [datetime(2012, Monat, Tag)
for (Monat, Tag) in geburten_nach_datum.index]
births_by_date.head()
Out[21]: Geburten
2012-01-01 4009.225
2012-01-02 4247.400
2012-01-03 4500.900
2012-01-04 4571.350
2012-01-05 4603.625

Wenn wir uns nur auf den Monat und den Tag konzentrieren, haben wir jetzt eine Zeitreihe, die den durchschnittlichen

Anzahl der Geburten nach Datum des Jahres. Auf dieser Grundlage können wir mit der Plot-Methode Folgendes darstellen

die Daten. Dabei zeigen sich einige interessante Trends, wie Sie in Abbildung 21-3 sehen können.

In [22]: # Plotten Sie die Ergebnisse
fig, ax = plt.subplots(figsize=(12, 4))
geburten_nach_datum.plot(ax=ax);

Beispiel: Daten zur Geburtenrate | 183
4 Eine Version dieser Abbildung in voller Größe ist auf GitHub zu finden.
Abbildung 21-3. Durchschnittliche tägliche Geburten nach Datum^4

Auffallend an dieser Grafik ist insbesondere der Rückgang der Geburtenrate an US-Feiertagen

(z. B. Unabhängigkeitstag, Tag der Arbeit, Thanksgiving, Weihnachten, Neujahr),

obwohl dies wahrscheinlich eher Trends bei geplanten/induzierten Geburten widerspiegelt als eine tiefgreifende
psychosomatische Auswirkungen auf natürliche Geburten. Weitere Informationen über diesen Trend finden Sie in der

Analyse und Links im Blogbeitrag von Andrew Gelman zu diesem Thema. Wir kehren zu dieser
Abbildung in Kapitel 32 zurückkehren, wo wir die Werkzeuge von Matplotlib verwenden werden, um dieses Diagramm mit Anmerkungen zu versehen.

Anhand dieses kurzen Beispiels können Sie sehen, dass viele der Python- und Pandas-Tools

die wir bisher gesehen haben, können kombiniert und genutzt werden, um Erkenntnisse aus einer Vielzahl von
Datensätzen zu gewinnen. Wir werden einige anspruchsvollere Anwendungen dieser Datenmanipulationen sehen.

in zukünftigen Kapiteln!

184 | Kapitel 21: Pivot-Tabellen

KAPITEL 22

Vektorisierte String-Operationen
Eine Stärke von Python ist die relativ einfache Handhabung und Manipulation von String-Daten.
Pandas baut darauf auf und bietet einen umfassenden Satz von vektorisierten String-Operationen

die ein wichtiger Bestandteil der Art des Mungens sind, die bei der Arbeit mit (lies:

Aufräumen) von realen Daten. In diesem Kapitel gehen wir durch einige der Pandas
String-Operationen und werfen dann einen Blick auf deren Verwendung zur teilweisen Bereinigung eines sehr

unübersichtlicher Datensatz mit Rezepten aus dem Internet.

Einführung in die String-Operationen von Pandas
Wir haben in den vorherigen Kapiteln gesehen, wie Werkzeuge wie NumPy und Pandas arithmetische Operationen verallgemeinern
Operationen verallgemeinern, so dass wir einfach und schnell die gleiche Operation auf vielen

Array-Elemente. Zum Beispiel:

In [1]: import numpy as np
x = np.array([2, 3, 5, 7, 11, 13])
x * 2
Out[1]: array([ 4, 6, 10, 14, 22, 26])

Diese Vektorisierung von Operationen vereinfacht die Syntax für die Bearbeitung von Datenfeldern:

müssen wir uns nicht mehr um die Größe oder Form des Arrays kümmern, sondern nur noch darum, was

Operation, die wir durchführen wollen. Für Arrays von Strings bietet NumPy keinen so einfachen
Zugriff, und daher müssen Sie eine ausführlichere Schleifensyntax verwenden:

In [2]: data = ['peter', 'Paul', 'MARY', 'gUIDO']
[s.capitalize() for s in data]
Out[2]: ['Peter', 'Paul', 'Mary', 'Guido']

185
Dies ist vielleicht ausreichend, um mit einigen Daten zu arbeiten, aber es wird nicht funktionieren, wenn es irgendwelche

fehlende Werte, so dass dieser Ansatz zusätzliche Prüfungen erfordert:

In [3]: data = ['peter', 'Paul', None , 'MARY', 'gUIDO']
[s if s is None else s.capitalize() for s in data]
Out[3]: ['Peter', 'Paul', None , 'Mary', 'Guido']

Dieser manuelle Ansatz ist nicht nur langwierig und umständlich, sondern auch fehleranfällig.

Pandas enthält Funktionen, die sowohl diesen Bedarf an vektorisierten String-Operationen als auch

sowie die Notwendigkeit der korrekten Behandlung fehlender Daten über das str-Attribut von Pandas

Series- und Index-Objekte, die Zeichenketten enthalten. Wenn wir also zum Beispiel ein Pandas

Serie mit diesen Daten können wir direkt die Methode str.capitalize aufrufen, die über

Behandlung fehlender Werte eingebaut:

In [4]: import pandas as pd
names = pd.Series(Daten)
names.str.capitalize()
Out[4]: 0 Peter
1 Paul
2 Keine
3 Maria
4 Guido
dTyp: Objekt

Tabellen von Pandas String-Methoden
Wenn Sie ein gutes Verständnis der Stringmanipulation in Python haben, sind die meisten der Pan-
das String-Syntax ist so intuitiv, dass es wahrscheinlich ausreicht, die verfügbaren Methoden aufzulisten.

fähigen Methoden. Damit beginnen wir hier, bevor wir uns mit einigen der

Feinheiten. Die Beispiele in diesem Abschnitt verwenden das folgende Objekt Serie:

In [5]: monte = pd.Series(['Graham Chapman', 'John Cleese', 'Terry Gilliam',
'Eric Idle', 'Terry Jones', 'Michael Palin'])

Methoden ähnlich den Python String-Methoden
Fast alle in Python eingebauten String-Methoden werden von einer vektorisierten Pandas-Methode gespiegelt

string-Methode. Die folgenden Pandas-Str-Methoden spiegeln die Python-String-Methoden wider:

len lower übersetzen islower ljust
upper startswith isupper rjust find
endswith isnumeric center rfind isalnum
isdezimal zfill index isalpha split
streifen rindex istZiffer rsplit rstrip
capitalize isspace partition lstrip swapcase
186 | Kapitel 22: Vektorisierte String-Operationen

Beachten Sie, dass diese verschiedene Rückgabewerte haben. Einige, wie lower, geben eine Reihe von
Zeichenketten zurück:

In [6]: monte.str.lower()
Out[6]: 0 graham chapman
1 john cleese
2 terry gilliam
3 eric idle
4 terry jones
5 michael palin
dTyp: Objekt

Aber einige andere liefern Zahlen:

In [7]: monte.str.len()
Out[7]: 0 14
1 11
2 13
3 9
4 11
5 13
dTyp: int64

Oder boolesche Werte:

In [8]: monte.str.startswith('T')
Out[8]: 0 Falsch
1 Falsch
2 Wahr
3 Falsch
4 Richtig
5 Falsch
dtype: bool

Wieder andere geben Listen oder andere zusammengesetzte Werte für jedes Element zurück:

In [9]: monte.str.split()
Out[9]: 0 [Graham, Chapman]
1 [John, Cleese]
2 [Terry, Gilliam]
3 [Eric, Idle]
4 [Terry, Jones]
5 [Michael, Palin]
dtype: Objekt

Wir werden weitere Manipulationen dieser Art von Listenreihen-Objekten sehen, wenn wir fortfahren

unsere Diskussion.

Methoden mit regulären Ausdrücken
Darüber hinaus gibt es mehrere Methoden, die reguläre Ausdrücke (regexps) akzeptieren, um

den Inhalt jedes Zeichenkettenelements untersuchen und einige der API-Konventionen befolgen

von Pythons eingebautem re-Modul (siehe Tabelle 22-1).

Tabellen zu Pandas String-Methoden | 187
Tabelle 22-1. Zuordnung zwischen Pandas-Methoden und Funktionen in Pythons re-Modul

Methode Beschreibung
match Ruft re.match für jedes Element auf und gibt einen booleschen Wert zurück.
extract Ruft re.match für jedes Element auf und gibt übereinstimmende Gruppen als Strings zurück.
findall Ruft re.findall für jedes Element auf
replace Ersetzt Vorkommen des Musters durch eine andere Zeichenkette
contains Ruft re.search für jedes Element auf und gibt einen booleschen Wert zurück.
count Zählt die Vorkommen des Musters
split Äquivalent zu str.split, akzeptiert aber Regexps
rsplit Äquivalent zu str.rsplit, akzeptiert aber Regexps
Mit diesen können wir eine Vielzahl von Operationen durchführen. Zum Beispiel können wir die

ersten Namen von jedem Element, indem Sie nach einer zusammenhängenden Gruppe von Zeichen an der

Anfang eines jeden Elements:

In [10]: monte.str.extract('([A-Za-z]+)', expand= False )
Out[10]: 0 Graham
1 Johannes
2 Terry
3 Eric
4 Terry
5 Michael
dTyp: Objekt

Oder wir können etwas Komplizierteres tun, z. B. alle Namen finden, die mit

mit einem Konsonanten, unter Verwendung der regulären Zeichenfolgenanfangs- (^) und Zeichenfolgenendzeichen ($)

Ausdruckszeichen:

In [11]: monte.str.findall(r'^[^AEIOU].*[^aeiou]$')
Out[11]: 0 [Graham Chapman]
1 []
2 [Terry Gilliam]
3 []
4 [Terry Jones]
5 [Michael Palin]
dtype: Objekt

Die Fähigkeit, reguläre Ausdrücke auf Serien- oder DataFrame-Einträge anzuwenden

eröffnet viele Möglichkeiten zur Analyse und Bereinigung von Daten.

Verschiedene Methoden
In Tabelle 22-2 sind schließlich verschiedene Methoden aufgeführt, die andere praktische

Operationen.

188 | Kapitel 22: Vektorisierte String-Operationen

Tabelle 22-2. Andere Pandas-String-Methoden

Methode Beschreibung
get Indiziert jedes Element
slice Schneidet jedes Element in Scheiben
slice_replace Ersetzt slice in jedem Element durch den übergebenen Wert
cat Verkettet Zeichenketten
repeat Wiederholt Werte
normalize Gibt die Unicode-Form von Zeichenketten zurück
pad Fügt Leerzeichen auf der linken, rechten oder beiden Seiten von Zeichenketten ein
wrap Teilt lange Zeichenketten in Zeilen mit einer Länge kleiner als eine bestimmte Breite
join Verbindet Zeichenketten in jedem Element der Reihe mit dem übergebenen Trennzeichen
get_dummies Extrahiert Dummy-Variablen als DataFrame
Vektorisierter Elementzugriff und Slicing

Die Get- und Slice-Operationen ermöglichen insbesondere den vektorisierten Elementzugriff von

jedes Array. Zum Beispiel können wir einen Ausschnitt der ersten drei Zeichen jedes Arrays erhalten

mit str.slice(0, 3). Dieses Verhalten ist auch durch Pythons normale

Indizierungssyntax; zum Beispiel ist df.str.slice(0, 3) äquivalent zu df.str[0:3]:

In [12]: monte.str[0:3]
Out[12]: 0 Gra
1 Joh
2 Ter
3 Eri
4 Ter
5 Mic
dTyp: Objekt

Die Indizierung über df.str.get(i) und df.str[i] ist ebenfalls ähnlich.

Mit diesen Indexierungsmethoden können Sie auch auf Elemente von Arrays zugreifen, die durch Split zurückgegeben werden. Für

Beispiel: Um den Nachnamen eines jeden Eintrags zu extrahieren, kombinieren Sie split mit str indexing:

In [13]: monte.str.split().str[-1]
Out[13]: 0 Chapman
1 Cleese
2 Gilliam
3 Idle
4 Jones
5 Palin
dTyp: Objekt

Indikator-Variablen

Eine weitere Methode, die ein wenig mehr Erklärung erfordert, ist die Methode get_dummies.
Sie ist nützlich, wenn Ihre Daten eine Spalte enthalten, die eine Art kodierten Indikator enthält.

Tabellen von Pandas String-Methoden | 189
Wir könnten zum Beispiel einen Datensatz haben, der Informationen in Form von Codes enthält,

wie z. B. A = "in Amerika geboren", B = "im Vereinigten Königreich geboren", C = "mag Käse", D = "mag Spam".
D = "mag Spam":

In [14]: full_monte = pd.DataFrame({'name': monte,
'info': ['B|C|D', 'B|D', 'A|C',
'B|D', 'B|C', 'B|C|D']})
full_monte
Out[14]: Name Info
0 Graham Chapman B|C|D
1 John Cleese B|D
2 Terry Gilliam A|C
3 Eric Idle B|D
4 Terry Jones B|C
5 Michael Palin B|C|D

Mit der Routine get_dummies können wir diese Indikatorvariablen in einen DataFrame aufteilen:

In [15]: full_monte['info'].str.get_dummies('|')
Out[15]: A B C D
0 0 1 1 1
1 0 1 0 1
2 1 0 1 0
3 0 1 0 1
4 0 1 1 0
5 0 1 1 1

Mit diesen Operationen als Bausteine können Sie eine endlose Reihe von Zeichenketten konstruieren

Verarbeitungsverfahren bei der Bereinigung Ihrer Daten.

Wir werden hier nicht weiter auf diese Methoden eingehen, aber ich möchte Sie ermutigen, Folgendes zu lesen

"Arbeiten mit Textdaten" in der Pandas-Online-Dokumentation oder in den
Ressourcen, die unter "Weitere Ressourcen" auf Seite 221 aufgeführt sind.

Beispiel: Rezeptur-Datenbank
Diese vektorisierten String-Operationen sind besonders nützlich bei der Bereinigung von

chaotischen, realen Daten. Hier werde ich ein Beispiel dafür durchgehen, indem ich eine offene

Rezeptdatenbank, die aus verschiedenen Quellen im Internet zusammengestellt wurde. Unser Ziel wird es sein, die
der Rezeptdaten in Zutatenlisten zu analysieren, so dass wir schnell ein Rezept auf der Grundlage einiger Zutaten finden können.

Zutaten, die wir vorrätig haben. Die Skripte, die zum Kompilieren verwendet werden, finden Sie auf
GitHub, und der Link zur neuesten Version der Datenbank ist ebenfalls dort zu finden.

Diese Datenbank ist ca. 30 MB groß und kann mit den folgenden Dateien heruntergeladen und entpackt werden

Befehle:

In [16]: _# repo = "https://raw.githubusercontent.com/jakevdp/open-recipe-data/master"

!cd data && curl -O {repo}/recipeitems.json.gz
!gunzip data/recipeitems.json.gz_
190 | Kapitel 22: Vektorisierte String-Operationen

Die Datenbank liegt im JSON-Format vor, so dass wir sie mit pd.read_json einlesen (lines=True
ist für diesen Datensatz erforderlich, da jede Zeile der Datei ein JSON-Eintrag ist):

In [17]: recipes = pd.read_json('data/recipeitems.json', lines= True )
rezepte.form
Out[17]: (173278, 17)

Wir sehen, dass es fast 175.000 Rezepte und 17 Spalten gibt. Werfen wir einen Blick auf eine Zeile
um zu sehen, was wir haben:

In [18]: rezepte.iloc[0]
Out[18]: _id {'$oid': '5160756b96cc62079cc2db15'}
name Drop Biscuits und Würstchensoße
Zutaten Biscuits\n3 Tassen Allzweckmehl\n2 Esslöffel...
url http://thepioneerwoman.com/cooking/2013/03/dro...
image http://static.thepioneerwoman.com/cooking/file...
ts {'$date': 1365276011104}
cookTime PT30M
Quelle: thepioneerwoman
RezeptAusbeute 12
datePublished 2013-03-11
prepTime PT10M
description Am späten Samstagnachmittag, nachdem Marlboro Man...
totalZeit NaN
Ersteller NaN
RezeptKategorie NaN
DatumÄnderung NaN
recipeInstructions NaN
Name: 0, dtype: Objekt

Es gibt dort viele Informationen, aber viele davon sind in einer sehr unübersichtlichen Form, wie es typisch ist
für Daten aus dem Internet typisch ist. Vor allem die Zutatenliste ist im String-Format;

müssen wir die Informationen, an denen wir interessiert sind, sorgfältig extrahieren. Beginnen wir
indem wir uns die Inhaltsstoffe genauer ansehen:

In [19]: rezepte.zutaten.str.len().beschreiben()
Out[19]: Anzahl 173278.000000
Mittelwert 244.617926
std 146.705285
min 0.000000
25% 147.000000
50% 221.000000
75% 314.000000
max 9067.000000
Name: Zutaten, dtype: float64

Die Zutatenlisten sind durchschnittlich 250 Zeichen lang, mit einem Minimum von 0 und einem Maximum von

von fast 10.000 Zeichen!

Nur aus Neugierde: Welches Rezept hat die längste Zutatenliste?

Beispiel: Rezepturdatenbank | 191
In [20]: recipes.name[np.argmax(recipes.ingredients.str.len())]
Out[20]: 'Karotten-Ananas-Gewürz & Brownie-Schichtkuchen mit Schlagsahne &

Frischkäse-Glasur und Marzipan-Möhren

Wir können weitere Aggregatuntersuchungen durchführen; zum Beispiel können wir sehen, wie viele der
Rezepte für Frühstücksgerichte sind (mit der Syntax des regulären Ausdrucks, um sowohl die niederen als auch die

Groß- und Kleinbuchstaben):

In [21]: rezepte.beschreibung.str.enthält('[Bb]reakfast').sum()
Out [21]: 3524

Oder wie viele der Rezepte Zimt als Zutat enthalten:

In [22]: recipes.ingredients.str.contains('[Cc]innamon').sum()
Out [22]: 10526

Wir könnten sogar nachsehen, ob es Rezepte gibt, in denen die Zutat falsch als "Zimt" geschrieben wird:

In [23]: recipes.ingredients.str.contains('[Cc]inamon').sum()
Out [23]: 11

Dies ist die Art von Datenexploration, die mit Pandas String-Tools möglich ist. Es sind Daten

Diese Art des Mungerns ist die große Stärke von Python.

Ein einfaches Rezept-Empfehlungssystem
Gehen wir noch einen Schritt weiter und beginnen wir mit der Arbeit an einem einfachen System zur Empfehlung von Rezepten:

Bei einer Liste von Zutaten wollen wir alle Rezepte finden, die alle diese Zutaten verwenden.
Obwohl die Aufgabe konzeptionell einfach ist, wird sie durch die Heterogenität der

der Daten: Es ist nicht einfach, aus jeder Zeile eine saubere Liste der Zutaten zu
aus jeder Zeile zu extrahieren. Also werden wir ein wenig schummeln: Wir beginnen mit einer Liste der üblichen Zutaten,

und suchen Sie einfach nach, ob sie in der Zutatenliste des jeweiligen Rezepts enthalten sind. Für einfaches...

Bleiben wir vorerst bei Kräutern und Gewürzen:

In [24]: spice_list = ['salt', 'pepper', 'oregano', 'sage', 'parsley',
Rosmarin', 'Estragon', 'Thymian', 'Paprika', 'Kümmel']

Wir können dann einen booleschen DataFrame erstellen, der aus True- und False-Werten besteht, die auf die

ob die einzelnen Inhaltsstoffe in der Liste aufgeführt sind:

In [25]: import re
spice_df = pd.DataFrame({
spice: recipes.ingredients.str.contains(spice, re.IGNORECASE)
for spice in spice_list})
spice_df.head()
Out[25]: Salz Pfeffer Oregano Salbei Petersilie Rosmarin Estragon Thymian
0 Falsch Falsch Falsch Falsch Wahr Falsch Falsch Falsch Falsch
1 Falsch Falsch Falsch Falsch Falsch Falsch Falsch Falsch Falsch
2 Wahr Wahr Falsch Falsch Falsch Falsch Falsch Falsch Falsch Falsch
3 Falsch Falsch Falsch Falsch Falsch Falsch Falsch Falsch Falsch Falsch
4 Falsch Falsch Falsch Falsch Falsch Falsch Falsch Falsch Falsch Falsch

192 | Kapitel 22: Vektorisierte String-Operationen

Paprika-Kreuzkümmel
0 Falsch Falsch
1 Falsch Falsch
2 Falsch Richtig
3 Falsch Falsch
4 Falsch Falsch

Nehmen wir als Beispiel an, wir möchten ein Rezept finden, das Petersilie, Paprika und

Estragon. Mit der Abfragemethode von DataFrames können wir dies sehr schnell berechnen,
die in Kapitel 24 näher erläutert wird:

In [26]: Auswahl = spice_df.query('Petersilie & Paprika & Estragon')
len(Auswahl)
Out[26]: 10

Wir finden nur 10 Rezepte mit dieser Kombination. Benutzen wir den Index, der von diesem

Auswahl, um die Namen dieser Rezepte zu entdecken:

In [27]: rezepte.name[auswahl.index]
Out[27]: 2069 All cremat mit einem Little Gem, Löwenzahn und Wa...
74964 Hummer mit Thermidor-Butter
93768 Burton's Southern Fried Chicken mit weißer Soße
113926 Mijo's Slow Cooker Geschnetzeltes Rindfleisch
137686 Spargelsuppe mit pochierten Eiern
140530 Gebratene Austern-Po'boys
158475 Lammschenkel-Tagine mit Kräuter-Tabbouleh
158486 Gebratenes Südstaaten-Huhn in Buttermilch
163175 Gebratene Hähnchen-Schieber mit Essiggurken und Salat
165243 Bar Tartine Blumenkohlsalat
Name: name, dtype: objekt

Nun, da wir unsere Rezeptauswahl von 175.000 auf 10 reduziert haben, sind wir in einer

in der Lage, eine fundiertere Entscheidung darüber zu treffen, was wir zum Abendessen kochen möchten.

Weitergehen mit Rezepten
Ich hoffe, dieses Beispiel hat Ihnen einen kleinen Einblick in die Arten von Daten gegeben, die bereinigt werden können.

Operationen, die durch Pandas String-Methoden effizient ermöglicht werden. Aber natürlich
würde der Aufbau eines robusten Rezept-Empfehlungssystems viel mehr Arbeit erfordern!

Das Extrahieren vollständiger Zutatenlisten aus jedem Rezept wäre ein wichtiger Teil der Aufgabe.
Leider macht die große Vielfalt der verwendeten Formate dies zu einer relativ zeitaufwendigen Aufgabe.

verbrauchenden Prozess. Dies verweist auf die Binsenweisheit, dass in der Datenwissenschaft das Bereinigen und

mung von Daten aus der realen Welt macht oft den Großteil der Arbeit aus - und Pandas
bietet die Werkzeuge, die Ihnen dabei helfen können.

Beispiel: Rezepturdatenbank | 193
KAPITEL 23

Arbeiten mit Zeitreihen
Pandas wurde ursprünglich im Zusammenhang mit der Finanzmodellierung entwickelt.
Es enthält daher erwartungsgemäß einen umfangreichen Satz von Werkzeugen für die Arbeit mit Daten, Zeiten und Zeitreihen.

indizierte Daten. Datums- und Zeitdaten gibt es in verschiedenen Varianten, die wir hier erörtern:

Zeitstempel
Bestimmte Zeitpunkte (z. B. der 4. Juli 2021 um 7:00 Uhr).

Zeitintervalle und Zeiträume
Eine Zeitspanne zwischen einem bestimmten Anfangs- und Endpunkt, zum Beispiel der
Monat Juni 2021. Zeiträume beziehen sich in der Regel auf einen Sonderfall von Zeitintervallen, bei dem
in dem jedes Intervall gleich lang ist und sich nicht überschneidet (z. B. 24 Stunden lange
Perioden, die Tage umfassen).

Zeitdeltas oder Dauern
Eine genaue Zeitspanne (z. B. eine Dauer von 22,56 Sekunden).

In diesem Kapitel erfahren Sie, wie Sie mit jedem dieser Datums-/Zeitdatentypen in

Pandas. Dies ist keineswegs ein vollständiger Leitfaden zu den in Python oder Pandas verfügbaren Zeitreihenwerkzeugen.
Python oder Pandas zur Verfügung stehen, sondern soll einen umfassenden Überblick darüber geben, wie Sie als Benutzer

die Arbeit mit Zeitreihen angehen sollten. Wir beginnen mit einer kurzen Diskussion über
Werkzeuge für den Umgang mit Datums- und Zeitangaben in Python, bevor wir uns speziell mit einem

Diskussion der von Pandas bereitgestellten Werkzeuge. Abschließend werden wir einige kurze Prüfungs- und

Beispiele für die Arbeit mit Zeitreihendaten in Pandas.

194
Datums- und Zeitangaben in Python
In der Python-Welt gibt es eine Reihe von Darstellungen von Datumsangaben, Zeiten und Deltas,

und Zeitspannen. Die von Pandas bereitgestellten Zeitreihentools sind zwar die
nützlichsten für datenwissenschaftliche Anwendungen sind, ist es hilfreich, ihre Beziehung zu anderen Tools zu sehen

in Python verwendet.

Native Python-Daten und -Zeiten: datetime und dateutil
Die grundlegenden Objekte von Python für die Arbeit mit Datums- und Zeitangaben befinden sich in der eingebauten

datetime-Modul. Zusammen mit dem Modul dateutil eines Drittanbieters können Sie dieses Modul verwenden, um

können Sie schnell eine Reihe nützlicher Funktionen für Datums- und Zeitangaben ausführen. Zum Beispiel können Sie

kann manuell ein Datum unter Verwendung des Datentyps datetime erstellen:

In [1]: from datetime import datetime
datetime(year=2021, month=7, day=4)
Out[1]: datetime.datetime(2021, 7, 4, 0, 0)

Mit dem Modul dateutil können Sie Datumsangaben aus einer Vielzahl von Zeichenkettenformaten parsen:

In [2]: from dateutil import parser
Datum = parser.parse("4. Juli, 2021")
Datum
Out[2]: datetime.datetime(2021, 7, 4, 0, 0)

Sobald Sie ein datetime-Objekt haben, können Sie z. B. den Wochentag ausdrucken:

In [3]: date.strftime('%A')
Out[3]: 'Sonntag'

Hier haben wir einen der Standard-Stringformatcodes für den Ausdruck von Datumsangaben verwendet ('%A'),

die Sie im strftime-Abschnitt der datetime-Dokumentation von Python nachlesen können.

tion. Die Dokumentation anderer nützlicher Datumsprogramme finden Sie in dateutil's online

Dokumentation. Ein verwandtes Paket ist pytz, das Werkzeuge enthält für

Arbeit mit dem migräneauslösendsten Element von Zeitreihendaten: Zeitzonen.

Die Stärke von datetime und dateutil liegt in ihrer Flexibilität und einfachen Syntax: Sie können
diese Objekte und ihre eingebauten Methoden verwenden, um nahezu jede Operation auszuführen

die für Sie von Interesse sein könnten. Das Problem ist, wenn Sie mit folgenden Personen zusammenarbeiten möchten

große Arrays von Daten und Zeiten: genauso wie Listen von numerischen Python-Variablen subopti-

m Vergleich zu typisierten numerischen Arrays im NumPy-Stil sind Listen von Python datetime
Objekten im Vergleich zu typisierten Arrays mit kodierten Daten suboptimal.

Datums- und Zeitangaben in Python | 195
Typisierte Arrays von Zeiten: NumPy's datetime64
NumPy's datetime64 dtype kodiert Datumsangaben als 64-Bit-Ganzzahlen und ermöglicht so die kompakte Darstellung von Arrays aus
Datumsangaben kompakt dargestellt und auf effiziente Weise verarbeitet werden. Die

datetime64 erfordert ein bestimmtes Eingabeformat:

In [4]: import numpy as np
Datum = np.array('2021-07-04', dtype=np.datetime64)
Datum
Out[4]: array('2021-07-04', dtype='datetime64[D]')

Sobald wir Daten in dieser Form haben, können wir schnell vektorisierte Operationen damit durchführen:

In [5]: Datum + np.arange(12)
Out[5]: array(['2021-07-04', '2021-07-05', '2021-07-06', '2021-07-07',
'2021-07-08', '2021-07-09', '2021-07-10', '2021-07-11',
'2021-07-12', '2021-07-13', '2021-07-14', '2021-07-15'],
dtype='datetime64[D]')

Aufgrund des einheitlichen Typs in NumPy datetime64-Arrays kann diese Art von Operation

viel schneller erreicht werden, als wenn wir direkt mit Python arbeiten würden.

Datetime-Objekte, insbesondere wenn die Arrays groß werden (wir haben diese Art der Vektorisierung

in Kapitel 6).

Ein Detail der datetime64- und verwandten timedelta64-Objekte ist, dass sie auf

eine grundlegende Zeiteinheit. Da das datetime64-Objekt auf eine Genauigkeit von 64 Bit beschränkt ist,

der Bereich der kodierbaren Zeiten beträgt das 264-fache dieser Grundeinheit. Mit anderen Worten,

datetime64 erzwingt einen Kompromiss zwischen Zeitauflösung und maximaler Zeitspanne.

Wenn Sie zum Beispiel eine Zeitauflösung von 1 Nanosekunde wünschen, haben Sie nur genügend

Informationen, um einen Bereich von 264 Nanosekunden oder knapp 600 Jahren zu kodieren. NumPy

leitet aus der Eingabe die gewünschte Einheit ab; hier zum Beispiel eine tagesbasierte Datumszeit:

In [6]: np.datetime64('2021-07-04')
Out[6]: numpy.datetime64('2021-07-04')

Hier ist eine minutenbasierte Datumsangabe:

In [7]: np.datetime64('2021-07-04 12:00')
Out[7]: numpy.datetime64('2021-07-04T12:00')

Sie können jede gewünschte Grundeinheit mit einem der vielen Formatcodes erzwingen; zum Beispiel
In diesem Beispiel wird eine auf Nanosekunden basierende Zeit erzwungen:

In [8]: np.datetime64('2021-07-04 12:59:59.50', 'ns')
Out[8]: numpy.datetime64('2021-07-04T12:59:59.500000000')

Tabelle 23-1, die der NumPy datetime64-Dokumentation entnommen ist, listet die verfügbaren
Formatcodes zusammen mit den relativen und absoluten Zeitspannen, die sie kodieren können.

196 | Kapitel 23: Arbeiten mit Zeitreihen

Tabelle 23-1. Beschreibung der Datums- und Zeitcodes

Code Bedeutung Zeitspanne (relativ) Zeitspanne (absolut)
Y Jahr ± 9,2e18 Jahre [9,2e18 v. Chr., 9,2e18 n. Chr.]
M Monat ± 7,6e17 Jahre [7,6e17 v. Chr., 7,6e17 n. Chr.]
W Woche ± 1,7e17 Jahre [1,7e17 v. Chr., 1,7e17 n. Chr.]
D Tag ± 2,5e16 Jahre [2,5e16 v. Chr., 2,5e16 n. Chr.]
h Stunde ± 1,0e15 Jahre [1,0e15 v. Chr., 1,0e15 n. Chr.]
m Minute ± 1,7e13 Jahre [1,7e13 v. Chr., 1,7e13 n. Chr.]
s Sekunde ± 2,9e12 Jahre [ 2,9e9 v. Chr., 2,9e9 n. Chr.]
ms Millisekunde ± 2,9e9 Jahre [ 2,9e6 v. Chr., 2,9e6 n. Chr.]
us Mikrosekunde ± 2,9e6 Jahre [290301 v. Chr., 294241 n. Chr.]
ns Nanosekunde ± 292 Jahre [ 1678 n. Chr., 2262 n. Chr.]
ps Pikosekunde ± 106 Tage [ 1969 n. Chr., 1970 n. Chr.]
fs Femtosekunde ± 2,6 Stunden [ 1969 n. Chr., 1970 n. Chr.]
as Attosekunde ± 9,2 Sekunden [ 1969 n. Chr., 1970 n. Chr.]
Für die Arten von Daten, die wir in der realen Welt sehen, ist datetime64[ns] ein nützlicher Standard, da er

kann eine nützliche Reihe von modernen Daten mit einer angemessenen Genauigkeit kodieren.

Schließlich ist zu beachten, dass der Datentyp datetime64 zwar einige der Mängel von

dem eingebauten Python-Datetime-Typ, fehlen ihm viele der praktischen Methoden und

Funktionen, die von datetime und insbesondere von dateutil bereitgestellt werden. Weitere Informationen können sein

in der datetime64-Dokumentation von NumPy zu finden.

Daten und Zeiten in Pandas: Das Beste aus beiden Welten
Pandas baut auf den soeben besprochenen Werkzeugen auf und bietet ein Timestamp-Objekt, das

kombiniert die Benutzerfreundlichkeit von datetime und dateutil mit der effizienten Speicherung und

vektorisierte Schnittstelle von numpy.datetime64. Aus einer Gruppe dieser Timestamp-Objekte,

Pandas kann einen DatetimeIndex konstruieren, der verwendet werden kann, um Daten in einer Serie oder

DataFrame.

Wir können zum Beispiel Pandas-Tools verwenden, um die Demonstration von vorhin zu wiederholen. Wir

kann eine flexibel formatierte Datumszeichenfolge analysieren und Formatcodes verwenden, um den Tag der Woche auszugeben
der Woche ausgeben, wie folgt:

In [9]: import pandas as pd
date = pd.to_datetime("4. Juli, 2021")
date
Out[9]: Timestamp('2021-07-04 00:00:00')

In [10]: date.strftime('%A')
Out[10]: 'Sonntag'

Daten und Zeiten in Python | 197
Darüber hinaus können wir vektorisierte Operationen im Stil von NumPy direkt mit demselben

Objekt:

In [11]: Datum + pd.to_timedelta(np.arange(12), 'D')
Out[11]: DatetimeIndex(['2021-07-04', '2021-07-05', '2021-07-06', '2021-07-07',
'2021-07-08', '2021-07-09', '2021-07-10', '2021-07-11',
'2021-07-12', '2021-07-13', '2021-07-14', '2021-07-15'],
dtype='datetime64[ns]', freq= None )

Im nächsten Abschnitt werden wir uns näher mit der Bearbeitung von Zeitreihendaten mit

die von Pandas bereitgestellten Werkzeuge.

Pandas Zeitreihen: Indizierung nach Zeit
Die Pandas-Tools für Zeitreihen werden erst dann wirklich nützlich, wenn Sie beginnen, die Daten nach

Zeitstempel. Wir können zum Beispiel ein Series-Objekt konstruieren, das zeitindizierte

Daten:

In [12]: index = pd.DatetimeIndex(['2020-07-04', '2020-08-04',
'2021-07-04', '2021-08-04'])
Daten = pd.Reihe([0, 1, 2, 3], index=index)
daten
Out[12]: 2020-07-04 0
2020-08-04 1
2021-07-04 2
2021-08-04 3
dTyp: int64

Und jetzt, wo wir diese Daten in einer Serie haben, können wir alle Serien
Indizierungsmuster verwenden, die wir in früheren Kapiteln besprochen haben, indem wir Werte übergeben, die

zu Dates gezwungen werden:

In [13]: data['2020-07-04':'2021-07-04']
Out[13]: 2020-07-04 0
2020-08-04 1
2021-07-04 2
dTyp: int64

Es gibt zusätzliche spezielle Indexierungsoperationen, die nur das Datum betreffen, wie z.B. die Übergabe eines Jahres, um
um einen Ausschnitt aller Daten aus diesem Jahr zu erhalten:

In [14]: data['2021']
Out[14]: 2021-07-04 2
2021-08-04 3
dtTyp: int64

Später werden wir weitere Beispiele für die Bequemlichkeit von Datumsangaben als Indizes sehen. Aber zuerst,
wollen wir uns zunächst die verfügbaren Zeitreihendatenstrukturen genauer ansehen.

198 | Kapitel 23: Arbeiten mit Zeitreihen

Pandas Zeitreihen-Datenstrukturen
In diesem Abschnitt werden die grundlegenden Pandas-Datenstrukturen für die Arbeit mit

Zeitreihendaten:

Für Zeitstempel bietet Pandas den Typ Timestamp. Wie bereits erwähnt, ist dieser
im Wesentlichen ein Ersatz für Pythons natives datetime, aber er basiert auf dem
effizienteren Datentyp numpy.datetime64. Die zugehörige Index-Struktur ist
DatetimeIndex.
Für Zeiträume stellt Pandas den Typ Period zur Verfügung. Dieser kodiert ein festes
Frequenzintervall basierend auf numpy.datetime64. Die zugehörige Index-Struktur ist
PeriodIndex.
Für Zeitdeltas oder Zeitdauern bietet Pandas den Typ Timedelta an. Timedelta ist ein
effizienterer Ersatz für Pythons nativen datetime.timedelta-Typ und basiert
basiert auf numpy.timedelta64. Die zugehörige Indexstruktur ist TimedeltaIndex.
Die grundlegendsten dieser Datums-/Zeitobjekte sind Timestamp und Datetime

Index-Objekte. Obwohl diese Klassenobjekte direkt aufgerufen werden können, ist es üblicher, dass

die Funktion pd.to_datetime verwenden, die eine Vielzahl von Formaten parsen kann. Übergabe von

ein einzelnes Datum an pd.to_datetime ergibt einen Zeitstempel; die Übergabe einer Reihe von Daten durch

ergibt standardmäßig einen DatetimeIndex, wie Sie hier sehen können:

In [15]: dates = pd.to_datetime([datetime(2021, 7, 3), '4. Juli, 2021',
'2021-Jul-6', '07-07-2021', '20210708'])
Daten
Out[15]: DatetimeIndex(['2021-07-03', '2021-07-04', '2021-07-06', '2021-07-07',
'2021-07-08'],
dtype='datetime64[ns]', freq= None )

Jeder DatetimeIndex kann mit der Funktion to_period in einen PeriodIndex umgewandelt werden,

unter Hinzufügung eines Häufigkeitscodes; in diesem Fall wird "D" für die tägliche Häufigkeit verwendet:

In [16]: dates.to_period('D')
Out[16]: PeriodIndex(['2021-07-03', '2021-07-04', '2021-07-06', '2021-07-07',
'2021-07-08'],
dtype='Zeitraum[D]')

Ein TimedeltaIndex wird z. B. erstellt, wenn ein Datum von einem anderen subtrahiert wird:

In [17]: Daten - Daten[0]
Out[17]: TimedeltaIndex(['0 Tage', '1 Tage', '3 Tage', '4 Tage', '5 Tage'],

dtype='timedelta64[ns]', freq= None )

Pandas Zeitreihen-Datenstrukturen | 199
Reguläre Sequenzen: pd.date_range
Um die Erstellung von regulären Datumsfolgen zu erleichtern, bietet Pandas einige

Funktionen für diesen Zweck: pd.date_range für Zeitstempel, pd.period_range für

Zeiträume, und pd.timedelta_range für Zeitdeltas. Wir haben gesehen, dass Pythons range und

NumPy's np.arange nimmt einen Startpunkt, einen Endpunkt und eine optionale Schrittweite und gibt eine

Sequenz. Ähnlich akzeptiert pd.date_range ein Startdatum, ein Enddatum und einen optionalen
Frequenzcode, um eine regelmäßige Folge von Daten zu erstellen:

In [18]: pd.date_range('2015-07-03', '2015-07-10')
Out[18]: DatetimeIndex(['2015-07-03', '2015-07-04', '2015-07-05', '2015-07-06',
'2015-07-07', '2015-07-08', '2015-07-09', '2015-07-10'],
dtype='datetime64[ns]', freq='D')

Alternativ kann der Datumsbereich nicht mit einem Start- und Endpunkt, sondern mit
einem Startpunkt und einer Anzahl von Perioden:

In [19]: pd.date_range('2015-07-03', periods=8)
Out[19]: DatetimeIndex(['2015-07-03', '2015-07-04', '2015-07-05', '2015-07-06',
'2015-07-07', '2015-07-08', '2015-07-09', '2015-07-10'],
dtype='datetime64[ns]', freq='D')

Die Abstände können durch Ändern des Arguments freq geändert werden, das standardmäßig auf D eingestellt ist.

Beispiel konstruieren wir hier eine Reihe von stündlichen Zeitstempeln:

In [20]: pd.date_range('2015-07-03', periods=8, freq='H')
Out[20]: DatetimeIndex(['2015-07-03 00:00:00', '2015-07-03 01:00:00',
'2015-07-03 02:00:00', '2015-07-03 03:00:00',
'2015-07-03 04:00:00', '2015-07-03 05:00:00',
'2015-07-03 06:00:00', '2015-07-03 07:00:00'],
dtype='datetime64[ns]', freq='H')

Um regelmäßige Sequenzen von Perioden- oder Timedelta-Werten zu erstellen, müssen die ähnlichen

Die Funktionen pd.period_range und pd.timedelta_range sind nützlich. Hier sind einige

monatliche Perioden:

In [21]: pd.period_range('2015-07', periods=8, freq='M')
Out[21]: PeriodIndex(['2015-07', '2015-08', '2015-09',
'2015-10', '2015-11', '2015-12',
'2016-01', '2016-02'],
dtype='period[M]')

Und eine Folge von Dauern, die um eine Stunde zunehmen:

In [22]: pd.timedelta_range(0, periods=6, freq='H')
Out[22]: TimedeltaIndex(['0 Tage 00:00:00', '0 Tage 01:00:00', '0 Tage 02:00:00',
'0 Tage 03:00:00', '0 Tage 04:00:00', '0 Tage 05:00:00'],
dtype='timedelta64[ns]', freq='H')

All dies erfordert ein Verständnis der Pandas-Frequenzcodes, die zusammengefasst werden.

im nächsten Abschnitt erläutert.

200 | Kapitel 23: Arbeiten mit Zeitreihen

Frequenzen und Offsets
Grundlegend für diese Pandas-Zeitreihen-Tools ist das Konzept einer Frequenz oder eines Datums

Offset. Die folgende Tabelle gibt einen Überblick über die wichtigsten verfügbaren Codes; wie bei D (Tag)

und H (Stunde)-Codes, die in den vorangegangenen Abschnitten gezeigt wurden, können wir diese verwenden, um Folgendes festzulegen

beliebige Frequenzabstände. Tabelle 23-2 gibt einen Überblick über die wichtigsten verfügbaren Codes.

Tabelle 23-2. Auflistung der Pandas-Frequenzcodes

Code Beschreibung Code Beschreibung
D Kalendertag B Geschäftstag
W Wöchentlich
M Monatsende BM Geschäftsmonatsende
Q Quartalsende BQ Geschäftsquartalsende
A Jahresende BA Ende des Geschäftsjahrs
H Stunden BH Geschäftsstunden
T Minuten
S Sekunden
L Millisekunden
U Mikrosekunden
N Nanosekunden
Die monatlichen, vierteljährlichen und jährlichen Frequenzen sind alle am Ende der Spezifikation angegeben.

fierten Zeitraum. Durch Hinzufügen eines Suffixes werden diese stattdessen am Anfang markiert
am Anfang markiert (siehe Tabelle 23-3).

Tabelle 23-3. Auflistung der startindizierten Frequenzcodes

Code Beschreibung Code Beschreibung
MS Monatsbeginn BMS Geschäftsmonatsbeginn
QS Beginn des Quartals BQS Beginn des Geschäftsquartals
AS Jahresbeginn BAS Beginn des Geschäftsjahres
Außerdem können Sie den Monat ändern, der zur Kennzeichnung eines Quartals- oder Jahrescodes verwendet wird

durch Hinzufügen eines dreibuchstabigen Monatscodes als Suffix:

Q-JAN, BQ-FEB, QS-MAR, BQS-APR, usw.
A-JAN, BA-FEB, AS-MAR, BAS-APR, usw.
In gleicher Weise kann der Splitpunkt des Wochenrhythmus durch Hinzufügen eines

Wochentagscode mit drei Buchstaben: W-SON, W-MON, W-TUE, W-WED, usw.

Frequenzen und Offsets | 201
Darüber hinaus können Codes mit Zahlen kombiniert werden, um andere Frequenzen anzugeben. Für

Bei einer Frequenz von 2 Stunden und 30 Minuten können wir beispielsweise die Stunde (H) und die

Minute (T) Codes wie folgt:

In [23]: pd.timedelta_range(0, periods=6, freq="2H30T")
Out[23]: TimedeltaIndex(['0 Tage 00:00:00', '0 Tage 02:30:00', '0 Tage 05:00:00',
'0 Tage 07:30:00', '0 Tage 10:00:00', '0 Tage 12:30:00'],
dtype='timedelta64[ns]', freq='150T')

Alle diese Kurzcodes beziehen sich auf bestimmte Instanzen von Pandas-Zeitreihen-Offsets, die

können im Modul pd.tseries.offsets gefunden werden. Wir können zum Beispiel einen Busi-

ness-Tag direkt wie folgt verrechnet:

In [24]: from pandas.tseries.offsets import BDay
pd.date_range('2015-07-01', periods=6, freq=BDay())
Out[24]: DatetimeIndex(['2015-07-01', '2015-07-02', '2015-07-03', '2015-07-06',
'2015-07-07', '2015-07-08'],
dtype='datetime64[ns]', freq='B')

Weitere Informationen über die Verwendung von Frequenzen und Offsets finden Sie im Abschnitt DateOffset

der Pandas-Dokumentation.

Neuabtastung, Verschiebung und Fensterung
Die Möglichkeit, Datums- und Zeitangaben als Indizes zu verwenden, um Daten intuitiv zu organisieren und auf sie zuzugreifen, ist
ist ein wichtiger Aspekt der Pandas-Zeitreihenwerkzeuge. Die Vorteile von indizierten Daten in

(automatischer Abgleich bei Operationen, intuitives Daten-Slicing und Zugriff,
usw.) gelten nach wie vor, und Pandas bietet mehrere zusätzliche zeitreihenspezifische

Operationen.

Wir werden uns hier einige davon am Beispiel von Aktienkursdaten ansehen.
Da Pandas größtenteils in einem Finanzkontext entwickelt wurde, enthält es einige sehr spezielle...

spezifische Werkzeuge für Finanzdaten. Zum Beispiel kann der zugehörige pandas-datareader

Paket (installierbar über pip install pandas-datareader) weiß, wie man
Daten aus verschiedenen Online-Quellen zu importieren. Hier werden wir einen Teil des S&P 500-Kursverlaufs laden:

In [25]: from pandas_datareader import data

sp500 = data.DataReader('^GSPC', start='2018', end='2022',
data_source='yahoo')
sp500.head()
Out[25]: High Low Open Close Volume
Datum
2018-01-02 2695.889893 2682.360107 2683.729980 2695.810059 3367250000
2018-01-03 2714.370117 2697.770020 2697.850098 2713.060059 3538660000
2018-01-04 2729.290039 2719.070068 2719.310059 2723.989990 3695260000
2018-01-05 2743.449951 2727.919922 2731.330078 2743.149902 3236620000
2018-01-08 2748.510010 2737.600098 2742.669922 2747.709961 3242650000

202 | Kapitel 23: Arbeiten mit Zeitreihen

Adj Schließen
Datum
2018-01-02 2695.810059
2018-01-03 2713.060059
2018-01-04 2723.989990
2018-01-05 2743.149902
2018-01-08 2747.709961

Der Einfachheit halber werden wir nur den Schlusskurs verwenden:

In [26]: sp500 = sp500['Close']

Wir können dies mit der Plot-Methode visualisieren, nach der normalen Matplotlib-Einrichtung

Boilerplate (siehe Teil IV); das Ergebnis ist in Abbildung 23-1 dargestellt.

In [27]: % matplotlib inline
import matplotlib.pyplot as plt
plt.style.use('seaborn-whitegrid')
sp500.plot();

Abbildung 23-1. S&P500-Schlusskurs im Zeitverlauf

Neuabtastung und Konvertierung von Frequenzen
Eine häufige Notwendigkeit beim Umgang mit Zeitreihendaten ist das Resampling mit einer höheren oder

niedrigere Frequenz. Dies kann mit der Resample-Methode oder der viel einfacheren

alsfreq -Methode. Der Hauptunterschied zwischen den beiden Verfahren besteht darin, dass die Resample-Methode funda-

im Grunde eine Datenaggregation, während asfreq im Grunde eine Datenselektion ist.

Vergleichen wir, was die beiden Ergebnisse bringen, wenn wir die Schlusskurse des S&P 500 herunterrechnen
Daten. Hier werden wir die Daten am Ende des Geschäftsjahres neu abfragen; Abbildung 23-2 zeigt

das Ergebnis.

Neuabtastung, Verschiebung und Fensterung | 203
In [28]: sp500.plot(alpha=0.5, style='-')
sp500.resample('BA').mean().plot(style=':')
sp500.asfreq('BA').plot(style='--');
plt.legend(['input', 'resample', 'asfreq'],
loc='upper left');

Abbildung 23-2. Neuabtastung des S&P500-Schlusskurses

Beachten Sie den Unterschied: Resample meldet zu jedem Zeitpunkt den Durchschnitt des vorangegangenen Jahres,

während asfreq den Wert am Ende des Jahres angibt.

Beim Upsampling sind resample und asfreq weitgehend gleichwertig, obwohl resample
viel mehr Optionen zur Verfügung. In diesem Fall ist die Vorgabe für beide Methoden, die

upsampled points leer, d. h. mit NA-Werten gefüllt. Wie die Funktion pd.fillna

wie in Kapitel 16 beschrieben, akzeptiert asfreq ein Methodenargument, um anzugeben, wie die Werte
unterstellt werden. In diesem Fall werden die Daten des Geschäftstages mit einer täglichen Frequenz neu abgetastet (d. h.,

einschließlich Wochenenden); Abbildung 23-3 zeigt das Ergebnis.

In [29]: fig, ax = plt.subplots(2, sharex= True )
Daten = sp500.iloc[:20]

data.asfreq('D').plot(ax=ax[0], marker='o')

data.asfreq('D', method='bfill').plot(ax=ax[1], style='-o')
data.asfreq('D', method='ffill').plot(ax=ax[1], style='--o')
ax[1].legend(["back-fill", "forward-fill"]);

204 | Kapitel 23: Arbeiten mit Zeitreihen

Abbildung 23-3. Vergleich zwischen Forward-Fill- und Back-Fill-Interpolation

Da die Daten des S&P 500 nur für Geschäftstage vorliegen, weist das obere Feld Lücken auf.

der NA-Werte. Das untere Feld zeigt die Unterschiede zwischen zwei Strategien für

Auffüllen der Lücken: Vorwärtsfüllung und Rückwärtsfüllung.

Zeitverschiebungen
Eine weitere häufige zeitreihenspezifische Operation ist das Verschieben von Daten in der Zeit. Hierfür,

Pandas stellt die Shift-Methode zur Verfügung, mit der man Daten um eine bestimmte Anzahl verschieben kann.

Anzahl von Einträgen. Bei Zeitreihendaten, die in regelmäßigen Abständen abgerufen werden, kann dies eine
Möglichkeit, Trends im Laufe der Zeit zu untersuchen.

In diesem Beispiel werden die Daten auf Tageswerte umgerechnet und um 364 verschoben, um Folgendes zu berechnen

die 1-Jahres-Investitionsrendite des S&P 500 im Zeitverlauf (siehe Abbildung 23-4).

In [30]: sp500 = sp500.asfreq('D', method='pad')

ROI = 100 * (sp500.shift(-365) - sp500) / sp500
ROI.plot()
plt.ylabel('% Kapitalrendite nach 1 Jahr');

Neuabtastung, Verschiebung und Fensterung | 205
Abbildung 23-4. Rentabilität der Investition nach einem Jahr

Die schlechteste Einjahresrendite wurde um den März 2019 herum erreicht, als die durch das Coronavirus

Marktcrash genau ein Jahr später. Wie zu erwarten, war die beste Einjahresrendite im März
im März 2020 zu finden, und zwar für diejenigen, die genug Weitsicht oder Glück hatten, niedrig zu kaufen.

Rollierende Fenster
Die Berechnung der rollenden Statistik ist eine dritte Art von zeitreihenspezifischer Operation, die implementiert wird.

die von Pandas verwaltet werden. Dies kann über das Rolling-Attribut von Series und

DataFrame-Objekte, die eine ähnliche Ansicht wie bei der Groupby-Ansicht liefern

Operation (siehe Kapitel 20). Diese rollierende Ansicht stellt standardmäßig eine Reihe von Aggrega-
standardmäßig eine Reihe von Aggregationsoperationen zur Verfügung.

Zum Beispiel können wir den einjährigen zentrierten gleitenden Mittelwert und die Standardabweichung betrachten.

tion der Aktienkurse (siehe Abbildung 23-5).

In [31]: rolling = sp500.rolling(365, center= True )

data = pd.DataFrame({'input': sp500,
einjähriger rollierender_Mittelwert': rollierender.Mittelwert(),
einjähriger rollierender_Median': rollierender.Median()})
ax = data.plot(style=['-', '--', ':'])
ax.lines[0].set_alpha(0.3)

206 | Kapitel 23: Arbeiten mit Zeitreihen

Abbildung 23-5. Rollierende Statistiken zum S&P500-Index

Wie bei den Groupby-Operationen können die Methoden Aggregate und Apply für Cus-

Tom-Rolling-Berechnungen.

Wo Sie mehr erfahren können

Dieses Kapitel enthält nur eine kurze Zusammenfassung einiger der wichtigsten Funktionen
der von Pandas zur Verfügung gestellten Zeitreihentools gegeben; eine ausführlichere Diskussion finden Sie im
finden Sie im Abschnitt "Time Series/Date Functionality" in der Online-Dokumentation von Pandas.
Eine weitere ausgezeichnete Quelle ist das Buch Python for Data Analysis von Wes McKinney
(O'Reilly). Es ist eine unschätzbare Ressource für die Verwendung von Pandas. Insbesondere dieses Buch
Dieses Buch legt den Schwerpunkt auf Zeitreihentools im Kontext von Wirtschaft und Finanzen und konzentriert sich viel
und konzentriert sich viel mehr auf besondere Details von Geschäftskalendern, Zeitzonen und ähnlichen Themen.
Wie immer können Sie auch die IPython-Hilfefunktionalität nutzen, um weitere Optionen der Funktionen und Methoden zu erforschen und auszuprobieren.
weitere Optionen für die hier besprochenen Funktionen und Methoden auszuprobieren. Ich finde, dies ist oft
der beste Weg, ein neues Python-Werkzeug zu erlernen.
Neuabtastung, Verschiebung und Fensterung | 207
Beispiel: Visualisierung der Fahrradzählungen in Seattle
Als komplexeres Beispiel für die Arbeit mit Zeitreihendaten sehen wir uns die Fahrradzählung an.

Zählungen auf der Fremont Bridge in Seattle. Diese Daten stammen von einem automatischen Fahrrad
Fahrradzähler, der Ende 2012 installiert wurde und über induktive Sensoren auf der Ost- und Westseite verfügt.

Gehwege der Brücke. Die stündlichen Fahrradzählungen können von http:// heruntergeladen werden.
data.seattle.gov heruntergeladen werden; der Fremont Bridge Bicycle Counter-Datensatz ist verfügbar unter dem

Kategorie Transport.

Die für dieses Buch verwendete CSV-Datei kann wie folgt heruntergeladen werden:

In [32]: _# url = ('https://raw.githubusercontent.com/jakevdp/'

'fahrrad-daten/haupt/FremontBridge.csv')
!curl -O {url}_
Sobald dieser Datensatz heruntergeladen ist, können wir Pandas verwenden, um die CSV-Ausgabe in eine

DataFrame. Wir geben an, dass wir die Spalte "Datum" als Index verwenden wollen, und wir wollen

diese Daten automatisch geparst werden:

In [33]: data = pd.read_csv('FremontBridge.csv', index_col='Datum', parse_dates= True )
data.head()
Out[33]: Fremont Bridge Total Fremont Bridge East Sidewalk
Datum
2019-11-01 00:00:00 12.0 7.0
2019-11-01 01:00:00 7.0 0.0
2019-11-01 02:00:00 1.0 0.0
2019-11-01 03:00:00 6.0 6.0
2019-11-01 04:00:00 6.0 5.0

Fremont Bridge West Bürgersteig
Datum
2019-11-01 00:00:00 5.0
2019-11-01 01:00:00 7.0
2019-11-01 02:00:00 1.0
2019-11-01 03:00:00 0.0
2019-11-01 04:00:00 1.0

Der Einfachheit halber werden wir die Spaltennamen abkürzen:

In [34]: data.columns = ['Total', 'East', 'West']

Werfen wir nun einen Blick auf die zusammenfassenden Statistiken für diese Daten:

In [35]: data.dropna().describe()
Out[35]: Gesamt Ost West
Anzahl 147255.000000 147255.000000 147255.000000
Mittelwert 110.341462 50.077763 60.263699
std 140,422051 64,634038 87,252147
min 0,000000 0,000000 0,000000
25% 14.000000 6.000000 7.000000
50% 60.000000 28.000000 30.000000

208 | Kapitel 23: Arbeiten mit Zeitreihen

75% 145.000000 68.000000 74.000000
max 1097,000000 698,000000 850,000000

Visualisierung der Daten
Wir können einen Einblick in den Datensatz gewinnen, indem wir ihn visualisieren. Beginnen wir mit dem Plotten der

Rohdaten (siehe Abbildung 23-6).

In [36]: data.plot()
plt.ylabel('Hourly Bicycle Count');

Abbildung 23-6. Stündliche Fahrradzählungen auf der Fremont Bridge in Seattle

Die ~150.000 stündlichen Stichproben sind viel zu dicht, als dass wir damit etwas anfangen könnten. Wir können

einen besseren Einblick erhalten, indem Sie die Daten auf einem gröberen Raster neu abtasten. Nehmen wir eine neue Stichprobe pro Woche
(siehe Abbildung 23-7).

In [37]: weekly = data.resample('W').sum()
weekly.plot(style=['-', ':', '--'])
plt.ylabel('Wöchentliche Fahrradanzahl');

Dabei lassen sich einige Trends erkennen: Wie zu erwarten, fahren die Menschen im Sommer mehr Fahrrad als im Winter.
als im Winter, und selbst innerhalb einer bestimmten Jahreszeit variiert die Fahrradnutzung von

von Woche zu Woche (wahrscheinlich abhängig vom Wetter; siehe Kapitel 42, wo wir dies
weiter untersuchen). Außerdem sind die Auswirkungen der COVID-19-Pandemie auf die Pendlerströme

ganz klar, ab Anfang 2020.

Beispiel: Visualisierung der Fahrradzählungen in Seattle | 209
Abbildung 23-7. Wöchentliche Fahrradüberquerungen der Fremont Bridge in Seattle

Eine weitere Option, die sich für die Aggregation der Daten anbietet, ist die Verwendung eines gleitenden Mittelwerts,

unter Verwendung der Funktion pd.rolling_mean. Hier untersuchen wir den gleitenden 30-Tage-Mittelwert

unserer Daten und achten Sie darauf, das Fenster zu zentrieren (siehe Abbildung 23-8).

In [38]: daily = data.resample('D').sum()
daily.rolling(30, center= True ).sum().plot(style=['-', ':', '--'])
plt.ylabel('mittlere stündliche Anzahl');

Abbildung 23-8. Rollierender Mittelwert der wöchentlichen Fahrradzählungen

210 | Kapitel 23: Arbeiten mit Zeitreihen

Die Zerklüftung des Ergebnisses ist auf die harte Abgrenzung des Fensters zurückzuführen. Wir können eine

eine glattere Version eines gleitenden Mittelwerts unter Verwendung einer Fensterfunktion, z. B. eines Gaus-
sianisches Fenster, wie in Abbildung 23-9 gezeigt. Der folgende Code spezifiziert sowohl die Breite des

das Fenster (hier 50 Tage) und die Breite des Gauß-Fensters (hier 10 Tage):

In [39]: daily.rolling(50, center= True ,
win_type='gaussian').sum(std=10).plot(style=['-', ':', '--']);

Abbildung 23-9. Gauß-geglättete wöchentliche Fahrradzählungen

Detailliertere Betrachtung der Daten
Diese geglätteten Datenansichten sind zwar nützlich, um sich ein Bild vom allgemeinen Trend in den Daten zu machen
Daten zu bekommen, verbergen sie einen Großteil der Struktur. Zum Beispiel könnten wir uns die Durchschnitts- und

Alter des Datenverkehrs in Abhängigkeit von der Tageszeit. Wir können dies mit der Funktion groupby tun.

tionalität, die in Kapitel 20 erörtert wurde (siehe Abbildung 23-10).

In [40]: by_time = data.groupby(data.index.time).mean()
stündliche_ticks = 4 * 60 * 60 * np.arange(6)
by_time.plot(xticks=hourly_ticks, style=['-', ':', '--']);

Beispiel: Visualisierung der Fahrradzählungen in Seattle | 211
Abbildung 23-10. Durchschnittliche stündliche Fahrradzählungen

Der stündliche Verkehr ist eine stark bimodale Abfolge, mit Spitzenwerten um 8:00 Uhr morgens und
17:00 Uhr. Dies ist wahrscheinlich ein Hinweis auf eine starke Komponente des Pendlerverkehrs, der die

die Brücke. Es gibt auch eine richtungsabhängige Komponente: Den Daten zufolge wird der östliche
Daten zufolge wird der östliche Gehweg während des morgendlichen Pendelverkehrs stärker genutzt, der westliche Gehweg mehr

während des Pendelverkehrs am Nachmittag.

Vielleicht sind wir auch neugierig darauf, wie sich die Dinge je nach Wochentag verändern.

Auch hier können wir dies mit einem einfachen Groupby tun (siehe Abbildung 23-11).

In [41]: by_weekday = data.groupby(data.index.dayofweek).mean()
by_weekday.index = ['Mon', 'Tues', 'Wed', 'Thurs', 'Fri', 'Sat', 'Sun']
by_weekday.plot(style=['-', ':', '--']);

212 | Kapitel 23: Arbeiten mit Zeitreihen

Abbildung 23-11. Durchschnittliche tägliche Fahrradzählungen

Hier zeigt sich ein deutlicher Unterschied zwischen den Gesamtzahlen an Wochentagen und am Wochenende, wobei etwa
doppelt so viele durchschnittliche Fahrer die Brücke von Montag bis Freitag überqueren als an

Samstag und Sonntag.

Lassen Sie uns daher eine zusammengesetzte Gruppierung vornehmen und die stündlichen Trends für

Wochentage und Wochenenden. Wir beginnen mit der Gruppierung nach Fahnen, die das Wochenende und
der Tageszeit:

In [42]: weekend = np.where(data.index.weekday < 5, 'Weekday', 'Weekend')
by_time = data.groupby([Wochenende, data.index.time]).mean()

Nun werden wir einige der Matplotlib-Werkzeuge, die in Kapitel 31 beschrieben werden, zum Plotten verwenden

zwei Platten nebeneinander, wie in Abbildung 23-12 dargestellt.

In [43]: import matplotlib.pyplot as plt
fig, ax = plt.subplots(1, 2, figsize=(14, 5))
by_time.loc['Wochentag'].plot(ax=ax[0], title='Wochentage',
xticks=hourly_ticks, style=['-', ':', '--'])
by_time.loc['Weekend'].plot(ax=ax[1], title='Weekends',
xticks=hourly_ticks, style=['-', ':', '--']);

Beispiel: Visualisierung der Fahrradzählungen in Seattle | 213
Abbildung 23-12. Durchschnittliche stündliche Fahrradzählungen an Wochentagen und Wochenenden

Das Ergebnis zeigt ein bimodales Pendelverhalten während der Arbeitswoche und ein unimo-

dal-Freizeitverhalten an den Wochenenden. Es könnte interessant sein, diese Daten
diese Daten genauer zu untersuchen und die Auswirkungen von Wetter, Temperatur und Jahreszeit zu ermitteln,

und andere Faktoren auf das Pendelverhalten der Menschen; weitere Informationen finden Sie in meinem Blog

Beitrag "Is Seattle Really Seeing an Uptick in Cycling?", der eine Teilmenge dieser Daten verwendet.
Wir werden diesen Datensatz auch im Zusammenhang mit der Modellierung in Kapitel 42 wieder aufgreifen.

214 | Kapitel 23: Arbeiten mit Zeitreihen

KAPITEL 24

Leistungsstarke Pandas: eval und Query
Wie wir bereits in früheren Kapiteln gesehen haben, beruht die Leistungsfähigkeit des PyData-Stacks
auf der Fähigkeit von NumPy und Pandas auf, grundlegende Operationen in untergeordnete Kom-

gepackten Code über eine intuitive Syntax auf höherer Ebene: Beispiele werden vektorisiert/broadcasted

Operationen in NumPy und gruppierungsartige Operationen in Pandas. Während diese Abstraktionen
für viele gängige Anwendungsfälle effizient und effektiv sind, beruhen sie oft auf dem

Erstellung von temporären Zwischenobjekten, die einen unangemessenen Overhead an
Rechenzeit und Speicherverbrauch verursachen kann.

Um dieses Problem zu lösen, enthält Pandas einige Methoden, mit denen Sie direkt auf C-

Geschwindigkeitsoperationen ohne kostspielige Zuweisung von Zwischenfeldern: eval und query,

die sich auf das NumExpr-Paket stützen. In diesem Kapitel werde ich Sie durch ihre Verwendung führen
und gebe Ihnen einige Faustregeln, wann Sie sie verwenden sollten.

Motivierende Abfragen und Auswertungen: Zusammengesetzte Ausdrücke
Wir haben bereits gesehen, dass NumPy und Pandas schnelle vektorisierte Operationen unterstützen; für

zum Beispiel beim Addieren der Elemente von zwei Arrays:

In [1]: import numpy as np
rng = np.random.default_rng(42)
x = rng.random(1000000)
y = rng.random(1000000)
% timeit x + y
Out[1]: 2.21 ms ± 142 μs pro Schleife (Mittelwert ± std. dev. von 7 Durchläufen, je 100 Schleifen)

215
Wie in Kapitel 6 besprochen, ist dies viel schneller als die Addition über eine Python

Schleife oder Verstehen:

In [2]: % timeit np.fromiter((xi + yi for xi, yi in zip(x, y)),
dtype=x.dtype, count=len(x))
Out[2]: 263 ms ± 43,4 ms pro Schleife (Mittelwert ± std. Abweichung von 7 Läufen, je 1 Schleife)

Diese Abstraktion kann jedoch bei der Berechnung von zusammengesetzten Ausdrücken weniger effizient sein.

sionen. Betrachten Sie zum Beispiel den folgenden Ausdruck:

In [3]: Maske = (x > 0,5) & (y < 0,5)

Da NumPy jeden Unterausdruck auswertet, ist dies in etwa äquivalent zu folgendem

sen:

In [4]: tmp1 = (x > 0,5)
tmp2 = (y < 0,5)
Maske = tmp1 & tmp2

Mit anderen Worten: Jeder Zwischenschritt wird explizit im Speicher abgelegt. Wenn die x- und y
Arrays sehr groß sind, kann dies zu erheblichem Speicher- und Berechnungsaufwand führen.

Kopf. Die NumExpr-Bibliothek bietet die Möglichkeit, diese Art von zusammengesetzten Werten zu berechnen

Ausdruck Element für Element, ohne die Notwendigkeit, vollständige Zwischenfelder zuzuweisen.
Die NumExpr-Dokumentation enthält mehr Details, aber für den Moment ist es ausreichend

zu sagen, dass die Bibliothek eine Zeichenkette akzeptiert, die den NumPy-Ausdruck angibt, den Sie berechnen möchten
berechnen möchten:

In [5]: import numexpr
mask_numexpr = numexpr.evaluate('(x > 0.5) & (y < 0.5)')
np.all(mask == mask_numexpr)
Out[5]: True

Der Vorteil dabei ist, dass NumExpr den Ausdruck so auswertet, dass temporäre Arrays möglichst vermieden werden.
Arrays vermeidet und somit viel effizienter sein kann als NumPy,

insbesondere bei langen Berechnungssequenzen auf großen Arrays. Die Pandas eval und

Abfragetools, die wir hier besprechen werden, sind konzeptionell ähnlich und sind im Wesentlichen
Pandas-spezifische Umhüllungen der NumExpr-Funktionalität.

pandas.eval für effiziente Operationen
Die eval-Funktion in Pandas verwendet String-Ausdrücke, um Operationen effizient zu berechnen

auf DataFrame-Objekte. Betrachten Sie zum Beispiel die folgenden Daten:

In [6]: import pandas as pd
nrows, ncols = 100000, 100
df1, df2, df3, df4 = (pd.DataFrame(rng.random((nrows, ncols)))
for i in range(4))

Um die Summe aller vier DataFrames mit dem typischen Pandas-Ansatz zu berechnen, können wir

schreiben Sie einfach die Summe:

216 | Kapitel 24: Leistungsstarke Pandas: eval und Query

In [7]: % timeit df1 + df2 + df3 + df4
Out[7]: 73,2 ms ± 6,72 ms pro Schleife (Mittelwert ± std. Abweichung von 7 Durchläufen, jeweils 10 Schleifen)

Das gleiche Ergebnis kann mit pd.eval berechnet werden, indem der Ausdruck als

String:

In [8]: % timeit pd.eval('df1 + df2 + df3 + df4')
Out[8]: 34 ms ± 4,2 ms pro Schleife (Mittelwert ± std. Abweichung von 7 Durchläufen, je 10 Schleifen)

Die eval-Version dieses Ausdrucks ist etwa 50 % schneller (und verbraucht viel weniger Speicher),

bei gleichem Ergebnis:

In [9]: np.allclose(df1 + df2 + df3 + df4,
pd.eval('df1 + df2 + df3 + df4'))
Out[9]: True

pd.eval unterstützt eine breite Palette von Operationen. Um diese zu demonstrieren, werden wir die folgenden
folgenden Integer-Daten:

In [10]: df1, df2, df3, df4, df5 = (pd.DataFrame(rng.integers(0, 1000, (100, 3)))
for i in range(5))

Hier ist eine Zusammenfassung der Operationen, die pd.eval unterstützt:

Arithmetische Operatoren

pd.eval unterstützt alle arithmetischen Operatoren. Zum Beispiel:

In [11]: result1 = -df1 * df2 / (df3 + df4) - df5
ergebnis2 = pd.eval('-df1 * df2 / (df3 + df4) - df5')
np.allclose(result1, result2)
Out[11]: True
Vergleichsoperatoren

pd.eval unterstützt alle Vergleichsoperatoren, einschließlich verketteter Ausdrücke:

In [12]: result1 = (df1 < df2) & (df2 <= df3) & (df3 != df4)
Ergebnis2 = pd.eval('df1 < df2 <= df3 != df4')
np.allclose(result1, result2)
Out[12]: True
Bitweise Operatoren

pd.eval unterstützt die bitweisen Operatoren & und |:

In [13]: result1 = (df1 < 0.5) & (df2 < 0.5) | (df3 < df4)
ergebnis2 = pd.eval('(df1 < 0.5) & (df2 < 0.5) | (df3 < df4)')
np.allclose(result1, result2)
Out[13]: True
Darüber hinaus unterstützt es die Verwendung des Literales und und oder in booleschen Ausdrücken:

In [14]: result3 = pd.eval('(df1 < 0.5) und (df2 < 0.5) oder (df3 < df4)')
np.allclose(result1, result3)
Out[14]: True
pandas.eval für effiziente Operationen | 217
Objektattribute und Indizes

pd.eval unterstützt den Zugriff auf Objektattribute über die Syntax obj.attr und Indizes
über die obj[index]-Syntax:
In [15]: result1 = df2.T[0] + df3.iloc[1]
ergebnis2 = pd.eval('df2.T[0] + df3.iloc[1]')
np.allclose(result1, result2)
Out[15]: True
Andere Operationen

Andere Operationen, wie Funktionsaufrufe, bedingte Anweisungen, Schleifen und andere
und andere kompliziertere Konstrukte sind derzeit nicht in pd.eval implementiert. Wenn Sie
diese komplizierteren Arten von Ausdrücken ausführen möchten, können Sie die
NumExpr-Bibliothek selbst verwenden.
DataFrame.eval für spaltenweise Operationen
So wie Pandas eine pd.eval-Funktion auf oberster Ebene hat, haben DataFrame-Objekte eine eval

Methode, die auf ähnliche Weise funktioniert. Der Vorteil der eval-Methode ist, dass Spalten
mit Namen referenziert werden können. Wir werden dieses beschriftete Array als Beispiel verwenden:

In [16]: df = pd.DataFrame(rng.random((1000, 3)), columns=['A', 'B', 'C'])
df.head()
Out[16]: A B C
0 0.850888 0.966709 0.958690
1 0.820126 0.385686 0.061402
2 0.059729 0.831768 0.652259
3 0.244774 0.140322 0.041711
4 0.818205 0.753384 0.578851

Mit pd.eval können wir, wie im vorigen Abschnitt, Ausdrücke mit den drei

Spalten wie diese:

In [17]: result1 = (df['A'] + df['B']) / (df['C'] - 1)
ergebnis2 = pd.eval("(df.A + df.B) / (df.C - 1)")
np.allclose(result1, result2)
Out[17]: True

Die DataFrame.eval-Methode ermöglicht eine viel prägnantere Auswertung von Ausdrücken
mit den Spalten:

In [18]: result3 = df.eval('(A + B) / (C - 1)')
np.allclose(result1, result3)
Out[18]: True

Beachten Sie hier, dass wir Spaltennamen als Variablen innerhalb des ausgewerteten Ausdrucks behandeln,
und das Ergebnis ist so, wie wir es uns wünschen würden.

218 | Kapitel 24: Leistungsstarke Pandas: eval und Query

Zuweisung in DataFrame.eval
Zusätzlich zu den gerade besprochenen Optionen erlaubt DataFrame.eval auch die Zuweisung an

eine beliebige Spalte. Verwenden wir den DataFrame von vorhin, der die Spalten 'A', 'B', und

'C':

In [19]: df.head()
Out[19]: A B C
0 0.850888 0.966709 0.958690
1 0.820126 0.385686 0.061402
2 0.059729 0.831768 0.652259
3 0.244774 0.140322 0.041711
4 0.818205 0.753384 0.578851

Wir können df.eval verwenden, um eine neue Spalte 'D' zu erstellen und ihr einen Wert zuzuweisen, der
aus den anderen Spalten berechnet wird:

In [20]: df.eval('D = (A + B) / C', inplace= True )
df.head()
Out[20]: A B C D
0 0.850888 0.966709 0.958690 1.895916
1 0.820126 0.385686 0.061402 19.638139
2 0.059729 0.831768 0.652259 1.366782
3 0.244774 0.140322 0.041711 9.232370
4 0.818205 0.753384 0.578851 2.715013

Auf die gleiche Weise kann jede bestehende Spalte geändert werden:

In [21]: df.eval('D = (A - B) / C', inplace= True )
df.head()
Out[21]: A B C D
0 0.850888 0.966709 0.958690 -0.120812
1 0.820126 0.385686 0.061402 7.075399
2 0.059729 0.831768 0.652259 -1.183638
3 0.244774 0.140322 0.041711 2.504142
4 0.818205 0.753384 0.578851 0.111982

Lokale Variablen in DataFrame.eval
Die Methode DataFrame.eval unterstützt eine zusätzliche Syntax, die die Arbeit mit

lokale Python-Variablen. Betrachten Sie das Folgende:

In [22]: spalten_mittel = df.mittel(1)
ergebnis1 = df['A'] + spalten_mittelwert
ergebnis2 = df.eval('A + @spalten_mittel')
np.allclose(result1, result2)
Out[22]: True

Das @-Zeichen markiert hier einen Variablennamen und nicht einen Spaltennamen und ermöglicht es Ihnen

effizient Ausdrücke auswerten, die die beiden "Namespaces" einbeziehen: den Namespace von

Spalten und den Namespace von Python-Objekten. Beachten Sie, dass dieses @ Zeichen nur

DataFrame.eval für spaltenweise Operationen | 219
von der Methode DataFrame.eval unterstützt, nicht von der Funktion pandas.eval, weil

die Funktion pandas.eval hat nur Zugriff auf den einen (Python-)Namensraum.

Die Methode DataFrame.query
Der DataFrame verfügt über eine weitere Methode, die auf ausgewerteten Zeichenketten basiert und query genannt wird. 
Sie das Folgende:

In [23]: result1 = df[(df.A < 0.5) & (df.B < 0.5)]
result2 = pd.eval('df[(df.A < 0.5) & (df.B < 0.5)]')
np.allclose(result1, result2)
Out[23]: True

Wie in dem Beispiel, das wir in unserer Diskussion über DataFrame.eval verwendet haben, ist dies ein Ausdruck

mit Spalten des DataFrame. Sie kann jedoch nicht mit dem DataFrame ausgedrückt werden.

Frame.eval-Syntax! Stattdessen können Sie für diese Art der Filterung die Abfrage
Methode verwenden:

In [24]: result2 = df.query('A < 0.5 und B < 0.5')
np.allclose(result1, result2)
Out[24]: True

Die Berechnung ist nicht nur effizienter als die Maskierungsausdrücke, sondern auch

Dies ist viel einfacher zu lesen und zu verstehen. Beachten Sie, dass die Abfragemethode auch

akzeptiert das @-Flag zur Kennzeichnung lokaler Variablen:

In [25]: Cmean = df['C'].mean()
result1 = df[(df.A < Cmean) & (df.B < Cmean)]
result2 = df.query('A < @Cmean und B < @Cmean')
np.allclose(result1, result2)
Out[25]: True

Leistung: Wann diese Funktionen verwendet werden sollten
Bei der Überlegung, ob eval und query verwendet werden sollen, gibt es zwei Überlegungen: com-

und Speicherverbrauch. Der Speicherbedarf ist der am besten vorhersehbare Aspekt. Wie bereits

erwähnt, jeder zusammengesetzte Ausdruck, der NumPy-Arrays oder Pandas Data

Frames führen zur impliziten Erstellung von temporären Arrays. Zum Beispiel dies:

In [26]: x = df[(df.A < 0.5) & (df.B < 0.5)]

entspricht in etwa dem hier:

In [27]: tmp1 = df.A < 0,5
tmp2 = df.B < 0,5
tmp3 = tmp1 & tmp2
x = df[tmp3]

220 | Kapitel 24: Leistungsstarke Pandas: eval und Query

Wenn die Größe der temporären DataFrames im Vergleich zu den verfügbaren Systemkapazitäten erheblich ist.

tem Speicher (typischerweise mehrere Gigabyte), dann ist es eine gute Idee, eine Eval- oder

Abfrageausdruck. Sie können die ungefähre Größe Ihres Arrays in Bytes überprüfen, indem Sie

dies:

In [28]: df.Werte.nbytes
Out[28]: 32000

Was die Leistung angeht, so kann eval schneller sein, auch wenn Sie nicht die volle Leistung Ihrer

Systemspeicher. Die Frage ist, wie Ihre temporären Objekte im Vergleich zur Größe des

L1- oder L2-CPU-Cache auf Ihrem System (in der Regel ein paar Megabyte); wenn sie viel größer sind-

ger, dann kann eval einige potenziell langsame Bewegungen von Werten zwischen den verschiedenen
verschiedenen Speicher-Caches vermeiden. In der Praxis habe ich festgestellt, dass der Unterschied in der Berechnungszeit

zwischen den traditionellen Methoden und der eval/query-Methode ist in der Regel nicht signifi-
Wenn überhaupt, dann ist die traditionelle Methode bei kleineren Arrays schneller! Der Vorteil der

eval/query liegt hauptsächlich im gespeicherten Speicher, und die manchmal sauberere Syntax, die sie

Angebot.

Wir haben die meisten Details von eval und query hier behandelt; für weitere Informationen über

können Sie in der Pandas-Dokumentation nachlesen. Insbesondere können verschiedene Parser und
Engines für die Ausführung dieser Abfragen spezifiziert werden; Details dazu finden Sie in der Diskussio-

im Abschnitt "Verbesserung der Leistung" der Dokumentation.

Weitere Ressourcen
In diesem Teil des Buches haben wir viele der Grundlagen für die effektive Nutzung von Pandas behandelt

für die Datenanalyse. Dennoch wurde in unserer Diskussion vieles ausgelassen. Um mehr zu erfahren
über Pandas zu erfahren, empfehle ich die folgenden Ressourcen:

Pandas Online-Dokumentation
Dies ist die erste Anlaufstelle für die vollständige Dokumentation des Pakets. Während die
Beispiele in der Dokumentation dazu neigen, auf kleinen generierten Datensätzen zu basieren, ist die
Beschreibung der Optionen ist vollständig und im Allgemeinen sehr nützlich für das Verständnis
die Verwendung der verschiedenen Funktionen.

Python für die Datenanalyse
Dieses Buch wurde von Wes McKinney (dem ursprünglichen Erfinder von Pandas) geschrieben und enthält
viel mehr Details über das Pandas-Paket, als wir in diesem Kapitel unterbringen konnten.
Insbesondere taucht McKinney tief in die Werkzeuge für Zeitreihen ein, die
sein Brot und Butter als Finanzberater waren. Das Buch enthält auch viele unterhal-
Beispiele für die Anwendung von Pandas, um Erkenntnisse aus realen Datensätzen zu gewinnen.

Weitere Ressourcen | 221
Effektive Pandas

Dieses kurze E-Book von Pandas-Entwickler Tom Augspurger bietet eine prägnante Out-
die volle Leistungsfähigkeit der Pandas-Bibliothek auf effektive und idiomatische Weise zu nutzen.
Weise.
Pandas auf PyVideo
Von der PyCon über die SciPy bis hin zu PyData, viele Konferenzen haben Tutorials von
Pandas-Entwicklern und Power-Usern. Insbesondere die PyCon-Tutorials werden in der Regel
von sehr gut geprüften Referenten gehalten.

Mit Hilfe dieser Ressourcen in Verbindung mit der in diesen Kapiteln beschriebenen Vorgehensweise kann mein

hoffen, dass Sie in der Lage sind, mit Pandas jedes Datenanalyseproblem zu lösen, das Sie
zu lösen!

222 | Kapitel 24: Leistungsstarke Pandas: eval und Query

TEIL IV
Visualisierung mit Matplotlib

Wir werden nun einen detaillierten Blick auf das Matplotlib-Paket für die Visualisierung in Python werfen.
Matplotlib ist eine Multiplattform-Bibliothek zur Datenvisualisierung, die auf NumPy-Arrays und

entwickelt, um mit dem breiteren SciPy-Stack zu arbeiten. Es wurde von John Hunter im Jahr
2002 entwickelt, ursprünglich als Patch für IPython, um interaktives Plotten im Stil von MATLAB zu ermöglichen

über gnuplot von der IPython-Befehlszeile aus. Der Erfinder von IPython, Fernando Perez, war
zu dieser Zeit damit beschäftigt, seine Promotion abzuschließen, und ließ John wissen, dass er keine Zeit haben würde, um

den Patch mehrere Monate lang zu überprüfen. John nahm dies zum Anlass, sich auf eigene Faust auf den Weg zu machen, und
Das Matplotlib-Paket war geboren und wurde in Version 0.1 im Jahr 2003 veröffentlicht. Es erhielt eine

frühen Aufschwung, als es als bevorzugtes Plot-Paket des Space Tele-

scope Science Institute (die Leute hinter dem Hubble-Teleskop), das die Entwicklung von Matplotlib finanziell
die Entwicklung von Matplotlib unterstützte und seine Fähigkeiten stark erweiterte.

Eine der wichtigsten Eigenschaften von Matplotlib ist seine Fähigkeit, mit vielen Betriebssystemen und
Betriebssystemen und Grafik-Backends. Matplotlib unterstützt Dutzende von Backends und

Ausgabearten, d.h. Sie können sich darauf verlassen, dass es unabhängig vom Betriebssystem funktioniert.

System, das Sie verwenden oder welches Ausgabeformat Sie wünschen. Dieser plattformübergreifende,
alles-zu-jedem-Ansatz ist eine der großen Stärken von Matplotlib. Es

hat zu einer großen Nutzerbasis geführt, was wiederum zu einer aktiven Entwicklerbasis geführt hat und
Matplotlibs leistungsstarke Werkzeuge und seine Allgegenwart in der wissenschaftlichen Python-Welt.

In den letzten Jahren haben jedoch die Schnittstelle und der Stil von Matplotlib begonnen zu zeigen

ihr Alter. Neuere Werkzeuge wie ggplot und ggvis in der Sprache R, zusammen mit Webvisu-

alisierungs-Toolkits, die auf D3js und HTML5 Canvas basieren, lassen Matplotlib oft
klobig und altmodisch. Dennoch bin ich der Meinung, dass wir Matplotlibs Vorteile nicht ignorieren können.

Stärke als gut getestete, plattformübergreifende Grafik-Engine. Neueste Matplotlib-Versionen

machen es relativ einfach, neue globale Darstellungsstile festzulegen (siehe Kapitel 34), und die Leute

haben neue Pakete entwickelt, die auf ihren leistungsfähigen Interna aufbauen, um die Mat-
plotlib über sauberere, modernere APIs anzusteuern - zum Beispiel Seaborn (besprochen in Kap.

ter 36), ggpy, HoloViews, und sogar Pandas selbst können als Wrapper um
Matplotlibs API verwendet werden. Selbst mit solchen Wrappern ist es oft sinnvoll, in die Mat-

Plotlib-Syntax, um die endgültige Plotausgabe anzupassen. Aus diesem Grund glaube ich, dass Matplotlib
selbst ein wichtiger Bestandteil der Datenvisualisierung bleiben wird, auch wenn neue Werkzeuge bedeuten

die Gemeinschaft entfernt sich allmählich von der direkten Verwendung der Matplotlib-API.

KAPITEL 25

Allgemeine Matplotlib-Tipps
Bevor wir in die Details der Erstellung von Visualisierungen mit Matplotlib eintauchen, gibt es ein paar
einige nützliche Dinge, die Sie über die Verwendung des Pakets wissen sollten.

Matplotlib importieren
So wie wir die Abkürzung np für NumPy und die Abkürzung pd für Pandas verwenden, werden wir

einige Standardabkürzungen für Matplotlib-Importe verwenden:

In [1]: import matplotlib as mpl
import matplotlib.pyplot as plt

Die plt-Schnittstelle werden wir am häufigsten verwenden, wie Sie in diesem Teil sehen werden

des Buches.

Stile einstellen
Wir werden die plt.style-Direktive verwenden, um geeignete ästhetische Stile für unser Bild zu wählen.

Darstellungen. Hier stellen wir den klassischen Stil ein, der sicherstellt, dass die von uns erstellten Diagramme den
klassischen Matplotlib-Stil verwenden:

In [2]: plt.style.use('classic')

Im Laufe dieses Kapitels werden wir diesen Stil je nach Bedarf anpassen. Für weitere Informationen über

Stylesheets, siehe Kapitel 34.

225
anzeigen oder nicht anzeigen? Wie Sie Ihre Diagramme anzeigen
Eine Visualisierung, die Sie nicht sehen können, ist nicht von großem Nutzen, aber wie Sie Ihre Mat-

plotlib-Plots hängt vom jeweiligen Kontext ab. Die beste Verwendung von Matplotlib hängt davon ab
wie Sie es verwenden; grob gesagt, sind die drei anwendbaren Kontexte die Verwendung von Matplotlib in einem

Skript, in einem IPython-Terminal oder in einem Jupyter-Notebook.

Plotten aus einem Skript heraus
Wenn Sie Matplotlib aus einem Skript heraus verwenden, ist die Funktion plt.show Ihr Freund.

plt.show startet eine Ereignisschleife, sucht alle derzeit aktiven Figure-Objekte und öffnet

ein oder mehrere interaktive Fenster, die Ihre Figur oder Figuren anzeigen.

So können Sie beispielsweise eine Datei mit dem Namen myplot.py haben, die Folgendes enthält:

# Datei: myplot.py
import matplotlib.pyplot as plt
import numpy as np
x = np.linspace(0, 10, 100)
plt.plot(x, np.sin(x))
plt.plot(x, np.cos(x))
plt.show()
Sie können dieses Skript dann von der Befehlszeile aus ausführen, was dazu führt, dass ein
Fenster öffnet, in dem Ihre Abbildung angezeigt wird:

$ python myplot.py
Der Befehl plt.show macht eine Menge unter der Haube, da er mit dem interaktiven grafischen Backend Ihres Systems interagieren muss.
tems interaktivem grafischen Backend interagieren muss. Die Details dieser Operation können sehr unterschiedlich sein

von System zu System und sogar von Installation zu Installation, aber Matplotlib tut sein
sein Bestes, um all diese Details vor Ihnen zu verbergen.

Eine Sache ist zu beachten: Der Befehl plt.show sollte nur einmal pro

Python-Sitzung und ist meist am Ende des Skripts zu sehen. Mehrere show
Befehle können zu unvorhersehbarem Backend-abhängigem Verhalten führen und sollten

meist vermieden werden.

226 | Kapitel 25: Allgemeine Matplotlib-Tipps

Plotten aus einer IPython-Shell
Matplotlib funktioniert auch nahtlos innerhalb einer IPython-Shell (siehe Teil I). IPython ist aufgebaut

funktioniert gut mit Matplotlib, wenn Sie den Matplotlib-Modus angeben. Um diesen Modus zu aktivieren,

können Sie nach dem Start von ipython den Befehl %matplotlib magic verwenden:

In [1]: % matplotlib
Verwendung von matplotlib backend: TkAgg

In [2]: import matplotlib.pyplot as plt

Zu diesem Zeitpunkt führt jeder plt-Plot-Befehl zum Öffnen eines Abbildungsfensters, und weitere

können Befehle ausgeführt werden, um die Darstellung zu aktualisieren. Einige Änderungen (wie z.B. das Ändern von Eigen
von Linien, die bereits gezeichnet sind) werden nicht automatisch gezeichnet: Sie müssen eine Aktualisierung erzwingen,

plt.draw verwenden. Die Verwendung von plt.show im Matplotlib-Modus von IPython ist nicht erforderlich.

Plotten aus einem Jupyter-Notizbuch
Das Jupyter-Notizbuch ist ein browserbasiertes, interaktives Datenanalysewerkzeug, mit dem sich
das Erzählungen, Code, Grafiken, HTML-Elemente und vieles mehr in einer einzigen Ex- perte zusammenfasst.

ausschneidbares Dokument (siehe Teil I).

Das interaktive Plotten innerhalb eines Jupyter-Notebooks kann mit der %matplotlib

und funktioniert auf ähnliche Weise wie die IPython-Shell. Sie haben auch die Möglichkeit
Sie haben auch die Möglichkeit, Grafiken direkt in das Notizbuch einzubinden, wobei es zwei Optionen gibt:

%matplotlib inline führt zu statischen Bildern Ihres Plots, die in das
Notizbuch.
%matplotlib notebook führt zu interaktiven Plots, die in das Notiz-
Buch.
In diesem Buch werden wir im Allgemeinen bei der Standardeinstellung bleiben, bei der die Zahlen als statische Bilder dargestellt werden.
dargestellt werden (siehe Abbildung 25-1 für das Ergebnis dieses einfachen Plot-Beispiels):

In [3]: % matplotlib inline

In [4]: import numpy as np
x = np.linspace(0, 10, 100)

fig = plt.figure()
plt.plot(x, np.sin(x), '-')
plt.plot(x, np.cos(x), '--');

Anzeigen oder nicht anzeigen? Wie Sie Ihre Diagramme anzeigen | 227
Abbildung 25-1. Grundlegendes Beispiel für das Plotten

Speichern von Zahlen in einer Datei
Ein nettes Feature von Matplotlib ist die Möglichkeit, Figuren in einer Vielzahl von for-

Matten. Das Speichern einer Figur kann mit dem Befehl savefig durchgeführt werden. Zum Beispiel, um die
die vorherige Abbildung als PNG-Datei zu speichern, können wir dies ausführen:

In [5]: fig.savefig('meine_Abbildung.png')

Wir haben jetzt eine Datei namens my_figure.png im aktuellen Arbeitsverzeichnis:

In [6]: !ls -lh meine_Figur.png
Out[6]: -rw-r--r-- 1 jakevdp staff 26K Feb 1 06:15 my_figure.png

Um zu bestätigen, dass sie das enthält, was wir denken, verwenden wir das IPython Image
Objekt, um den Inhalt dieser Datei anzuzeigen (siehe Abbildung 25-2).

In [7]: from IPython.display import Image
Image('meine_Figur.png')

228 | Kapitel 25: Allgemeine Matplotlib-Tipps

Abbildung 25-2. PNG-Rendering des Basisplots

In savefig wird das Dateiformat aus der Erweiterung des angegebenen Dateinamens abgeleitet.
Je nachdem, welche Backends Sie installiert haben, gibt es viele verschiedene Dateiformate

verfügbar. Die Liste der unterstützten Dateitypen finden Sie für Ihr System mit der Funktion

folgende Methode des Objekts figure canvas:

In [8]: fig.canvas.get_supported_filetypes()
Out[8]: {'eps': 'Encapsulated Postscript',
jpg': 'Joint Photographic Experts Group',
jpeg': 'Joint Photographic Experts Group', 'jpeg': 'Joint Photographic Experts Group',
pdf": "Portable Document Format",
pgf": "PGF-Code für LaTeX",
png": "Portable Netzwerkgrafik",
ps": "Postscript",
'raw': Rohes RGBA-Bitmap',
rgba': 'Rohes RGBA-Bitmap',
svg": "Skalierbare Vektorgrafik",
svgz": "Skalierbare Vektorgrafik", "svgz": "Skalierbare Vektorgrafik
tif": "Tagged Image File Format",
tiff': 'Tagged Image File Format'}

Beachten Sie, dass es beim Speichern Ihrer Abbildung nicht notwendig ist, plt.show oder ähnliche Kom- ponenten zu verwenden.
Kommandos zu verwenden.

Anzeigen oder nicht anzeigen? Wie Sie Ihre Plots anzeigen | 229
Zwei Schnittstellen zum Preis von einer
Ein potenziell verwirrendes Merkmal von Matplotlib sind seine zwei Schnittstellen: eine bequeme

MATLAB-ähnliche zustandsbasierte Schnittstelle und eine leistungsfähigere objektorientierte Schnittstelle.
Ich werde hier kurz die Unterschiede zwischen den beiden hervorheben.

MATLAB-ähnliche Schnittstelle

Matplotlib wurde ursprünglich als Python-Alternative für MATLAB-Anwender konzipiert, und
ein Großteil der Syntax spiegelt diese Tatsache wider. Die MATLAB-ähnlichen Werkzeuge sind in der

pyplot (plt) Schnittstelle. Zum Beispiel wird der folgende Code wahrscheinlich ziemlich gut aussehen

die MATLAB-Benutzer kennen (Abbildung 25-3 zeigt das Ergebnis).

In [9]: plt.figure() # Erstellen einer Plotfigur

# Erstellen Sie das erste von zwei Feldern und setzen Sie die aktuelle Achse
plt.subplot(2, 1, 1) # (Zeilen, Spalten, Feldnummer)
plt.plot(x, np.sin(x))

# Erstellen Sie das zweite Panel und setzen Sie die aktuelle Achse
plt.subplot(2, 1, 2)
plt.plot(x, np.cos(x));

Abbildung 25-3. Subplots unter Verwendung der MATLAB-ähnlichen Schnittstelle

Es ist wichtig zu wissen, dass diese Schnittstelle zustandsbehaftet ist: Sie verfolgt die "aktuelle"

Figur und Achsen, auf die alle plt-Befehle angewendet werden. Sie können eine Referenz...

plt.gcf (get current figure) und plt.gca (get current axes) auf diese zugreifen.

Routinen.

230 | Kapitel 25: Allgemeine Matplotlib-Tipps

Diese zustandsbehaftete Schnittstelle ist zwar schnell und bequem für einfache Diagramme, aber es ist einfach, die

zu Problemen führen. Wie können wir zum Beispiel, sobald das zweite Panel erstellt ist, zurückgehen
und etwas zum ersten hinzufügen? Dies ist über die MATLAB-ähnliche Schnittstelle möglich,

aber ein bisschen umständlich. Zum Glück gibt es einen besseren Weg.

Objektorientierte Schnittstelle

Für diese komplizierteren Situationen steht die objektorientierte Schnittstelle zur Verfügung, und

für den Fall, dass Sie mehr Kontrolle über Ihre Figur wünschen. Anstatt sich auf eine
einer "aktiven" Figur oder Achsen abhängt, werden in der objektorientierten Schnittstelle die Plotfunktionen

tionen sind Methoden expliziter Figure- und Axes-Objekte. Um die vorherige Darstellung
neu zu erstellen, wie in Abbildung 25-4 gezeigt, könnten Sie wie folgt vorgehen:

In [10]: _# Erstellen Sie zunächst ein Raster von Plots

ax wird ein Array mit zwei Achsen-Objekten sein_
fig, ax = plt.subplots(2)

# Aufruf der Methode plot() für das entsprechende Objekt
ax[0].plot(x, np.sin(x))
ax[1].plot(x, np.cos(x));

Abbildung 25-4. Teilplots mit objektorientierter Schnittstelle

Bei einfacheren Plots ist die Wahl des Stils weitgehend eine Frage der Vorliebe, aber
Der objektorientierte Ansatz kann zu einer Notwendigkeit werden, wenn die Plots komplizierter werden.

kiert. In den folgenden Kapiteln werden wir zwischen der MATLAB-ähnlichen
und objektorientierten Schnittstellen wechseln, je nachdem, was am bequemsten ist. In den meisten Fällen,

der Unterschied ist so gering wie der Wechsel von plt.plot zu ax.plot, aber es gibt ein paar

Probleme, die ich in den folgenden Kapiteln hervorheben werde.

Anzeigen oder nicht anzeigen? Wie Sie Ihre Diagramme anzeigen | 231
KAPITEL 26

Einfache Liniendiagramme
Die vielleicht einfachste aller Darstellungen ist die Visualisierung einer einzelnen Funktion y=fx. Hier
werden wir einen ersten Blick auf die Erstellung einer einfachen Darstellung dieses Typs werfen. Wie in allen folgenden

Kapiteln beginnen wir mit dem Einrichten des Notizbuchs zum Plotten und dem Importieren der Pack-

Alter, die wir verwenden werden:

In [1]: % matplotlib inline
import matplotlib.pyplot as plt
plt.style.use('seaborn-whitegrid')
import numpy as np

Für alle Matplotlib-Diagramme erstellen wir zunächst eine Figur und Achsen. In ihrer einfachsten Form,

Dies kann wie folgt geschehen (siehe Abbildung 26-1).

In [2]: fig = plt.figure()
ax = plt.axes()

In Matplotlib kann man sich die Figur (eine Instanz der Klasse plt.Figure) als eine

einen einzigen Container, der alle Objekte enthält, die Achsen, Grafiken, Text und

Beschriftungen. Die Achsen (eine Instanz der Klasse plt.Axes) sind das, was wir oben sehen: eine Begrenzungslinie

Box mit Häkchen, Gittern und Beschriftungen, die schließlich die Plot-Elemente enthält, die
die unsere Visualisierung ausmachen. In diesem Teil des Buches werde ich häufig die

Variablenname fig für eine Figureninstanz und ax für eine Achseninstanz oder

Gruppe von Achseninstanzen.

232
Abbildung 26-1. Ein leeres Achsennetz

Sobald wir eine Achse erstellt haben, können wir die Methode ax.plot verwenden, um einige Daten darzustellen. Lassen Sie uns
beginnen wir mit einer einfachen Sinuskurve, wie in Abbildung 26-2 dargestellt.

In [3]: fig = plt.figure()
ax = plt.axes()

x = np.linspace(0, 10, 1000)
ax.plot(x, np.sin(x));

Abbildung 26-2. Eine einfache Sinuskurve

Einfache Liniendiagramme | 233
Beachten Sie, dass das Semikolon am Ende der letzten Zeile beabsichtigt ist: Es unterdrückt die Tex-

Darstellung des Plots in der Ausgabe.

Alternativ können wir auch die PyLab-Schnittstelle verwenden und die Figur und die Achsen für

im Hintergrund (siehe Teil IV für eine Diskussion über diese beiden Schnittstellen); als

Abbildung 26-3 zeigt, dass das Ergebnis das gleiche ist.

In [4]: plt.plot(x, np.sin(x));

Abbildung 26-3. Eine einfache Sinuskurve über die objektorientierte Schnittstelle

Wenn wir eine einzelne Figur mit mehreren Linien erstellen wollen (siehe Abbildung 26-4), können wir einfach

die Plot-Funktion mehrfach aufrufen:

In [5]: plt.plot(x, np.sin(x))
plt.plot(x, np.cos(x));

Das ist alles, was es zum Plotten von einfachen Funktionen in Matplotlib zu wissen gibt! Wir werden jetzt in einige

weitere Einzelheiten zur Steuerung des Erscheinungsbildes der Achsen und Linien.

234 | Kapitel 26: Einfache Liniendiagramme

Abbildung 26-4. Überplotten mehrerer Linien

Anpassen des Plots: Linienfarben und -stile
Die erste Anpassung, die Sie an einem Diagramm vornehmen möchten, ist die Kontrolle der Linienfarben und

Stile. Die Funktion plt.plot nimmt zusätzliche Argumente entgegen, die verwendet werden können, um Folgendes anzugeben

diese. Um die Farbe anzupassen, können Sie das Schlüsselwort color verwenden, das eine Zeichenkette akzeptiert
Argument akzeptiert, das praktisch jede erdenkliche Farbe repräsentiert. Die Farbe kann in einer

siehe Abbildung 26-5 für die Ausgabe der folgenden Beispiele:

In [6]: plt.plot(x, np.sin(x - 0), color='blue') # Farbe mit Namen angeben
plt.plot(x, np.sin(x - 1), color='g') # kurzer Farbcode (rgbcmyk)
plt.plot(x, np.sin(x - 2), color='0.75') # Grauskala zwischen 0 und 1
plt.plot(x, np.sin(x - 3), color='#FFDD44') # Hex-Code (RRGGBB, 00 bis FF)
plt.plot(x, np.sin(x - 4), color=(1.0,0.2,0.3)) # RGB-Tupel, Werte 0 bis 1
plt.plot(x, np.sin(x - 5), color='chartreuse'); # HTML-Farbnamen werden unterstützt

Anpassen des Plots: Linienfarben und -stile | 235
Abbildung 26-5. Steuern der Farbe von Plot-Elementen

Wenn keine Farbe angegeben wird, schaltet Matplotlib automatisch durch eine Reihe von Standard
Farben für mehrere Zeilen durch.

In ähnlicher Weise kann der Linienstil mit dem Schlüsselwort linestyle angepasst werden (siehe
Abbildung 26-6).

In [7]: plt.plot(x, x + 0, linestyle='solid')
plt.plot(x, x + 1, linestyle='gestrichelt')
plt.plot(x, x + 2, linestyle='dashdot')
plt.plot(x, x + 3, linestyle='gepunktet');

# Kurz gesagt, können Sie die folgenden Codes verwenden:
plt.plot(x, x + 4, linestyle='-') # durchgezogen
plt.plot(x, x + 5, linestyle='--') # gestrichelt
plt.plot(x, x + 6, linestyle='-.') # dashdot
plt.plot(x, x + 7, linestyle=':'); # gepunktet

236 | Kapitel 26: Einfache Liniendiagramme

Abbildung 26-6. Beispiele für verschiedene Linienstile

Auch wenn es für jemanden, der Ihren Code liest, weniger klar sein mag, können Sie einige Schlüssel-

Striche durch die Kombination dieser Linienstil- und Farbcodes zu einem einzigen Nicht-Schlüsselwort

Argument für die Funktion plt.plot; Abbildung 26-7 zeigt das Ergebnis.

In [8]: plt.plot(x, x + 0, '-g') # einfarbig grün
plt.plot(x, x + 1, '--c') # gestricheltes Cyan
plt.plot(x, x + 2, '-.k') # strichliert schwarz
plt.plot(x, x + 3, ':r'); # gestrichelt rot

Abbildung 26-7. Steuerung von Farben und Stilen mit der Kurzsyntax

Anpassen des Plots: Linienfarben und -stile | 237
Diese einstelligen Farbcodes spiegeln die Standardabkürzungen im RGB

(Rot/Grün/Blau) und CMYK (Cyan/Magenta/Gelb/BlackK), die üblicherweise
häufig für digitale Farbgrafiken verwendet werden.

Es gibt viele weitere Schlüsselwortargumente, die zur Feinabstimmung des Erscheinungsbildes verwendet werden können.

ance des Plots; für Details lesen Sie bitte den Docstring der Funktion plt.plot

mit Hilfe der IPython-Hilfswerkzeuge (siehe Kapitel 1).

Anpassen des Plots: Achsenbegrenzungen
Matplotlib macht einen guten Job bei der Auswahl von Standard-Achsengrenzen für Ihren Plot, aber manchmal
manchmal ist es schön, eine feinere Kontrolle zu haben. Der einfachste Weg, die Grenzen einzustellen, ist die Verwendung von

die Funktionen plt.xlim und plt.ylim (siehe Abbildung 26-8).

In [9]: plt.plot(x, np.sin(x))

plt.xlim(-1, 11)
plt.ylim(-1,5, 1,5);

Abbildung 26-8. Beispiel für die Einstellung von Achsengrenzen

Wenn Sie aus irgendeinem Grund möchten, dass eine der beiden Achsen in umgekehrter Reihenfolge angezeigt wird, können Sie einfach
die Reihenfolge der Argumente umkehren (siehe Abbildung 26-9).

In [10]: plt.plot(x, np.sin(x))

plt.xlim(10, 0)
plt.ylim(1.2, -1.2);

238 | Kapitel 26: Einfache Liniendiagramme

Abbildung 26-9. Beispiel für die Umkehrung der y-Achse

Eine nützliche verwandte Methode ist plt.axis (beachten Sie hier die mögliche Verwechslung zwischen Achsen

mit einem e und Achse mit einem i), was eine qualitativere Spezifizierung der Achsengrenzen ermöglicht.
len. Zum Beispiel können Sie die Grenzen um den aktuellen Kon- sum automatisch enger ziehen.

Zelt, wie in Abbildung 26-10 dargestellt.

In [11]: plt.plot(x, np.sin(x))
plt.axis('tight');

Abbildung 26-10. Beispiel für ein "enges" Layout

Anpassen der Darstellung: Achsenbegrenzungen | 239
Sie können auch angeben, dass Sie ein gleiches Achsenverhältnis wünschen, so dass eine Einheit in x visuell ist

entspricht einer Einheit in y, wie in Abbildung 26-11 dargestellt.

In [12]: plt.plot(x, np.sin(x))
plt.axis('equal');

Abbildung 26-11. Beispiel für ein "gleiches" Layout, bei dem die Einheiten an die Ausgangsauflösung angepasst sind

Weitere Achsenoptionen sind "ein", "aus", "quadratisch", "Bild" und mehr. Für mehr

Informationen dazu finden Sie im plt.axis docstring.

Beschriftung von Diagrammen
Im letzten Teil dieses Kapitels werden wir uns kurz mit der Beschriftung von Diagrammen befassen: Titel, Achsen
Achsenbeschriftungen und einfache Legenden. Titel und Achsenbeschriftungen sind die einfachsten Beschriftungen - es gibt

sind Methoden, mit denen man sie schnell einstellen kann (siehe Abbildung 26-12).

In [13]: plt.plot(x, np.sin(x))
plt.title("Eine Sinuskurve")
plt.xlabel("x")
plt.ylabel("sin(x)");

240 | Kapitel 26: Einfache Liniendiagramme

Abbildung 26-12. Beispiele für Achsenbeschriftungen und Titel

Die Position, die Größe und der Stil dieser Beschriftungen können mit optionalen Argumenten angepasst werden
an die Funktionen angepasst werden, die in den Docstrings beschrieben sind.

Wenn mehrere Linien innerhalb einer einzigen Achse angezeigt werden, kann es nützlich sein, eine

Plotlegende, die jeden Linientyp beschriftet. Auch hier hat Matplotlib eine eingebaute Möglichkeit, schnell

Die Erstellung einer solchen Legende erfolgt über die (Sie haben es erraten) Methode plt.legend.
Obwohl es mehrere gültige Möglichkeiten gibt, dies zu verwenden, finde ich es am einfachsten, die Beschriftung anzugeben

jeder Linie mit dem Schlüsselwort label der Funktion plot (siehe Abbildung 26-13).

In [14]: plt.plot(x, np.sin(x), '-g', label='sin(x)')
plt.plot(x, np.cos(x), ':b', label='cos(x)')
plt.axis('equal')

plt.legend();

Plots beschriften | 241
Abbildung 26-13. Beispiel für eine Plotlegende

Wie Sie sehen können, verfolgt die Funktion plt.legend den Linienstil und die Farbe und
stimmt diese mit der richtigen Beschriftung ab. Weitere Informationen zum Festlegen und Formatieren

Plot-Legenden können im plt.legend docstring gefunden werden; zusätzlich werden wir behandeln

einige erweiterte Legendenoptionen in Kapitel 29.

Matplotlib-Gotchas
Während die meisten plt-Funktionen direkt in ax-Methoden übersetzt werden (plt.plot → ax.plot,

plt.legend → ax.legend, usw.), ist dies nicht bei allen Befehlen der Fall. Insbesondere die
Funktionen zum Setzen von Grenzwerten, Beschriftungen und Titeln sind leicht verändert. Für den Übergang

zwischen MATLAB-ähnlichen Funktionen und objektorientierten Methoden zu finden, machen Sie folgendes

Änderungen:

plt.xlabel → ax.set_xlabel
plt.ylabel → ax.set_ylabel
plt.xlim → ax.set_xlim
plt.ylim → ax.set_ylim
plt.title → ax.set_title
In der objektorientierten Schnittstelle zum Plotten werden diese Funktionen nicht indi-

ist es oft bequemer, die ax.set-Methode zu verwenden, um all diese Eigenschaften zu setzen.

auf einmal zu binden (siehe Abbildung 26-14).

242 | Kapitel 26: Einfache Liniendiagramme

In [15]: ax = plt.axes()
ax.plot(x, np.sin(x))
ax.set(xlim=(0, 10), ylim=(-2, 2),
xlabel='x', ylabel='sin(x)',
title='A Simple Plot');

Abbildung 26-14. Beispiel für die Verwendung von ax.set zum gleichzeitigen Setzen mehrerer Eigenschaften

Matplotlib-Gotchas | 243
KAPITEL 27

Einfache Streuungsdiagramme
Eine weitere häufig verwendete Darstellungsart ist die einfache Punktwolke, ein enger Verwandter der Linienwolke.
Darstellung. Anstelle von Punkten, die durch Liniensegmente verbunden sind, werden hier die Punkte dargestellt

einzeln mit einem Punkt, einem Kreis oder einer anderen Form. Wir beginnen mit dem Einrichten des Notizbuchs

zum Plotten und Importieren der Pakete, die wir verwenden werden:

In [1]: % matplotlib inline
import matplotlib.pyplot as plt
plt.style.use('seaborn-whitegrid')
import numpy as np

Streudiagramme mit plt.plot
Im vorigen Kapitel haben wir uns die Verwendung von plt.plot/ax.plot zur Erstellung von Linienplots angesehen. Es

stellt sich heraus, dass dieselbe Funktion auch Streudiagramme erzeugen kann (siehe Abbildung 27-1).

In [2]: x = np.linspace(0, 10, 30)
y = np.sin(x)

plt.plot(x, y, 'o', color='black');

244
Abbildung 27-1. Beispiel für ein Streudiagramm

Das dritte Argument im Funktionsaufruf ist ein Zeichen, das den Typ des Sym-

bol für die grafische Darstellung verwendet. Genauso wie Sie Optionen wie '-' oder '--' angeben können, um die

Während Sie den Linienstil steuern können, verfügt der Markierungsstil über einen eigenen Satz kurzer Zeichenfolgencodes. Die vollständige Liste der

verfügbaren Symbole finden Sie in der Dokumentation von plt.plot oder in der Online-Dokumentation von Matplotlib
Online-Dokumentation. Die meisten der Möglichkeiten sind ziemlich intuitiv, und eine Reihe von

Die gängigsten sind hier dargestellt (siehe Abbildung 27-2).

In [3]: rng = np.random.default_rng(0)
for marker in ['o', '.', ',', 'x', '+', 'v', '^', '<', '>', 's', 'd']:
plt.plot(rng.random(2), rng.random(2), marker, color='black',
label="marker='{0}'".format(marker))
plt.legend(numpoints=1, fontsize=13)
plt.xlim(0, 1.8);

Streudiagramme mit plt.plot | 245
Abbildung 27-2. Demonstration von Punktnummern

Um noch mehr Möglichkeiten zu haben, können diese Zeichencodes zusammen mit Zeilen- und

Farbcodes, um Punkte zusammen mit einer Verbindungslinie zu zeichnen (siehe Abbildung 27-3).

In [4]: plt.plot(x, y, '-ok');

Abbildung 27-3. Kombination von Linien- und Punktmarkern

246 | Kapitel 27: Einfache Streudiagramme

Zusätzliche Schlüsselwortargumente für plt.plot spezifizieren eine breite Palette von Eigenschaften der
Linien und Markierungen, wie in Abbildung 27-4 zu sehen ist.

In [5]: plt.plot(x, y, '-p', color='gray',
markerize=15, linewidth=4,
markerfacecolor='white',
markeredgecolor='gray',
markeredgewidth=2)
plt.ylim(-1.2, 1.2);

Abbildung 27-4. Anpassen von Linien- und Punktmarkierungen

Diese Art von Optionen machen plt.plot zum wichtigsten Arbeitspferd für zweidimensionale

Plots in Matplotlib. Eine vollständige Beschreibung der verfügbaren Optionen finden Sie in der

plt.plot-Dokumentation.

Streudiagramme mit plt.scatter
Eine zweite, leistungsfähigere Methode zur Erstellung von Streudiagrammen ist die Funktion plt.scatter.

die sehr ähnlich wie die Funktion plt.plot verwendet werden kann (siehe Abbildung 27-5).

In [6]: plt.scatter(x, y, marker='o');

Streudiagramme mit plt.scatter | 247
Abbildung 27-5. Eine einfache Punktwolke

Der Hauptunterschied zwischen plt.scatter und plt.plot besteht darin, dass plt.scatter verwendet werden kann, um

Streudiagramme, in denen die Eigenschaften jedes einzelnen Punktes (Größe, Flächenfarbe, Kantenfarbe,

usw.) können individuell gesteuert oder auf Daten abgebildet werden.

Um dies zu verdeutlichen, erstellen wir eine zufällige Punktwolke mit Punkten in verschiedenen Farben und Größen.

Um die sich überschneidenden Ergebnisse besser sehen zu können, verwenden wir auch das Schlüsselwort alpha, um

passen Sie die Transparenzstufe an (siehe Abbildung 27-6).

In [7]: rng = np.random.default_rng(0)
x = rng.normal(size=100)
y = rng.normal(größe=100)
farben = rng.random(100)
Größen = 1000 * rng.random(100)

plt.scatter(x, y, c=Farben, s=Größen, alpha=0.3)
plt.colorbar(); # Farbskala anzeigen

248 | Kapitel 27: Einfache Streudiagramme

Abbildung 27-6. Ändern der Größe und Farbe von Streupunkten

Beachten Sie, dass das Farbargument automatisch auf eine Farbskala abgebildet wird (hier gezeigt

durch den Befehl colorbar), und dass das Argument Größe in Pixeln angegeben wird. Auf diese Weise,

Die Farbe und Größe der Punkte kann verwendet werden, um Informationen in der Visualisierung zu vermitteln, um
um mehrdimensionale Daten zu visualisieren.

Wir könnten zum Beispiel den Iris-Datensatz von Scikit-Learn verwenden, bei dem jede Probe
eine von drei Blütentypen ist, bei denen die Größe der Blütenblätter und Kelchblätter sorgfältig

gemessen (siehe Abbildung 27-7).

In [8]: from sklearn.datasets import load_iris
iris = load_iris()
Merkmale = iris.data.T

plt.scatter(features[0], features[1], alpha=0.4,
s=100*Merkmale[3], c=iris.target, cmap='viridis')
plt.xlabel(iris.feature_names[0])
plt.ylabel(iris.feature_names[1]);

Streudiagramme mit plt.scatter | 249
1 Eine vollfarbige Version dieser Abbildung ist auf GitHub zu finden.
Abbildung 27-7. Verwendung von Punkteigenschaften zur Codierung von Merkmalen der Irisdaten^1

Wir können sehen, dass dieses Streudiagramm uns die Möglichkeit gegeben hat, gleichzeitig zu untersuchen

vier verschiedene Dimensionen der Daten: Die Lage (x, y) jedes Punktes entspricht

die Länge und Breite des Kelchblattes, die Größe der Spitze hängt von der Breite des Blütenblattes ab, und die
die Farbe steht im Zusammenhang mit der jeweiligen Blumenart. Multicolor und Multifeature-Streuung

Darstellungen wie diese können sowohl für die Untersuchung als auch für die Darstellung von Daten nützlich sein.

Plot Versus Scatter: Ein Hinweis zur Effizienz
Abgesehen von den unterschiedlichen Funktionen von plt.plot und plt.scatter, warum könnte

Sie sich für das eine oder das andere entscheiden? Während es für kleine Unternehmen nicht so wichtig ist

Datenmengen, wenn die Datensätze größer als ein paar tausend Punkte sind, kann plt.plot

spürbar effizienter als plt.scatter. Der Grund dafür ist, dass plt.scatter die

die Möglichkeit, für jeden Punkt eine andere Größe und/oder Farbe zu rendern, daher muss der Renderer

die zusätzliche Arbeit, jeden Punkt einzeln zu konstruieren. Mit plt.plot, auf der Seite

Andererseits sind die Marker für jeden Punkt garantiert identisch, so dass die Arbeit der
Bestimmung des Aussehens der Punkte nur einmal für den gesamten Datensatz durchgeführt wird.

Bei großen Datensätzen kann dieser Unterschied zu erheblichen Leistungsunterschieden führen, und bei

Aus diesem Grund sollte plt.plot bei großen Datensätzen gegenüber plt.scatter bevorzugt werden.

250 | Kapitel 27: Einfache Streudiagramme

Visualisierung von Unsicherheiten
Bei jeder wissenschaftlichen Messung ist die genaue Erfassung der Unsicherheiten fast genauso wichtig wie die

Ebenso wichtig, wenn nicht sogar wichtiger, ist die genaue Angabe der Zahl selbst. Ein Beispiel,
Stellen Sie sich vor, ich verwende einige astrophysikalische Beobachtungen, um das Hubble-Kon-

stante, die lokale Messung der Expansionsrate des Universums. Ich weiß, dass die
Literatur einen Wert von etwa 70 (km/s)/Mpc vorschlägt, und ich messe einen Wert von

74 (km/s)/Mpc mit meiner Methode. Sind die Werte konsistent? Die einzig richtige Antwort,

In Anbetracht dieser Informationen kann ich nur sagen: Es gibt keine Möglichkeit, das zu wissen.

Nehmen wir an, ich ergänze diese Informationen um gemeldete Unsicherheiten: Die aktuelle Literatur

ture schlägt einen Wert von 70 ± 2,5 (km/s)/Mpc vor, und meine Methode hat einen Wert von
74 ± 5 (km/s)/Mpc gemessen. Sind die Werte nun konsistent? Das ist eine Frage, die man

quantitativ beantwortet.

Bei der Visualisierung von Daten und Ergebnissen kann ein Diagramm durch die effektive Darstellung dieser Fehler
viel umfassendere Informationen vermitteln.

Grundlegende Fehlerbalken
Eine Standardmethode zur Visualisierung von Unsicherheiten ist die Verwendung einer Fehlerleiste. Ein einfacher Fehlerbalken kann
mit einem einzigen Matplotlib-Funktionsaufruf erstellt werden, wie in Abbildung 27-8 gezeigt.

In [1]: % matplotlib inline
import matplotlib.pyplot as plt
plt.style.use('seaborn-whitegrid')
import numpy as np

In [2]: x = np.linspace(0, 10, 50)
dy = 0.8
y = np.sin(x) + dy * np.random.randn(50)

plt.errorbar(x, y, yerr=dy, fmt='.k');

Hier ist fmt ein Formatcode, der das Aussehen von Linien und Punkten steuert, und er

hat die gleiche Syntax wie die in plt.plot verwendete Kurzschrift, die im vorherigen Abschnitt

Kapitel und früher in diesem Kapitel.

Visualisierung von Unsicherheiten | 251
Abbildung 27-8. Beispiel einer Fehlerleiste

Zusätzlich zu diesen grundlegenden Optionen bietet die Fehlerbalkenfunktion zahlreiche Optionen zur Feinabstimmung
Feinabstimmung der Ausgaben. Mit diesen zusätzlichen Optionen können Sie die Ästhetik und die

ik Ihres Fehlerbalken-Diagramms. Ich finde es oft hilfreich, insbesondere bei überfüllten Diagrammen, die
die Fehlerbalken heller zu machen als die Punkte selbst (siehe Abbildung 27-9).

In [3]: plt.errorbar(x, y, yerr=dy, fmt='o', color='black',
ecolor='lightgray', elinewidth=3, capsize=0);

Abbildung 27-9. Anpassen der Fehlerbalken

252 | Kapitel 27: Einfache Streudiagramme

Zusätzlich zu diesen Optionen können Sie auch horizontale Fehlerbalken, einseitige

Fehlerbalken und viele andere Varianten. Für weitere Informationen über die verfügbaren Optionen,

beziehen sich auf den Docstring von plt.errorbar.

Kontinuierliche Fehler
In manchen Situationen ist es wünschenswert, Fehlerbalken für kontinuierliche Größen anzuzeigen. Obwohl

Matplotlib hat keine eingebaute Komfortroutine für diese Art von Anwendung,

Es ist relativ einfach, Primitive wie plt.plot und plt.fill_between zu kombinieren, um eine

nützliches Ergebnis.

Hier führen wir eine einfache Gauß'sche Prozessregression unter Verwendung der Scikit-Learn-API durch

(siehe Kapitel 38 für Details). Dies ist eine Methode zur Anpassung eines sehr flexiblen nichtparametrischen

Funktion auf Daten mit einem kontinuierlichen Maß für die Unsicherheit. Wir werden uns an dieser Stelle nicht mit den
Details der Gauß'schen Prozessregression eingehen, sondern uns stattdessen darauf konzentrieren, wie

können Sie sich eine solche kontinuierliche Fehlermessung vorstellen:

In [4]: from sklearn.gaussian_process import GaussianProcessRegressor

# Definieren Sie das Modell und zeichnen Sie einige Daten
model = lambda x: x * np.sin(x)
xDaten = np.array([1, 3, 5, 6, 8])
ydata = model(xdata)

# Berechnung der Gaußschen Prozessanpassung
gp = GaußscherProzessRegressor()
gp.fit(xDaten[:, np.neueAchse], yDaten)

xfit = np.linspace(0, 10, 1000)
yfit, dyfit = gp.predict(xfit[:, np.newaxis], return_std= True )

Wir haben jetzt xfit, yfit und dyfit, die die kontinuierliche Anpassung an unsere Daten abfragen. Wir

diese an die Funktion plt.errorbar wie im vorherigen Abschnitt übergeben, aber wir wollen nicht
wollen wir nicht wirklich 1.000 Punkte mit 1.000 Fehlerbalken darstellen. Stattdessen können wir die Funktion

plt.fill_between Funktion mit einer hellen Farbe, um diesen kontinuierlichen Fehler zu visualisieren (siehe
Abbildung 27-10).

In [5]: # Visualisieren Sie das Ergebnis
plt.plot(xdata, ydata, 'oder')
plt.plot(xfit, yfit, '-', color='gray')
plt.fill_between(xfit, yfit - dyfit, yfit + dyfit,
color='grau', alpha=0.2)
plt.xlim(0, 10);

Visualisierung von Unsicherheiten | 253
Abbildung 27-10. Darstellung von kontinuierlicher Unsicherheit mit gefüllten Regionen

Schauen Sie sich die Signatur des fill_between-Aufrufs an: Wir übergeben einen x-Wert, dann den unteren y-

Grenze, dann die obere y-Grenze, und das Ergebnis ist, dass der Bereich zwischen diesen Regionen
gefüllt wird.

Die sich daraus ergebende Abbildung vermittelt einen intuitiven Eindruck davon, was die Gaußsche Prozessregression

Algorithmus: In Regionen in der Nähe eines gemessenen Datenpunktes ist das Modell stark
belastet, was sich in den kleinen Modellunsicherheiten widerspiegelt. In Regionen, die weit von einem

gemessenen Datenpunktes ist das Modell nicht stark eingeschränkt, und die Modellunsicherheiten
Unsicherheiten zunehmen.

Für weitere Informationen über die Optionen in plt.fill_between (und die nahe

verwandte Funktion plt.fill), siehe den Docstring der Funktion oder die Dokumentation von Matplotlib.

Wenn Ihnen das zu wenig ist, lesen Sie bitte Kapitel 36, in dem wir

das Seaborn-Paket zu erörtern, das eine optimierte API für die Visualisierung dieser

Art der kontinuierlichen Fehlerleiste.

254 | Kapitel 27: Einfache Streudiagramme

KAPITEL 28

Dichte- und Konturdiagramme
Manchmal ist es sinnvoll, dreidimensionale Daten in zwei Dimensionen darzustellen, indem man
Konturen oder farbcodierten Regionen darzustellen. Es gibt drei Matplotlib-Funktionen, mit denen man

hilfreich für diese Aufgabe: plt.contour für Konturplots, plt.contourf für gefüllte Konturen

Plots und plt.imshow für die Darstellung von Bildern. In diesem Kapitel werden mehrere Beispiele für die
Verwendung dieser Programme. Wir beginnen mit dem Einrichten des Notizbuchs für das Plotten und importieren die

Funktionen, die wir verwenden werden:

In [1]: % matplotlib inline
import matplotlib.pyplot as plt
plt.style.use('seaborn-weiß')
import numpy as np

Visualisierung einer dreidimensionalen Funktion
Unser erstes Beispiel demonstriert ein Konturdiagramm mit einer Funktion z=fx,y, wobei die
f (wir haben dies bereits in Kapitel 8 gesehen, als wir es verwendet haben

als motivierendes Beispiel für Array Broadcasting):

In [2]: def f(x, y):
return np.sin(x) ** 10 + np.cos(10 + y * x) * np.cos(x)

Ein Konturdiagramm kann mit der Funktion plt.contour erstellt werden. Sie nimmt drei Argumente
Argumente: ein Raster von x-Werten, ein Raster von y-Werten und ein Raster von z-Werten. Die x- und y-Werte

stellen Positionen auf dem Diagramm dar, und die z-Werte werden durch die Kontur
Ebenen dargestellt. Der vielleicht einfachste Weg, solche Daten vorzubereiten, ist die Verwendung der

np.meshgrid-Funktion, die zweidimensionale Gitter aus eindimensionalen
Arrays erstellt:

In [3]: x = np.linspace(0, 5, 50)
y = np.linspace(0, 5, 40)

255
X, Y = np.meshgrid(x, y)
Z = f(X, Y)

Betrachten wir dies nun mit einem Standard-Konturdiagramm, das nur aus Linien besteht (siehe Abbildung 28-1).

In [4]: plt.contour(X, Y, Z, colors='black');

Abbildung 28-1. Visualisierung dreidimensionaler Daten mit Konturen

Beachten Sie, dass bei Verwendung einer einzigen Farbe negative Werte durch gestrichelte
Linien und positive Werte durch durchgezogene Linien dargestellt werden. Alternativ können die Linien auch farblich kodiert werden durch

indem wir mit dem Argument cmap eine Farbkarte angeben. Hier werden wir auch angeben, dass wir

weitere Linien, die in 20 gleichmäßigen Abständen innerhalb des Datenbereichs zu zeichnen sind, wie gezeigt

in Abbildung 28-2.

In [5]: plt.contour(X, Y, Z, 20, cmap='RdGy');

256 | Kapitel 28: Dichte- und Konturdiagramme

Abbildung 28-2. Visualisierung dreidimensionaler Daten mit farbigen Konturen

Hier haben wir die RdGy-Farbkarte (kurz für Red-Gray) gewählt, die eine gute Wahl ist für

divergente Daten: (d.h. Daten mit positiver und negativer Variation um den Nullpunkt). Matplot-
lib verfügt über eine breite Palette von Farbkarten, die Sie in IPython leicht durchsuchen können

indem Sie das Modul plt.cm mit einem Tabulator abschließen:

plt.cm.<TAB>
Unser Diagramm sieht nun besser aus, aber die Abstände zwischen den Zeilen können ein wenig ablenken.

Wir können dies ändern, indem wir zu einer gefüllten Konturdarstellung wechseln, indem wir die Funktion plt.contourf

Funktion, die weitgehend die gleiche Syntax wie plt.contour verwendet.

Außerdem fügen wir einen Befehl plt.colorbar hinzu, der eine zusätzliche Achse
mit beschrifteten Farbinformationen für das Diagramm erstellt (siehe Abbildung 28-3).

In [6]: plt.contourf(X, Y, Z, 20, cmap='RdGy')
plt.colorbar();

Visualisierung einer dreidimensionalen Funktion | 257
Abbildung 28-3. Visualisierung dreidimensionaler Daten mit gefüllten Konturen

Der Farbbalken verdeutlicht, dass die schwarzen Regionen "Spitzen" sind, während die roten Regionen

sind "Täler".

Ein mögliches Problem bei dieser Darstellung ist, dass sie etwas fleckig ist: Die Farbschritte sind diskret
und nicht kontinuierlich, was nicht immer erwünscht ist. Dies könnte behoben werden

indem man die Anzahl der Konturen auf eine sehr hohe Zahl setzt, aber dies führt zu einer eher
ineffiziente Darstellung: Matplotlib muss für jeden Schritt im Level ein neues Polygon rendern. A

Eine bessere Möglichkeit, eine glatte Darstellung zu erzeugen, ist die Verwendung der Funktion plt.imshow,

die das Argument der Interpolation bietet, um eine glatte zweidimensionale
Darstellung der Daten (siehe Abbildung 28-4).

In [7]: plt.imshow(Z, extent=[0, 5, 0, 5], origin='lower', cmap='RdGy',
interpolation='gaussian', aspect='equal')
plt.colorbar();

258 | Kapitel 28: Dichte- und Konturdiagramme

Abbildung 28-4. Darstellung von dreidimensionalen Daten als Bild

Es gibt jedoch einige potenzielle Probleme mit plt.imshow:

Es akzeptiert kein x- und y-Raster, daher müssen Sie die Ausdehnung [xmin,
xmax, ymin, ymax] des Bildes auf dem Plot angeben.
Standardmäßig folgt es der Standard-Bildarray-Definition, bei der der Ursprung in der
der Ursprung links oben und nicht wie bei den meisten Konturdiagrammen links unten liegt. Dies muss geändert werden
wenn Sie gerasterte Daten anzeigen.
Das Seitenverhältnis der Achsen wird automatisch an die Eingabedaten angepasst; dies kann
kann mit dem Argument aspect geändert werden.
Schließlich kann es manchmal nützlich sein, Konturdiagramme und Bilddiagramme zu kombinieren. Für

Beispiel wird hier ein teilweise transparentes Hintergrundbild verwendet (mit Transparenz

über den Alpha-Parameter eingestellt) und überlagern Sie die Konturen mit Beschriftungen auf den Konturen selbst.

selbst mit der Funktion plt.clabel (siehe Abbildung 28-5).

In [8]: Konturen = plt.contour(X, Y, Z, 3, colors='black')
plt.clabel(Konturen, inline= True , fontsize=8)

plt.imshow(Z, extent=[0, 5, 0, 5], origin='lower',
cmap='RdGy', alpha=0.5)
plt.colorbar();

Visualisierung einer dreidimensionalen Funktion | 259
Abbildung 28-5. Beschriftete Konturen über einem Bild

Die Kombination dieser drei Funktionen - plt.contour, plt.contourf und

plt.imshow bietet nahezu unbegrenzte Möglichkeiten zur Darstellung dieser Art von dreidimensionalen
dimensionalen Daten in einer zweidimensionalen Darstellung. Für weitere Informationen über die

Optionen, die in diesen Funktionen verfügbar sind, finden Sie in den entsprechenden Dokumentationen. Wenn Sie interessiert sind an

dreidimensionale Visualisierungen dieser Art von Daten, siehe Kapitel 35.

Histogramme, Binnings und Dichte
Ein einfaches Histogramm kann ein guter erster Schritt zum Verständnis eines Datensatzes sein. Vorhin haben wir
eine Vorschau auf die Histogramm-Funktion von Matplotlib (besprochen in Kapitel 9) gesehen, die

erstellt ein einfaches Histogramm in einer Zeile, nachdem die normalen Boilerplate-Importe durchgeführt wurden (siehe
Abbildung 28-6).

In [1]: % matplotlib inline
import numpy as np
import matplotlib.pyplot as plt
plt.style.use('seaborn-weiß')

rng = np.random.default_rng(1701)
Daten = rng.normal(size=1000)

In [2]: plt.hist(data);

260 | Kapitel 28: Dichte- und Konturdiagramme

Abbildung 28-6. Ein einfaches Histogramm

Die Funktion hist verfügt über viele Optionen, um sowohl die Berechnung als auch die Anzeige zu optimieren;
Ein Beispiel für ein angepasstes Histogramm finden Sie in Abbildung 28-7.

In [3]: plt.hist(data, bins=30, density= True , alpha=0.5,
histtype='stepfilled', color='steelblue',
edgecolor='none');

Abbildung 28-7. Ein angepasstes Histogramm

Histogramme, Binnings und Dichte | 261
1 Eine vollfarbige Version dieser Abbildung ist auf GitHub zu finden.
Das plt.hist docstring enthält weitere Informationen zu anderen verfügbaren Anpassungen

Optionen. Ich finde diese Kombination von histtype='stepfilled' zusammen mit einigen trans-

parency alpha beim Vergleich von Histogrammen verschiedener Verteilungen hilfreich sein (siehe

Abbildung 28-8).

In [4]: x1 = rng.normal(0, 0.8, 1000)
x2 = rng.normal(-2, 1, 1000)
x3 = rng.normal(3, 2, 1000)

kwargs = dict(histtype='stepfilled', alpha=0.3, density= True , bins=40)

plt.hist(x1, **kwargs)
plt.hist(x2, **kwargs)
plt.hist(x3, **kwargs);

Abbildung 28-8. Überplotten mehrerer Histogramme^1

Wenn Sie an der Berechnung, aber nicht an der Anzeige des Histogramms interessiert sind (d. h. an der Zähl- und

die Anzahl der Punkte in einem bestimmten Bin), können Sie die Funktion np.histogram verwenden:

In [5]: counts, bin_edges = np.histogram(data, bins=5)
print(counts)
Out[5]: [ 23 241 491 224 21]

262 | Kapitel 28: Dichte- und Konturdiagramme

Zweidimensionale Histogramme und Binnings
So wie wir Histogramme in einer Dimension erstellen, indem wir die Zahlenreihe in Bins unterteilen,

können wir auch zweidimensionale Histogramme erstellen, indem wir die Punkte auf zweidimensionale
dimensionalen Bins. Wir sehen uns kurz verschiedene Möglichkeiten an, dies zu tun. Beginnen wir mit der Definition...

ing einige Daten - ein x- und y-Array, das aus einer multivariaten Gauß-Verteilung gezogen wurde:

In [6]: mean = [0, 0]
cov = [[1, 1], [1, 2]]
x, y = rng.multivariate_normal(mean, cov, 10000).T

plt.hist2d: Zweidimensionales Histogramm
Eine einfache Möglichkeit, ein zweidimensionales Histogramm darzustellen, ist die Verwendung von Matplotlibs

plt.hist2d-Funktion (siehe Abbildung 28-9).

In [7]: plt.hist2d(x, y, bins=30)
cb = plt.colorbar()
cb.set_label('zählt in bin')

Abbildung 28-9. Ein zweidimensionales Histogramm mit plt.hist2d

Wie plt.hist verfügt auch plt.hist2d über eine Reihe zusätzlicher Optionen zur Feinabstimmung der Darstellung
und das Binning, die im Docstring der Funktion ausführlich beschrieben sind. Außerdem, genau wie

plt.hist hat ein Gegenstück in np.histogram, plt.hist2d hat ein Gegenstück in

np.histogram2d:

In [8]: counts, xedges, yedges = np.histogram2d(x, y, bins=30)
print(counts.shape)
Out[8]: (30, 30)

Zweidimensionale Histogramme und Binnings | 263
Für die Verallgemeinerung dieses Histogramm-Binnings, wenn es mehr als zwei

Dimensionen, siehe die Funktion np.histogramdd.

plt.hexbin: Sechseckige Binnings
Das zweidimensionale Histogramm erzeugt eine Tesselierung von Quadraten über die Achsen.
Eine andere natürliche Form für eine solche Tesselierung ist das regelmäßige Sechseck. Zu diesem Zweck,

Matplotlib bietet die Routine plt.hexbin, die einen zweidimensionalen
Datensatz in einem Gitter aus Sechsecken darstellt (siehe Abbildung 28-10).

In [9]: plt.hexbin(x, y, gridsize=30)
cb = plt.colorbar(label='count in bin')

Abbildung 28-10. Ein zweidimensionales Histogramm mit plt.hexbin

plt.hexbin bietet eine Reihe zusätzlicher Optionen, darunter die Möglichkeit, Folgendes anzugeben

Gewichte für jeden Punkt und um die Ausgabe in jedem Bin in ein beliebiges NumPy-Aggregat zu ändern

(Mittelwert der Gewichte, Standardabweichung der Gewichte usw.).

Kernel-Dichte-Schätzung
Eine weitere gängige Methode zur Schätzung und Darstellung von Dichten in mehreren

Dimensionen ist die Kernel-Dichte-Schätzung (KDE). Dies wird in Kapitel 49 ausführlicher besprochen.
Kapitel 49 ausführlicher behandelt, aber für den Moment möchte ich nur erwähnen, dass KDE als eine Möglichkeit angesehen werden kann, um

die Punkte im Raum "auswischen" und das Ergebnis addieren, um eine glatte Funktion zu erhalten.

Eine extrem schnelle und einfache KDE-Implementierung gibt es in der Datei scipy.stats

Paket. Hier ist ein kurzes Beispiel für die Verwendung von KDE (siehe Abbildung 28-11).

264 | Kapitel 28: Dichte- und Konturdiagramme

In [10]: from scipy.stats import gaussian_kde

# ein Array der Größe [Ndim, Nsamples] einpassen
Daten = np.vstack([x, y])
kde = gaussian_kde(Daten)

# Auswertung auf einem regelmäßigen Gitter
xgrid = np.linspace(-3.5, 3.5, 40)
ygrid = np.linspace(-6, 6, 40)
Xgrid, Ygrid = np.meshgrid(xgrid, ygrid)
Z = kde.evaluate(np.vstack([Xgrid.ravel(), Ygrid.ravel()])

# Plotten Sie das Ergebnis als Bild
plt.imshow(Z.reshape(Xgrid.shape),
origin='lower', aspect='auto',
extent=[-3.5, 3.5, -6, 6])
cb = plt.colorbar()
cb.set_label("Dichte")

Abbildung 28-11. Darstellung einer Kernel-Dichte einer Verteilung

KDE hat eine Glättungslänge, die effektiv den Regler zwischen Detail und

Glattheit (ein Beispiel für den allgegenwärtigen Kompromiss zwischen Verzerrung und Varianz). Die Literatur

zur Wahl einer geeigneten Glättungslänge ist enorm; gaussian_kde verwendet eine Regel von

Daumen, um zu versuchen, eine nahezu optimale Glättungslänge für die Eingabedaten zu finden.

Andere KDE-Implementierungen sind innerhalb des SciPy-Ökosystems verfügbar, jede mit ihrer

eigene Stärken und Schwächen; siehe z.B. sklearn.neighbors.KernelDensity

und statsmodels.nonparametric.KDEMultivariate.

Zweidimensionale Histogramme und Binnings | 265
Für Visualisierungen, die auf KDE basieren, ist die Verwendung von Matplotlib tendenziell übermäßig langwierig. Die

Die Seaborn-Bibliothek, die in Kapitel 36 besprochen wird, bietet eine viel kompaktere API für die Erstellung
Erstellung von KDE-basierten Visualisierungen.

266 | Kapitel 28: Dichte- und Konturdiagramme

KAPITEL 29

Plot-Legenden anpassen
Darstellungslegenden verleihen einer Visualisierung Bedeutung, indem sie den verschiedenen Darstellungselementen
Elementen zu. Wir haben bereits gesehen, wie man eine einfache Legende erstellt; hier sehen wir uns Folgendes an

Anpassen der Platzierung und Ästhetik der Legende in Matplotlib.

Die einfachste Legende kann mit dem Befehl plt.legend erstellt werden, der automatisch

erstellt eine Legende für alle beschrifteten Plot-Elemente (siehe Abbildung 29-1).

In [1]: import matplotlib.pyplot as plt
plt.style.use('seaborn-whitegrid')

In [2]: % matplotlib inline
import numpy as np

In [3]: x = np.linspace(0, 10, 1000)
fig, ax = plt.subplots()
ax.plot(x, np.sin(x), '-b', label='Sinus')
ax.plot(x, np.cos(x), '--r', label='Cosinus')
ax.axis('gleich')
leg = ax.legend()

267
Abbildung 29-1. Eine Standard-Plotlegende

Es gibt jedoch viele Möglichkeiten, eine solche Legende anzupassen. Zum Beispiel können wir

können Sie den Standort festlegen und den Rahmen einschalten (siehe Abbildung 29-2).

In [4]: ax.legend(loc='upper left', frameon= True )
Abb.

Abbildung 29-2. Eine angepasste Diagrammlegende

268 | Kapitel 29: Anpassen von Plot-Legenden

Mit dem Befehl ncol können Sie die Anzahl der Spalten in der Legende angeben, wie
in Abbildung 29-3 gezeigt.

In [5]: ax.legend(loc='lower center', ncol=2)
Abb.

Abbildung 29-3. Eine zweispaltige Diagrammlegende

Und wir können einen abgerundeten Rahmen (Fancybox) verwenden oder einen Schatten hinzufügen, die Transparenz
(Alphawert) des Rahmens oder die Auffüllung des Textes ändern (siehe Abbildung 29-4).

In [6]: ax.legend(frameon= True , fancybox= True , framealpha=1,
shadow= True , borderpad=1)
Abb.

Weitere Informationen zu den verfügbaren Legendenoptionen finden Sie im plt.legend docstring.

Anpassen von Darstellungslegenden | 269
Abbildung 29-4. Eine Fancybox-Plotlegende

Auswählen von Elementen für die Legende
Wie wir bereits gesehen haben, enthält die Legende standardmäßig alle beschrifteten Elemente aus der

Darstellung. Wenn dies nicht erwünscht ist, können wir die Anzeige der Elemente und Beschriftungen feinabstimmen

in der Legende mit Hilfe der von Plot-Befehlen zurückgegebenen Objekte. plt.plot ist in der Lage

erstellt mehrere Zeilen auf einmal und gibt eine Liste der erstellten Zeileninstanzen zurück. Die Übergabe einer der

plt.legend die zu identifizierenden Elemente und die Beschriftungen mitteilt, die wir in der

angeben (siehe Abbildung 29-5).

In [7]: y = np.sin(x[:, np.newaxis] + np.pi * np.arange(0, 2, 0.5))
lines = plt.plot(x, y)

# lines ist eine Liste von plt.Line2D-Instanzen
plt.legend(lines[:2], ['first', 'second'], frameon= True );

270 | Kapitel 29: Anpassen von Plot-Legenden

1 Eine vollfarbige Version dieser Abbildung ist auf GitHub zu finden.
Abbildung 29-5. Anpassen von Legendenelementen^1

In der Praxis hat sich gezeigt, dass es klarer ist, die erste Methode zu verwenden, bei der die Beschriftungen auf
die Sie in der Legende darstellen möchten (siehe Abbildung 29-6).

In [8]: plt.plot(x, y[:, 0], label='erste')
plt.plot(x, y[:, 1], label='zweite')
plt.plot(x, y[:, 2:])
plt.legend(frameon= True );

Beachten Sie, dass die Legende alle Elemente ignoriert, für die kein Label-Attribut festgelegt wurde.

Auswählen von Elementen für die Legende | 271
2 Eine vollfarbige Version dieser Abbildung ist auf GitHub zu finden.
Abbildung 29-6. Alternative Methode zum Anpassen von Legendenelementen^2

Legende für die Größe der Punkte
Manchmal sind die Legendenvorgaben für die gegebene Visualisierung nicht ausreichend. Zum Beispiel...

Vielleicht verwenden Sie die Größe der Punkte, um bestimmte Merkmale der Daten zu markieren, und
Sie möchten eine Legende erstellen, die dies widerspiegelt. Hier ein Beispiel, bei dem die Größe von

Punkte, um die Einwohnerzahlen der kalifornischen Städte anzuzeigen. Wir hätten gerne eine Legende, die die folgenden Punkte angibt

Skalierung der Punktgrößen, und wir erreichen dies, indem wir einige beschriftete Daten
ohne Einträge (siehe Abbildung 29-7).

In [9]: _# Uncomment to download the data

url = ('https://raw.githubusercontent.com/jakevdp/
PythonDataScienceHandbook/''master/notebooks/data/
california_cities.csv')
!cd data && curl -O {url}_
In [10]: import pandas as pd
Städte = pd.read_csv('data/california_cities.csv')

# Extrahieren der Daten, die uns interessieren
lat, lon = cities['latd'], cities['longd']
population, area = cities['population_total'], cities['area_total_km2']

# Streuung der Punkte, mit Größe und Farbe, aber ohne Beschriftung
plt.scatter(lon, lat, label= None ,
c=np.log10(population), cmap='viridis',
s=Fläche, linewidth=0, alpha=0.5)

272 | Kapitel 29: Anpassen von Plot-Legenden

plt.axis('gleich')
plt.xlabel('Längengrad')
plt.ylabel('Breitengrad')
plt.colorbar(label='log$_{10}$(Bevölkerung)')
plt.clim(3, 7)

_# Hier erstellen wir eine Legende:

wir zeichnen leere Listen mit der gewünschten Größe und Beschriftung_
für Bereich in [100, 300, 500]:
plt.scatter([], [], c='k', alpha=0.3, s=area,
label=str(area) + ' km$^2$')
plt.legend(scatterpoints=1, frameon= False , labelspacing=1,
title='Stadtgebiet')

plt.title('California Cities: Area and Population');

Abbildung 29-7. Lage, geografische Größe und Einwohnerzahl der kalifornischen Städte

Die Legende bezieht sich immer auf ein Objekt, das sich auf dem Plot befindet.
eine bestimmte Form anzeigen möchten, müssen wir sie plotten. In diesem Fall werden die gewünschten Objekte (graue Kreise)

cles) sind nicht auf dem Plot, also täuschen wir sie vor, indem wir leere Listen plotten. Erinnern Sie sich, dass die
Legende nur Plot-Elemente auflistet, für die eine Beschriftung angegeben wurde.

Durch das Plotten von leeren Listen werden beschriftete Plotobjekte erstellt, die von der Legende aufgegriffen werden,

und jetzt sagt uns unsere Legende einige nützliche Informationen. Diese Strategie kann nützlich sein für
anspruchsvollere Visualisierungen zu erstellen.

Legende für die Größe der Punkte | 273
Mehrere Legenden
Bei der Erstellung eines Diagramms möchten Sie manchmal mehrere Legenden zu denselben Achsen hinzufügen.

Leider macht Matplotlib dies nicht einfach: über die Standard-Legendenschnittstelle,

ist es nur möglich, eine einzige Legende für das gesamte Diagramm zu erstellen. Wenn Sie versuchen, eine

Wenn Sie eine zweite Legende mit plt.legend oder ax.legend erstellen, wird die erste Legende einfach überschrieben.

Wir können dieses Problem umgehen, indem wir eine neue Legende Künstler von Grund auf neu erstellen (Künstler ist die

Basisklasse, die Matplotlib für visuelle Attribute verwendet), und dann mit der untergeordneten

ax.add_artist, um den zweiten Künstler manuell zum Diagramm hinzuzufügen (siehe
Abbildung 29-8).

In [11]: fig, ax = plt.subplots()

Zeilen = []
styles = ['-', '--', '-.', ':']
x = np.linspace(0, 10, 1000)

for i in range(4):
lines += ax.plot(x, np.sin(x - i * np.pi / 2),
styles[i], color='black')
ax.axis('equal')

# Zeilen und Beschriftungen der ersten Legende festlegen
ax.legend(lines[:2], ['line A', 'line B'], loc='upper right')

# Erstellen Sie die zweite Legende und fügen Sie den Künstler manuell hinzu
from matplotlib.legend import Legende
leg = Legend(ax, lines[2:], ['line C', 'line D'], loc='lower right')
ax.add_artist(leg);

Dies ist ein Einblick in die Low-Level-Künstler-Objekte, die jede Matplotlib-Plot enthalten. Wenn

untersuchen Sie den Quellcode von ax.legend (Sie erinnern sich, dass Sie dies mit innerhalb von

das Jupyter-Notebook mit ax.legend??) werden Sie sehen, dass die Funktion einfach aus

einer Logik, um einen geeigneten Legendenkünstler zu erstellen, der dann in der legend_ gespeichert wird

Attribut und wird der Abbildung hinzugefügt, wenn die Darstellung gezeichnet wird.

274 | Kapitel 29: Anpassen von Plot-Legenden

Abbildung 29-8. Legende einer geteilten Darstellung

Mehrere Legenden | 275
KAPITEL 30

Farbbalken anpassen
Diagrammlegenden kennzeichnen diskrete Beschriftungen diskreter Punkte. Für kontinuierliche Beschriftungen, die auf
der Farbe von Punkten, Linien oder Regionen, kann ein beschrifteter Farbbalken ein hervorragendes Werkzeug sein. In Mat-

plotlib wird ein Farbbalken als separate Achse gezeichnet, der einen Schlüssel für die Bedeutung der Farbe liefern kann

von Farben in einer Handlung. Da das Buch in schwarz-weiß gedruckt ist, gibt es zu diesem Kapitel eine
zu diesem Kapitel eine Online-Ergänzung, in der Sie die Abbildungen in voller Farbe sehen können. Wir werden

Beginnen Sie damit, das Notebook für das Plotten einzurichten und die Funktionen zu importieren, die wir verwenden werden:

In [1]: import matplotlib.pyplot as plt
plt.style.use('seaborn-white')

In [2]: % matplotlib inline
import numpy as np

Wie wir bereits mehrfach gesehen haben, kann der einfachste Farbbalken mit der

plt.colorbar Funktion (siehe Abbildung 30-1).

In [3]: x = np.linspace(0, 10, 1000)
I = np.sin(x) * np.cos(x[:, np.newaxis])

plt.imshow(I)
plt.colorbar();

Vollfarbige Abbildungen sind in den ergänzenden Materialien auf
GitHub.
Wir werden nun einige Ideen für die Anpassung dieser Farbbalken und ihre effektive Nutzung diskutieren.

in verschiedenen Situationen.

276
Abbildung 30-1. Eine einfache Farbbalkenlegende

Farbbalken anpassen
Die Farbkarte kann mit dem cmap-Argument der Plotting-Funktion angegeben werden, die

wird die Visualisierung erstellt (siehe Abbildung 30-2).

In [4]: plt.imshow(I, cmap='Blues');

Abbildung 30-2. Eine Farbkarte mit blauer Skala

Farbbalken anpassen | 277
Die Namen der verfügbaren Farbkarten befinden sich im plt.cm-Namensraum; mit IPythons Tabulator
Vervollständigungsfunktion von IPython erhalten Sie eine vollständige Liste der eingebauten Möglichkeiten:

plt.cm.<TAB>
Aber die Auswahl einer Farbkarte ist nur der erste Schritt: Wichtiger ist, wie man

Entscheiden Sie sich zwischen den Möglichkeiten! Die Wahl ist viel subtiler, als man zunächst
als Sie vielleicht zunächst erwarten.

Auswahl der Farbkarte
Eine vollständige Behandlung der Farbauswahl in Visualisierungen würde den Rahmen dieses Buches sprengen,
aber unterhaltsame Lektüre zu diesem und anderen Themen finden Sie in dem Artikel "Zehn einfache

Regeln für bessere Zahlen" von Nicholas Rougier, Michael Droettboom und Philip

Bourne. In der Online-Dokumentation von Matplotlib gibt es auch eine interessante Diskussion über die Wahl der Farb- und
ormap-Auswahl.

Im Großen und Ganzen sollten Sie sich über drei verschiedene Kategorien von Farbkarten im Klaren sein:

Sequentielle Farbkarten

Diese bestehen aus einer fortlaufenden Folge von Farben (z. B. binär oder
viridis).
Divergierende Farbkarten

Diese enthalten in der Regel zwei verschiedene Farben, die positive und negative Abweichungen
Abweichungen von einem Mittelwert anzeigen (z. B. RdBu oder PuOr).
Qualitative Farbkarten

Diese mischen Farben ohne bestimmte Reihenfolge (z. B. Regenbogen oder Strahl).

Die Jet-Farbkarte, die in Matplotlib vor Version 2.0 die Standardeinstellung war, ist eine

Beispiel für eine qualitative Farbkarte. Ihr Status als Standard war ziemlich unglücklich,
denn qualitative Karten sind oft eine schlechte Wahl für die Darstellung quantitativer Daten.

Zu den Problemen gehört die Tatsache, dass qualitative Karten in der Regel keine einheitli-

eine Zunahme der Helligkeit mit zunehmender Skala.

Wir können dies sehen, indem wir den Farbbalken des Strahls in Schwarz und Weiß umwandeln (siehe Abbildung 30-3).

In [5]: from matplotlib.colors import LinearSegmentedColormap

def grayscale_cmap(cmap):
"""Gibt eine Graustufenversion der angegebenen Farbkarte zurück"""
cmap = plt.cm.get_cmap(cmap)
colors = cmap(np.arange(cmap.N))

RGBA in wahrgenommene Graustufenluminanz umwandeln

vgl. http://alienryderflex.com/hsp.html_
RGB_Gewicht = [0,299, 0,587, 0,114]
Leuchtdichte = np.sqrt(np.dot(colors[:, :3] ** 2, RGB_weight))

278 | Kapitel 30: Farbbalken anpassen

colors[:, :3] = luminance[:, np.newaxis]

return LinearSegmentedColormap.from_list(
cmap.name + "_gray", colors, cmap.N)

def view_colormap(cmap):
"""Plotten einer Farbkarte mit ihrem Graustufen-Äquivalent"""
cmap = plt.cm.get_cmap(cmap)
colors = cmap(np.arange(cmap.N))

cmap = grayscale_cmap(cmap)
grayscale = cmap(np.arange(cmap.N))

fig, ax = plt.subplots(2, figsize=(6, 2),
subplot_kw=dict(xticks=[], yticks=[]))
ax[0].imshow([Farben], extent=[0, 10, 0, 1])
ax[1].imshow([grayscale], extent=[0, 10, 0, 1])

In [6]: view_colormap('jet')

Abbildung 30-3. Die Jet-Farbkarte und ihre ungleichmäßige Leuchtdichteskala

Beachten Sie die hellen Streifen auf dem Graustufenbild. Selbst in Vollfarbe bedeutet diese ungleichmäßige Helligkeit
bedeutet diese ungleichmäßige Helligkeit, dass das Auge auf bestimmte Teile des Farbbereichs gelenkt wird, die

wird möglicherweise unwichtige Teile des Datensatzes hervorheben. Es ist besser, eine Farb-

Karte wie viridis (die Standardeinstellung ab Matplotlib 2.0), die speziell für die Erstellung von

Helligkeitsschwankungen über den gesamten Bereich zu haben; daher spielt es nicht nur gut mit unserer
mit unserer Farbwahrnehmung, sondern lässt sich auch gut in Graustufen drucken (siehe

Abbildung 30-4).

In [7]: view_colormap('viridis')

Farbbalken anpassen | 279
Abbildung 30-4. Die viridis-Farbkarte und ihre gleichmäßige Luminanzskala

Für andere Situationen, wie z. B. das Aufzeigen positiver und negativer Abweichungen von bestimmten

bedeuten, sind zweifarbige Farbbalken wie RdBu (Rot-Blau) hilfreich. Sie können jedoch

Abbildung 30-5 zu sehen ist, ist es wichtig zu beachten, dass die positiven/negativen Informationen

bei der Übersetzung in Graustufen verloren!

In [8]: view_colormap('RdBu')

Abbildung 30-5. Die RdBu-Farbkarte und ihre Leuchtdichte

Wir werden im weiteren Verlauf Beispiele für die Verwendung einiger dieser Farbkarten sehen.

Es gibt eine große Anzahl von Farbkarten, die in Matplotlib verfügbar sind; hier finden Sie eine Liste davon,

können Sie IPython verwenden, um das Submodul plt.cm zu erkunden. Für eine prinzipiellere

Ansatz für Farben in Python zu finden, können Sie die Werkzeuge und die Dokumentation der
der Seaborn-Bibliothek (siehe Kapitel 36).

Farbbegrenzungen und Erweiterungen
Matplotlib erlaubt eine große Bandbreite an Farbbalkenanpassungen. Der Farbbalken selbst ist

einfach eine Instanz von plt.Axes, so dass alle Achsen und Tick-Formatierung Tricks, die wir gesehen haben
bisher gesehen haben, anwendbar sind. Der Farbbalken hat eine interessante Flexibilität: Zum Beispiel können wir

kann die Farbgrenzen eingrenzen und die Werte, die außerhalb der Grenzen liegen, mit einem dreieckigen

Pfeil am oberen und unteren Rand durch Setzen der Eigenschaft extend. Dies könnte in

Dies ist z. B. praktisch, wenn ein Bild mit Rauschen angezeigt werden soll (siehe Abbildung 30-6).

In [9]: # Rauschen in 1% der Bildpixel erzeugen
speckles = (np.random.random(I.shape) < 0.01)
I[speckles] = np.random.normal(0, 3, np.count_nonzero(speckles))

280 | Kapitel 30: Farbbalken anpassen

1 Eine Version dieser Abbildung in voller Größe ist auf GitHub zu finden.
plt.figure(figsize=(10, 3.5))

plt.subplot(1, 2, 1)
plt.imshow(I, cmap='RdBu')
plt.colorbar()

plt.subplot(1, 2, 2)
plt.imshow(I, cmap='RdBu')
plt.colorbar(extend='both')
plt.clim(-1, 1)

Abbildung 30-6. Festlegen von Farbkartenerweiterungen^1

Beachten Sie, dass im linken Bereich die Standardfarbgrenzen auf die verrauschten Pixel reagieren, und

der Bereich des Rauschens verwischt das Muster, an dem wir interessiert sind, vollständig. In der
rechten Feld legen wir die Farbgrenzen manuell fest und fügen Erweiterungen hinzu, um Werte anzuzeigen

die über oder unter diesen Grenzwerten liegen. Das Ergebnis ist eine viel nützlichere Visualisierung der
unserer Daten.

Diskrete Farbbalken
Farbkarten sind standardmäßig kontinuierlich, aber manchmal möchte man diskrete Farben darstellen.

Werte. Der einfachste Weg, dies zu tun, ist die Verwendung der Funktion plt.cm.get_cmap und die Übergabe der

Name einer geeigneten Farbkarte und die Anzahl der gewünschten Bins (siehe Abbildung 30-7).

In [10]: plt.imshow(I, cmap=plt.cm.get_cmap('Blues', 6))
plt.colorbar(extend='both')
plt.clim(-1, 1);

Farbbalken anpassen | 281
Abbildung 30-7. Eine diskretisierte Farbkarte

Die diskrete Version einer Farbkarte kann genau wie jede andere Farbkarte verwendet werden.

Beispiel: Handgeschriebene Ziffern
Als Beispiel dafür, wo dies angewendet werden kann, sehen wir uns eine interessante Visualisierung von
einiger handgeschriebener Ziffern aus dem Digits-Datensatz, der in Scikit-Learn enthalten ist; sie besteht aus

von fast 2.000 8 × 8 Miniaturbildern, die verschiedene handschriftliche Ziffern zeigen.

Beginnen wir mit dem Herunterladen des Digits-Datensatzes und der Visualisierung einiger der

Beispielbilder mit plt.imshow (siehe Abbildung 30-8).

In [11]: # lade Bilder der Ziffern 0 bis 5 und visualisiere mehrere von ihnen
from sklearn.datasets import load_digits
Ziffern = load_digits(n_class=6)

fig, ax = plt.subplots(8, 8, figsize=(6, 6))
for i, axi in enumerate(ax.flat):
axi.imshow(digits.images[i], cmap='binary')
axi.set(xticks=[], yticks=[])

282 | Kapitel 30: Farbbalken anpassen

Abbildung 30-8. Beispiel für handgeschriebene Zifferndaten

Da jede Ziffer durch den Farbton ihrer 64 Pixel definiert ist, können wir jede Ziffer als

ein Punkt im 64-dimensionalen Raum sein: jede Dimension stellt die Helligkeit eines
eines Pixels. Die Visualisierung solch hochdimensionaler Daten kann schwierig sein, aber eine Möglichkeit ist es

Diese Aufgabe lässt sich am besten mit einem Verfahren zur Dimensionalitätsreduzierung wie dem

Lernens, um die Dimensionalität der Daten zu reduzieren und gleichzeitig die Beziehungen
von Interesse. Die Dimensionalitätsreduktion ist ein Beispiel für unüberwachtes maschinelles Lernen.

Wir werden sie in Kapitel 37 ausführlicher behandeln.

Lassen wir die Diskussion dieser Details beiseite und schauen wir uns ein zweidimensionales Mani-

Fold-Learning-Projektion der Zifferndaten (Einzelheiten siehe Kapitel 46):

In [12]: # Projektion der Ziffern in 2 Dimensionen mit Isomap
from sklearn.manifold import Isomap
iso = Isomap(n_Komponenten=2, n_Nachbarn=15)
projection = iso.fit_transform(ziffern.daten)

Beispiel: Handgeschriebene Ziffern | 283
Wir verwenden unsere diskrete Farbkarte, um die Ergebnisse zu sehen, und setzen die Ticks und Clim, um
um die Ästhetik des resultierenden Farbbalkens zu verbessern (siehe Abbildung 30-9).

In [13]: # Plotten der Ergebnisse
plt.scatter(projektion[:, 0], projektion[:, 1], lw=0.1,
c=digits.target, cmap=plt.cm.get_cmap('plasma', 6))
plt.colorbar(ticks=range(6), label='Stellenwert')
plt.clim(-0.5, 5.5)

Abbildung 30-9. Vielfältige Einbettung von Pixeln handgeschriebener Ziffern

Die Projektion gibt uns auch einige Einblicke in die Beziehungen innerhalb des Datensatzes: zum Beispiel
zum Beispiel überschneiden sich die Bereiche von 2 und 3 in dieser Projektion fast, was darauf hinweist, dass einige

Handgeschriebene 2er und 3er sind schwer zu unterscheiden und können eher als Betrug gewertet werden.

durch einen automatischen Klassifizierungsalgorithmus fusioniert. Andere Werte, wie 0 und 1, sind weiter
weiter voneinander entfernt und können weniger leicht verwechselt werden.

Wir werden in Teil V auf das vielfältige Lernen und die Klassifizierung von Zahlen zurückkommen.

284 | Kapitel 30: Farbbalken anpassen

KAPITEL 31

Mehrere Teilplots
Manchmal ist es hilfreich, verschiedene Ansichten von Daten nebeneinander zu vergleichen. Zu diesem Zweck,
Matplotlib das Konzept der Subplots: Gruppen von kleineren Achsen, die zusammen existieren können

innerhalb einer einzigen Abbildung. Bei diesen Teilflächen kann es sich um Einschübe, Raster von Flächen oder andere

komplizierte Layouts. In diesem Kapitel werden wir vier Routinen zur Erstellung von Subplots
in Matplotlib. Wir beginnen mit dem Importieren der Pakete, die wir verwenden werden:

In [1]: % matplotlib inline
import matplotlib.pyplot as plt
plt.style.use('seaborn-weiß')
import numpy as np

plt.axes: Subplots von Hand
Die einfachste Methode zur Erstellung von Achsen ist die Verwendung der Funktion plt.axes. Wie wir
gesehen haben, wird damit standardmäßig ein Standard-Achsenobjekt erstellt, das die gesamte Abbildung ausfüllt.

plt.axes nimmt auch ein optionales Argument entgegen, das eine Liste von vier Zahlen in der

Koordinatensystem der Abbildung ([ links , unten , Breite , Höhe ]), das von 0
unten links in der Abbildung bis 1 oben rechts in der Abbildung reicht.

Wir könnten zum Beispiel eine Einschubachse in der oberen rechten Ecke einer anderen Achse erstellen, indem wir
die x- und y-Position auf 0,65 setzen (d. h., wir beginnen bei 65 % der Breite und 65 % der

die Höhe der Figur) und die x- und y-Ausdehnung auf 0,2 (d. h. die Größe der Achsen ist

20% der Breite und 20% der Höhe der Abbildung). Abbildung 31-1 zeigt das Ergebnis:

In [2]: ax1 = plt.axes() # Standard-Achsen
ax2 = plt.axes([0.65, 0.65, 0.2, 0.2])

285
Abbildung 31-1. Beispiel für einen Achseneinschub

Die Entsprechung dieses Befehls in der objektorientierten Schnittstelle lautet

fig.add_axes. Verwenden wir dies, um zwei vertikal gestapelte Achsen zu erstellen, wie sie in

Abbildung 31-2.

In [3]: fig = plt.figure()
ax1 = fig.add_axes([0.1, 0.5, 0.8, 0.4],
xticklabels=[], ylim=(-1.2, 1.2))
ax2 = fig.add_axes([0.1, 0.1, 0.8, 0.4],
ylim=(-1.2, 1.2))

x = np.linspace(0, 10)
ax1.plot(np.sin(x))
ax2.plot(np.cos(x));

Wir haben jetzt zwei Achsen (die obere ohne Tick-Labels), die sich gerade berühren: die untere

des oberen Feldes (an Position 0,5) mit dem oberen Ende des unteren Feldes (an Position
0.1 + 0.4).

286 | Kapitel 31: Mehrere Nebenhandlungen

Abbildung 31-2. Beispiel für vertikal gestapelte Achsen

plt.subplot: Einfache Raster von Subplots
Ausgerichtete Spalten oder Zeilen von Subplots sind so häufig erforderlich, dass Matplotlib
Matplotlib über mehrere Routinen verfügt, mit denen sie leicht zu erstellen sind. Die unterste Ebene dieser Routinen

ist plt.subplot, das einen einzelnen Teilplot innerhalb eines Gitters erstellt. Wie Sie sehen können, benötigt dieser
Befehl drei ganzzahlige Argumente - die Anzahl der Zeilen, die Anzahl der Spalten

und den Index des zu erstellenden Plots in diesem Schema, das von links oben nach rechts unten verläuft
von oben links nach unten rechts (siehe Abbildung 31-3).

In [4]: for i in range(1, 7):
plt.subplot(2, 3, i)
plt.text(0.5, 0.5, str((2, 3, i)),
fontsize=18, ha='center')

plt.subplot: Einfache Raster von Teilplots | 287
Abbildung 31-3. Ein plt.subplot-Beispiel

Mit dem Befehl plt.subplots_adjust können die Abstände zwischen
diesen Plots anzupassen. Der folgende Code verwendet den entsprechenden objektorientierten Befehl,

fig.add_subplot; Abbildung 31-4 zeigt das Ergebnis:

In [5]: fig = plt.figure()
fig.subplots_adjust(hspace=0.4, wspace=0.4)
for i in range(1, 7):
ax = fig.add_subplot(2, 3, i)
ax.text(0.5, 0.5, str((2, 3, i)),
fontsize=18, ha='center')

Hier haben wir die Argumente hspace und wspace von plt.subplots_adjust verwendet, die

geben Sie den Abstand entlang der Höhe und Breite der Abbildung an, und zwar in Einheiten der Größe der Teilfläche
Größe (in diesem Fall beträgt der Abstand 40 % der Breite und Höhe der Teilfläche).

288 | Kapitel 31: Mehrere Nebenhandlungen

Abbildung 31-4. plt.subplot mit angepassten Rändern

plt.subplots: Das gesamte Raster in einem Durchgang
Der soeben beschriebene Ansatz wird schnell mühsam, wenn Sie ein großes Raster von
Subplots zu erstellen, insbesondere wenn Sie die Beschriftungen der x- und y-Achse in den inneren Plots ausblenden möchten. Für

plt.subplots ist für diesen Zweck das einfachere Werkzeug (beachten Sie das s am Ende von

Teilflächen). Anstatt einen einzelnen Subplot zu erstellen, erstellt diese Funktion ein komplettes Raster von
Subplots in einer einzigen Zeile und gibt sie in einem NumPy-Array zurück. Die Argumente sind die

Anzahl der Zeilen und Anzahl der Spalten, zusammen mit den optionalen Schlüsselwörtern sharex und

sharey, mit denen Sie die Beziehungen zwischen verschiedenen Achsen festlegen können.

Erstellen wir ein 2 × 3-Gitter von Teilbildern, bei dem alle Achsen in derselben Zeile ihre y-Achse teilen

Skala, und alle Achsen in derselben Spalte haben die gleiche Skala für die x-Achse (siehe Abbildung 31-5).

In [6]: fig, ax = plt.subplots(2, 3, sharex='col', sharey='row')

plt.subplots: Das gesamte Raster in einem Durchgang | 289
Abbildung 31-5. Gemeinsame x- und y-Achsen in plt.subplots

Durch die Angabe von sharex und sharey haben wir automatisch die inneren Beschriftungen auf dem
Gitter entfernt, um das Diagramm übersichtlicher zu gestalten. Das resultierende Achsengitter wird in einer

NumPy-Array, das eine bequeme Spezifikation der gewünschten Achsen unter Verwendung von Standard

Array-Indexierungsschreibweise (siehe Abbildung 31-6).

In [7]: # Achsen sind in einem zweidimensionalen Array, indiziert durch [row, col]
for i in range(2):
for j in range(3):
ax[i, j].text(0.5, 0.5, str((i, j)),
fontsize=18, ha='center')
Abb.

Im Vergleich zu plt.subplot ist plt.subplots konsistenter mit Pythons Kon-

ventionalen nullbasierten Indizierung, während plt.subplot die MATLAB-übliche einbasierte

Indizierung.

290 | Kapitel 31: Mehrere Nebenhandlungen

Abbildung 31-6. Identifizierung von Plots in einem Subplot-Raster

plt.GridSpec: Kompliziertere Anordnungen
Um über ein normales Raster hinauszugehen und Teilflächen zu erstellen, die sich über mehrere Zeilen und Spalten erstrecken,

plt.GridSpec ist das beste Werkzeug. plt.GridSpec erstellt nicht selbst einen Plot; es ist

sondern eine bequeme Schnittstelle, die von dem Befehl plt.subplot erkannt wird. Für

Beispiel eine GridSpec für ein Gitter mit zwei Zeilen und drei Spalten mit bestimmten

Der Raum für Breite und Höhe sieht wie folgt aus:

In [8]: grid = plt.GridSpec(2, 3, wspace=0.4, hspace=0.3)

Von hier aus können wir die Positionen und Ausdehnungen der Unterdiagramme mit der bekannten Python-Slicing-Syntax
Syntax angeben (siehe Abbildung 31-7).

In [9]: plt.subplot(grid[0, 0])
plt.subplot(raster[0, 1:])
plt.subplot(raster[1, :2])
plt.subplot(grid[1, 2]);

plt.GridSpec: Kompliziertere Anordnungen | 291
Abbildung 31-7. Unregelmäßige Teilplots mit plt.GridSpec

Diese Art der flexiblen Gitterausrichtung ist sehr vielseitig einsetzbar. Ich verwende sie am häufigsten, wenn ich
bei der Erstellung von Histogrammen mit mehreren Achsen, wie in Abbildung 31-8 gezeigt.

In [10]: # Erstellen Sie einige normalverteilte Daten
mean = [0, 0]
cov = [[1, 1], [1, 2]]
rng = np.random.default_rng(1701)
x, y = rng.multivariate_normal(mean, cov, 3000).T

# Einrichten der Achsen mit GridSpec
fig = plt.figure(figsize=(6, 6))
grid = plt.GridSpec(4, 4, hspace=0.2, wspace=0.2)
main_ax = fig.add_subplot(grid[:-1, 1:])
y_hist = fig.add_subplot(grid[:-1, 0], xticklabels=[], sharey=main_ax)
x_hist = fig.add_subplot(grid[-1, 1:], yticklabels=[], sharex=main_ax)

# Streupunkte auf den Hauptachsen
main_ax.plot(x, y, 'ok', markersize=3, alpha=0.2)

# Histogramm auf den angehängten Achsen
x_hist.hist(x, 40, histtype='stepfilled',
orientation='vertical', color='gray')
x_hist.invert_yaxis()

y_hist.hist(y, 40, histtype='stepfilled',
orientation='horizontal', color='gray')
y_hist.invert_xaxis()

292 | Kapitel 31: Mehrere Nebenhandlungen

Abbildung 31-8. Visualisierung mehrdimensionaler Verteilungen mit plt.GridSpec

Diese Art der Verteilung, die zusammen mit ihren Rändern dargestellt wird, ist so häufig, dass sie

seine eigene Plot-API im Seaborn-Paket; siehe Kapitel 36 für weitere Einzelheiten.

plt.GridSpec: Kompliziertere Anordnungen | 293
KAPITEL 32

Text und Anmerkungen
Zu einer guten Visualisierung gehört es, den Leser so zu führen, dass die Abbildung eine Geschichte erzählt.
Geschichte erzählt. In einigen Fällen kann diese Geschichte auf rein visuelle Weise erzählt werden, ohne dass

In anderen Fällen sind jedoch kleine textliche Hinweise und Beschriftungen erforderlich. Vielleicht

Die grundlegendsten Arten von Anmerkungen, die Sie verwenden werden, sind Achsenbeschriftungen und Titel, aber die
Optionen gehen darüber hinaus. Werfen wir einen Blick auf einige Daten und wie wir sie visualisieren können und

mit Anmerkungen versehen, um interessante Informationen zu vermitteln. Wir beginnen mit dem Einrichten des Notizbuchs
Buchs für das Plotten einrichten und die Funktionen importieren, die wir verwenden werden:

In [1]: % matplotlib inline
import matplotlib.pyplot as plt
import matplotlib as mpl
plt.style.use('seaborn-whitegrid')
import numpy as np
import pandas as pd

Beispiel: Auswirkung von Feiertagen auf Geburten in den USA
Kehren wir zu einigen Daten zurück, mit denen wir bereits in "Beispiel" gearbeitet haben: Daten zur Geburtenrate" auf
Seite 180 gearbeitet haben, wo wir ein Diagramm der durchschnittlichen Geburten im Laufe des Kalenders erstellt haben

Jahr. Wir beginnen mit dem gleichen Reinigungsverfahren, das wir dort verwendet haben, und zeichnen die Ergebnisse auf
(siehe Abbildung 32-1).

In [2]: _# Shell-Befehl zum Herunterladen der Daten:

!cd data && curl -O \
https://raw.githubusercontent.com/jakevdp/data-CDCbirths/master/
geburten.csv_
In [3]: from datetime import datetime

Geburten = pd.read_csv('daten/geburten.csv')

Quartile = np.percentile(geburten['geburten'], [25, 50, 75])

294
1 Eine Version dieser Abbildung in voller Größe ist auf GitHub zu finden.
mu, sig = quartiles[1], 0.74 * (quartiles[2] - quartiles[0])
geburten = geburten.abfrage('(geburten > @mu - 5 * @sig) &
(Geburten < @mu + 5 * @sig)')

births['day'] = births['day'].astype(int)

geburten.index = pd.to_datetime(10000 * geburten.jahr +
100 * geburten.monat +
geburten.tag, format='%Y%m%d')
geburten_nach_datum = geburten.pivot_table('geburten',
[geburten.index.monat, geburten.index.tag])
geburten_nach_datum.index = [datetime(2012, Monat, Tag)
for (Monat, Tag) in geburten_nach_datum.index]

In [4]: fig, ax = plt.subplots(figsize=(12, 4))
geburten_nach_datum.plot(ax=ax);

Abbildung 32-1. Durchschnittliche tägliche Geburten nach Datum^1

Wenn wir Daten wie diese visualisieren, ist es oft nützlich, bestimmte Merkmale der Daten mit Anmerkungen zu versehen.

die Darstellung, um die Aufmerksamkeit des Lesers zu erregen. Dies kann manuell mit der Funktion plt.text/

ax.text-Funktionen, die den Text an einem bestimmten x/y-Wert platzieren (siehe Abbildung 32-2).

In [5]: fig, ax = plt.subplots(figsize=(12, 4))
geburten_nach_datum.plot(ax=ax)

# Beschriftungen zum Diagramm hinzufügen
style = dict(size=10, color='gray')

ax.text('2012-1-1', 3950, "Neujahr", **style)
ax.text('2012-7-4', 4250, "Unabhängigkeitstag", ha='center', **style)
ax.text('2012-9-4', 4850, "Tag der Arbeit", ha='center', **style)
ax.text('2012-10-31', 4600, "Halloween", ha='rechts', **style)
ax.text('2012-11-25', 4450, "Erntedankfest", ha='Mitte', **style)
ax.text('2012-12-25', 3850, "Weihnachten", ha='rechts', **style)

Beispiel: Auswirkung von Feiertagen auf Geburten in den USA | 295
2 Eine Version dieser Abbildung in voller Größe ist auf GitHub zu finden.
# Beschriften Sie die Achsen
ax.set(title='USA-Geburten nach Tag des Jahres (1969-1988)',
ylabel='durchschnittliche tägliche Geburten')

# Formatieren der x-Achse mit zentrierten Monatsbeschriftungen
ax.xaxis.set_major_locator(mpl.dates.MonthLocator())
ax.xaxis.set_minor_locator(mpl.dates.MonthLocator(bymonthday=15))
ax.xaxis.set_major_formatter(plt.NullFormatter())
ax.xaxis.set_minor_formatter(mpl.dates.DateFormatter('%h'));

Abbildung 32-2. Kommentierte durchschnittliche tägliche Geburten nach Datum^2

Die ax.text-Methode nimmt eine x-Position, eine y-Position, eine Zeichenkette und einen optionalen Schlüssel.

Wörter, die die Farbe, Größe, den Stil, die Ausrichtung und andere Eigenschaften des Textes festlegen.

Hier haben wir ha='right' und ha='center' verwendet, wobei ha die Abkürzung für horizontale Ausrichtung ist.

ment. Siehe die Dokumentationen von plt.text und mpl.text.Text für weitere Informationen über

die verfügbaren Optionen.

Transformationen und Textposition
Im vorherigen Beispiel haben wir unsere Textanmerkungen an Datenpositionen verankert. Manchmal -
Manchmal ist es besser, den Text an einer festen Position auf den Achsen oder in der Abbildung zu verankern.

von den Daten abhängen. In Matplotlib wird dies durch eine Änderung der Transformation erreicht.

Matplotlib verwendet einige verschiedene Koordinatensysteme: Ein Datenpunkt bei
x,y = 1, 1 entspricht einer bestimmten Stelle auf den Achsen oder der Figur, die wiederum

einem bestimmten Pixel auf dem Bildschirm entspricht. Mathematisch gesehen ist das Transformieren
zwischen solchen Koordinatensystemen ist relativ einfach, und Matplotlib hat eine

eine Reihe gut entwickelter Tools, die sie intern zur Durchführung dieser Umwandlungen verwendet (diese

Werkzeuge können im Untermodul matplotlib.transforms erkundet werden).

296 | Kapitel 32: Text und Anmerkungen

Ein typischer Benutzer muss sich selten um die Details der Transformationen kümmern, aber es ist hilfreich.

Dieses Wissen ist wichtig, wenn es um die Platzierung von Text auf einer Abbildung geht. Es gibt
drei vordefinierte Transformationen, die in dieser Situation nützlich sein können:

ax.transData
Mit den Datenkoordinaten verbundene Transformation

ax.transAxes

Mit den Achsen verbundene Transformation (in Einheiten der Achsenabmessungen)

fig.transFigure

Mit der Figur verbundene Transformation (in Einheiten der Figurendimensionen)

Sehen wir uns ein Beispiel für das Zeichnen von Text an verschiedenen Stellen unter Verwendung dieser Transformationen an

(siehe Abbildung 32-3).

In [6]: fig, ax = plt.subplots(facecolor='lightgray')
ax.axis([0, 10, 0, 10])

# transform=ax.transData ist der Standardwert, aber wir geben ihn trotzdem an
ax.text(1, 5, ". Daten: (1, 5)", transform=ax.transData)
ax.text(0.5, 0.1, ". Achsen: (0.5, 0.1)", transform=ax.transAxes)
ax.text(0.2, 0.2, ". Abbildung: (0.2, 0.2)", transform=fig.transFigure);

Abbildung 32-3. Vergleich der Koordinatensysteme von Matplotlib

Die Standardtextausrichtung von Matplotlib ist so, dass das "." am Anfang jeder Zeichenkette

markiert ungefähr die angegebene Koordinatenposition.

Die transData-Koordinaten geben die üblichen Datenkoordinaten an, die mit den x- und

y-Achsenbeschriftungen. Die transAxes-Koordinaten geben die Position von unten links an

Transformationen und Textposition | 297
Ecke der Achsen (das weiße Kästchen), als Bruchteil der Gesamtgröße der Achsen. Die transFig

ure-Koordinaten sind ähnlich, geben aber die Position von der linken unteren Ecke des

der Abbildung (der graue Kasten) als Bruchteil der Gesamtgröße der Abbildung.

Beachten Sie nun, dass bei einer Änderung der Achsengrenzen nur die transData-Koordinaten
betroffen sind, während die anderen unverändert bleiben (siehe Abbildung 32-4).

In [7]: ax.set_xlim(0, 2)
ax.set_ylim(-6, 6)
Abbildung

Abbildung 32-4. Vergleich der Koordinatensysteme von Matplotlib

Dieses Verhalten wird noch deutlicher, wenn man die Achsengrenzen interaktiv verändert: Wenn

Sie diesen Code in einem Notebook ausführen, können Sie dies durch Ändern der

%matplotlib inline zu %matplotlib notebook und mit Hilfe des Menüs jedes Plots zu inter-

mit der Handlung handeln.

Pfeile und Beschriftungen
Neben den Häkchen und dem Text ist der einfache Pfeil eine weitere nützliche Anmerkungsmarkierung.

Es gibt zwar eine Funktion plt.arrow, aber ich würde nicht empfehlen, sie zu benutzen: die Pfeile
sind SVG-Objekte, die dem variierenden Seitenverhältnis Ihrer Diagramme unterworfen sein werden,

was es schwierig macht, sie richtig zu machen. Stattdessen würde ich vorschlagen, die Funktion plt.annotate zu verwenden.
zu verwenden, die einen Text und einen Pfeil erzeugt und es ermöglicht, die Pfeile sehr flexibel zu gestalten.

angegeben.

Hier ist eine Demonstration von annotate mit einigen seiner Optionen (siehe Abbildung 32-5).

298 | Kapitel 32: Text und Anmerkungen

In [8]: fig, ax = plt.subplots()

x = np.linspace(0, 20, 1000)
ax.plot(x, np.cos(x))
ax.axis('equal')

ax.annotate('lokales Maximum', xy=(6.28, 1), xytext=(10, 4),
arrowprops=dict(facecolor='black', shrink=0.05))

ax.annotate('lokales Minimum', xy=(5 * np.pi, -1), xytext=(2, -6),
arrowprops=dict(arrowstyle="->",
connectionstyle="angle3,angleA=0,angleB=-90"));

Abbildung 32-5. Beispiele für Beschriftungen

Der Pfeilstil wird über das Wörterbuch arrowprops gesteuert, das über numerische

verschiedene Optionen zur Verfügung. Diese Optionen sind in der Online-Doku von Matplotlib gut dokumentiert.
Dokumentation von Matplotlib dokumentiert, daher ist es wahrscheinlich sinnvoller, sie hier nicht zu wiederholen, sondern zu zeigen

einige Beispiele. Lassen Sie uns einige der möglichen Optionen anhand der Geburtenrate demonstrieren

Plot von vorher (siehe Abbildung 32-6).

In [9]: fig, ax = plt.subplots(figsize=(12, 4))
geburten_nach_datum.plot(ax=ax)

# Beschriftungen zum Diagramm hinzufügen
ax.annotate("New Year's Day", xy=('2012-1-1', 4100), xycoords='data',
xytext=(50, -30), textcoords='offset points',
arrowprops=dict(arrowstyle="->",
connectionstyle="arc3,rad=-0.2"))

Pfeile und Beschriftung | 299
ax.annotate("Independence Day", xy=('2012-7-4', 4250), xycoords='data',
bbox=dict(boxstyle="round", fc="none", ec="gray"),
xytext=(10, -40), textcoords='offset points', ha='center',
arrowprops=dict(arrowstyle="->"))

ax.annotate('Labor Day Weekend', xy=('2012-9-4', 4850), xycoords='data',
ha='center', xytext=(0, -20), textcoords='offset points')
ax.annotate('', xy=('2012-9-1', 4850), xytext=('2012-9-7', 4850),
xycoords='data', textcoords='data',
arrowprops={'arrowstyle': '|-|,widthA=0.2,widthB=0.2', })

ax.annotate('Halloween', xy=('2012-10-31', 4600), xycoords='data',
xytext=(-80, -40), textcoords='offset points',
arrowprops=dict(arrowstyle="fancy",
fc="0.6", ec="none",
connectionstyle="angle3,angleA=0,angleB=-90"))

ax.annotate('Thanksgiving', xy=('2012-11-25', 4500), xycoords='data',
xytext=(-120, -60), textcoords='offset points',
bbox=dict(boxstyle="round4,pad=.5", fc="0.9"),
arrowprops=dict(
arrowstyle="->",
connectionstyle="angle,angleA=0,angleB=80,rad=20"))

ax.annotate('Weihnachten', xy=('2012-12-25', 3850), xycoords='data',
xytext=(-30, 0), textcoords='offset points',
size=13, ha='right', va="center",
bbox=dict(boxstyle="round", alpha=0.1),
arrowprops=dict(arrowstyle="wedge,tail_width=0.5", alpha=0.1));

# Beschriften Sie die Achsen
ax.set(title='USA Geburten nach Tag des Jahres (1969-1988)',
ylabel='durchschnittliche tägliche Geburten')

# Formatieren der x-Achse mit zentrierten Monatsbeschriftungen
ax.xaxis.set_major_locator(mpl.dates.MonthLocator())
ax.xaxis.set_minor_locator(mpl.dates.MonthLocator(bymonthday=15))
ax.xaxis.set_major_formatter(plt.NullFormatter())
ax.xaxis.set_minor_formatter(mpl.dates.DateFormatter('%h'));

ax.set_ylim(3600, 5400);

Die Vielfalt der Optionen macht Annotate leistungsstark und flexibel: Sie können fast

jede beliebige Pfeilart, die Sie wünschen. Leider bedeutet dies auch, dass diese Art von Merkmalen
oft manuell angepasst werden müssen, was sehr zeitaufwändig sein kann, wenn

Erstellung von Grafiken in Publikationsqualität! Abschließend möchte ich anmerken, dass die vorangegangene Mischung aus

Stile sind keineswegs die beste Praxis für die Darstellung von Daten, sondern dienen der
einige der verfügbaren Optionen zu demonstrieren.

300 | Kapitel 32: Text und Anmerkungen

3 Eine Version dieser Abbildung in voller Größe ist auf GitHub zu finden.
Weitere Diskussionen und Beispiele für verfügbare Pfeil- und Anmerkungsstile finden Sie unter

im Matplotlib Annotations Tutorial.

Abbildung 32-6. Kommentierte durchschnittliche Geburtenraten nach Tag^3

Pfeile und Beschriftungen | 301
KAPITEL 33

Anpassen von Ticks
Die Standard-Tick-Locators und -Formatierer von Matplotlib sind so konzipiert, dass sie
in vielen gängigen Situationen ausreichen, sind aber keineswegs für jede Darstellung optimal. Dieses Kapitel

werden mehrere Beispiele für die Anpassung der Position der Häkchen und der Formatierung für die Par-

Sie interessieren sich für einen bestimmten Grundstückstyp.

Bevor wir uns jedoch mit den Beispielen befassen, sollten wir etwas mehr über die Objekthierarchie erfahren

von Matplotlib-Plots. Matplotlib zielt darauf ab, ein Python-Objekt zu haben, das alles repräsentiert

die auf dem Diagramm erscheint: Denken Sie zum Beispiel daran, dass die Abbildung der Begrenzungsrahmen ist

innerhalb derer Plot-Elemente erscheinen. Jedes Matplotlib-Objekt kann auch als Container fungieren

von Unterobjekten: Jede Abbildung kann beispielsweise ein oder mehrere Achsenobjekte enthalten, von denen jedes
von denen jedes wiederum andere Objekte enthält, die den Plotinhalt darstellen.

Die Häkchen bilden da keine Ausnahme. Jede Achse hat die Attribute xaxis und yaxis, die ihrerseits
wiederum Attribute haben, die alle Eigenschaften der Linien, Häkchen und Beschriftungen enthalten, die

bilden die Achsen.

Große und kleine Ticks
Innerhalb jeder Achse gibt es das Konzept eines großen und eines kleinen Tickmarks. Wie
die Namen andeuten, sind größere Ticks in der Regel größer oder ausgeprägter, während kleinere

Ticks sind in der Regel kleiner. Standardmäßig macht Matplotlib selten Gebrauch von kleineren Ticks, aber

Ein Ort, an dem man sie sehen kann, ist in logarithmischen Diagrammen (siehe Abbildung 33-1).

In [1]: import matplotlib.pyplot as plt
plt.style.use('classic')
import numpy as np

% matplotlib inline

302
In [2]: ax = plt.axes(xscale='log', yscale='log')
ax.set(xlim=(1, 1E3), ylim=(1, 1E3))
ax.grid( True );

Abbildung 33-1. Beispiel für logarithmische Skalen und Beschriftungen

In diesem Diagramm zeigt jeder große Tick ein großes Häkchen, eine Beschriftung und eine Gitternetzlinie, während jeder
Jeder kleinere Tick zeigt ein kleineres Tickmark ohne Beschriftung oder Gitternetzlinie.

Diese Zeckeneigenschaften - also Positionen und Beschriftungen - können durch Setzen der

Formatierer und Locator-Objekte für jede Achse. Untersuchen wir diese für die x-Achse der Tabelle

soeben gezeigte Handlung:

In [3]: print(ax.xaxis.get_major_locator())
print(ax.xaxis.get_minor_locator())
Out[3]: <matplotlib.ticker.LogLocator object at 0x1129b9370>
<matplotlib.ticker.LogLocator-Objekt bei 0x1129aaf70>

In [4]: print(ax.xaxis.get_major_formatter())
print(ax.xaxis.get_minor_formatter())
Out[4]: <matplotlib.ticker.LogFormatterSciNotation Objekt bei 0x1129aaa00>
<matplotlib.ticker.LogFormatterSciNotation-Objekt bei 0x1129aac10>

Wir sehen, dass sowohl die Haupt- als auch die Nebentickmarkierungen ihre Position durch eine

LogLocator (was für eine logarithmische Darstellung sinnvoll ist). Kleinere Zecken haben jedoch

deren Beschriftungen mit einem NullFormatter formatiert sind: Dies bedeutet, dass keine Beschriftungen angezeigt werden.

Im Folgenden werden wir uns einige Beispiele für die Einstellung dieser Locatoren und Formatierer für verschiedene
Diagramme.

Große und kleine Zecken | 303
Ausblenden von Häkchen oder Beschriftungen
Der vielleicht häufigste Vorgang bei der Formatierung von Häkchen und Etiketten ist das Ausblenden von Häkchen oder

Bezeichnungen. Dies kann mit plt.NullLocator und plt.NullFormatter geschehen, wie gezeigt

hier (siehe Abbildung 33-2).

In [5]: ax = plt.axes()
rng = np.random.default_rng(1701)
ax.plot(rng.random(50))
ax.grid()

ax.yaxis.set_major_locator(plt.NullLocator())
ax.xaxis.set_major_formatter(plt.NullFormatter())

Abbildung 33-2. Darstellung mit ausgeblendeten Tick-Labels (x-Achse) und ausgeblendeten Ticks (y-Achse)

Wir haben die Beschriftungen von der x-Achse entfernt (aber die Häkchen/Gitterlinien beibehalten) und die

die Häkchen (und damit auch die Beschriftungen und Gitternetzlinien) von der y-Achse. Keine Häkchen bei

können in vielen Situationen nützlich sein - zum Beispiel, wenn Sie ein Raster von Bildern anzeigen möchten.
Bilder zeigen will. Betrachten Sie zum Beispiel Abbildung 33-3, die Bilder verschiedener Gesichter enthält,

ein Beispiel, das häufig bei Problemen des überwachten maschinellen Lernens verwendet wird (siehe z. B.,
Kapitel 43):

In [6]: fig, ax = plt.subplots(5, 5, figsize=(5, 5))
fig.subplots_adjust(hspace=0, wspace=0)

# Gesichtsdaten von Scikit-Learn holen
from sklearn.datasets import fetch_olivetti_faces
Gesichter = fetch_olivetti_faces().images

304 | Kapitel 33: Zecken anpassen

for i in range(5):
for j in range(5):
ax[i, j].xaxis.set_major_locator(plt.NullLocator())
ax[i, j].yAchse.set_major_locator(plt.NullLocator())
ax[i, j].imshow(faces[10 * i + j], cmap='binary_r')

Abbildung 33-3. Ausblenden von Häkchen in Bilddiagrammen

Jedes Bild wird in seinen eigenen Achsen angezeigt, und wir haben die Tick-Locators auf Null gesetzt, weil
die Tickwerte (in diesem Fall Pixelzahlen) keine relevanten Informationen für

diese spezielle Visualisierung.

Ausblenden von Häkchen oder Etiketten | 305
Verringern oder Erhöhen der Anzahl der Häkchen
Ein häufiges Problem bei den Standardeinstellungen ist, dass kleinere Teilflächen zu

mit überfüllten Beschriftungen. Dies ist in dem hier gezeigten Raster zu sehen (siehe Abbildung 33-4).

In [7]: fig, ax = plt.subplots(4, 4, sharex= True , sharey= True )

Abbildung 33-4. Ein Standardplot mit überfüllten Zecken

Vor allem bei der x-Achse überschneiden sich die Zahlen fast, was es schwierig macht.

zu entschlüsseln. Eine Möglichkeit, dies zu ändern, ist plt.MaxNLocator, das uns erlaubt

geben Sie die maximale Anzahl der anzuzeigenden Häkchen an. Angesichts dieser maximalen
Anzahl verwendet Matplotlib eine interne Logik, um die einzelnen Tick-Positionen auszuwählen (siehe

Abbildung 33-5).

In [8]: # Für jede Achse den x- und y-Hauptlocator setzen
for axi in ax.flat:
axi.xaxis.set_major_locator(plt.MaxNLocator(3))
axi.yaxis.set_major_locator(plt.MaxNLocator(3))
Abb.

306 | Kapitel 33: Zecken anpassen

Abbildung 33-5. Anpassen der Anzahl der Häkchen

Das macht die Dinge viel sauberer. Wenn Sie noch mehr Kontrolle über die Standorte von

regelmäßig verteilte Ticks, können Sie auch plt.MultipleLocator verwenden, was wir noch besprechen werden

im folgenden Abschnitt.

Ausgefallene Tick-Formate
Die Standard-Tick-Formatierung von Matplotlib lässt viel zu wünschen übrig: Sie funktioniert gut als
Standard, aber manchmal möchte man etwas anderes machen. Betrachten Sie diesen Plot

einer Sinus- und einer Kosinuskurve (siehe Abbildung 33-6).

In [9]: # Plotten einer Sinus- und Kosinuskurve
fig, ax = plt.subplots()
x = np.linspace(0, 3 * np.pi, 1000)
ax.plot(x, np.sin(x), lw=3, label='Sinus')
ax.plot(x, np.cos(x), lw=3, label='Cosinus')

# Raster, Legende und Grenzen einrichten
ax.grid( True )
ax.legend(frameon= False )
ax.axis('equal')
ax.set_xlim(0, 3 * np.pi);

Ausgefallene Zeckenformate | 307
Abbildung 33-6. Ein Standardplot mit ganzzahligen Ticks

Vollfarbige Abbildungen sind in den ergänzenden Materialien auf
GitHub.
Es gibt ein paar Änderungen, die wir hier vornehmen sollten. Erstens ist es natürlicher für

diese Daten, um die Ticks und Gitternetzlinien in Vielfachen von π zu platzieren.

MultipleLocator, der die Zecken mit einem Vielfachen der von uns angegebenen Zahl lokalisiert. Für

Zur Sicherheit fügen wir sowohl Dur- als auch Moll-Ticks in Vielfachen von π/2 und π/4 hinzu (siehe
Abbildung 33-7).

In [10]: ax.xaxis.set_major_locator(plt.MultipleLocator(np.pi / 2))
ax.xaxis.set_minor_locator(plt.MultipleLocator(np.pi / 4))
Abb.

308 | Kapitel 33: Zecken anpassen

Abbildung 33-7. Ticks bei Vielfachen von π/2 und π/4

Aber jetzt sehen diese Häkchen etwas albern aus: Wir können sehen, dass sie Vielfache von π sind,
aber die Dezimaldarstellung vermittelt dies nicht sofort. Um dies zu beheben, können wir

das Tick-Formatierungsprogramm ändern. Es gibt keinen eingebauten Formatierer für das, was wir tun wollen, also

verwenden wir stattdessen plt.FuncFormatter, das eine benutzerdefinierte Funktion akzeptiert, die

feinkörnige Kontrolle über die Tick-Ausgaben (siehe Abbildung 33-8).

In [11]: def format_func(value, tick_number):
# Anzahl der Vielfachen von pi/2 ermitteln
N = int(np.round(2 * Wert / np.pi))
wenn N == 0:
return "0"
elif N == 1:
return r"$\pi/2$"
elif N == 2:
return r"$\pi$"
elif N % 2 > 0:
return rf"${N}\pi/2$"
else :
return rf"${N // 2}\pi$"

ax.xaxis.set_major_formatter(plt.FuncFormatter(format_func))
Abb.

Das ist viel besser! Beachten Sie, dass wir die LaTeX-Unterstützung von Matplotlib genutzt haben.
indem wir die Zeichenkette in Dollarzeichen einschließen. Dies ist sehr praktisch für die Anzeige von

mathematische Symbole und Formeln: In diesem Fall wird "$\pi$" als das griechische

Zeichen π.

Ausgefallene Zeckenformate | 309
Abbildung 33-8. Häkchen mit benutzerdefinierten Beschriftungen

Zusammenfassung der Formatierer und Locatoren
Wir haben einige der verfügbaren Formatierer und Locatoren kennengelernt; ich schließe dieses Kapitel
mit einer Auflistung aller eingebauten Locator-Optionen (Tabelle 33-1) und Formatierungsoptionen

(Tabelle 33-2). Weitere Informationen dazu finden Sie in den Docstrings oder in der Mat-

plotlib-Dokumentation. Jeder der folgenden Punkte ist im plt-Namensraum verfügbar.

Tabelle 33-1. Matplotlib-Locator-Optionen

Locator-Klasse Beschreibung
NullLocator Keine Häkchen
FixedLocator Zeckenpositionen sind fest
IndexLocator Locator für Indexplots (z. B. mit x = range(len(y)))
LinearLocator Gleichmäßig verteilte Ticks von min bis max
LogLocator Logarithmisch verteilte Ticks von min bis max
MultipleLocator Ticks und Bereich sind ein Vielfaches der Basis
MaxNLocator Findet bis zu einer maximalen Anzahl von Ticks an schönen Stellen
AutoLocator (Default) MaxNLocator mit einfachen Standardwerten
AutoMinorLocator Lokalisierer für Minor Ticks
310 | Kapitel 33: Zecken anpassen

Tabelle 33-2. Matplotlib-Formatierungsoptionen

Formatter-Klasse Beschreibung
NullFormatter Keine Beschriftung der Häkchen
IndexFormatter Setzen Sie die Zeichenfolgen aus einer Liste von Beschriftungen
FixedFormatter Manuelles Festlegen der Zeichenketten für die Beschriftungen
FuncFormatter Benutzerdefinierte Funktion setzt die Beschriftungen
FormatStrFormatter Verwendung einer Formatzeichenfolge für jeden Wert
ScalarFormatter Standardformatierung für skalare Werte
LogFormatter Standardformatierung für Log-Achsen
Im weiteren Verlauf des Buches werden wir weitere Beispiele für diese Formate sehen.

Zusammenfassung der Formatierer und Locatoren | 311
KAPITEL 34

Matplotlib anpassen:
Konfigurationen und Stylesheets
Während viele der in den vorherigen Kapiteln behandelten Themen die Anpassung des Stils der
Plot-Elemente einzeln angepasst werden, bietet Matplotlib auch Mechanismen zur Anpassung der gesamten

Stil eines Diagramms auf einmal. In diesem Kapitel gehen wir durch einige der Matplotlib-Funktionen.

Zeitkonfigurationsoptionen (rc), und werfen Sie einen Blick auf die Funktion Stylesheets, die
einige schöne Sätze von Standardkonfigurationen enthält.

Manuelle Anpassung des Plots
In diesem Teil des Buches haben Sie gesehen, wie man einzelne Plots anpassen kann.

Plot-Einstellungen, um etwas zu erhalten, das ein wenig besser aussieht als die Standardeinstellung. Es ist
ist es auch möglich, diese Anpassungen für jedes einzelne Diagramm vorzunehmen. Zum Beispiel ist hier

ein recht tristes Standardhistogramm, wie in Abbildung 34-1 dargestellt.

In [1]: import matplotlib.pyplot as plt
plt.style.use('classic')
import numpy as np

% matplotlib inline

In [2]: x = np.random.randn(1000)
plt.hist(x);

312
Abbildung 34-1. Ein Histogramm im Standardstil von Matplotlib

Wir können dies von Hand anpassen, um die Darstellung optisch ansprechender zu gestalten, wie Sie sehen können

siehe Abbildung 34-2.

In [3]: # einen grauen Hintergrund verwenden
fig = plt.figure(facecolor='white')
ax = plt.axes(facecolor='#E6E6E6')
ax.set_axisbelow( True )

# einfarbige weiße Gitterlinien zeichnen
plt.grid(color='w', linestyle='solid')

# Achsenstacheln ausblenden
for spine in ax.spines.values():
spine.set_visible( False )

# Ticks oben und rechts ausblenden
ax.xaxis.tick_bottom()
ax.yaxis.tick_left()

# Ticks und Beschriftungen aufhellen
ax.tick_params(colors='gray', direction='out')
for tick in ax.get_xticklabels():
tick.set_color('gray')
for tick in ax.get_yticklabels():
tick.set_color('gray')

# Flächen- und Kantenfarbe des Histogramms steuern
ax.hist(x, edgecolor='#E6E6E6', color='#EE6666');

Manuelle Plotanpassung | 313
Abbildung 34-2. Ein Histogramm mit manuellen Anpassungen

Dies sieht besser aus, und Sie werden vielleicht erkennen, dass das Aussehen von der R-Sprache inspiriert ist.

ge's ggplot Visualisierungspaket. Aber das war eine ganze Menge Arbeit! Wir wollen definitiv
wollen nicht jedes Mal, wenn wir ein Diagramm erstellen, all diese Anpassungen vornehmen müssen. Zum Glück gibt es

ist eine Möglichkeit, diese Standardwerte einmal so anzupassen, dass sie für alle Grundstücke gelten.

Ändern der Standardeinstellungen: rcParams
Jedes Mal, wenn Matplotlib geladen wird, definiert es eine Laufzeitkonfiguration, die die Standard

Stile für jedes von Ihnen erstellte Plot-Element. Diese Konfiguration kann jederzeit angepasst werden

Zeit mit Hilfe der Komfortroutine plt.rc. Schauen wir uns an, wie wir die rc
Parameter so ändern können, dass unser Standarddiagramm ähnlich aussieht wie das vorherige.

Wir können die Funktion plt.rc verwenden, um einige dieser Einstellungen zu ändern:

In [4]: from matplotlib import cycler
colors = cycler('color',
['#EE6666', '#3388BB', '#9988DD',
'#EECC55', '#88BB44', '#FFBBBB'])
plt.rc('figure', facecolor='white')
plt.rc('axes', facecolor='#E6E6E6', edgecolor='none',
axisbelow= True , grid= True , prop_cycle=colors)
plt.rc('grid', color='w', linestyle='solid')
plt.rc('xtick', direction='out', color='gray')
plt.rc('ytick', Richtung='out', Farbe='grau')
plt.rc('patch', edgecolor='#E6E6E6')
plt.rc('lines', linewidth=2)

314 | Kapitel 34: Matplotlib anpassen: Konfigurationen und Stylesheets

Mit diesen Einstellungen können wir nun ein Diagramm erstellen und unsere Einstellungen in Aktion sehen

(siehe Abbildung 34-3).

In [5]: plt.hist(x);

Abbildung 34-3. Ein angepasstes Histogramm mit rc-Einstellungen

Schauen wir uns an, wie einfache Liniendiagramme mit diesen rc-Parametern aussehen (siehe Abbildung 34-4).

In [6]: for i in range(4):
plt.plot(np.random.rand(10))

Abbildung 34-4. Ein Liniendiagramm mit angepassten Stilen

Ändern der Standardeinstellungen: rcParams | 315
Für Diagramme, die auf dem Bildschirm und nicht auf dem Drucker angezeigt werden, finde ich dies viel ästhetischer

gefälliger als das Standard-Styling. Wenn Sie mit meinem ästhetischen Empfinden nicht einverstanden sind, ist die gute
Die gute Nachricht ist, dass Sie die rc-Parameter an Ihren eigenen Geschmack anpassen können! Wahlweise,

Diese Einstellungen können in einer .matplotlibrc-Datei gespeichert werden, über die Sie in der

Matplotlib-Dokumentation.

Stylesheets
Ein neuerer Mechanismus zum Anpassen der allgemeinen Diagrammstile ist die Matplotlib-Style-Modifikation.

die eine Reihe von Standard-Stylesheets enthält, aber auch die Möglichkeit, eigene Styles zu erstellen und zu verpacken.
eigene Stile zu erstellen und zu verpacken. Diese Stylesheets sind ähnlich formatiert wie die .matplotlibrc

Dateien, müssen aber mit der Erweiterung .mplstyle benannt werden.

Auch wenn Sie nicht so weit gehen, Ihren eigenen Stil zu kreieren, finden Sie vielleicht, was Sie suchen.

in den eingebauten Stylesheets. plt.style.available enthält eine Liste der verfügbaren
verfügbaren Stile - hier werden der Kürze halber nur die ersten fünf aufgeführt:

In [7]: plt.style.available[:5]
Out[7]: ['Solarize_Light2', '_classic_test_patch', 'bmh', 'classic',

dark_background']

Der Standardweg, um zu einem Stylesheet zu wechseln, ist der Aufruf von style.use:

plt.style.use(' stylename ')
Denken Sie aber daran, dass Sie damit den Stil für den Rest der Python-Sitzung ändern!

Alternativ können Sie auch den Stilkontextmanager verwenden, der einen Stil vorübergehend festlegt:

mit plt.style.context(' stylename '):
make_a_plot()
Um diese Stile zu demonstrieren, wollen wir eine Funktion erstellen, die zwei grundlegende Arten von
Plots erstellt:

In [8]: def hist_and_lines():
np.random.seed(0)
fig, ax = plt.subplots(1, 2, figsize=(11, 4))
ax[0].hist(np.random.randn(1000))
for i in range(3):
ax[1].plot(np.random.rand(10))
ax[1].legend(['a', 'b', 'c'], loc='lower left')

Wir werden dies nutzen, um zu untersuchen, wie diese Diagramme unter Verwendung der verschiedenen integrierten Stile aussehen.

Vollfarbige Abbildungen sind in den ergänzenden Materialien auf
GitHub.
316 | Kapitel 34: Matplotlib anpassen: Konfigurationen und Stylesheets

Standard-Stil
Der Standardstil von Matplotlib wurde in der Version 2.0 aktualisiert; schauen wir uns diesen zuerst an
(siehe Abbildung 34-5).

In [9]: mit plt.style.context('default'):
hist_and_lines()

Abbildung 34-5. Matplotlibs Standardstil

FiveThiryEight-Stil
Der fivethirtyeight-Stil ahmt die Grafiken auf der beliebten FiveThir-
tyEight-Website zu finden sind. Wie Sie in Abbildung 34-6 sehen können, zeichnet er sich durch kräftige Farben und dicke Linien aus,

und transparente Achsen:

In [10]: mit plt.style.context('fivethirtyeight'):
hist_and_lines()

Abbildung 34-6. Der fivethirtyeight-Stil

Stylesheets | 317
ggplot Stil
Das ggplot-Paket in der Sprache R ist ein beliebtes Visualisierungstool unter Datenwissenschaftlern.

entists. Der ggplot-Stil von Matplotlib ahmt die Standardstile aus diesem Paket nach (siehe

Abbildung 34-7).

In [11]: mit plt.style.context('ggplot'):
hist_and_lines()

Abbildung 34-7. Der ggplot-Stil

Bayes'sche Methoden für Hacker
Es gibt ein nettes kurzes Online-Buch namens Probabilistic Programming and Bayesian

Methoden für Hacker von Cameron Davidson-Pilon mit Figuren, die mit

Matplotlib und verwendet eine Reihe von rc-Parametern, um eine konsistente und visuelle

ansprechender Stil im gesamten Buch. Dieser Stil wird im Stylesheet bmh wiedergegeben
wiedergegeben (siehe Abbildung 34-8).

In [12]: mit plt.style.context('bmh'):
hist_and_lines()

Abbildung 34-8. Der bmh-Stil

318 | Kapitel 34: Matplotlib anpassen: Konfigurationen und Stylesheets

Dunkler Hintergrundstil
Bei Abbildungen, die in Präsentationen verwendet werden, ist es oft sinnvoll, einen dunklen statt einen hellen Hintergrund zu verwenden.

Hintergrund. Der Stil dark_background ermöglicht dies (siehe Abbildung 34-9).

In [13]: mit plt.style.context('dark_background'):
hist_and_lines()

Abbildung 34-9. Der Stil dark_background

Graustufen-Stil
Es kann vorkommen, dass Sie Zahlen für eine Druckpublikation vorbereiten, die keine Graustufen akzeptiert.

farbige Abbildungen. Hierfür kann der Graustufenstil (siehe Abbildung 34-10) nützlich sein.

In [14]: mit plt.style.context('grayscale'):
hist_and_lines()

Abbildung 34-10. Der Graustufenstil

Stylesheets | 319
Seaborn-Stil
Matplotlib hat auch mehrere Stylesheets, die von der Seaborn-Bibliothek inspiriert sind (besprochen

ausführlicher in Kapitel 36). Ich habe diese Einstellungen als sehr angenehm empfunden und verwende
und verwende sie als Standardeinstellungen für meine eigene Datenexploration (siehe Abbildung 34-11).

In [15]: mit plt.style.context('seaborn-whitegrid'):
hist_and_lines()

Abbildung 34-11. Der Seaborn-Darstellungsstil

Nehmen Sie sich etwas Zeit, um die eingebauten Optionen zu erkunden und eine zu finden, die Sie anspricht!

In diesem Buch werde ich im Allgemeinen eine oder mehrere dieser Stilkonventionen verwenden
bei der Erstellung von Plots.

320 | Kapitel 34: Matplotlib anpassen: Konfigurationen und Stylesheets

KAPITEL 35

Dreidimensionales Plotten in Matplotlib
Matplotlib wurde ursprünglich nur mit Blick auf zweidimensionale Plots entwickelt.
Um die Zeit der Veröffentlichung von Version 1.0 wurden einige dreidimensionale Plotprogramme

die auf der zweidimensionalen Darstellung von Matplotlib aufbaut, und das Ergebnis ist eine praktische (wenn auch

etwas begrenzten) Satz von Werkzeugen für die dreidimensionale Datenvisualisierung. Drei-

dimensionalen Plots wird durch den Import des mplot3d-Toolkits ermöglicht, das in der
Hauptinstallation von Matplotlib enthalten ist:

In [1]: from mpl_toolkits import mplot3d

Sobald dieses Submodul importiert ist, kann eine dreidimensionale Achse erstellt werden, indem man

das Schlüsselwort projection='3d' an eine der Routinen zur Erstellung normaler Achsen, wie gezeigt

hier (siehe Abbildung 35-1).

In [2]: % matplotlib inline
import numpy as np
import matplotlib.pyplot as plt

In [3]: fig = plt.figure()
ax = plt.axes(projection='3d')

Wenn diese dreidimensionalen Achsen aktiviert sind, können wir nun eine Vielzahl von dreidimensionalen
dimensionalen Plot-Typen. Die dreidimensionale Darstellung ist eine der Funktionalitäten, die

profitiert immens von der interaktiven statt statischen Betrachtung von Zahlen in der

Notizbuch; erinnern Sie sich daran, dass Sie für interaktive Figuren %matplotlib notebook verwenden können

und nicht %matplotlib inline, wenn Sie diesen Code ausführen.

321
Abbildung 35-1. Eine leere dreidimensionale Achse

Dreidimensionale Punkte und Linien
Die grundlegendste dreidimensionale Darstellung ist eine Linie oder eine Sammlung von Streudiagrammen, die
aus Mengen von (x, y, z) Dreiergruppen. In Analogie zu den häufigeren zweidimensionalen Darstellungen

können diese mit den Funktionen ax.plot3D und ax.scatter3D erstellt werden.
Funktionen erstellt werden. Die Aufrufsignatur für diese Funktionen ist nahezu identisch mit der ihrer zweidimensionalen

Sie können daher in den Kapiteln 26 und 27 weitere Informationen zur Steuerung der Ausgabe finden.
trolling der Ausgabe. Hier zeichnen wir eine trigonometrische Spirale zusammen mit einigen Punkten

zufällig in der Nähe der Linie gezeichnet (siehe Abbildung 35-2).

In [4]: ax = plt.axes(projection='3d')

# Daten für eine dreidimensionale Linie
zline = np.linspace(0, 15, 1000)
xLinie = np.sin(zLinie)
yline = np.cos(zline)
ax.plot3D(xline, yline, zline, 'grau')

# Daten für dreidimensional verstreute Punkte
zdata = 15 * np.random.random(100)
xdata = np.sin(zdata) + 0,1 * np.random.randn(100)
ydata = np.cos(zdata) + 0,1 * np.random.randn(100)
ax.scatter3D(xdata, ydata, zdata, c=zdata, cmap='Greens');

322 | Kapitel 35: Dreidimensionales Plotten in Matplotlib

Abbildung 35-2. Punkte und Linien in drei Dimensionen

Beachten Sie, dass die Transparenz der Streupunkte angepasst wurde, um ein Gefühl von Tiefe zu vermitteln.

die Seite. Der dreidimensionale Effekt ist manchmal schwer zu erkennen, wenn man

statischen Bildes kann eine interaktive Ansicht zu einer guten Intuition über die Anordnung der Punkte führen.
der Punkte.

Dreidimensionale Konturdiagramme
Analog zu den Konturplots, die wir in Kapitel 28 untersucht haben, enthält mplot3d Werkzeuge, um

erstellt dreidimensionale Reliefdiagramme unter Verwendung der gleichen Eingaben. Wie ax.contour, ax.con

Für tour3D müssen alle Eingabedaten in Form von zweidimensionalen regelmäßigen Gittern vorliegen,

wobei die z-Daten an jedem Punkt ausgewertet werden. Hier zeigen wir ein dreidimensionales Konturdiagramm
Konturdiagramm einer dreidimensionalen Sinusfunktion (siehe Abbildung 35-3).

In [5]: def f(x, y):
return np.sin(np.sqrt(x ** 2 + y ** 2))

x = np.linspace(-6, 6, 30)
y = np.linspace(-6, 6, 30)

X, Y = np.meshgrid(x, y)
Z = f(X, Y)

In [6]: fig = plt.figure()
ax = plt.axes(projection='3d')
ax.contour3D(X, Y, Z, 40, cmap='binary')
ax.set_xlabel('x')
ax.set_ylabel('y')
ax.set_zlabel('z');

Dreidimensionale Konturplots | 323
Abbildung 35-3. Eine dreidimensionale Konturdarstellung

Manchmal ist der voreingestellte Blickwinkel nicht optimal; in diesem Fall können wir die

view_init-Methode, um die Elevations- und Azimutalwinkel festzulegen. In der folgenden Beispiel-

Abbildung 35-4 veranschaulicht, verwenden wir eine Elevation von 60 Grad (d. h. 60 Grad
über der x-y-Ebene) und einen Azimut von 35 Grad (d. h. um 35 Grad gedreht

gegen den Uhrzeigersinn um die z-Achse):

In [7]: ax.view_init(60, 35)
Abb.

Beachten Sie bitte, dass diese Art der Rotation interaktiv durch Klicken und Ziehen mit einem der interaktiven Backends von Matplotlib durchgeführt werden kann.
und Ziehen erreicht werden kann, wenn eines der interaktiven Backends von Matplotlib verwendet wird.

324 | Kapitel 35: Dreidimensionales Plotten in Matplotlib

Abbildung 35-4. Einstellen des Blickwinkels für eine dreidimensionale Darstellung

Wireframes und Oberflächendiagramme
Zwei weitere Arten von dreidimensionalen Diagrammen, die mit gerasterten Daten arbeiten, sind Wireframes

und Oberflächenplots. Diese nehmen ein Raster von Werten und projizieren es auf die angegebene drei-

dimensionalen Oberfläche und kann die daraus resultierenden dreidimensionalen Formen recht einfach
zu visualisieren. Hier ist ein Beispiel für die Verwendung eines Drahtmodells (siehe Abbildung 35-5).

In [8]: fig = plt.figure()
ax = plt.axes(projection='3d')
ax.plot_Wireframe(X, Y, Z)
ax.set_title('wireframe');

Wireframes und Oberflächenplots | 325
Abbildung 35-5. Ein Drahtgitterplot

Ein Oberflächenplot ist wie ein Drahtgitterplot, aber jede Fläche des Drahtgitters ist ein gefülltes Polygon.
gon. Das Hinzufügen einer Farbkarte zu den gefüllten Polygonen kann die Wahrnehmung der Topologie von

die zu visualisierende Fläche, wie in Abbildung 35-6 zu sehen ist.

In [9]: ax = plt.axes(projection='3d')
ax.plot_surface(X, Y, Z, rstride=1, cstride=1,
cmap='viridis', edgecolor='none')
ax.set_title('surface');

326 | Kapitel 35: Dreidimensionales Plotten in Matplotlib

Abbildung 35-6. Ein dreidimensionales Oberflächendiagramm

Das Wertegitter für eine Oberflächendarstellung muss zwar zweidimensional sein, aber nicht
geradlinig sein. Hier ist ein Beispiel für die Erstellung eines partiellen polaren Gitters, das bei Verwendung

mit dem surface3D-Plot kann uns einen Einblick in die Funktion geben, die wir visualisieren (siehe

Abbildung 35-7).

In [10]: r = np.linspace(0, 6, 20)
theta = np.linspace(-0.9 * np.pi, 0.8 * np.pi, 40)
r, theta = np.meshgrid(r, theta)

X = r * np.sin(theta)
Y = r * np.cos(theta)
Z = f(X, Y)

ax = plt.axes(projection='3d')
ax.plot_surface(X, Y, Z, rstride=1, cstride=1,
cmap='viridis', edgecolor='none');

Wireframes und Oberflächenplots | 327
Abbildung 35-7. Eine polare Oberflächendarstellung

Oberflächen-Triangulationen
Für einige Anwendungen sind die von den vorangegangenen Routinen benötigten gleichmäßig abgetasteten Gitter

zu restriktiv sind. In solchen Situationen können auf Triangulation basierende Diagramme sehr nützlich sein.
Was wäre, wenn wir statt einer gleichmäßigen Zeichnung eines kartesischen oder polaren Gitters stattdessen ein

eine Reihe von Zufallsziehungen?

In [11]: theta = 2 * np.pi * np.random.random(1000)
r = 6 * np.random.random(1000)
x = np.ravel(r * np.sin(theta))
y = np.ravel(r * np.cos(theta))
z = f(x, y)

Wir könnten ein Streudiagramm der Punkte erstellen, um eine Vorstellung von der Fläche zu bekommen, die wir beproben

aus, wie in Abbildung 35-8 dargestellt.

In [12]: ax = plt.axes(projection='3d')
ax.scatter(x, y, z, c=z, cmap='viridis', linewidth=0.5);

328 | Kapitel 35: Dreidimensionales Plotten in Matplotlib

Abbildung 35-8. Eine dreidimensional abgetastete Oberfläche

Diese Punktwolke lässt viel zu wünschen übrig. Die Funktion, die uns in diesem Fall helfen wird, ist

ax.plot_trisurf, das eine Oberfläche erzeugt, indem es zunächst eine Menge von Dreiecken findet, die

zwischen benachbarten Punkten (es sei daran erinnert, dass x, y und z hier eindimensionale Arrays sind);
Abbildung 35-9 zeigt das Ergebnis:

In [13]: ax = plt.axes(projection='3d')
ax.plot_trisurf(x, y, z,
cmap='viridis', edgecolor='none');

Das Ergebnis ist sicherlich nicht so sauber, wie wenn es mit einem Gitter aufgetragen wird, aber die Flexibilität
einer solchen Triangulation ermöglicht einige wirklich interessante dreidimensionale Darstellungen. Für

Beispiel ist es tatsächlich möglich, auf diese Weise ein dreidimensionales Möbiusband zu zeichnen, wie
wir als nächstes sehen werden.

Flächentriangulationen | 329
Abbildung 35-9. Eine triangulierte Oberfläche

Beispiel: Visualisierung eines Möbiusbandes
Ein Möbiusband ist vergleichbar mit einem Papierstreifen, der mit einer halben Drehung in eine Schleife geklebt wird, so dass -

in einem Objekt mit nur einer Seite! Hier werden wir ein solches Objekt mit den
Matplotlibs dreidimensionale Werkzeuge. Der Schlüssel zur Erstellung des Möbiusbandes ist, sich Folgendes vorzustellen

über seine Parametrisierung: Es ist ein zweidimensionaler Streifen, also brauchen wir zwei intrinsische
Dimensionen. Nennen wir sie θ, die von 0 bis 2 π um die Schleife reicht, und w,

der über die Breite des Streifens von -1 bis 1 reicht:

In [14]: theta = np.linspace(0, 2 * np.pi, 30)
w = np.linspace(-0.25, 0.25, 8)
w, theta = np.meshgrid(w, theta)

Ausgehend von dieser Parametrisierung müssen wir nun die (x, y, z)-Positionen der

eingebetteter Streifen.

Wenn wir darüber nachdenken, werden wir feststellen, dass es zwei Rotationen gibt: die eine ist die

Position der Schleife um ihren Mittelpunkt (was wir θ genannt haben), während die andere die Verdrehung ist.

des Streifens um seine Achse (wir nennen dies φ). Für ein Möbiusband müssen wir das Band
Band während einer vollen Schleife eine halbe Drehung machen, oder Δφ=Δθ/2:

In [15]: phi = 0,5 * theta

Nun nutzen wir unsere Erinnerung an die Trigonometrie, um die dreidimensionale Einbettung abzuleiten.

ding. Wir definieren r, den Abstand jedes Punktes vom Zentrum, und verwenden dies, um die
eingebetteten x,y,z-Koordinaten:

330 | Kapitel 35: Dreidimensionales Plotten in Matplotlib

In [16]: # Radius in der x-y-Ebene
r = 1 + w * np.cos(phi)

x = np.ravel(r * np.cos(theta))
y = np.ravel(r * np.sin(theta))
z = np.ravel(w * np.sin(phi))

Um schließlich das Objekt zu zeichnen, müssen wir sicherstellen, dass die Triangulation korrekt ist. Die beste

Eine Möglichkeit, dies zu tun, besteht darin, die Triangulation innerhalb der zugrunde liegenden Parametrisierung zu definieren,

und lassen Sie dann Matplotlib diese Triangulation in den dreidimensionalen Raum des
des Möbiusbandes projizieren. Dies lässt sich wie folgt bewerkstelligen (siehe Abbildung 35-10).

In [17]: # Triangulation in der zugrunde liegenden Parametrisierung
from matplotlib.tri import Triangulation
tri = Triangulation(np.ravel(w), np.ravel(theta))

ax = plt.axes(projection='3d')
ax.plot_trisurf(x, y, z, triangles=tri.triangles,
cmap='Greys', linewidths=0.2);

ax.set_xlim(-1, 1); ax.set_ylim(-1, 1); ax.set_zlim(-1, 1)
ax.axis('off');

Abbildung 35-10. Visualisierung eines Möbiusbandes

Durch die Kombination all dieser Techniken ist es möglich, eine Vielzahl von dreidimensionalen Objekten und Mustern in Matplotlib zu erstellen und darzustellen.
dreidimensionaler Objekte und Muster in Matplotlib.

Beispiel: Visualisierung eines Möbiusbandes | 331
KAPITEL 36

Visualisierung mit Seaborn
Matplotlib ist seit Jahrzehnten das Herzstück der wissenschaftlichen Visualisierung in Python, aber
selbst begeisterte Benutzer werden zugeben, dass sie oft zu wünschen übrig lässt. Es gibt mehrere Kom-

Beschwerden über Matplotlib, die häufig auftauchen:

Eine häufige frühe Beschwerde, die jetzt überholt ist: Vor Version 2.0 waren die Farb- und Stilvorgaben von Mat-
plotlibs Farb- und Stilvorgaben waren manchmal schlecht und sahen veraltet aus.
Die API von Matplotlib ist relativ einfach gehalten. Anspruchsvolle statistische Visualisierung
Visualisierung ist zwar möglich, erfordert aber oft eine Menge Boilerplate-Code.
Matplotlib ist mehr als ein Jahrzehnt älter als Pandas und ist daher nicht für die
Verwendung mit Pandas DataFrame-Objekten konzipiert. Um Daten aus einem DataFrame zu visualisieren,
müssen Sie jede Serie extrahieren und sie oft in das richtige Format zusammenfügen.
Format bringen. Es wäre schöner, eine Plot-Bibliothek zu haben, die intelligent die
DataFrame-Beschriftungen in einem Diagramm verwenden kann.
Eine Antwort auf diese Probleme ist Seaborn. Seaborn bietet eine API über der Matplot-
lib, die vernünftige Auswahlmöglichkeiten für Plotstil und Farbvorgaben bietet, einfache High-Level

Funktionen für gängige statistische Diagrammtypen und integriert sich mit der Funktionalität pro-

von Pandas bereitgestellt.

Fairerweise muss man sagen, dass sich das Matplotlib-Team an die veränderte Landschaft angepasst hat: Es hat die

plt.style-Tools, die in Kapitel 34 besprochen wurden, und Matplotlib fängt an, mit Pandas

Daten nahtloser zu verarbeiten. Aber aus all den soeben erörterten Gründen bleibt Seaborn ein nützliches

Add-on.

332
Konventionell wird Seaborn oft als sns importiert:

In [1]: % matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd

sns.set() # Methode von seaborn zum Einstellen des Diagrammstils

Vollfarbige Abbildungen sind in den ergänzenden Materialien auf
GitHub.
Seaborn-Plots erforschen
Die Hauptidee von Seaborn ist, dass es High-Level-Befehle zum Erstellen einer Vielzahl von

von Diagrammtypen, die für die statistische Datenuntersuchung und sogar für einige statistische Modell
Anpassung.

Werfen wir einen Blick auf einige der in Seaborn verfügbaren Datensätze und Diagrammtypen. Beachten Sie, dass
alle folgenden Schritte auch mit einfachen Matplotlib-Befehlen durchgeführt werden können (dies ist in der Tat der Fall,

was Seaborn unter der Haube tut), aber die Seaborn-API ist viel bequemer.

Histogramme, KDE und Dichten
Bei der Visualisierung statistischer Daten geht es oft nur um die Darstellung von Histogrammen und gemeinsamen Dis-

Verteilungen von Variablen. Wir haben gesehen, dass dies in Matplot relativ einfach ist.

lib (siehe Abbildung 36-1).

In [2]: Daten = np.random.multivariate_normal([0, 0], [[5, 2], [2, 2]], size=2000)
Daten = pd.DataFrame(Daten, Spalten=['x', 'y'])

for col in 'xy':
plt.hist(data[col], density= True , alpha=0.5)

Erkundung von Seaborn-Grundstücken | 333
Abbildung 36-1. Histogramme zur Visualisierung von Verteilungen

Anstatt nur ein Histogramm als visuelle Ausgabe zu liefern, können wir eine glatte Schätzung der
mate der Verteilung mit Hilfe der Kernel-Dichte-Schätzung (eingeführt in Kapitel 28),

was Seaborn mit sns.kdeplot tut (siehe Abbildung 36-2).

In [3]: sns.kdeplot(data=data, shade= True );

Abbildung 36-2. Kernel-Dichte-Schätzungen zur Visualisierung von Verteilungen

334 | Kapitel 36: Visualisierung mit Seaborn

Wenn wir kdeplot die Spalten x und y übergeben, erhalten wir stattdessen eine zweidimensionale Visualisierung
tion der gemeinsamen Dichte (siehe Abbildung 36-3).

In [4]: sns.kdeplot(data=data, x='x', y='y');

Abbildung 36-3. Eine zweidimensionale Kerndichtedarstellung

Wir können die gemeinsame Verteilung und die Randverteilungen zusammen sehen, indem wir

sns.jointplot, auf das wir später in diesem Kapitel näher eingehen werden.

Paarige Diagramme
Wenn Sie Joint Plots auf Datensätze mit größeren Dimensionen verallgemeinern, erhalten Sie
Paar-Diagramme. Diese sind sehr nützlich für die Untersuchung von Korrelationen zwischen mehrdimensionalen

Daten, wenn Sie alle Wertepaare gegeneinander aufzeichnen möchten.

Wir werden dies anhand des bekannten Iris-Datensatzes demonstrieren, der Messungen von Blütenblättern enthält

und Kelchblätter von drei Iris-Arten:

In [5]: iris = sns.load_dataset("iris")
iris.head()
Out[5]: sepal_length sepal_width petal_length petal_width species
0 5.1 3.5 1.4 0.2 setosa
1 4,9 3,0 1,4 0,2 setosa
2 4,7 3,2 1,3 0,2 setosa
3 4.6 3.1 1.5 0.2 setosa
4 5,0 3,6 1,4 0,2 setosa

Erforschung von Seaborn Plots | 335
1 Die in diesem Abschnitt verwendeten Daten zum Restaurantpersonal unterteilen die Mitarbeiter in zwei Geschlechter: weiblich und männlich. Das biologische Geschlecht
ist nicht binär, aber die folgenden Diskussionen und Visualisierungen sind durch diese Daten begrenzt.
Die Visualisierung der mehrdimensionalen Beziehungen zwischen den Stichproben ist so einfach wie ein Anruf.

ing sns.pairplot (siehe Abbildung 36-4).

In [6]: sns.pairplot(iris, hue='species', height=2.5);

Abbildung 36-4. Ein Paardiagramm, das die Beziehungen zwischen vier Variablen zeigt

Facettierte Histogramme
Manchmal lassen sich Daten am besten durch Histogramme von Teilmengen darstellen, wie in

Abbildung 36-5. Das FacetGrid von Seaborn macht dies einfach. Werfen wir einen Blick auf einige Daten

die die Höhe der Trinkgelder für das Restaurantpersonal anhand verschiedener Indikatoren anzeigt

Daten:^1

336 | Kapitel 36: Visualisierung mit Seaborn

In [7]: tips = sns.load_dataset('tips')
tips.head()
Out[7]: total_bill tip Geschlecht Raucher Tag Zeit Größe
0 16.99 1.01 Weiblich Nein Sonne Abendessen 2
1 10.34 1.66 Männlich Keine Sonne Abendessen 3
2 21.01 3.50 Männlich Kein Sonntagsessen 3
3 23.68 3.31 Männlich Kein Abendessen an der Sonne 2
4 24.59 3.61 Weiblich Kein Sonnenuntergang 4

In [8]: tips['tip_pct'] = 100 * tips['tip'] / tips['total_bill']

grid = sns.FacetGrid(tips, row="sex", col="time", margin_titles= True )
grid.map(plt.hist, "tip_pct", bins=np.linspace(0, 40, 15));

Abbildung 36-5. Ein Beispiel für ein facettiertes Histogramm

Erforschung von Seaborn Plots | 337
Das Facettendiagramm gibt uns einige schnelle Einblicke in den Datensatz: Wir sehen zum Beispiel

dass sie weit mehr Daten über männliche Kellner während der Abendessenszeit enthält als andere Kategorien.
und die typischen Trinkgeldbeträge scheinen zwischen 10 % und 20 % zu liegen, mit

einige Ausreißer an beiden Enden.

Kategorische Diagramme
Auch kategoriale Darstellungen können für diese Art der Visualisierung nützlich sein. Diese ermöglichen Ihnen

um die Verteilung eines Parameters innerhalb der durch einen anderen Parameter definierten Bins anzuzeigen, wie
in Abbildung 36-6 gezeigt.

In [9]: with sns.axes_style(style='ticks'):
g = sns.catplot(x="day", y="total_bill", hue="sex",
data=tips, kind="box")
g.set_axis_labels("Tag", "Gesamtrechnung");

Abbildung 36-6. Ein Beispiel für ein Faktorendiagramm, das Verteilungen bei verschiedenen diskreten

Faktoren

338 | Kapitel 36: Visualisierung mit Seaborn

Gemeinsame Verteilungen
Ähnlich wie beim Paarplot, den wir zuvor gesehen haben, können wir mit sns.jointplot die gemeinsame
Verteilung zwischen verschiedenen Datensätzen, zusammen mit den zugehörigen marginalen Verteilun-

tionen (siehe Abbildung 36-7).

In [10]: mit sns.axes_style('white'):
sns.jointplot(x="total_bill", y="tip", data=tips, kind='hex')

Abbildung 36-7. Ein gemeinsames Verteilungsdiagramm

Die gemeinsame Darstellung kann sogar eine automatische Kerndichteschätzung und Regression durchführen,
wie in Abbildung 36-8 gezeigt.

In [11]: sns.jointplot(x="total_bill", y="tip", data=tips, kind='reg');

Kategoriale Darstellungen | 339
Abbildung 36-8. Eine gemeinsame Verteilungsdarstellung mit einer Regressionsanpassung

Balkendiagramme
Zeitreihen können mit sns.factorplot aufgezeichnet werden. Im folgenden Beispiel verwenden wir
den Planetendatensatz, den wir in Kapitel 20 zum ersten Mal gesehen haben; das Ergebnis ist in Abbildung 36-9 zu sehen.

In [12]: planets = sns.load_dataset('planets')
planets.head()
Out[12]: methode nummer orbital_periode masse entfernung jahr
0 Radialgeschwindigkeit 1 269.300 7.10 77.40 2006
1 Radialgeschwindigkeit 1 874,774 2,21 56,95 2008
2 Radialgeschwindigkeit 1 763,000 2,60 19,84 2011
3 Radialgeschwindigkeit 1 326,030 19,40 110,62 2007
4 Radialgeschwindigkeit 1 516,220 10,50 119,47 2009

In [13]: mit sns.axes_style('white'):
g = sns.catplot(x="year", data=planets, aspect=2,
kind="count", color='steelblue')
g.set_xticklabels(step=5)

340 | Kapitel 36: Visualisierung mit Seaborn

Abbildung 36-9. Ein Histogramm als Spezialfall eines Faktorendiagramms

Wir können mehr erfahren, wenn wir uns die Methode der Entdeckung jedes dieser Planeten ansehen (siehe

Abbildung 36-10).

In [14]: with sns.axes_style('white'):
g = sns.catplot(x="year", data=planets, aspect=4.0, kind='count',
hue='method', order=range(2001, 2015))
g.set_ylabels('Anzahl der entdeckten Planeten')

Abbildung 36-10. Anzahl der entdeckten Planeten nach Jahr und Typ

Weitere Informationen zum Plotten mit Seaborn finden Sie in der Seaborn-Dokumentation, und

insbesondere die Beispielgalerie.

Kategoriale Diagramme | 341
2 Die in diesem Abschnitt verwendeten Marathondaten teilen die Läufer in zwei Geschlechter ein: Männer und Frauen. Das Geschlecht ist zwar ein
Spektrum ist, verwenden die folgende Diskussion und die Visualisierungen dieses Binärformat, weil sie von den Daten abhängen.
3 Wenn Sie an der Verwendung von Python für Web Scraping interessiert sind, empfehle ich Ihnen Web Scraping with Python von
Ryan Mitchell, ebenfalls von O'Reilly.
Beispiel: Untersuchung von Marathon-Zielzeiten
Hier sehen wir uns die Verwendung von Seaborn an, um die Zieleinlaufergebnisse zu visualisieren und zu verstehen

von einem Marathon.^2 Ich habe die Daten aus Quellen im Internet zusammengesucht, sie aggregiert und
und alle identifizierenden Informationen entfernt und sie auf GitHub gestellt, wo sie

heruntergeladen.^3

Wir beginnen mit dem Herunterladen der Daten und laden sie in Pandas:

In [15]: _# url = ('https://raw.githubusercontent.com/jakevdp/'

'marathon-data/master/marathon-data.csv')
!cd data && curl -O {url}_
In [16]: daten = pd.read_csv('daten/marathon-daten.csv')
data.head()
Out[16]: Alter Geschlecht split final
0 33 M 01:05:38 02:08:51
1 32 M 01:06:26 02:09:28
2 31 M 01:06:49 02:10:42
3 38 M 01:06:16 02:13:45
4 31 M 01:06:32 02:13:59

Beachten Sie, dass Pandas die Zeitspalten als Python-Strings (Typ Objekt) geladen hat; wir können

Sie können dies anhand des Attributs dtypes des DataFrame erkennen:

In [17]: data.dtypes
Out[17]: Alter int64
gender Objekt
split-Objekt
endgültiges Objekt
dtype: objekt

Wir sollten das Problem lösen, indem wir einen Umrechner für die Zeiten bereitstellen:

In [18]: import datetime

def convert_time(s):
h, m, s = map(int, s.split(':'))
return datetime.timedelta(hours=h, minutes=m, seconds=s)

data = pd.read_csv('data/marathon-data.csv',
Konverter={'split':convert_time, 'final':convert_time})
data.head()
Out[18]: Alter Geschlecht split final
0 33 M 0 Tage 01:05:38 0 Tage 02:08:51
1 32 M 0 Tage 01:06:26 0 Tage 02:09:28

342 | Kapitel 36: Visualisierung mit Seaborn

2 31 M 0 Tage 01:06:49 0 Tage 02:10:42
3 38 M 0 Tage 01:06:16 0 Tage 02:13:45
4 31 M 0 Tage 01:06:32 0 Tage 02:13:59

In [19]: data.dtypes
Out[19]: Alter int64
Geschlecht Objekt
split timedelta64[ns]
endgültig timedelta64[ns]
dTyp: Objekt

Das wird die Bearbeitung der zeitlichen Daten erleichtern. Für die Zwecke unserer Sea-

Nach der Erstellung von Diagrammen fügen wir nun Spalten hinzu, die die Zeiten in Sekunden angeben:

In [20]: data['split_sec'] = data['split'].view(int) / 1E9
data['final_sec'] = data['final'].view(int) / 1E9
data.head()
Out[20]: Alter Geschlecht split final split_sec final_sec
0 33 M 0 Tage 01:05:38 0 Tage 02:08:51 3938.0 7731.0
1 32 M 0 Tage 01:06:26 0 Tage 02:09:28 3986.0 7768.0
2 31 M 0 Tage 01:06:49 0 Tage 02:10:42 4009.0 7842.0
3 38 M 0 Tage 01:06:16 0 Tage 02:13:45 3976.0 8025.0
4 31 M 0 Tage 01:06:32 0 Tage 02:13:59 3992.0 8039.0

Um eine Vorstellung davon zu bekommen, wie die Daten aussehen, können wir ein Jointplot über die Daten zeichnen;
Abbildung 36-11 zeigt das Ergebnis.

In [21]: mit sns.axes_style('white'):
g = sns.jointplot(x='split_sec', y='final_sec', data=data, kind='hex')
g.ax_joint.plot(np.linspace(4000, 16000),
np.linspace(8000, 32000), ':k')

Die gestrichelte Linie zeigt, wo die Zeit einer Person liegen würde, wenn sie den Marathon in einem
gleichmäßigem Tempo laufen würde. Die Tatsache, dass die Verteilung oberhalb dieser Linie liegt, zeigt (wie Sie

Die meisten Menschen werden im Laufe des Marathons langsamer, als man erwarten würde. Wenn Sie
Wenn Sie schon einmal Wettkämpfe gelaufen sind, werden Sie wissen, dass diejenigen, die das Gegenteil tun - schneller laufen

in der zweiten Hälfte des Rennens - gelten als "Negativsplit" des Rennens.

Beispiel: Untersuchung der Marathon-Endzeiten | 343
Abbildung 36-11. Die Beziehung zwischen dem Split für den ersten Halbmarathon und der Endzeit für den
und der Zielzeit für den Vollmarathon

Legen wir eine weitere Spalte in den Daten an, den Splitanteil, der den Grad
inwieweit jeder Läufer einen negativen oder positiven Split im Rennen hat:

In [22]: data['split_frac'] = 1 - 2 * data['split_sec'] / data['final_sec']
data.head()
Out[22]: Alter Geschlecht split final split_sec final_sec
0 33 M 0 Tage 01:05:38 0 Tage 02:08:51 3938.0 7731.0
1 32 M 0 Tage 01:06:26 0 Tage 02:09:28 3986.0 7768.0
2 31 M 0 Tage 01:06:49 0 Tage 02:10:42 4009.0 7842.0
3 38 M 0 Tage 01:06:16 0 Tage 02:13:45 3976.0 8025.0
4 31 M 0 Tage 01:06:32 0 Tage 02:13:59 3992.0 8039.0

split_frac
0 -0.018756
1 -0.026262
2 -0.022443
3 0.009097
4 0.006842

344 | Kapitel 36: Visualisierung mit Seaborn

Ist diese Split-Differenz kleiner als Null, so hat die Person einen Negativ-Split um diese

Fraktion. Erstellen wir ein Verteilungsdiagramm für diesen aufgeteilten Anteil (siehe Abbildung 36-12).

In [23]: sns.displot(data['split_frac'], kde= False )
plt.axvline(0, color="k", linestyle="--");

Abbildung 36-12. Die Verteilung der Splitfraktionen; 0,0 bedeutet, dass ein Läufer die

die erste und zweite Halbzeit in identischer Zeit

In [24]: sum(data.split_frac < 0)
Out[24]: 251

Von fast 40.000 Teilnehmern haben nur 250 Personen einen Negativ-Split bei ihrem
Marathon.

Wir wollen sehen, ob es eine Korrelation zwischen diesem Splitanteil und anderen Variablen gibt.

ables. Dazu verwenden wir ein PairGrid, das alle diese Korrelationen grafisch darstellt (siehe
Abbildung 36-13).

In [25]: g = sns.PairGrid(data, vars=['age', 'split_sec', 'final_sec', 'split_frac'],
hue='gender', palette='RdBu_r')
g.map(plt.scatter, alpha=0.8)
g.add_legend();

Beispiel: Untersuchung der Zieleinlaufzeiten beim Marathon | 345
Abbildung 36-13. Die Beziehung zwischen Größen im Marathon-Datensatz

Es sieht so aus, als ob der Splitanteil nicht sonderlich mit dem Alter korreliert, aber es gibt eine Korrelation.

mit der Endzeit: Schnellere Läufer neigen dazu, ihre Marathon-Zeit annähernd gleichmäßig aufzuteilen.
Zeit. Vergrößern wir das Histogramm der Splitanteile, getrennt nach Geschlecht,

in Abbildung 36-14 dargestellt.

In [26]: sns.kdeplot(data.split_frac[data.gender=='M'], label='men', shade= True )
sns.kdeplot(data.split_frac[data.gender=='W'], label='Frauen', shade= True )
plt.xlabel('split_frac');

346 | Kapitel 36: Visualisierung mit Seaborn

Abbildung 36-14. Die Verteilung der aufgeteilten Fraktionen nach Geschlecht

Interessant ist dabei, dass es viel mehr Männer als Frauen gibt, die

die fast gleichmäßig verteilt sind! Es sieht fast nach einer bimodalen Verteilung zwischen
Männern und Frauen. Mal sehen, ob wir herausfinden können, was los ist, indem wir uns die Verteilung ansehen.

tionen in Abhängigkeit vom Alter.

Eine gute Möglichkeit, Verteilungen zu vergleichen, ist die Verwendung eines Violindiagramms, wie in Abbildung 36-15 dargestellt.

In [27]: sns.violinplot(x="gender", y="split_frac", data=data,
palette=["hellblau", "hellrosa"]);

Beispiel: Untersuchung der Zieleinlaufzeiten beim Marathon | 347
Abbildung 36-15. Ein Violindiagramm, das die Aufteilung nach Geschlecht zeigt

Schauen wir uns das Ganze etwas genauer an und vergleichen diese Geigenplots in Abhängigkeit vom Alter (siehe

Abbildung 36-16). Wir beginnen mit der Erstellung einer neuen Spalte im Array, die den Altersbereich
Altersbereich, in dem sich jede Person befindet, nach Jahrzehnt angibt:

In [28]: data['age_dec'] = data.age.map( lambda age: 10 * (age // 10))
data.head()
Out[28]: Alter Geschlecht split final split_sec final_sec
0 33 M 0 Tage 01:05:38 0 Tage 02:08:51 3938.0 7731.0
1 32 M 0 Tage 01:06:26 0 Tage 02:09:28 3986.0 7768.0
2 31 M 0 Tage 01:06:49 0 Tage 02:10:42 4009.0 7842.0
3 38 M 0 Tage 01:06:16 0 Tage 02:13:45 3976.0 8025.0
4 31 M 0 Tage 01:06:32 0 Tage 02:13:59 3992.0 8039.0

split_frac age_dec
0 -0.018756 30
1 -0.026262 30
2 -0.022443 30
3 0.009097 30
4 0.006842 30

In [29]: Männer = (data.gender == 'M')
Frauen = (daten.geschlecht == 'W')

mit sns.axes_style(style= None ):
sns.violinplot(x="age_dec", y="split_frac", hue="gender", data=data,
split=True , inner="quartile",
palette=["lightblue", "lightpink"]);

348 | Kapitel 36: Visualisierung mit Seaborn

Abbildung 36-16. Ein Violindiagramm, das den aufgeteilten Anteil nach Geschlecht und Alter zeigt

Wir können sehen, wo die Verteilungen bei Männern und Frauen unterschiedlich sind: die geteilte Verteilung

Die Daten von Männern zwischen 20 und 50 Jahren zeigen eine ausgeprägte Überdichte in Richtung niedrigerer Splits
im Vergleich zu Frauen im gleichen Alter (oder in jedem Alter).

Erstaunlich ist auch, dass die 80-jährigen Frauen anscheinend besser abschneiden als alle anderen.

der Zwischenzeit, obwohl es sich dabei wahrscheinlich um einen kleinen Effekt handelt, da es nur
nur eine Handvoll Läufer in diesem Bereich sind:

In [30]: (data.age > 80).sum()
Out[30]: 7

Zurück zu den Männern mit negativen Splits: Wer sind diese Läufer? Ist dieser Bruchteil

mit einem schnellen Abschluss korrelieren? Wir können dies sehr einfach grafisch darstellen. Wir werden regplot verwenden,

wodurch automatisch ein lineares Regressionsmodell an die Daten angepasst wird (siehe Abbildung 36-17).

In [31]: g = sns.lmplot(x='final_sec', y='split_frac', col='gender', data=data,
markers=".", scatter_kws=dict(color='c'))
g.map(plt.axhline, y=0.0, color="k", ls=":");

Beispiel: Untersuchung der Zieleinlaufzeiten beim Marathon | 349
Abbildung 36-17. Aufgeteilter Anteil gegenüber der Endzeit nach Geschlecht

Offensichtlich sind sowohl bei den Männern als auch bei den Frauen die Personen mit den schnellen Spurts tendenziell schneller

Läufer, die innerhalb von ~15.000 Sekunden, also in etwa 4 Stunden, ins Ziel kommen. Menschen, die langsamer
langsamer sind, haben viel weniger Chancen auf einen schnellen Sekundensplit.

Weitere Ressourcen
Ein einzelner Teil eines Buches kann niemals alle verfügbaren Funktionen und Handlungsmöglichkeiten abdecken

Typen, die in Matplotlib verfügbar sind. Wie bei anderen Paketen, die wir gesehen haben, kann die großzügige Verwendung von IPy-
thon's Tabulatorvervollständigung und Hilfefunktionen (siehe Kapitel 1) sehr hilfreich sein, wenn

Erforschung der Matplotlib-API. Darüber hinaus kann die Online-Dokumentation von Matplotlib eine

hilfreiche Referenz. Siehe insbesondere die Matplotlib-Galerie, die Miniaturansichten von
Hunderten von verschiedenen Diagrammtypen zeigt, von denen jeder mit einer Seite mit dem Python-Code verlinkt ist.

pet verwendet, um es zu erzeugen. Auf diese Weise können Sie ein breites Spektrum an
verschiedene Darstellungsstile und Visualisierungstechniken kennen.

Für eine Behandlung von Matplotlib in Buchlänge würde ich Interactive Applica-

tions Using Matplotlib (Packt), geschrieben von Matplotlib-Kernentwickler Ben Root.

350 | Kapitel 36: Visualisierung mit Seaborn

Andere Python-Visualisierungsbibliotheken
Obwohl Matplotlib die bekannteste Python-Visualisierungsbibliothek ist, gibt es weitere

andere, modernere Instrumente, die es ebenfalls wert sind, erforscht zu werden. Ich werde ein paar von ihnen
hier kurz erwähnen:

Bokeh ist eine JavaScript-Visualisierungsbibliothek mit einem Python-Frontend, die
hochgradig interaktive Visualisierungen erstellt, die sehr große und/oder strömende
Datenmengen.
Plotly ist das gleichnamige Open-Source-Produkt des Unternehmens Plotly und ähnelt
dem Geist von Bokeh. Es wird aktiv weiterentwickelt und bietet eine breite Palette von inter
aktiven Diagrammtypen.
HoloViews ist eine deklarative, einheitliche API zur Erzeugung von Diagrammen in einer Vielzahl von
Backends, einschließlich Bokeh und Matplotlib.
Vega und Vega-Lite sind deklarative Grafikdarstellungen und das Produkt
Sie sind das Ergebnis jahrelanger Forschung im Bereich der Datenvisualisierung und Interaktion.
Die Referenzimplementierung für das Rendering ist JavaScript, und das Altair-Paket
bietet eine Python-API zur Erzeugung dieser Diagramme.
Die Visualisierungslandschaft in der Python-Welt entwickelt sich ständig weiter, und ich erwarte

dass diese Liste zum Zeitpunkt der Veröffentlichung dieses Buches bereits veraltet sein kann. Zusätzlich,
weil Python in so vielen Bereichen verwendet wird, werden Sie viele andere Visualisierungsprogramme finden.

Werkzeuge, die für spezifischere Anwendungsfälle entwickelt wurden. Es kann schwierig sein, den Überblick zu behalten, aber eine
aber eine gute Ressource, um mehr über diese große Vielfalt an Visualisierungswerkzeugen zu erfahren, ist PyViz, eine

offene, von der Gemeinschaft betriebene Website mit Anleitungen und Beispielen zu vielen verschiedenen

Visualisierungswerkzeuge.

Andere Python-Visualisierungsbibliotheken | 351
TEIL V
Maschinelles Lernen

Dieser letzte Teil ist eine Einführung in das sehr breite Thema des maschinellen Lernens, hauptsächlich
über das Python-Paket Scikit-Learn. Sie können sich maschinelles Lernen als eine Klasse von

Algorithmen, die es einem Programm ermöglichen, bestimmte Muster in einem Datensatz zu erkennen und so
aus den Daten zu "lernen" und daraus Schlüsse zu ziehen. Dies soll kein umfassender Überblick sein.

Einführung in das Gebiet des maschinellen Lernens; das ist ein großes Thema und

erfordert einen technischeren Ansatz, als wir ihn hier verfolgen. Es ist auch nicht dazu gedacht, ein
umfassendes Handbuch für die Verwendung des Scikit-Learn-Pakets sein (dafür können Sie die

zu den unter "Weitere Ressourcen zum maschinellen Lernen" auf Seite 550 aufgeführten Ressourcen).
Vielmehr sind die Ziele hier:

Einführung in das grundlegende Vokabular und die Konzepte des maschinellen Lernens
Einführung in die Scikit-Learn-API und Vorstellung einiger Anwendungsbeispiele
Einen tieferen Einblick in die Details einiger der wichtigsten klassischen
Ansätze des maschinellen Lernens und entwickeln ein Gespür dafür, wie sie funktionieren und
wann und wo sie anwendbar sind
Ein großer Teil dieses Materials stammt aus den Scikit-Learn-Tutorials und -Workshops, die ich

bei mehreren Gelegenheiten auf der PyCon, SciPy, PyData und anderen Konferenzen gehalten. Jede
Klarheit auf den folgenden Seiten ist wahrscheinlich auf die vielen Workshop-Teilnehmer und Co

Ausbilderinnen und Ausbilder, die mir über die Jahre hinweg wertvolles Feedback zu diesem Material gegeben haben!

KAPITEL 37

Was ist maschinelles Lernen?
Bevor wir einen Blick auf die Details verschiedener Methoden des maschinellen Lernens werfen, sollten wir zunächst einmal
was maschinelles Lernen ist, und was es nicht ist. Maschinelles Lernen wird oft als Kate-

als Teilgebiet der künstlichen Intelligenz bezeichnet, aber ich finde, dass diese Kategorisierung falsch sein kann.

führend. Das Studium des maschinellen Lernens ist sicherlich aus der Forschung in diesem Zusammenhang hervorgegangen,
aber bei der datenwissenschaftlichen Anwendung von Methoden des maschinellen Lernens ist es hilfreicher, die

betrachten maschinelles Lernen als ein Mittel zur Erstellung von Datenmodellen.

In diesem Zusammenhang kommt das "Lernen" ins Spiel, wenn wir diesen Modellen abstimmbare Parameter geben.

ters, die an die beobachteten Daten angepasst werden können; auf diese Weise kann das Programm als

die aus den Daten "lernen" sollen. Sobald diese Modelle an zuvor gesehene Daten angepasst wurden
Daten angepasst wurden, können sie verwendet werden, um Aspekte neu beobachteter Daten vorherzusagen und zu verstehen. Ich werde

überlasse ich dem Leser den philosophischen Exkurs über die Frage, inwieweit
inwieweit diese Art des mathematischen, modellbasierten "Lernens" dem "Lernen" ähnelt, das ausgestellt wird.

des menschlichen Gehirns.

Das Verständnis der Problemstellung beim maschinellen Lernen ist für die effektive Nutzung dieser
Wir beginnen daher mit einer groben Kategorisierung der verschiedenen Arten von

Ansätze, die wir hier diskutieren werden.

Alle Abbildungen in diesem Kapitel wurden auf der Grundlage tatsächlicher
Berechnungen des maschinellen Lernens erstellt; der Code dahinter ist
finden Sie im Online-Anhang.
Kategorien des maschinellen Lernens
Maschinelles Lernen kann in zwei Haupttypen unterteilt werden: überwachtes Lernen und

unüberwachtes Lernen.

355
Beim überwachten Lernen geht es um die Modellierung der Beziehung zwischen gemessenen

Merkmale der Daten und einige mit den Daten verknüpfte Kennzeichnungen; sobald dieses Modell bestimmt
Sobald dieses Modell ermittelt ist, kann es verwendet werden, um neue, unbekannte Daten zu beschriften. Dies wird manchmal weiter

unterteilt in Klassifizierungs- und Regressionsaufgaben: Bei der Klassifizierung werden die Etiketten

sind diskrete Kategorien, während bei der Regression die Bezeichnungen kontinuierliche Größen sind. Sie
werden im folgenden Abschnitt Beispiele für beide Arten des überwachten Lernens sehen.

Unüberwachtes Lernen beinhaltet die Modellierung der Merkmale eines Datensatzes ohne Bezug auf
Kennzeichnung. Zu diesen Modellen gehören Aufgaben wie Clustering und Dimensionalitätsreduktion.

Clustering-Algorithmen identifizieren unterschiedliche Gruppen von Daten, während die Dimensionalitätsreduktion

Algorithmen suchen nach prägnanteren Darstellungen der Daten. Sie werden im folgenden Abschnitt auch
Beispiele für beide Arten des unüberwachten Lernens finden Sie im folgenden Abschnitt.

Darüber hinaus gibt es so genannte halbüberwachte Lernmethoden, die zwischen überwachtem und unüberwachtem Lernen liegen.
die irgendwo zwischen überwachtem und unüberwachtem Lernen liegen. Halbüberwachtes

Lernmethoden sind oft nützlich, wenn nur unvollständige Beschriftungen verfügbar sind.

Qualitative Beispiele für Anwendungen des maschinellen Lernens
Um diese Ideen zu konkretisieren, lassen Sie uns einige sehr einfache Beispiele für eine

Aufgabe des maschinellen Lernens. Diese Beispiele sollen einen intuitiven, nicht quantitativen
quantitativen Überblick über die Arten von Aufgaben des maschinellen Lernens geben, mit denen wir uns in

diesem Teil des Buches. In späteren Kapiteln werden wir die einzelnen Modelle und ihre
Modelle und deren Verwendung eingehen. Für eine Vorschau auf diese eher technischen Aspekte,

Den Python-Quellcode, der die Zahlen generiert, finden Sie im Online-Anhang.

Klassifizierung: Vorhersage von diskreten Bezeichnungen
Wir werden uns zunächst eine einfache Klassifizierungsaufgabe ansehen, bei der wir eine Reihe von

markierte Punkte und möchten diese verwenden, um einige nicht markierte Punkte zu klassifizieren.

Stellen Sie sich vor, wir haben die in Abbildung 37-1 dargestellten Daten. Diese Daten sind zweidimensional:
Das heißt, wir haben zwei Merkmale für jeden Punkt, dargestellt durch die (x,y)-Positionen der

Punkte auf der Ebene. Darüber hinaus gibt es für jeden Punkt eine von zwei Klassenbezeichnungen, die hier
dargestellt durch die Farben der Punkte. Aus diesen Merkmalen und Bezeichnungen möchten wir

um ein Modell zu erstellen, mit dem wir entscheiden können, ob ein neuer Punkt als "blau" gekennzeichnet werden soll

oder "rot".

356 | Kapitel 37: Was ist maschinelles Lernen?

Abbildung 37-1. Ein einfacher Datensatz für die Klassifizierung

Es gibt eine Reihe von möglichen Modellen für eine solche Klassifizierungsaufgabe, aber wir beginnen mit

mit einer sehr einfachen. Wir gehen davon aus, dass die beiden Gruppen
durch eine gerade Linie durch die Ebene zwischen ihnen getrennt werden können, so dass die Punkte auf

auf beiden Seiten der Linie alle in dieselbe Gruppe fallen. Hier ist das Modell eine quantitative Version

der Aussage "eine gerade Linie trennt die Klassen", und die Modellparameter sind
die einzelnen Zahlen sind, die die Lage und Ausrichtung dieser Linie für unsere

Daten. Die optimalen Werte für diese Modellparameter werden aus den Daten gelernt (dies ist
das "Lernen" beim maschinellen Lernen), was oft als Training des Modells bezeichnet wird.

Abbildung 37-2 zeigt eine visuelle Darstellung, wie das trainierte Modell für diesen Fall aussieht

Daten.

Qualitative Beispiele für Anwendungen des maschinellen Lernens | 357
Abbildung 37-2. Ein einfaches Klassifikationsmodell

Nachdem dieses Modell trainiert wurde, kann es nun auf neue, nicht beschriftete Daten verallgemeinert werden. Unter

Mit anderen Worten, wir können einen neuen Datensatz nehmen, diese Linie durch ihn ziehen und Etiketten zuweisen

zu den neuen Punkten auf der Grundlage dieses Modells (siehe Abbildung 37-3). Diese Phase wird gewöhnlich als
Vorhersage.

Abbildung 37-3. Anwenden eines Klassifikationsmodells auf neue Daten

358 | Kapitel 37: Was ist maschinelles Lernen?

Dies ist der Grundgedanke einer Klassifizierungsaufgabe beim maschinellen Lernen, bei der "Klassifizie-

tion" bedeutet, dass die Daten diskrete Klassenbezeichnungen haben. Auf den ersten Blick mag dies trivial erscheinen
trivial erscheinen: Es ist einfach, unsere Daten zu betrachten und eine solche diskriminierende Linie zu ziehen, um die

diese Klassifizierung. Ein Vorteil des Ansatzes des maschinellen Lernens ist jedoch, dass er

auf viel größere Datensätze in viel mehr Dimensionen verallgemeinern. Dies ist zum Beispiel
ähnlich wie die Aufgabe der automatischen Spam-Erkennung bei E-Mails. In diesem Fall könnten wir Folgendes verwenden

die folgenden Merkmale und Kennzeichnungen:

Merkmal 1, Merkmal 2 usw. normalisierte Anzahl wichtiger Wörter oder Phrasen
("Viagra", "Verlängerte Garantie", usw.)
Kennzeichnung "Spam" oder "kein Spam"
Für die Trainingsmenge können diese Kennzeichnungen durch individuelle Inspektion einer

eine kleine repräsentative Stichprobe von E-Mails; für die übrigen E-Mails würde die Kennzeichnung
anhand des Modells bestimmt. Für einen entsprechend trainierten Klassifizierungsalgorithmus mit

genügend gut strukturierte Merkmale (in der Regel Tausende oder Millionen von Wörtern oder

Phrasen), kann diese Art von Ansatz sehr effektiv sein. Wir werden ein Beispiel für eine solche
textbasierten Klassifizierung in Kapitel 41.

Einige wichtige Klassifizierungsalgorithmen, auf die wir noch näher eingehen werden, sind Gaus-
sian Naive Bayes (siehe Kapitel 41), Support-Vektor-Maschinen (siehe Kapitel 43), und

random forest classification (siehe Kapitel 44).

Regression: Vorhersage von kontinuierlichen Bezeichnungen
Im Gegensatz zu den diskreten Kennzeichnungen eines Klassifizierungsalgorithmus werden wir als nächstes eine

einfache Regressionsaufgabe, bei der die Bezeichnungen kontinuierliche Größen sind.

Betrachten Sie die in Abbildung 37-4 gezeigten Daten, die aus einer Reihe von Punkten bestehen, die jeweils eine
kontinuierlichen Etikett.

Qualitative Beispiele für Anwendungen des maschinellen Lernens | 359
Abbildung 37-4. Ein einfacher Datensatz für die Regression

Wie beim Klassifizierungsbeispiel haben wir zweidimensionale Daten, d. h. es gibt

zwei Merkmale, die jeden Datenpunkt beschreiben. Die Farbe eines jeden Punktes stellt die
tinuierliche Bezeichnung für diesen Punkt.

Es gibt eine Reihe von möglichen Regressionsmodellen, die wir für diese Art von Daten verwenden können,
Hier wird jedoch ein einfaches lineares Regressionsmodell zur Vorhersage der Punkte verwendet. Dieses sim-

Das Modell geht davon aus, dass wir, wenn wir das Etikett als eine dritte räumliche Dimension betrachten, eine

Ebene zu den Daten. Dies ist eine übergeordnete Verallgemeinerung des bekannten Problems der
Anpassung einer Linie an Daten mit zwei Koordinaten.

In Abbildung 37-5 wird dieser Aufbau veranschaulicht.

360 | Kapitel 37: Was ist maschinelles Lernen?

Abbildung 37-5. Eine dreidimensionale Ansicht der Regressionsdaten

Beachten Sie, dass die Ebene von Merkmal 1 bis Merkmal 2 hier dieselbe ist wie in der zweidimensionalen

in Abbildung 37-4; in diesem Fall haben wir jedoch die Beschriftungen sowohl farblich als auch optisch dargestellt

und der dreidimensionalen Achsenposition. Aus dieser Sicht scheint es vernünftig, dass die Anpassung einer
Ebene durch diese dreidimensionalen Daten es uns ermöglichen würde, die erwarteten

Etikett für jeden Satz von Eingabeparametern. Zurück zu der zweidimensionalen Projektion,
Wenn wir eine solche Ebene einpassen, erhalten wir das in Abbildung 37-6 gezeigte Ergebnis.

Qualitative Beispiele für Anwendungen des maschinellen Lernens | 361
Abbildung 37-6. Eine Darstellung des Regressionsmodells

Diese Anpassungsebene gibt uns das, was wir brauchen, um die Beschriftungen für neue Punkte vorherzusagen. Visuell können wir

erhalten Sie die in Abbildung 37-7 dargestellten Ergebnisse.

Abbildung 37-7. Anwenden des Regressionsmodells auf neue Daten

Wie beim Klassifizierungsbeispiel mag diese Aufgabe bei einer geringen Anzahl von Personen trivial erscheinen.

Dimensionen. Die Stärke dieser Methoden liegt jedoch darin, dass sie ohne Umschweife

bei Daten mit vielen, vielen Merkmalen angewandt und bewertet werden. Dies ist zum Beispiel
ähnlich der Aufgabe, die Entfernung von Galaxien zu berechnen, die durch ein Teleobjektiv

In diesem Fall könnten wir die folgenden Merkmale und Bezeichnungen verwenden:

Merkmal 1, Merkmal 2, usw. Helligkeit jeder Galaxie bei einer von mehreren Wellenlängen
oder Farben
Bezeichnung der Entfernung oder Rotverschiebung der Galaxie
362 | Kapitel 37: Was ist maschinelles Lernen?

Die Entfernungen für eine kleine Anzahl dieser Galaxien könnten durch eine

unabhängiger Satz von (in der Regel teureren oder komplexeren) Beobachtungen. Die Entfernungen zu
verbleibenden Galaxien könnten dann mit Hilfe eines geeigneten Regressionsmodells geschätzt werden,

ohne die Notwendigkeit, die teurere Beobachtung für die gesamte Menge zu verwenden. Unter

In Astronomiekreisen ist dies als das Problem der "photometrischen Rotverschiebung" bekannt.

Einige wichtige Regressionsalgorithmen, die wir besprechen werden, sind die lineare Regression (siehe

Kapitel 42), Support-Vektor-Maschinen (siehe Kapitel 43) und Random-Forest-Regression
(siehe Kapitel 44).

Clustering: Ableitung von Bezeichnungen aus unbenannten Daten
Die Klassifizierungs- und Regressionsdarstellungen, die wir gerade gesehen haben, sind Beispiele für überwachte
Lernalgorithmen, bei denen wir versuchen, ein Modell zu erstellen, das Bezeichnungen vorhersagt

für neue Daten. Unüberwachtes Lernen umfasst Modelle, die Daten beschreiben, ohne sich

zu allen bekannten Etiketten.

Ein häufiger Fall von unüberwachtem Lernen ist das "Clustering", bei dem Daten automatisch

einer bestimmten Anzahl von diskreten Gruppen zugewiesen werden. Zum Beispiel könnten wir einige
zweidimensionale Daten wie die in Abbildung 37-8 dargestellten.

Abbildung 37-8. Beispieldaten für Clustering

Für das Auge ist es klar, dass jeder dieser Punkte zu einer bestimmten Gruppe gehört. Angesichts dieser Eingabe,

ein Clustermodell nutzt die innere Struktur der Daten, um zu bestimmen, welche
Punkte miteinander verbunden sind. Mit Hilfe des sehr schnellen und intuitiven k-means-Algorithmus (siehe Kap.

ter 47) finden wir die in Abbildung 37-9 dargestellten Cluster.

Qualitative Beispiele für Anwendungen des maschinellen Lernens | 363
Abbildung 37-9. Mit einem k-means Clustering-Modell beschriftete Daten

k-means passt sich einem Modell an, das aus k Clusterzentren besteht; es wird angenommen, dass die optimalen Zentren

die den Abstand eines jeden Punktes von seinem zugewiesenen Zentrum minimieren. Nochmals,

Dies mag wie eine triviale Übung in zwei Dimensionen erscheinen, aber da unsere Daten immer
und komplexer werden, können solche Clustering-Algorithmen weiterhin eingesetzt werden, um

nützliche Informationen aus dem Datensatz zu extrahieren.

Wir werden den k-means-Algorithmus in Kapitel 47 ausführlicher behandeln. Andere wichtige

Clustering-Algorithmen gehören Gaußsche Mischmodelle (siehe Kapitel 48) und spektrale

Clustering (siehe Scikit-Learn's clustering documentation).

Dimensionalitätsreduktion: Ableitung der Struktur von unmarkierten Daten
Die Dimensionalitätsreduktion ist ein weiteres Beispiel für einen unüberwachten Algorithmus, bei dem

Beschriftungen oder andere Informationen werden aus der Struktur des Datensatzes selbst abgeleitet.
Die Dimensionalitätsreduktion ist etwas abstrakter als die Beispiele, die wir uns angesehen haben

aber im Allgemeinen geht es darum, eine niedrigdimensionale Darstellung der Daten zu finden
die in gewisser Weise die relevanten Eigenschaften des vollständigen Datensatzes bewahrt. Verschiedene Dimensions-

alitätsreduktionsroutinen messen diese relevanten Qualitäten auf unterschiedliche Weise, wie wir

siehe Kapitel 46.

Ein Beispiel hierfür sind die in Abbildung 37-10 dargestellten Daten.

364 | Kapitel 37: Was ist maschinelles Lernen?

Abbildung 37-10. Beispieldaten für die Dimensionalitätsreduktion

Auf den ersten Blick wird deutlich, dass diese Daten eine gewisse Struktur aufweisen: Sie stammen aus einem ein-

dimensionalen Linie, die in diesem zweidimensionalen Raum spiralförmig angeordnet ist. In gewissem
könnte man sagen, dass diese Daten "an sich" nur eindimensional sind, obwohl diese

eindimensionale Daten werden in den zweidimensionalen Raum eingebettet. Ein geeignetes Dimensions-
alitätsreduktionsmodell würde in diesem Fall auf diese nichtlineare Einbettung reagieren

Struktur und in der Lage sein, diese Darstellung mit geringerer Dimension zu erkennen.

Abbildung 37-11 zeigt eine Visualisierung der Ergebnisse des Isomap-Algorithmus, eines vielfältigen
Lernalgorithmus, der genau dies tut.

Beachten Sie, dass die Farben (die die extrahierte eindimensionale latente Variable darstellen)
gleichmäßig entlang der Spirale verändern, was darauf hindeutet, dass der Algorithmus in der Tat

die Struktur zu erkennen, die wir mit dem Auge gesehen haben. Wie bei den vorherigen Beispielen ist die Leistung der

Algorithmen zur Dimensionalitätsreduktion wird in höherdimensionalen Fällen deutlicher.
Zum Beispiel könnten wir wichtige Beziehungen innerhalb eines Datensatzes visualisieren wollen, der

100 oder 1.000 Merkmale hat. Die Visualisierung von 1.000-dimensionalen Daten ist eine Herausforderung, und eine
eine Möglichkeit, dies handhabbarer zu machen, ist die Verwendung einer Technik zur Dimensionalitätsreduzierung.

um die Daten auf 2 oder 3 Dimensionen zu reduzieren.

Einige wichtige Algorithmen zur Dimensionalitätsreduktion, die wir erörtern werden, sind die princi-
Komponentenanalyse (siehe Kapitel 45) und verschiedene Algorithmen für das Lernen von Mannigfaltigkeiten,

einschließlich Isomap und lokal lineare Einbettung (siehe Kapitel 46).

Qualitative Beispiele für Anwendungen des maschinellen Lernens | 365
Abbildung 37-11. Daten mit durch Dimensionalitätsreduktion gelernten Labels

Zusammenfassung
Wir haben hier ein paar einfache Beispiele für einige der Grundtypen des maschinellen Lernens gesehen.
Ansätze gesehen. Natürlich gibt es eine Reihe von wichtigen praktischen Details, die

haben wir nur gestreift, aber dieses Kapitel sollte Ihnen eine grundlegende Vorstellung davon vermitteln, was

Arten von Problemen, die mit Hilfe des maschinellen Lernens gelöst werden können.

Kurz gesagt, wir haben Folgendes gesehen:

Überwachtes Lernen: Modelle, die auf der Grundlage markierter Trainingsdaten Bezeichnungen vorhersagen können
-Klassifizierung: Modelle, die Beschriftungen als zwei oder mehr diskrete Kategorien vorhersagen
-Regression: Modelle, die kontinuierliche Kennzeichnungen vorhersagen

Unüberwachtes Lernen: Modelle, die Strukturen in unbeschrifteten Daten erkennen
-Clustering: Modelle, die unterschiedliche Gruppen in den Daten erkennen und identifizieren
-Dimensionalitätsreduktion: Modelle, die niederdimensionale Strukturen in
Struktur in höherdimensionalen Daten
In den folgenden Kapiteln werden wir diese Kategorien noch weiter vertiefen,

und sehen Sie einige weitere interessante Beispiele dafür, wie diese Konzepte nützlich sein können.

366 | Kapitel 37: Was ist maschinelles Lernen?

KAPITEL 38

Einführung in Scikit-Learn
Mehrere Python-Bibliotheken bieten solide Implementierungen einer Reihe von Algorithmen für maschinelles Lernen.
lernenden Algorithmen. Eine der bekanntesten ist Scikit-Learn, ein Paket, das effi-

Versionen einer großen Anzahl von gängigen Algorithmen. Scikit-Learn ist gekennzeichnet durch

durch eine saubere, einheitliche und schlanke API sowie durch eine sehr nützliche und vollständige Dokumentation.
Dokumentation. Ein Vorteil dieser Einheitlichkeit ist, dass Sie, sobald Sie die grundlegende Verwendung

und Syntax von Scikit-Learn für einen Modelltyp ist der Wechsel zu einem neuen Modell oder Algo-
rithmus ist unkompliziert.

Dieses Kapitel gibt einen Überblick über die Scikit-Learn API. Ein solides Verständnis von

Diese API-Elemente bilden die Grundlage für das Verständnis der tieferen praktischen
Diskussion der Algorithmen und Ansätze des maschinellen Lernens in den folgenden Kapiteln.

Wir beginnen mit der Datendarstellung in Scikit-Learn, gehen dann auf die Esti-
mator API und schließlich ein interessantes Beispiel für die Verwendung dieser Werkzeuge für

eine Reihe von Bildern handgeschriebener Ziffern zu untersuchen.

Datendarstellung in Scikit-Learn
Beim maschinellen Lernen geht es darum, Modelle aus Daten zu erstellen; aus diesem Grund beginnen wir mit

Diskussion darüber, wie Daten dargestellt werden können. Der beste Weg, um über Daten in
Scikit-Learn in Form von Tabellen zu denken.

Eine Basistabelle ist ein zweidimensionales Datengitter, in dem die Zeilen die einzelnen Elemente des
einzelnen Elemente des Datensatzes darstellen und die Spalten die zu den einzelnen Elementen gehörenden Mengen

diese Elemente. Nehmen wir zum Beispiel den Iris-Datensatz, der berühmt geworden ist durch die Analyse von Ronald

Fisher im Jahr 1936. Wir können diesen Datensatz in Form eines Pandas DataFrame herunterladen

mit der Seaborn-Bibliothek und sehen Sie sich die ersten Einträge an:

367
In [1]: import seaborn as sns
iris = sns.load_dataset('iris')
iris.head()
Out[1]: sepal_length sepal_width petal_length petal_width species
0 5.1 3.5 1.4 0.2 setosa
1 4,9 3,0 1,4 0,2 setosa
2 4,7 3,2 1,3 0,2 setosa
3 4.6 3.1 1.5 0.2 setosa
4 5,0 3,6 1,4 0,2 setosa

Hier bezieht sich jede Zeile der Daten auf eine einzelne beobachtete Blüte, und die Anzahl der Zeilen

ist die Gesamtzahl der Blumen im Datensatz. Im Allgemeinen beziehen wir uns auf die Zeilen von

die Matrix als Samples und die Anzahl der Zeilen als n_samples.

Ebenso bezieht sich jede Spalte der Daten auf eine bestimmte quantitative Information.

die jede Probe beschreibt. Im Allgemeinen werden wir uns auf die Spalten der Matrix beziehen

als Features und die Anzahl der Spalten als n_features.

Die Funktionsmatrix
Das Tabellenlayout macht deutlich, dass die Informationen als zweidimensionales
dimensionales numerisches Feld oder eine Matrix, die wir als Merkmalsmatrix bezeichnen werden. Durch Kon-

wird diese Matrix häufig in einer Variablen mit dem Namen X gespeichert. Die Merkmalsmatrix ist

wird als zweidimensional angenommen, mit der Form [n_Stichproben, n_Merkmale], und ist meist

oft in einem NumPy-Array oder einem Pandas DataFrame enthalten, obwohl einige Scikit-Learn
Modelle auch dünn besetzte SciPy-Matrizen akzeptieren.

Die Stichproben (d. h. die Zeilen) beziehen sich immer auf die einzelnen Objekte, die durch den Datensatz beschrieben werden.

Eine Probe kann zum Beispiel eine Blume, eine Person, ein Dokument, ein Bild, eine
Tondatei, ein Video, ein astronomisches Objekt oder etwas anderes, das Sie mit einem

eine Reihe von quantitativen Messungen.

Die Merkmale (d. h. die Spalten) beziehen sich immer auf die einzelnen Beobachtungen, die Folgendes beschreiben

jede Probe in quantitativer Hinsicht. Merkmale sind oft reellwertig, können aber auch

Boolesche oder in einigen Fällen diskret-wertig.

Das Ziel-Array
Neben der Merkmalsmatrix X arbeiten wir im Allgemeinen auch mit einem Label- oder Target-Array,

das wir in der Regel y nennen. Das Zielfeld ist in der Regel ein-

dimensional, mit der Länge n_samples, und ist im Allgemeinen in einem NumPy-Array oder

Pandas-Reihe. Das Ziel-Array kann kontinuierliche numerische Werte oder diskrete Werte haben

Klassen/Labels. Während einige Scikit-Learn-Schätzer mit mehreren Zielwerten in

die Form eines zweidimensionalen [n_Proben, n_Ziele]-Zielfeldes, werden wir pri-

in der Regel mit einem eindimensionalen Zielfeld arbeiten.

368 | Kapitel 38: Einführung in Scikit-Learn

1 Eine farbige Version dieser Abbildung in voller Größe ist auf GitHub zu finden.
Ein häufiger Punkt der Verwirrung ist, wie sich das Ziel-Array von den anderen Merkmalen unterscheidet

Spalten. Das charakteristische Merkmal des Zielfeldes ist, dass es normalerweise die
Größe ist, die wir aus den Merkmalen vorhersagen wollen: statistisch gesehen ist es die abhängige

Variable. Anhand der vorangegangenen Daten könnten wir zum Beispiel ein Modell konstruieren

die auf der Grundlage der anderen Messungen die Art der Blume vorhersagen kann; in diesem Fall,

wird die Spalte "Art" als Ziel-Array betrachtet.

Mit diesem Zielfeld im Hinterkopf können wir Seaborn (siehe Kapitel 36) verwenden, um die

Die Daten lassen sich auf einfache Weise visualisieren (siehe Abbildung 38-1).

In [2]: % matplotlib inline
import seaborn as sns
sns.pairplot(iris, hue='species', height=1.5);

Abbildung 38-1. Eine Visualisierung des Iris-Datensatzes^1

Datendarstellung in Scikit-Learn | 369
2 Der Code zur Erstellung dieser Abbildung ist im Online-Anhang zu finden.
Für die Verwendung in Scikit-Learn extrahieren wir die Merkmalsmatrix und das Zielarray aus der Datei

DataFrame, was wir mit einigen der in Pandas DataFrame beschriebenen Operationen tun können.

die in Teil III behandelt werden:

In [3]: X_iris = iris.drop('species', axis=1)
X_iris.shape
Out[3]: (150, 4)

In [4]: y_iris = iris['species']
y_iris.shape
Out[4]: (150,)

Zusammenfassend ist die erwartete Anordnung der Merkmale und Zielwerte in
Abbildung 38-2.

Abbildung 38-2. Das Datenlayout von Scikit-Learn^2

Wenn diese Daten richtig formatiert sind, können wir uns mit der Estimate-API von Scikit-Learn beschäftigen.
tor-API.

Die Estimator-API
Die Scikit-Learn-API wurde unter Berücksichtigung der folgenden Leitprinzipien entwickelt

die im Scikit-Learn-API-Papier beschrieben sind:

370 | Kapitel 38: Einführung in Scikit-Learn

Konsistenz

Alle Objekte haben eine gemeinsame Schnittstelle, die aus einer begrenzten Anzahl von Methoden besteht, mit
konsistenter Dokumentation.
Inspektion

Alle angegebenen Parameterwerte werden als öffentliche Attribute angezeigt.

Begrenzte Objekthierarchie

Nur Algorithmen werden durch Python-Klassen dargestellt; Datensätze werden in
Standardformaten (NumPy-Arrays, Pandas DataFrame-Objekte, SciPy-Sparse-Matrizen)
ces) und Parameternamen verwenden Standard-Python-Strings.
Zusammensetzung

Viele Aufgaben des maschinellen Lernens lassen sich als Sequenzen von grundlegenderen Algorithmen ausdrücken.
Algorithmen ausgedrückt werden, und Scikit-Learn macht davon Gebrauch, wo immer dies möglich ist.
Sinnvolle Voreinstellungen

Wenn Modelle benutzerdefinierte Parameter erfordern, definiert die Bibliothek einen geeigneten
angemessenen Standardwert.
In der Praxis ist Scikit-Learn dank dieser Prinzipien sehr einfach zu benutzen, wenn man die grundlegenden Prinzipien
Prinzipien verstanden sind. Jeder Algorithmus für maschinelles Lernen in Scikit-Learn ist implementiert.

über die Estimator-API, die eine einheitliche Schnittstelle für eine Vielzahl von

von Anwendungen des maschinellen Lernens.

Grundlagen der API
In der Regel laufen die Schritte zur Verwendung der Scikit-Learn Estimator API wie folgt ab:

Wählen Sie eine Modellklasse, indem Sie die entsprechende Schätzerklasse aus Scikit-
Lernen.
Wählen Sie die Hyperparameter des Modells, indem Sie diese Klasse mit den gewünschten Werten instanziieren.
Ordnen Sie die Daten in einer Merkmalsmatrix und einem Zielvektor an, wie zuvor in diesem
Kapitel beschrieben.
Passen Sie das Modell an Ihre Daten an, indem Sie die fit-Methode der Modellinstanz aufrufen.
Wenden Sie das Modell auf neue Daten an:
Beim überwachten Lernen sagen wir oft die Beschriftungen für unbekannte Daten mit der
Methode vorhersagen.
Beim unüberwachten Lernen werden oft Eigenschaften der Daten transformiert oder abgeleitet
mit Hilfe der Transformations- oder Vorhersagemethode.
Wir werden nun einige einfache Beispiele für die Anwendung von überwachtem und unüberwachtem Lernen durchgehen.

pervised learning-Methoden.

Die Schätzer-API | 371
Beispiel zum überwachten Lernen: Einfache lineare Regression
Als Beispiel für diesen Prozess betrachten wir eine einfache lineare Regression, d. h. die

häufiger Fall der Anpassung einer Linie an x,y-Daten. Wir verwenden die folgenden einfachen Daten für
unser Regressionsbeispiel (siehe Abbildung 38-3).

In [5]: import matplotlib.pyplot as plt
import numpy as np

rng = np.random.RandomState(42)
x = 10 * rng.rand(50)
y = 2 * x - 1 + rng.randn(50)
plt.scatter(x, y);

Abbildung 38-3. Daten für die lineare Regression

Mit diesen Daten können wir das oben beschriebene Rezept verwenden. Wir gehen durch die

Prozess in den folgenden Abschnitten.

1. Wählen Sie eine Klasse von Modellen

In Scikit-Learn wird jede Modellklasse durch eine Python-Klasse repräsentiert. Also, zum Beispiel

Wenn wir ein einfaches LinearRegression-Modell berechnen möchten, können wir die

Klasse der linearen Regression:

In [6]: from sklearn.linear_model import LinearRegression

Beachten Sie, dass es auch andere, allgemeinere lineare Regressionsmodelle gibt; Sie können lesen

Mehr dazu finden Sie in der Dokumentation des Moduls sklearn.linear_model.

372 | Kapitel 38: Einführung in Scikit-Learn

2. Auswahl der Modell-Hyperparameter

Ein wichtiger Punkt ist, dass eine Klasse von Modellen nicht dasselbe ist wie eine Instanz eines Modells.

Sobald wir uns für eine Modellklasse entschieden haben, stehen uns noch einige Optionen offen.
Je nachdem, mit welcher Modellklasse wir arbeiten, müssen wir vielleicht eine oder mehrere Fragen beantworten

weitere Fragen wie die folgenden:

Möchten wir den Offset (d. h. den y-Achsenabschnitt) anpassen?
Möchten wir, dass das Modell normalisiert wird?
Möchten wir unsere Merkmale vorverarbeiten, um das Modell flexibler zu gestalten?
Welchen Grad der Regularisierung möchten wir in unserem Modell verwenden?
Wie viele Modellkomponenten möchten wir verwenden?
Dies sind Beispiele für die wichtigen Entscheidungen, die getroffen werden müssen, sobald die Modellklasse

ausgewählt. Diese Auswahlmöglichkeiten werden häufig als Hyperparameter dargestellt, d. h. als Parameter, die

muss festgelegt werden, bevor das Modell an die Daten angepasst wird. In Scikit-Learn werden die Hyperparameter
durch Übergabe von Werten bei der Modellinstanziierung ausgewählt. Wir werden untersuchen, wie Sie quanti-

Hyperparametern in Kapitel 39 zu wählen.

Für unser Beispiel der linearen Regression können wir die Klasse LinearRegression instanziieren und

angeben, dass wir den Achsenabschnitt mit dem Hyperparameter fit_intercept anpassen möchten:

In [7]: model = LinearRegression(fit_intercept= True )
Modell
Out[7]: LinearRegression()

Denken Sie daran, dass bei der Instanziierung des Modells die einzige Aktion das Speichern von

diese Hyperparameterwerte. Insbesondere haben wir das Modell noch nicht auf irgendwelche

Daten: Die Scikit-Learn-API macht einen klaren Unterschied zwischen der Wahl des Modells
und der Anwendung des Modells auf die Daten.

3. Anordnen der Daten in einer Merkmalsmatrix und einem Zielvektor

Zuvor haben wir die Scikit-Learn-Datendarstellung untersucht, die eine Zwei-

dimensionalen Merkmalsmatrix und einem eindimensionalen Zielfeld. Hier ist unsere Zielvari-

y hat bereits die richtige Form (ein Array der Länge n_Stichproben), aber wir müssen

die Daten x so massieren, dass eine Matrix der Größe [n_Stichproben, n_Merkmale] entsteht.

In diesem Fall handelt es sich um eine einfache Umformung des eindimensionalen Feldes:

In [8]: X = x[:, np.newaxis]
X.Form
Out[8]: (50, 1)

Die Estimator-API | 373
4. Anpassen des Modells an die Daten

Nun ist es an der Zeit, unser Modell auf die Daten anzuwenden. Dies kann mit der Anpassungsmethode
des Modells erfolgen:

In [9]: model.fit(X, y)
Out[9]: LinearRegression()

Dieser Fit-Befehl bewirkt, dass eine Reihe von modellabhängigen internen Berechnungen
und die Ergebnisse dieser Berechnungen werden in modellspezifischen

Attribute, die der Benutzer untersuchen kann. In Scikit-Learn werden standardmäßig alle Modellparame-

ters, die während des Anpassungsprozesses gelernt wurden, haben nachgestellte Unterstriche; zum Beispiel

in diesem linearen Modell ergibt sich Folgendes:

In [10]: model.coef_
Out[10]: array([1.9776566])

In [11]: model.intercept_
Out[11]: -0.9033107255311146

Diese beiden Parameter stellen die Steigung und den Achsenabschnitt der einfachen linearen Anpassung an die

Daten. Vergleicht man die Ergebnisse mit der Datendefinition, so stellt man fest, dass sie nahe an den
Werten liegen, die zur Erzeugung der Daten verwendet wurden: eine Steigung von 2 und ein Achsenabschnitt von -1.

Eine Frage, die sich häufig stellt, betrifft die Unsicherheit solcher internen
Modellparametern. Im Allgemeinen bietet Scikit-Learn keine Werkzeuge, um Schlüsse zu ziehen.

der internen Modellparameter selbst: Die Interpretation der Modellparameter ist

eher eine Frage der statistischen Modellierung als eine Frage des maschinellen Lernens. Maschinelles
Lernen konzentriert sich stattdessen auf die Vorhersagen des Modells. Wenn Sie sich mit den folgenden Fragen beschäftigen möchten

Bedeutung von Anpassungsparametern innerhalb des Modells zu ermitteln, stehen andere Instrumente zur Verfügung, darunter das

statsmodels Python-Paket.

5. Beschriftungen für unbekannte Daten vorhersagen

Sobald das Modell trainiert ist, besteht die Hauptaufgabe des überwachten maschinellen Lernens darin, die Ergebnisse zu bewerten.

auf der Grundlage dessen, was es über neue Daten aussagt, die nicht Teil des Trainingssatzes waren. Unter

Scikit-Learn kann dies mit der Vorhersagemethode geschehen. Für die Zwecke dieser Prüfung...

Unsere "neuen Daten" sind ein Raster von x-Werten, und wir fragen, welche y-Werte das Modell
vorhersagt:

In [12]: xfit = np.linspace(-1, 11)

Wie zuvor müssen wir diese x-Werte in eine [n_Stichproben, n_Merkmale] Merkmalsmatrix
Matrix überführen, die wir dann in das Modell einspeisen können:

In [13]: Xfit = xfit[:, np.newaxis]
yfit = model.predict(Xfit)

Zum Schluss wollen wir die Ergebnisse visualisieren, indem wir zuerst die Rohdaten und dann die Modellanpassung aufzeichnen

(siehe Abbildung 38-4).

374 | Kapitel 38: Einführung in Scikit-Learn

In [14]: plt.scatter(x, y)
plt.plot(xfit, yfit);

Abbildung 38-4. Eine einfache lineare Regressionsanpassung an die Daten

Normalerweise wird die Wirksamkeit des Modells durch den Vergleich seiner Ergebnisse mit einigen

bekannte Basislinie, wie wir im nächsten Beispiel sehen werden.

Beispiel für überwachtes Lernen: Iris-Klassifikation
Werfen wir einen Blick auf ein weiteres Beispiel für diesen Prozess, wobei wir den Iris-Datensatz verwenden, den wir besprochen haben

früher. Unsere Frage lautet: Wenn wir ein Modell mit einem Teil der Iris-Daten trainiert haben,
wie gut können wir die verbleibenden Etiketten vorhersagen?

Für diese Aufgabe verwenden wir ein einfaches generatives Modell, das als Gaussian Naive Bayes bekannt ist,

die von der Annahme ausgeht, dass jede Klasse aus einer achsengerechten Gaußschen Dis-
Verteilung gezogen wird (siehe Kapitel 41 für weitere Details). Weil es so schnell ist und keine Hyperpara-

Gaussian Naive Bayes ist oft ein gutes Modell, um eine erste Klassifizierung vorzunehmen.
Klassifizierung zu verwenden, bevor untersucht wird, ob Verbesserungen durch mehr

anspruchsvolle Modelle.

Wir möchten das Modell mit Daten evaluieren, die es noch nicht gesehen hat, und teilen daher die Daten in einen
Daten in einen Trainingssatz und einen Testsatz auf. Das könnte man auch von Hand machen, aber es ist mehr

die Utility-Funktion train_test_split zu verwenden:

In [15]: from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(X_iris, y_iris,
random_state=1)

Die Estimator-API | 375
Nachdem die Daten zusammengestellt sind, können wir unser Rezept zur Vorhersage der Beschriftungen anwenden:

In [16]: from sklearn.naive_bayes import GaussianNB # 1. Modellklasse wählen
model = GaussianNB() # 2. Modell instanziieren
model.fit(Xtrain, ytrain) # 3. Modell an Daten anpassen
y_model = model.predict(Xtest) # 4. Vorhersage für neue Daten

Schließlich können wir das Dienstprogramm accuracy_score verwenden, um den Anteil der vorhergesagten Bezeichnungen zu sehen

die mit ihren wahren Werten übereinstimmen:

In [17]: from sklearn.metrics import accuracy_score
accuracy_score(ytest, y_model)
Out [17]: 0.9736842105263158

Mit einer Genauigkeit von über 97 % sehen wir, dass selbst dieser sehr naive Klassifizierungsalgorithmus

rithmus ist für diesen speziellen Datensatz wirksam!

Beispiel für unüberwachtes Lernen: Dimensionalität von Iris
Als Beispiel für ein unüberwachtes Lernproblem wollen wir uns die Reduzierung der

Dimensionalität der Irisdaten, um sie besser visualisieren zu können. Erinnern Sie sich, dass die Irisdaten
vierdimensional sind: Für jede Probe werden vier Merkmale erfasst.

Die Aufgabe der Dimensionalitätsreduktion besteht darin, festzustellen, ob es eine
eine geeignete niederdimensionale Darstellung gibt, die die wesentlichen Merkmale der

Daten. Häufig wird die Dimensionalitätsreduktion als Hilfsmittel für die Visualisierung von Daten verwendet: Schließlich ist sie

Es ist viel einfacher, Daten in zwei Dimensionen darzustellen als in vier oder mehr Dimensionen!

Hier werden wir die Hauptkomponentenanalyse (PCA; siehe Kapitel 45) verwenden, die eine schnelle

linearen Dimensionalitätsreduktionstechnik. Wir werden das Modell bitten, zwei Kom- ponenten
Komponenten, d. h. eine zweidimensionale Darstellung der Daten.

Nach der oben beschriebenen Reihenfolge der Schritte ergibt sich folgendes Bild:

In [18]: from sklearn.decomposition import PCA # 1. Modellklasse wählen
model = PCA(n_components=2) # 2. Modell instanziieren
model.fit(X_iris) # 3. Modell an Daten anpassen
X_2D = model.transform(X_iris) # 4. Transformieren der Daten

Nun wollen wir die Ergebnisse grafisch darstellen. Eine schnelle Möglichkeit, dies zu tun, ist das Einfügen der Ergebnisse in die Originaldatei.

nal Iris DataFrame, und verwenden Sie Seaborns lmplot, um die Ergebnisse zu zeigen (siehe Abbildung 38-5).

In [19]: iris['PCA1'] = X_2D[:, 0]
Blende['PCA2'] = X_2D[:, 1]
sns.lmplot(x="PCA1", y="PCA2", hue='species', data=iris, fit_reg= False );

Wir sehen, dass die Arten in der zweidimensionalen Darstellung recht gut getrennt sind.
getrennt sind, obwohl der PCA-Algorithmus keine Kenntnis von den Artbezeichnungen hatte! Diese

lässt uns vermuten, dass eine relativ einfache Klassifizierung wahrscheinlich wirksam ist

auf den Datensatz, wie wir bereits gesehen haben.

376 | Kapitel 38: Einführung in Scikit-Learn

3 Eine vollfarbige Version dieser Abbildung ist auf GitHub zu finden.
Abbildung 38-5. Die auf zwei Dimensionen projizierten Iris-Daten^3

Beispiel für unüberwachtes Lernen: Iris-Clustering
Als Nächstes betrachten wir die Anwendung von Clustering auf die Irisdaten. Ein Clustering-Algorithmus versucht
unterschiedliche Datengruppen zu finden, ohne sich auf irgendwelche Kennzeichnungen zu beziehen. Hier werden wir einen

leistungsfähige Clustermethode, das so genannte Gaußsche Mischmodell (GMM), das in

ausführlicher in Kapitel 48. Ein GMM versucht, die Daten als eine Sammlung von
Gaußschen Klecksen zu modellieren.

Wir können das Gaußsche Mischungsmodell wie folgt anpassen:

In [20]: from sklearn.mixture import GaussianMixture # 1. Modellklasse wählen
model = GaussianMixture(n_components=3,
covariance_type='full') # 2. Modell instanziieren
model.fit(X_iris) # 3. Anpassung des Modells an die Daten
y_gmm = model.predict(X_iris) # 4. Beschriftungen bestimmen

Wie zuvor fügen wir die Clusterbeschriftung zum Iris DataFrame hinzu und verwenden Seaborn zum Plotten

die Ergebnisse (siehe Abbildung 38-6).

Die Estimator-API | 377
4 Eine farbige Version dieser Abbildung in voller Größe ist auf GitHub zu finden.
In [21]: iris['cluster'] = y_gmm
sns.lmplot(x="PCA1", y="PCA2", data=iris, hue='species',
col='cluster', fit_reg= False );

Abbildung 38-6. k-means-Cluster innerhalb der Iris-Daten^4

Durch die Aufteilung der Daten nach Clusternummern können wir genau sehen, wie gut der GMM-Algorithmus

hat die zugrundeliegenden Bezeichnungen wiederhergestellt: Die Setosa-Spezies ist perfekt in
Cluster 0 getrennt, während es eine geringe Vermischung zwischen versicolor und vir-

ginica. Das bedeutet, dass wir auch ohne einen Experten, der uns die Artbezeichnungen der einzelnen
der einzelnen Blüten zu bestimmen, sind die Maße dieser Blüten so eindeutig, dass wir sie

automatisch das Vorhandensein dieser verschiedenen Artengruppen mit einer einfachen

Clustering-Algorithmus! Diese Art von Algorithmus könnte den Experten auf dem Gebiet
Anhaltspunkte für die Beziehungen zwischen den von ihnen beobachteten Proben geben.

Anwendung: Erkundung handgeschriebener Ziffern
Um diese Prinzipien an einem interessanteren Problem zu demonstrieren, betrachten wir ein

Teil des Problems der optischen Zeichenerkennung: die Identifizierung von handgeschriebenen
Ziffern. In der freien Wildbahn umfasst dieses Problem sowohl das Auffinden als auch die Identifizierung von Zeichen in

ein Bild. Hier nehmen wir eine Abkürzung und verwenden den Satz vorformatierter Ziffern von Scikit-Learn,

die in die Bibliothek integriert ist.

Laden und Visualisieren der Zifferndaten
Wir können die Datenzugriffsschnittstelle von Scikit-Learn verwenden, um einen Blick auf diese Daten zu werfen:

In [22]: from sklearn.datasets import load_digits
Ziffern = load_digits()
digits.images.shape
Out[22]: (1797, 8, 8)

378 | Kapitel 38: Einführung in Scikit-Learn

Die Bilddaten sind ein dreidimensionales Array: 1.797 Proben, die jeweils aus einem 8 ×

8 Raster von Pixeln. Lassen Sie uns die ersten hundert davon visualisieren (siehe Abbildung 38-7).

In [23]: import matplotlib.pyplot as plt

fig, axes = plt.subplots(10, 10, figsize=(8, 8),
subplot_kw={'xticks':[], 'yticks':[]},
gridspec_kw=dict(hspace=0.1, wspace=0.1))

for i, ax in enumerate(axes.flat):
ax.imshow(digits.images[i], cmap='binary', interpolation='nearest')
ax.text(0.05, 0.05, str(ziffern.ziel[i]),
transform=ax.transAxes, color='grün')

Abbildung 38-7. Die Daten der handgeschriebenen Ziffern; jede Probe wird durch ein 8 × 8-Gitter aus

Pixel

Um mit diesen Daten in Scikit-Learn arbeiten zu können, benötigen wir ein zweidimensionales,

[n_samples, n_features] Darstellung. Wir können dies erreichen, indem wir jedes
Pixel im Bild als ein Merkmal behandelt wird: das heißt, die Pixelmatrix wird abgeflacht, so dass wir

ein Feld der Länge 64 mit Pixelwerten für jede Ziffer haben. Zusätzlich benötigen wir

das Zielfeld, das die zuvor festgelegte Bezeichnung für jede Ziffer enthält. Diese beiden

Anwendung: Erforschung handgeschriebener Ziffern | 379
Mengen werden in den Ziffern-Datensatz unter den Attributen Daten und Ziel eingebaut,
eingebaut:

In [24]: X = Ziffern.Daten
X.Form
Out[24]: (1797, 64)

In [25]: y = Ziffern.Ziel
y.Form
Out[25]: (1797,)

Wir sehen hier, dass es 1.797 Proben und 64 Merkmale gibt.

Beispiel für unüberwachtes Lernen: Dimensionalitätsreduktion
Wir würden gerne unsere Punkte im 64-dimensionalen Parameterraum visualisieren, aber es ist
aber es ist schwierig, Punkte in einem so hochdimensionalen Raum effektiv zu visualisieren. Stattdessen werden wir

die Anzahl der Dimensionen mit Hilfe einer unüberwachten Methode zu reduzieren. Hier werden wir
Algorithmus namens Isomap (siehe Kapitel 46) und transformieren

die Daten auf zwei Dimensionen:

In [26]: from sklearn.manifold import Isomap
iso = Isomap(n_Komponenten=2)
iso.fit(ziffern.daten)
data_projected = iso.transform(ziffern.daten)
print(daten_projiziert.form)
Out[26]: (1797, 2)

Wir sehen, dass die projizierten Daten nun zweidimensional sind. Stellen wir diese Daten dar, um zu sehen, ob wir
ob wir aus ihrer Struktur etwas lernen können (siehe Abbildung 38-8).

In [27]: plt.scatter(data_projected[:, 0], data_projected[:, 1], c=digits.target,
edgecolor='none', alpha=0.5,
cmap=plt.cm.get_cmap('viridis', 10))
plt.colorbar(label='Stellenbezeichnung', ticks=range(10))
plt.clim(-0.5, 9.5);

Dieses Diagramm gibt uns eine gute Vorstellung davon, wie gut verschiedene Zahlen voneinander getrennt sind

in dem größeren 64-dimensionalen Raum. Zum Beispiel haben Nullen und Einsen eine sehr geringe
Überlappung im Parameterraum. Intuitiv macht dies Sinn: eine Null ist in der Mitte leer

des Bildes, während bei einer Eins die Tinte im Allgemeinen in der Mitte liegt. Andererseits
scheint es ein mehr oder weniger kontinuierliches Spektrum zwischen Einsen und Vieren zu geben: Wir können

verstehen, dass manche Menschen "Hüte" zeichnen, die sie in den Händen halten.

lässt sie ähnlich wie Vieren aussehen.

Insgesamt scheinen die verschiedenen Gruppen jedoch trotz einer gewissen Vermischung an den Rändern zu sein

recht gut im Parameterraum lokalisiert: Dies legt nahe, dass selbst ein sehr geradliniger
überwachter Klassifizierungsalgorithmus eine angemessene Leistung auf dem gesamten hoch

dimensionalen Datensatz. Versuchen wir es mal.

380 | Kapitel 38: Einführung in Scikit-Learn

Abbildung 38-8. Eine Isomap-Einbettung der Zifferndaten

Klassifizierung von Ziffern
Wenden wir nun einen Klassifizierungsalgorithmus auf die Zifferndaten an. Wie bei den Iris-Daten
gemacht haben, werden wir die Daten in Trainings- und Testdatensätze aufteilen und eine Gaußsche naive

Bayes-Modell:

In [28]: Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, random_state=0)

In [29]: from sklearn.naive_bayes import GaussianNB
model = GaussianNB()
model.fit(Xtrain, ytrain)
y_model = model.predict(Xtest)

Da wir nun die Vorhersagen des Modells kennen, können wir seine Genauigkeit messen, indem wir die
die wahren Werte der Testmenge mit den Vorhersagen vergleichen:

In [30]: from sklearn.metrics import accuracy_score
accuracy_score(ytest, y_model)
Out [30]: 0.8333333333333334

Selbst mit diesem sehr einfachen Modell erreichen wir eine Genauigkeit von 83 % bei der Klassifizierung von

die Ziffern! Diese eine Zahl sagt uns jedoch nicht, wo wir uns geirrt haben. Eine

Eine gute Möglichkeit, dies zu tun, ist die Verwendung der Konfusionsmatrix, die wir mit Scikit-
Learn berechnen und mit Seaborn darstellen können (siehe Abbildung 38-9).

In [31]: from sklearn.metrics import confusion_matrix

mat = confusion_matrix(ytest, y_model)

sns.heatmap(mat, square= True , annot= True , cbar= False , cmap='Blues')

Anwendung: Erforschung handgeschriebener Ziffern | 381
plt.xlabel('vorhergesagter Wert')
plt.ylabel('wahrer Wert');

Dies zeigt uns, wo die falsch beschrifteten Punkte zu finden sind: zum Beispiel sind viele der Zweier
hier entweder als Einsen oder Achten fehlinterpretiert.

Abbildung 38-9. Eine Konfusionsmatrix, die die Häufigkeit von Fehlklassifizierungen durch unseren
Klassifikator

Eine weitere Möglichkeit, sich ein Bild von den Eigenschaften des Modells zu machen, ist die Darstellung der
Eingaben mit ihren vorhergesagten Bezeichnungen darzustellen. Wir verwenden Grün für korrekte Bezeichnungen und Rot für

falsche Etiketten; siehe Abbildung 38-10.

In [32]: fig, axes = plt.subplots(10, 10, figsize=(8, 8),
subplot_kw={'xticks':[], 'yticks':[]},
gridspec_kw=dict(hspace=0.1, wspace=0.1))

test_images = Xtest.reshape(-1, 8, 8)

for i, ax in enumerate(axes.flat):
ax.imshow(test_images[i], cmap='binary', interpolation='nearest')
ax.text(0.05, 0.05, str(y_model[i]),
transform=ax.transAxes,
color='grün' if (ytest[i] == y_model[i]) else 'rot')

Die Untersuchung dieser Teilmenge der Daten kann uns einen Einblick geben, wo der Algorithmus
möglicherweise nicht optimal arbeitet. Um über unsere 83%ige Klassifizierungserfolgsrate hinauszugehen,

könnten wir zu einem ausgefeilteren Algorithmus wie Support-Vektor-Maschinen wechseln

(siehe Kapitel 43), Random Forests (siehe Kapitel 44) oder einen anderen Klassifizierungsansatz.

382 | Kapitel 38: Einführung in Scikit-Learn

Abbildung 38-10. Daten mit richtigen (grün) und falschen (rot) Beschriftungen; eine Farbversion
dieser Darstellung finden Sie in der Online-Version des Buches

Zusammenfassung
In diesem Kapitel haben wir die wesentlichen Funktionen der Scikit-Learn-Datenrepräsenta- tion behandelt.

und der Estimator-API. Unabhängig vom Typ des verwendeten Schätzers gilt das gleiche
importieren/instantiieren/anpassen/vorhersagen gilt das gleiche Muster. Mit diesen Informationen ausgestattet, können Sie

erkunden Sie die Scikit-Learn-Dokumentation und probieren Sie verschiedene Modelle an Ihren Daten aus.

Im nächsten Kapitel werden wir das vielleicht wichtigste Thema des maschinellen Lernens
Lernens: die Auswahl und Validierung Ihres Modells.

Zusammenfassung | 383
KAPITEL 39

Hyperparameter und Modellvalidierung
Im vorherigen Kapitel haben wir das Grundrezept für die Anwendung eines überwachten maschinellen
Lernmodells kennengelernt:

Wählen Sie eine Klasse von Modellen.
Wählen Sie die Hyperparameter des Modells.
Passen Sie das Modell an die Trainingsdaten an.
Verwendung des Modells zur Vorhersage von Kennzeichnungen für neue Daten.
Die ersten beiden Teile dieses Prozesses - die Wahl des Modells und die Wahl der Hyperparameter - sind

vielleicht das Wichtigste, um diese Instrumente und Techniken wirksam einzusetzen. Unter

Um fundierte Entscheidungen treffen zu können, müssen wir überprüfen können, ob unser Modell und unsere
Hyperparameter eine gute Anpassung an die Daten darstellen. Dies mag zwar einfach klingen, aber es gibt

einige Fallstricke, die Sie vermeiden müssen, um dies effektiv zu tun.

Über die Modellvalidierung nachdenken
Im Prinzip ist die Modellvalidierung sehr einfach: Nach der Auswahl eines Modells und seiner Hyper-
Parameter, können wir abschätzen, wie effektiv es ist, indem wir es auf einige der Trainingsdaten anwenden.

Daten und vergleicht die Vorhersagen mit den bekannten Werten.

In diesem Abschnitt wird zunächst ein naiver Ansatz zur Modellvalidierung vorgestellt und erläutert, warum er scheitert,
bevor die Verwendung von Holdout-Sets und Kreuzvalidierung für robustere Modelle untersucht wird

Bewertung.

384
Modellvalidierung auf die falsche Art
Beginnen wir mit dem naiven Ansatz der Validierung anhand des Iris-Datensatzes, den wir gesehen haben

aus dem vorherigen Kapitel. Wir beginnen mit dem Laden der Daten:

In [1]: from sklearn.datasets import load_iris
iris = load_iris()
X = iris.data
y = iris.target

Als nächstes wählen wir ein Modell und Hyperparameter. Hier verwenden wir ein K-Nächste-Nachbarn-Modell

Klassifikator mit n_neighbors=1. Dies ist ein sehr einfaches und intuitives Modell, das besagt, dass "die

Die Bezeichnung eines unbekannten Punktes ist die gleiche wie die Bezeichnung des nächstgelegenen Trainingspunktes":

In [2]: from sklearn.neighbors import KNeighborsClassifier
model = KNeighborsClassifier(n_neighbors=1)

Dann trainieren wir das Modell und verwenden es zur Vorhersage von Bezeichnungen für Daten, deren Bezeichnungen wir bereits kennen.

wissen:

In [3]: model.fit(X, y)
y_model = model.predict(X)

Schließlich berechnen wir den Anteil der richtig beschrifteten Punkte:

In [4]: from sklearn.metrics import accuracy_score
accuracy_score(y, y_model)
Out[4]: 1.0

Wir sehen einen Genauigkeitswert von 1,0, was bedeutet, dass 100 % der Punkte von unserem Modell richtig
von unserem Modell richtig beschriftet wurden! Aber wird damit wirklich die erwartete Genauigkeit gemessen? Haben wir

Haben wir wirklich ein Modell gefunden, von dem wir erwarten, dass es in 100 % der Fälle richtig ist?

Wie Sie vielleicht schon gemerkt haben, lautet die Antwort nein. Tatsächlich enthält dieser Ansatz einen grundlegenden
einen grundlegenden Fehler: Das Modell wird auf denselben Daten trainiert und evaluiert. Außerdem ist dieser

Modell des nächsten Nachbarn ist ein instanzbasierter Schätzer, der einfach die Trainingsdaten speichert
Trainingsdaten speichert und Kennzeichnungen vorhersagt, indem es neue Daten mit diesen gespeicherten Punkten vergleicht: außer in Kon-

In den meisten Fällen wird eine 100%ige Genauigkeit erreicht!

Modellvalidierung auf die richtige Art: Holdout-Sets
Was kann also getan werden? Ein besseres Gefühl für die Leistung eines Modells erhält man durch die Verwendung von

ein sogenanntes Holdout-Set: Das heißt, wir halten eine Teilmenge der Daten von der

Training des Modells, und verwenden Sie dann diese Überbrückungsmenge, um die Leistung des Modells zu überprüfen.

Leistung. Diese Aufteilung kann mit dem Dienstprogramm train_test_split in Scikit-Learn vorgenommen werden:

In [5]: from sklearn.model_selection import train_test_split
# Aufteilung der Daten mit 50% in jedem Satz
X1, X2, y1, y2 = train_test_split(X, y, random_state=0,
train_size=0.5)

Nachdenken über Modellvalidierung | 385
1 Der Code zur Erstellung dieser Abbildung ist im Online-Anhang zu finden.
# Anpassung des Modells an einen Satz von Daten
model.fit(X1, y1)

# Auswertung des Modells anhand des zweiten Datensatzes
y2_model = model.predict(X2)
genauigkeit_punktzahl(y2, y2_modell)
Out[5]: 0.9066666666666666

Wir sehen hier ein vernünftigeres Ergebnis: der One-Nearest-Neighbor-Klassifikator ist etwa
90 % genau auf diesem Holdout-Set. Der Holdout-Satz ähnelt den unbekannten Daten, denn

das Modell hat es noch nicht "gesehen".

Modellvalidierung durch Kreuzvalidierung
Ein Nachteil der Verwendung eines Holdout-Sets für die Modellvalidierung ist, dass wir eine

Teil unserer Daten für das Modelltraining. Im vorangegangenen Fall trägt die Hälfte des Datensatzes
nicht zur Ausbildung des Modells beigetragen! Dies ist nicht optimal, insbesondere wenn die anfänglichen

Die Menge der Trainingsdaten ist klein.

Eine Möglichkeit, dieses Problem zu lösen, ist die Kreuzvalidierung, d. h. die Durchführung einer Reihe von Anpassungen
durchzuführen, wobei jede Teilmenge der Daten sowohl als Trainingsmenge als auch als Validierungsmenge verwendet wird.

Optisch könnte es etwa so aussehen wie in Abbildung 39-1.

Abbildung 39-1. Visualisierung der zweifachen Kreuzvalidierung^1

Hier führen wir zwei Validierungsversuche durch, bei denen wir abwechselnd die beiden Hälften der Daten als Wartezeit verwenden

setzen. Unter Verwendung der geteilten Daten von vorhin könnten wir dies wie folgt umsetzen:

In [6]: y2_model = model.fit(X1, y1).predict(X2)
y1_model = model.fit(X2, y2).predict(X1)
accuracy_score(y1, y1_model), accuracy_score(y2, y2_model)
Out[6]: (0.96, 0.90666666666666)

Das Ergebnis sind zwei Genauigkeitswerte, die wir kombinieren könnten (indem wir z. B. die

Mittelwert), um ein besseres Maß für die globale Modellleistung zu erhalten. Diese besondere Form
der Kreuzvalidierung ist eine zweifache Kreuzvalidierung, d. h. eine, bei der wir die

Die Daten wurden in zwei Sätze aufgeteilt und jeder Satz wurde als Validierungssatz verwendet.

386 | Kapitel 39: Hyperparameter und Modellvalidierung

2 Der Code zur Erstellung dieser Abbildung ist im Online-Anhang zu finden.
Wir könnten diese Idee ausbauen, um noch mehr Versuche und mehr Faltungen in den Daten zu verwenden - für

Beispiel zeigt Abbildung 39-2 eine visuelle Darstellung der fünffachen Kreuzvalidierung.

Abbildung 39-2. Visualisierung der fünffachen Kreuzvalidierung^2

Im Folgenden werden die Daten in fünf Gruppen aufgeteilt, die nacheinander zur Bewertung der Modellanpassung herangezogen werden

für die anderen vier Fünftel der Daten. Dies wäre ziemlich mühsam, wenn man es von Hand machen würde, aber

können wir die Komfortroutine cross_val_score von Scikit-Learn verwenden, um dies kurz und bündig zu tun:

In [7]: from sklearn.model_selection import cross_val_score
cross_val_score(model, X, y, cv=5)
Out[7]: array([0.96666667, 0.96666667, 0.93333333, 0.93333333, 1. ])

Durch die Wiederholung der Validierung in verschiedenen Teilmengen der Daten erhalten wir eine noch bessere

eine Vorstellung von der Leistung des Algorithmus.

Scikit-Learn implementiert eine Reihe von Kreuzvalidierungsschemata, die bei der Par-

Diese werden über Iteratoren in der model_selection-Module implementiert.
ule implementiert. Wir könnten zum Beispiel den Extremfall wählen, in dem unsere Anzahl von

Die Anzahl der Foldings ist gleich der Anzahl der Datenpunkte, d.h. wir trainieren mit allen Punkten außer einem in
jedem Versuch. Diese Art der Kreuzvalidierung wird als Leave-One-Out-Kreuzvalidierung bezeichnet,

und kann wie folgt verwendet werden:

In [8]: from sklearn.model_selection import LeaveOneOut
Ergebnisse = cross_val_score(model, X, y, cv=LeaveOneOut())
Punktzahlen
Out[8]: array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1..,
1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,

Nachdenken über Modellvalidierung | 387
1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,
1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])
Da wir 150 Stichproben haben, ergibt die Leave-One-Out-Kreuzvalidierung Scores für 150

und jede Punktzahl bedeutet entweder eine erfolgreiche (1,0) oder eine erfolglose (0,0) Vorhersage.
Diktion. Die Schätzung der Fehlerquote ergibt sich aus dem Mittelwert dieser Werte:

In [9]: ergebnisse.mittel()
Out[9]: 0.96

Andere Kreuzvalidierungsverfahren können in ähnlicher Weise verwendet werden. Für eine Beschreibung dessen, was ist

verfügbar in Scikit-Learn, verwenden Sie IPython, um die sklearn.model_selection sub-

Modul, oder werfen Sie einen Blick in die Dokumentation zur Kreuzvalidierung von Scikit-Learn.

Auswählen des besten Modells
Nachdem wir nun die Grundlagen der Validierung und der Kreuzvalidierung kennengelernt haben, werden wir uns mit folgenden Themen beschäftigen

etwas ausführlicher auf die Modellauswahl und die Auswahl von Hyperparametern eingehen.
Diese Themen gehören zu den wichtigsten Aspekten in der Praxis des maschinellen Lernens.

aber ich finde, dass diese Informationen in einführenden Tutorien zum maschinellen Lernen oft zu kurz kommen.
Tutorials zum maschinellen Lernen.

Von zentraler Bedeutung ist die folgende Frage: Wenn unser Schätzer unterdurchschnittlich abschneidet, wie

sollten wir vorankommen? Es gibt mehrere mögliche Antworten:

Verwenden Sie ein komplizierteres/flexibleres Modell.
Verwenden Sie ein weniger kompliziertes/weniger flexibles Modell.
Sammeln Sie mehr Trainingsmuster.
Sammeln Sie mehr Daten, um den einzelnen Stichproben Merkmale hinzuzufügen.
Die Antwort auf diese Frage ist oft kontraintuitiv. Insbesondere kann die Verwendung eines
komplizierteres Modell zu schlechteren Ergebnissen, und das Hinzufügen von mehr Trainingsstichproben

kann Ihre Ergebnisse nicht verbessern! Die Fähigkeit zu bestimmen, welche Schritte Ihr Modell verbessern
Modell verbessern, unterscheidet die erfolgreichen Praktiker des maschinellen Lernens von den

erfolglos.

388 | Kapitel 39: Hyperparameter und Modellvalidierung

3 Der Code zur Erstellung dieser Abbildung ist im Online-Anhang zu finden.
Der Kompromiss zwischen Verzerrung und Varianz
Grundsätzlich geht es bei der Suche nach dem "besten Modell" darum, einen "Sweet Spot" im Trade-off zu finden

zwischen Verzerrung und Varianz. Betrachten Sie Abbildung 39-3, die zwei Regressionsanpassungen
für denselben Datensatz zeigt.

Abbildung 39-3. Regressionsmodelle mit hoher Vorspannung und hoher Varianz^3

Es ist klar, dass keines dieser Modelle besonders gut zu den Daten passt, aber sie

auf unterschiedliche Weise scheitern.

Das Modell auf der linken Seite versucht, eine geradlinige Anpassung an die Daten zu finden. Weil in

In diesem Fall kann eine gerade Linie die Daten nicht genau aufteilen, das lineare Modell wird
niemals in der Lage sein, diesen Datensatz gut zu beschreiben. Bei einem solchen Modell spricht man von einer Unteranpassung der Daten:

d. h., sie ist nicht flexibel genug, um alle Merkmale in der Datenbank angemessen zu berücksichtigen.

Daten. Man könnte auch sagen, dass das Modell eine starke Verzerrung aufweist.

Mit dem Modell auf der rechten Seite wird versucht, ein Polynom hoher Ordnung an die Daten anzupassen.

In diesem Fall ist die Modellanpassung flexibel genug, um die feinen Merkmale in den Daten nahezu perfekt zu berücksichtigen.
Strukturen in den Daten zu berücksichtigen, aber auch wenn es die Trainingsdaten sehr genau beschreibt, ist seine

die genaue Form scheint die besonderen Rauscheigenschaften der Daten besser widerzuspiegeln

als von den eigentlichen Eigenschaften des Prozesses, der diese Daten erzeugt hat. Ein solches Modell
wird als "overfit" bezeichnet, d. h., es ist so flexibel, dass das Modell am Ende

die sowohl zufällige Fehler als auch die zugrunde liegende Datenverteilung berücksichtigt. Eine andere
Man kann auch sagen, dass das Modell eine hohe Varianz aufweist.

Um dies in einem anderen Licht zu betrachten, überlegen Sie, was passiert, wenn wir diese beiden Modelle verwenden, um

die y-Werte für einige neue Daten vorhersagen. In den Diagrammen in Abbildung 39-4 zeigen die roten/helleren
Punkte Daten an, die in der Trainingsmenge nicht enthalten sind.

Auswahl des besten Modells | 389
4 Der Code zur Erstellung dieser Abbildung ist im Online-Anhang zu finden.
Abbildung 39-4. Trainings- und Validierungsergebnisse in Modellen mit hohem Bias und hoher Varianz^4

Das Ergebnis hier ist der R^2-Wert oder das Bestimmtheitsmaß, das angibt, wie

wie gut ein Modell im Vergleich zu einem einfachen Mittelwert der Zielwerte abschneidet. R^2 = 1 zeigt an

eine perfekte Übereinstimmung, R^2 = 0 bedeutet, dass das Modell nicht besser ist als der
Mittelwert der Daten, und negative Werte bedeuten noch schlechtere Modelle. Aus den zugehörigen Punktzahlen

Im Zusammenhang mit diesen beiden Modellen können wir eine Beobachtung machen, die ganz allgemein gilt:

Bei Modellen mit hohem Bias ist die Leistung des Modells in der Validierungsmenge ähnlich
der Leistung in der Trainingsmenge.
Bei Modellen mit hoher Varianz ist die Leistung des Modells in der Validierungsmenge
weitaus schlechter als die Leistung auf dem Trainingsset.
Wenn wir uns vorstellen, dass wir die Komplexität des Modells einstellen können, würden wir
würden wir erwarten, dass sich die Trainings- und Validierungsergebnisse wie in Abbildung 39-5 dargestellt verhalten,

die oft als Validierungskurve bezeichnet wird, und die folgende Merkmale aufweist

Der Trainingswert ist überall höher als der Validierungswert. Dies ist generell
der Fall: Das Modell passt besser zu Daten, die es gesehen hat, als zu Daten, die es
nicht gesehen hat.
Bei einer sehr geringen Modellkomplexität (ein Modell mit hohem Bias) sind die Trainingsdaten unterangepasst,
was bedeutet, dass das Modell ein schlechter Prädiktor sowohl für die Trainingsdaten als auch für
alle zuvor nicht gesehenen Daten.
Bei einer sehr hohen Modellkomplexität (ein Modell mit hoher Varianz) sind die Trainingsdaten
Überanpassung, was bedeutet, dass das Modell die Trainingsdaten sehr gut vorhersagt, aber
für alle zuvor nicht gesehenen Daten.
390 | Kapitel 39: Hyperparameter und Modellvalidierung

5 Der Code zur Erstellung dieser Abbildung ist im Online-Anhang zu finden.
Bei einem Zwischenwert hat die Validierungskurve ein Maximum. Dieses Maß an
Komplexität zeigt einen geeigneten Kompromiss zwischen Verzerrung und Varianz an.
Die Mittel zur Abstimmung der Modellkomplexität variieren von Modell zu Modell; wenn wir

In späteren Kapiteln werden wir die einzelnen Modelle eingehend erörtern, um zu sehen, wie jedes Modell

ermöglicht eine solche Abstimmung.

Abbildung 39-5. Schematische Darstellung der Beziehung zwischen Modellkomplexität, Trainingsergebnis
und Validierungsergebnis^5

Validierungskurven in Scikit-Learn
Betrachten wir ein Beispiel für die Verwendung der Kreuzvalidierung zur Berechnung der Validierungskurve für
eine Klasse von Modellen. Hier werden wir ein polynomiales Regressionsmodell, ein verallgemeinertes lineares

Modell, bei dem der Grad des Polynoms ein abstimmbarer Parameter ist. Zum Beispiel kann ein

Polynom 1. Grades passt eine gerade Linie an die Daten an; für die Modellparameter a und b:

y=ax+b
Ein Polynom 3. Grades passt eine kubische Kurve an die Daten an; für die Modellparameter a,b,c,d:

y=ax^3 +bx^2 +cx+d
Auswahl des besten Modells | 391
Wir können dies auf eine beliebige Anzahl von polynomialen Merkmalen verallgemeinern. In Scikit-Learn können wir

dies mit einem linearen Regressionsklassifikator in Kombination mit dem polynomialen Prä
prozessor. Wir werden eine Pipeline verwenden, um diese Operationen aneinander zu reihen (wir werden die

Polynomiale Merkmale und Pipelines werden in Kapitel 40 ausführlicher behandelt):

In [10]: from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import make_pipeline

def PolynomialRegression(Grad=2, kwargs):
return make_pipeline(PolynomialFeatures(degree),
LinearRegression(kwargs))

Lassen Sie uns nun einige Daten erstellen, an die wir unser Modell anpassen wollen:

In [11]: import numpy as np

def make_data(N, err=1.0, rseed=1):
# Zufallsstichprobe der Daten
rng = np.random.RandomState(rseed)
X = rng.rand(N, 1) ** 2
y = 10 - 1. / (X.ravel() + 0.1)
if err > 0:
y += err * rng.randn(N)
X, y zurückgeben

X, y = make_data(40)

Wir können nun unsere Daten zusammen mit polynomialen Anpassungen verschiedener Grade visualisieren (siehe

Abbildung 39-6).

In [12]: % matplotlib inline
import matplotlib.pyplot as plt
plt.style.use('seaborn-whitegrid')

X_test = np.linspace(-0.1, 1.1, 500)[:, None ]

plt.scatter(X.ravel(), y, color='black')
axis = plt.axis()
für Grad in [1, 3, 5]:
y_test = PolynomialRegression(Grad).fit(X, y).predict(X_test)
plt.plot(X_test.ravel(), y_test, label='degree={0}'.format(degree))
plt.xlim(-0.1, 1.0)
plt.ylim(-2, 12)
plt.legend(loc='best');

Der Drehknopf, der die Komplexität des Modells steuert, ist in diesem Fall der Grad des Polynoms,

die eine beliebige nichtnegative ganze Zahl sein kann. Eine nützliche Frage, die es zu beantworten gilt, ist folgende: Welcher
Grad des Polynoms bietet einen geeigneten Kompromiss zwischen Verzerrung (Unteranpassung) und

Varianz (Overfitting)?

392 | Kapitel 39: Hyperparameter und Modellvalidierung

6 Eine vollfarbige Version dieser Abbildung ist auf GitHub zu finden.
Abbildung 39-6. Drei verschiedene Polynom-Modelle, angepasst an einen Datensatz^6

Wir können hier Fortschritte machen, indem wir die Validierungskurve für diese spezielle Situation visualisieren

Daten und Modell; dies kann einfach mit der von Scikit-Learn bereitgestellten Routine validation_curve con-
venience-Routine, die von Scikit-Learn bereitgestellt wird. Gegeben ein Modell, Daten, Parametername,

und einen zu untersuchenden Bereich ein, berechnet diese Funktion automatisch sowohl den Trainingswert
und den Validierungswert für den Bereich (siehe Abbildung 39-7).

In [13]: from sklearn.model_selection import validation_curve
Grad = np.arange(0, 21)
train_score, val_score = validation_curve(
PolynomialRegression(), X, y,
param_name='polynomialfeatures__degree',
param_range=Grad, cv=7)

plt.plot(grad, np.median(train_score, 1),
color='blue', label='training score')
plt.plot(grad, np.median(val_score, 1),
color='red', label='validation score')
plt.legend(loc='best')
plt.ylim(0, 1)
plt.xlabel('Grad')
plt.ylabel('score');

Auswählen des besten Modells | 393
Abbildung 39-7. Die Validierungskurven für die Daten in Abbildung 39-9

Dies zeigt genau das qualitative Verhalten, das wir erwartet haben: Die Ausbildungsnote ist immer

höher ist als der Validierungswert, verbessert sich der Trainingswert monoton
mit zunehmender Modellkomplexität, und die Validierungsbewertung erreicht ein Maximum, bevor

abnimmt, wenn das Modell überangepasst wird.

Anhand der Validierungskurve können wir feststellen, dass der optimale Kompromiss zwischen Verzerrung

und die Varianz wird für ein Polynom dritter Ordnung gefunden. Wir können dies berechnen und anzeigen

Fit über die Originaldaten wie folgt (siehe Abbildung 39-8).

In [14]: plt.scatter(X.ravel(), y)
lim = plt.axis()
y_test = PolynomialRegression(3).fit(X, y).predict(X_test)
plt.plot(X_test.ravel(), y_test);
plt.axis(lim);

394 | Kapitel 39: Hyperparameter und Modellvalidierung

Abbildung 39-8. Das kreuzvalidierte optimale Modell für die Daten in Abbildung 39-6

Um dieses optimale Modell zu finden, mussten wir nicht einmal die

Trainingspunkte, sondern die Untersuchung des Verhältnisses zwischen Trainingspunkten und Valida-

kann uns einen nützlichen Einblick in die Leistung des Modells geben.

Lernkurven
Ein wichtiger Aspekt der Modellkomplexität ist, dass das optimale Modell im Allgemeinen
von der Größe der Trainingsdaten abhängt. Lassen Sie uns zum Beispiel einen neuen Datensatz erzeugen

mit fünfmal so vielen Punkten (siehe Abbildung 39-9).

In [15]: X2, y2 = make_data(200)
plt.scatter(X2.ravel(), y2);

Lernkurven | 395
Abbildung 39-9. Daten zur Demonstration von Lernkurven

Nun duplizieren wir den vorangegangenen Code, um die Validierungskurve für diesen größeren Wert zu zeichnen

Datensatzes; zur Veranschaulichung überlagern wir auch die vorherigen Ergebnisse (siehe Abbildung 39-10).

In [16]: grad = np.arange(21)
train_score2, val_score2 = validation_curve(
PolynomialRegression(), X2, y2,
param_name='polynomialfeatures__degree',
param_range=Grad, cv=7)

plt.plot(grad, np.median(train_score2, 1),
color='blue', label='training score')
plt.plot(grad, np.median(val_score2, 1),
color='red', label='validation score')
plt.plot(grad, np.median(train_score, 1),
color='blau', alpha=0.3, linestyle='gestrichelt')
plt.plot(grad, np.median(val_score, 1),
color='rot', alpha=0.3, linestyle='gestrichelt')
plt.legend(loc='untere Mitte')
plt.ylim(0, 1)
plt.xlabel('Grad')
plt.ylabel('score');

Die durchgezogenen Linien zeigen die neuen Ergebnisse, während die schwächeren gestrichelten Linien die Ergebnisse
aus dem früheren kleineren Datensatz zeigen. Aus der Validierungskurve ist ersichtlich, dass die größeren

Datensatz kann ein viel komplizierteres Modell unterstützen: Der Spitzenwert ist hier wahrscheinlich

um einen Grad von 6, aber selbst ein Modell mit einem Grad von 20 passt sich den Daten nicht ernsthaft zu sehr an.
die Validierungs- und Trainingswerte liegen sehr nahe beieinander.

396 | Kapitel 39: Hyperparameter und Modellvalidierung

7 Eine vollfarbige Version dieser Abbildung ist auf GitHub zu finden.
Abbildung 39-10. Lernkurven für das Polynom-Modell, das an die Daten in Abbildung 39-9^7 angepasst wurde

Das Verhalten der Validierungskurve hat also nicht nur einen, sondern zwei wichtige Inputs: die

Modellkomplexität und die Anzahl der Trainingspunkte. Wir können weitere Erkenntnisse gewinnen, indem wir

Untersuchung des Verhaltens des Modells in Abhängigkeit von der Anzahl der Trainingspunkte,
Dies können wir tun, indem wir immer größere Teilmengen der Daten verwenden, um unser Modell anzupassen. A

Die Darstellung der Trainings-/Validierungsergebnisse in Abhängigkeit von der Größe der Trainingsmenge wird
wird manchmal als Lernkurve bezeichnet.

Das allgemeine Verhalten, das wir von einer Lernkurve erwarten würden, ist dieses:

Ein Modell mit einer bestimmten Komplexität wird einen kleinen Datensatz übererfüllen: Das bedeutet, dass die Trainings
wird relativ hoch sein, während die Validierungsergebnisse relativ niedrig sein werden.
Ein Modell mit einer bestimmten Komplexität passt nicht zu einem großen Datensatz: Dies bedeutet, dass die
Dies bedeutet, dass der Trainingswert sinkt, der Validierungswert jedoch steigt.
Ein Modell wird niemals, außer zufällig, eine bessere Bewertung für den Validierungssatz abgeben als
als der Trainingsdatensatz: Das bedeutet, dass sich die Kurven immer mehr annähern, aber
niemals kreuzen.
Unter Berücksichtigung dieser Merkmale würde man erwarten, dass eine Lernkurve qualitativ so aussieht
wie die in Abbildung 39-11 gezeigte.

Lernkurven | 397
8 Der Code zur Erstellung dieser Abbildung ist im Online-Anhang zu finden.
Abbildung 39-11. Schematische Darstellung der typischen Interpretation von Lernkurven^8

Das bemerkenswerte Merkmal der Lernkurve ist die Konvergenz zu einer bestimmten Punktzahl als

die Anzahl der Trainingsbeispiele wächst. Insbesondere, wenn Sie genügend Punkte haben
dass ein bestimmtes Modell konvergiert hat, hilft es Ihnen nicht, weitere Trainingsdaten hinzuzufügen!

Die einzige Möglichkeit, die Leistung des Modells in diesem Fall zu erhöhen, ist die Verwendung eines anderen (oft

komplexeres) Modell.

Scikit-Learn bietet ein praktisches Dienstprogramm zur Berechnung solcher Lernkurven aus Ihren

Modelle; hier berechnen wir eine Lernkurve für unseren Originaldatensatz mit einem Polynom zweiter
Ordnung und einem Polynom der neunten Ordnung (siehe Abbildung 39-12).

In [17]: from sklearn.model_selection import learning_curve

fig, ax = plt.subplots(1, 2, figsize=(16, 6))
fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)

for i, degree in enumerate([2, 9]):
N, train_lc, val_lc = learning_curve(
PolynomialRegression(Grad), X, y, cv=7,
train_sizes=np.linspace(0.3, 1, 25))

ax[i].plot(N, np.mean(train_lc, 1),
color='blue', label='training score')
ax[i].plot(N, np.mean(val_lc, 1),
color='red', label='validation score')
ax[i].hlines(np.mean([train_lc[-1], val_lc[-1]]), N[0],
N[-1], color='grau', linestyle='gestrichelt')

ax[i].set_ylim(0, 1)

398 | Kapitel 39: Hyperparameter und Modellvalidierung

9 Eine Version dieser Abbildung in voller Größe ist auf GitHub zu finden.
ax[i].set_xlim(N[0], N[-1])
ax[i].set_xlabel('Trainingsgröße')
ax[i].set_ylabel('Ergebnis')
ax[i].set_title('Grad = {0}'.format(Grad), size=14)
ax[i].legend(loc='best')

Abbildung 39-12. Lernkurven für ein Modell mit geringer Komplexität (links) und ein Modell mit hoher Komplexität

Modell (rechts)^9

Dies ist eine wertvolle Diagnose, denn sie gibt uns eine visuelle Darstellung, wie unser Modell

auf zunehmende Mengen von Trainingsdaten reagiert. Insbesondere, wenn die Lernkurve
Lernkurve bereits konvergiert ist (d. h., wenn die Trainings- und Validierungskurven bereits

nahe beieinander liegen), wird das Hinzufügen weiterer Trainingsdaten die Anpassung nicht wesentlich verbessern!

Diese Situation ist im linken Feld mit der Lernkurve für das Grad-2-Modell dargestellt.

Die einzige Möglichkeit, die konvergierte Punktzahl zu erhöhen, besteht in der Verwendung eines anderen (in der Regel kom-

pliziertes) Modell. Dies wird im rechten Feld deutlich: Durch den Wechsel zu einem viel komplizierteren Modell
Modells erhöht sich die Konvergenzrate (angezeigt durch die gestrichelte Linie), aber

auf Kosten einer höheren Modellvarianz (angegeben durch die Differenz zwischen der

Trainings- und Validierungsergebnisse). Wenn wir noch mehr Datenpunkte hinzufügen würden, würde die Lernkurve
Lernkurve für das kompliziertere Modell schließlich konvergieren.

Die Erstellung einer Lernkurve für das von Ihnen gewählte Modell und den Datensatz kann Ihnen helfen
bei der Entscheidung, wie Sie bei der Verbesserung Ihrer Analyse vorgehen sollen.

Die durchgezogenen Linien zeigen die neuen Ergebnisse, während die schwächeren gestrichelten Linien die Ergebnisse zeigen

auf dem vorherigen kleineren Datensatz. Aus der Validierungskurve wird deutlich, dass der größere
Datensatz ein viel komplizierteres Modell unterstützen kann: Die Spitze ist hier wahrscheinlich

um einen Grad von 6, aber selbst ein Modell mit einem Grad von 20 passt sich den Daten nicht ernsthaft zu sehr an.
Die Validierungs- und Trainingsergebnisse liegen sehr nahe beieinander.

Lernkurven | 399
Validierung in der Praxis: Gittersuche
Die vorangegangene Diskussion soll Ihnen ein Gefühl für die Abwägung geben

zwischen Verzerrung und Varianz und seine Abhängigkeit von der Modellkomplexität und der Größe der Trainingsmenge
Größe. In der Praxis haben Modelle in der Regel mehr als einen Knopf zum Drehen, was bedeutet, dass Diagramme von

Validierung und Lernkurven werden von Linien zu mehrdimensionalen Flächen. In
diesen Fällen sind solche Visualisierungen schwierig, und wir würden lieber einfach die Par-

bestimmten Modells, das den Validierungswert maximiert.

Scikit-Learn stellt einige Werkzeuge zur Verfügung, die diese Art der Suche vereinfachen: hier
werden wir die Verwendung der Gittersuche betrachten, um das optimale Polynom-Modell zu finden. Wir werden

ein zweidimensionales Raster von Modellmerkmalen untersuchen, nämlich den Polynomgrad und
das Flag, das uns sagt, ob der Achsenabschnitt angepasst werden soll. Dies kann mit der Scikit-Learn-Funktion

GridSearchCV Meta-Schätzer:

In [18]: from sklearn.model_selection import GridSearchCV

param_grid = {'polynomialfeatures__degree': np.arange(21),
'linearregression__fit_intercept': [ True , False ]}

grid = GridSearchCV(PolynomialRegression(), param_grid, cv=7)

Beachten Sie, dass dies wie ein normaler Schätzer noch nicht auf Daten angewendet wurde. Aufruf von

Die Anpassungsmethode passt das Modell an jedem Rasterpunkt an, wobei die Punktzahlen entlang

den Weg:

In [19]: grid.fit(X, y);

Da das Modell nun angepasst ist, können wir die besten Parameter wie folgt ermitteln:

In [20]: grid.best_params_
Out[20]: {'linearregression__fit_intercept': False , 'polynomialfeatures__degree': 4}

Schließlich können wir, wenn wir wollen, das beste Modell verwenden und die Anpassung an unsere Daten mit dem Code
aus dem vorherigen Beispiel (siehe Abbildung 39-13).

In [21]: model = grid.best_estimator_

plt.scatter(X.ravel(), y)
lim = plt.axis()
y_test = model.fit(X, y).predict(X_test)
plt.plot(X_test.ravel(), y_test);
plt.axis(lim);

400 | Kapitel 39: Hyperparameter und Modellvalidierung

Abbildung 39-13. Das über eine automatische Gittersuche ermittelte Best-Fit-Modell

Andere Optionen in GridSearchCV umfassen die Möglichkeit, eine benutzerdefinierte Scoring-Funktion zu spezifizieren.

tion, Parallelisierung der Berechnungen, randomisierte Suche und vieles mehr. Für weitere
Informationen finden Sie in den Beispielen in den Kapiteln 49 und 50 oder in der Scikit-Learn Grid

Dokumentation suchen.

Zusammenfassung
In diesem Kapitel haben wir begonnen, das Konzept der Modellvalidierung und der Hyperpara-
Optimierung, wobei wir uns auf intuitive Aspekte des Bias-Varianz-Abgleichs und

wie sie bei der Anpassung von Modellen an Daten ins Spiel kommt. Wir haben insbesondere festgestellt, dass die

Die Verwendung eines Validierungssatzes oder eines Kreuzvalidierungsansatzes ist bei der Abstimmung der Parameter von
um eine Überanpassung bei komplexeren/flexibleren Modellen zu vermeiden.

In späteren Kapiteln werden wir die Details besonders nützlicher Modelle erörtern, welche Einstellmöglichkeiten
für diese Modelle zur Verfügung stehen, und wie sich diese freien Parameter auf die Komplexität des Modells auswirken.

Behalten Sie die Lektionen dieses Kapitels im Hinterkopf, wenn Sie weiter lesen und etwas über diese

Ansätze des maschinellen Lernens!

Zusammenfassung | 401
KAPITEL 40

Merkmalstechnik
In den vorangegangenen Kapiteln wurden die grundlegenden Ideen des maschinellen Lernens dargelegt, aber alle

Bei den bisherigen Beispielen wurde davon ausgegangen, dass Sie numerische Daten in einem ordentlichen, [n_sam

ples, n_features] Format. In der realen Welt liegen die Daten selten in dieser Form vor. Mit

einer der wichtigsten Schritte bei der Anwendung des maschinellen Lernens in der Praxis ist
Feature-Engineering, d. h. die Verwendung aller Informationen, die Sie über Ihr Problem haben

und verwandelt sie in Zahlen, die Sie zum Aufbau Ihrer Merkmalsmatrix verwenden können.

In diesem Kapitel werden wir einige gängige Beispiele für Feature-Engineering-Aufgaben behandeln:
Wir werden uns Features für die Darstellung von kategorischen Daten, Text und Bildern ansehen. Zusätzlich,

werden wir abgeleitete Funktionen zur Erhöhung der Modellkomplexität und zur Imputation
fehlender Daten. Dieser Prozess wird gemeinhin als Vektorisierung bezeichnet, da er Folgendes beinhaltet

Umwandlung beliebiger Daten in brauchbare Vektoren.

Kategorische Merkmale
Eine häufige Art von nicht numerischen Daten sind kategoriale Daten. Stellen Sie sich zum Beispiel vor

Sie untersuchen einige Daten zu Wohnungspreisen, und neben numerischen Merkmalen
wie "Preis" und "Zimmer" haben Sie auch Informationen über die "Nachbarschaft". Zum Beispiel,

könnten Ihre Daten etwa so aussehen:

In [1]: Daten = [
{'Preis': 850000, 'rooms': 4, 'neighborhood': 'Queen Anne'},
{'price': 700000, 'rooms': 3, 'Nachbarschaft': 'Fremont'},
{'Preis': 650000, 'Zimmer': 3, 'Nachbarschaft': 'Wallingford'},
{'Preis': 600000, 'Zimmer': 2, 'Nachbarschaft': 'Fremont'}
]

402
Man könnte versucht sein, diese Daten mit einem einfachen numerischen Mapping zu kodieren:

In [2]: {'Queen Anne': 1, 'Fremont': 2, 'Wallingford': 3};

Es hat sich jedoch herausgestellt, dass dieser Ansatz in Scikit-Learn nicht generell sinnvoll ist. Die Pack-

Die Modelle von Age gehen von der Grundannahme aus, dass numerische Merkmale alge- ne Größen widerspiegeln.
braische Größen widerspiegeln, so dass eine solche Zuordnung zum Beispiel bedeuten würde, dass Queen Anne <

Fremont < Wallingford, oder sogar, dass Wallingford-Queen Anne = Fremont, was
(abgesehen von demografischen Nischenwitzen) nicht viel Sinn macht.

In diesem Fall ist eine bewährte Technik die Verwendung von One-Hot-Codierung, die effektiv eine

zusätzliche Spalten, die das Vorhandensein oder Nichtvorhandensein einer Kategorie mit einem Wert von 1 oder 0 anzeigen,
anzeigt. Wenn Ihre Daten die Form einer Liste von Wörterbüchern haben, kann Scikit-Learn's

DictVectorizer wird dies für Sie tun:

In [3]: from sklearn.feature_extraction import DictVectorizer
vec = DictVectorizer(sparse= False , dtype=int)
vec.fit_transform(data)
Out[3]: array([[ 0, 1, 0, 850000, 4],
[ 1, 0, 0, 700000, 3],
[ 0, 0, 1, 650000, 3],
[ 1, 0, 0, 600000, 2]])

Beachten Sie, dass die Spalte "Nachbarschaft" in drei separate Spalten erweitert wurde.

die drei Nachbarschaftsbezeichnungen darstellen, und dass jede Zeile eine 1 in der
Spalte, die mit ihrer Nachbarschaft verbunden ist. Mit diesen kategorialen Merkmalen werden also

ded, können Sie wie gewohnt mit der Anpassung eines Scikit-Learn-Modells fortfahren.

Um die Bedeutung der einzelnen Spalten zu erkennen, können Sie die Merkmalsnamen einsehen:

In [4]: vec.get_feature_names_out()
Out[4]: array(['neighborhood=Fremont', 'neighborhood=Queen Anne',
'neighborhood=Wallingford', 'price', 'rooms'], dtype=object)

Dieser Ansatz hat einen klaren Nachteil: Wenn Ihre Kategorie viele mögliche Werte hat
Werte hat, kann dies die Größe Ihres Datensatzes stark erhöhen. Da jedoch die Enco-

ded Daten überwiegend Nullen enthalten, kann eine spärliche Ausgabe eine sehr effiziente Lösung sein:

In [5]: vec = DictVectorizer(sparse= True , dtype=int)
vec.fit_transform(Daten)
Out[5]: <4x5 sparse Matrix vom Typ '<class 'numpy.int64'>'
mit 12 gespeicherten Elementen im komprimierten Sparse Row Format>

Fast alle Scikit-Learn-Schätzer akzeptieren solche spärlichen Eingaben bei der Anpassung und

Bewertung von Modellen. Zwei zusätzliche Werkzeuge, die Scikit-Learn enthält, unterstützen dies

Art der Kodierung sind sklearn.preprocessing.OneHotEncoder und sklearn

.feature_extraction.FeatureHasher.

Kategoriale Merkmale | 403
Textmerkmale
Eine weitere häufige Anforderung beim Feature Engineering ist die Umwandlung von Text in einen Satz von Repräsenta- tionen.

tive numerische Werte. Die meisten automatischen Auswertungen von Daten aus sozialen Medien beruhen zum Beispiel
auf einer Form der Kodierung des Textes als Zahlen. Eine der einfachsten Methoden zur

Kodierung dieser Art von Daten ist die Wortzählung: Sie nehmen jeden Textausschnitt, zählen die
Vorkommen jedes Worts darin und stellen die Ergebnisse in eine Tabelle.

Nehmen wir zum Beispiel die folgenden drei Sätze:

In [6]: sample = ['problem of evil',
'böse Königin',
'Horizontproblem']

Für eine Vektorisierung dieser Daten auf der Grundlage der Wortzahl könnten wir einzelne

Spalten, die die Wörter "Problem", "von", "böse" usw. darstellen. Während dies bei diesem einfachen Beispiel
Dies wäre für dieses einfache Beispiel zwar von Hand möglich, doch lässt sich diese Mühsal vermeiden, indem man

Der CountVectorizer von Scikit-Learn:

In [7]: from sklearn.feature_extraction.text import CountVectorizer

vec = CountVectorizer()
X = vec.fit_transform(sample)
X
Out[7]: <3x5 sparse matrix of type '<class 'numpy.int64'>'
mit 7 gespeicherten Elementen im komprimierten Sparse Row Format>

Das Ergebnis ist eine dünnbesetzte Matrix, in der die Anzahl des Auftretens jedes Wortes aufgezeichnet ist.

einfacher zu überprüfen, wenn wir dies in einen DataFrame mit beschrifteten Spalten umwandeln:

In [8]: import pandas as pd
pd.DataFrame(X.toarray(), columns=vec.get_feature_names_out())
Out[8]: böser Horizont der Problemkönigin
0 1 0 1 1 0
1 1 0 0 0 1
2 0 1 0 1 0

Es gibt jedoch einige Probleme bei der Verwendung einer einfachen rohen Wortzählung: Sie kann dazu führen, dass

Merkmale, die sehr häufig vorkommende Wörter zu stark gewichten, was dazu führen kann, dass

bei einigen Klassifizierungsalgorithmen suboptimal sein. Ein Ansatz, dies zu beheben, ist bekannt als
Term Frequency-Inverse Document Frequency (TF-IDF), bei der die Wortanzahl gewichtet wird

durch ein Maß dafür, wie oft sie in den Dokumenten vorkommen. Die Syntax zur Berechnung der
dieser Merkmale ist ähnlich wie im vorherigen Beispiel:

Die durchgezogenen Linien zeigen die neuen Ergebnisse, während die schwächeren gestrichelten Linien die Ergebnisse zeigen

auf dem vorherigen kleineren Datensatz. Aus der Validierungskurve wird deutlich, dass der größere
Datensatz ein viel komplizierteres Modell unterstützen kann: Die Spitze hier ist wahrscheinlich

um einen Grad von 6, aber selbst ein Modell mit einem Grad von 20 passt sich den Daten nicht ernsthaft zu sehr an.
die Validierungs- und Trainingswerte liegen sehr nahe beieinander.

404 | Kapitel 40: Feature Engineering

In [9]: from sklearn.feature_extraction.text import TfidfVectorizer
vec = TfidfVectorizer()
X = vec.fit_transform(Beispiel)
pd.DataFrame(X.toarray(), columns=vec.get_feature_names_out())
Out[9]: böser Horizont der Problemkönigin
0 0.517856 0.000000 0.680919 0.517856 0.000000
1 0.605349 0.000000 0.000000 0.000000 0.795961
2 0.000000 0.795961 0.000000 0.605349 0.000000

Ein Beispiel für die Verwendung von TF-IDF in einem Klassifikationsproblem finden Sie in Kapitel 41.

Bildmerkmale
Eine weitere häufige Anforderung ist die geeignete Kodierung von Bildern für die Analyse durch maschinelles Lernen.
Der einfachste Ansatz ist der, den wir für die Zifferndaten in Kapitel 38 verwendet haben: die einfache Verwendung von

die Pixelwerte selbst. Aber je nach Anwendung kann ein solcher Ansatz

nicht optimal sein.

Eine umfassende Zusammenfassung der Techniken zur Merkmalsextraktion für Bilder würde den Rahmen sprengen

den Rahmen dieses Kapitels, aber Sie können ausgezeichnete Implementierungen von vielen der
Standardansätze im Scikit-Image-Projekt. Für ein Beispiel der Verwendung von Scikit-

Learn und Scikit-Image zusammen, siehe Kapitel 50.

Abgeleitete Merkmale
Eine weitere nützliche Art von Merkmalen ist ein Merkmal, das mathematisch von einer Eingabe abgeleitet ist

Eigenschaften. Ein Beispiel dafür haben wir in Kapitel 39 gesehen, als wir polynomiale
Features aus unseren Eingabedaten konstruierten. Wir haben gesehen, dass wir eine lineare Regression in eine

Polynomielle Regression nicht durch Änderung des Modells, sondern durch Umwandlung der Eingabe!

Diese Daten lassen sich zum Beispiel nicht gut durch eine gerade Linie beschreiben (siehe

Abbildung 40-1):

In [10]: % matplotlib inline
import numpy as np
import matplotlib.pyplot as plt

x = np.array([1, 2, 3, 4, 5])
y = np.array([4, 2, 1, 3, 7])
plt.scatter(x, y);

Bildmerkmale | 405
Abbildung 40-1. Daten, die nicht gut durch eine gerade Linie beschrieben sind

Wir können immer noch eine Linie an die Daten mit LinearRegression anpassen und erhalten das optimale Ergebnis,
wie in Abbildung 40-2 gezeigt:

In [11]: from sklearn.linear_model import LinearRegression
X = x[:, np.newaxis]
model = LinearRegression().fit(X, y)
yfit = model.predict(X)
plt.scatter(x, y)
plt.plot(x, yfit);

Abbildung 40-2. Schlechte lineare Anpassung

406 | Kapitel 40: Feature Engineering

Aber es ist klar, dass wir ein ausgefeilteres Modell brauchen, um die Beziehung zu beschreiben

zwischen x und y.

Eine Möglichkeit besteht darin, die Daten umzuwandeln und zusätzliche Spalten mit Merkmalen hinzuzufügen, um

mehr Flexibilität in das Modell bringen. Zum Beispiel können wir polynomielle Merkmale hinzufügen zu

die Daten auf diese Weise:

In [12]: from sklearn.preprocessing import PolynomialFeatures
poly = PolynomialFeatures(degree=3, include_bias= False )
X2 = poly.fit_transform(X)
print(X2)
Out[12]: [[ 1. 1. 1.]
[ 2. 4. 8.]
[ 3. 9. 27.]
[ 4. 16. 64.]
[ 5. 25. 125.]]

Die abgeleitete Merkmalsmatrix hat eine Spalte, die x darstellt, eine zweite Spalte, die

die für x^2 steht, und eine dritte Spalte für x^3. Die Berechnung einer linearen Regression auf diese

Die erweiterte Eingabe führt zu einer wesentlich besseren Anpassung an unsere Daten, wie in Abbildung 40-3 zu sehen ist:

In [13]: model = LinearRegression().fit(X2, y)
yfit = model.predict(X2)
plt.scatter(x, y)
plt.plot(x, yfit);

Abbildung 40-3. Eine lineare Anpassung an aus den Daten abgeleitete polynomiale Merkmale

Diese Idee der Verbesserung eines Modells nicht durch Änderung des Modells, sondern durch Umwandlung der

Eingaben, ist grundlegend für viele der leistungsfähigeren Methoden des maschinellen Lernens.
Wir werden diese Idee in Kapitel 42 im Zusammenhang mit der Basisfunktionsregression näher untersuchen.

Abgeleitete Merkmale | 407
Allgemeiner ausgedrückt, ist dies ein motivierender Weg zu den leistungsstarken Techniken, die als

als Kernel-Methoden, die wir in Kapitel 43 untersuchen werden.

Imputation von fehlenden Daten
Eine weitere häufige Anforderung bei der Entwicklung von Merkmalen ist der Umgang mit fehlenden Daten. Wir dis-

In Kapitel 16 wurde die Behandlung fehlender Daten in DataFrame-Objekten besprochen, und es wurde festgestellt, dass

NaN wird häufig verwendet, um fehlende Werte zu markieren. Zum Beispiel könnte ein Datensatz folgende Werte enthalten

sieht so aus:

In [14]: from numpy import nan
X = np.array([[ nan, 0, 3 ],
[ 3, 7, 9 ],
[ 3, 5, 2 ],
[ 4, nan, 6 ],
[ 8, 8, 1 ]])
y = np.array([14, 16, -1, 8, -5])

Wenn wir ein typisches maschinelles Lernmodell auf solche Daten anwenden, müssen wir zunächst
die fehlenden Werte durch einen geeigneten Füllwert ersetzen. Dies wird als Imputation bezeichnet.

Die Strategien reichen von einfachen (z. B. Ersetzen fehlender Werte) bis hin zu

mit dem Mittelwert der Spalte) bis hin zu anspruchsvoll (z. B. durch Matrixvervollständigung oder ein
robustes Modell zur Verarbeitung solcher Daten).

Die ausgefeilten Ansätze sind in der Regel sehr anwendungsspezifisch, und wir werden hier nicht
sie hier nicht eingehen. Für einen grundlegenden Imputationsansatz, der den Mittelwert, den Median oder die meisten

häufig verwendet wird, bietet Scikit-Learn die Klasse SimpleImputer:

In [15]: from sklearn.impute import SimpleImputer
imp = SimpleImputer(strategy='mean')
X2 = imp.fit_transform(X)
X2
Out[15]: array([[4.5, 0. , 3. ],
[3. , 7. , 9. ],
[3. , 5. , 2. ],
[4. , 5. , 6. ],
[8. , 8. , 1. ]])

Wir sehen, dass in den resultierenden Daten die beiden fehlenden Werte durch die folgenden ersetzt wurden

Mittelwert der verbleibenden Werte in der Spalte. Diese unterstellten Daten können dann in die

beispielsweise direkt in einen LinearRegression-Schätzer einfließen:

In [16]: model = LinearRegression().fit(X2, y)
model.predict(X2)
Out[16]: array([13.14869292, 14.3784627 , -1.15539732, 10.96606197, -5.33782027])

408 | Kapitel 40: Feature Engineering

Feature-Pipelines
Bei jedem der vorangegangenen Beispiele kann es schnell mühsam werden, die Transfor-

Sie können diese Schritte auch von Hand ausführen, insbesondere wenn Sie mehrere Schritte aneinanderreihen möchten. Ein Beispiel,
könnten wir eine Verarbeitungspipeline wünschen, die etwa so aussieht:

Imputieren fehlender Werte mit Hilfe des Mittelwerts.
Transformieren Sie Merkmale in quadratische.
Anpassung eines linearen Regressionsmodells.
Um diese Art von Verarbeitungspipeline zu rationalisieren, bietet Scikit-Learn eine Pipeline

Objekt, das wie folgt verwendet werden kann:

In [17]: from sklearn.pipeline import make_pipeline

model = make_pipeline(SimpleImputer(strategy='mean'),
PolynomialFeatures(degree=2),
LinearRegression())

Diese Pipeline sieht aus und verhält sich wie ein Standard-Scikit-Learn-Objekt und wendet alle

spezifizierte Schritte zu beliebigen Eingabedaten:

In [18]: model.fit(X, y) # X mit fehlenden Werten, von oben
print(y)
print(model.predict(X))
Out[18]: [14 16 -1 8 -5]
[14. 16. -1. 8. -5.]

Alle Schritte des Modells werden automatisch ausgeführt. Der Einfachheit halber wird in dieser
der Einfachheit halber das Modell auf die Daten angewandt wurde, auf denen es trainiert wurde; deshalb wurde es

in der Lage sind, das Ergebnis perfekt vorherzusagen (siehe Kapitel 39 für weitere Informationen).

Einige Beispiele für Scikit-Learn-Pipelines in Aktion finden Sie im folgenden Kapitel über

Naive Bayes-Klassifikation sowie die Kapitel 42 und 43.

Feature-Pipelines | 409
KAPITEL 41

Vertiefung: Naive Bayes-Klassifikation
In den vorangegangenen vier Kapiteln wurde ein allgemeiner Überblick über die Konzepte des maschinellen
Lernens gegeben. Im weiteren Verlauf von Teil V werden wir zunächst vier Algorithmen genauer unter die Lupe nehmen

für überwachtes Lernen und dann vier Algorithmen für unüberwachtes Lernen. Wir

Wir beginnen hier mit unserer ersten überwachten Methode, der Naive-Bayes-Klassifikation.

Naive Bayes-Modelle sind eine Gruppe von extrem schnellen und einfachen Klassifizierungsalgorithmen

die oft für sehr hochdimensionale Datensätze geeignet sind. Weil sie so schnell sind
sind und nur wenige einstellbare Parameter haben, eignen sie sich als schnelle und unkomplizierte

Grundlage für ein Klassifizierungsproblem. In diesem Kapitel wird eine intuitive Erklärung

die Funktionsweise von Naive-Bayes-Klassifikatoren, gefolgt von einigen Beispielen, die sie in Aktion
für einige Datensätze.

Bayes'sche Klassifizierung
Naive Bayes-Klassifikatoren beruhen auf Bayes'schen Klassifizierungsmethoden. Diese beruhen auf

Bayes' Theorem, eine Gleichung, die die Beziehung zwischen bedingten
Wahrscheinlichkeiten von statistischen Größen beschreibt. Bei der Bayes'schen Klassifikation sind wir interessiert an

die Wahrscheinlichkeit eines Etiketts L bei bestimmten beobachteten Merkmalen zu ermitteln, die wir wie folgt schreiben können

als PL-Merkmale. Das Bayes'sche Theorem sagt uns, wie wir dies in Form von Größen ausdrücken können
die wir direkter berechnen können:

PL-Merkmale =
Pfeatures LPL
Merkmale
410
Wenn wir versuchen, zwischen zwei Etiketten - nennen wir sie L 1 und L 2 - zu entscheiden, dann ist eine Möglichkeit

um diese Entscheidung zu treffen, ist die Berechnung des Verhältnisses der posterioren Wahrscheinlichkeiten für jede

Etikett:

PL 1-Merkmale
PL 2-Merkmale
=
Merkmale L 1
Merkmale L 2
PL 1
PL 2
Alles, was wir jetzt noch brauchen, ist ein Modell, mit dem wir Pfeatures Li für jede

Etikett. Ein solches Modell wird als generatives Modell bezeichnet, weil es die hypothetischen

Zufallsprozess, der die Daten erzeugt. Die Spezifikation dieses generativen Modells für jedes
Label ist der Hauptteil des Trainings eines solchen Bayes'schen Klassifikators. Die allgemeine Ver-

Die Durchführung eines solchen Ausbildungsschrittes ist eine sehr schwierige Aufgabe, aber wir können sie vereinfachen durch

die Verwendung einiger vereinfachender Annahmen über die Form dieses Modells.

Hier kommt das "naiv" in "naive Bayes" ins Spiel: Wenn wir sehr naive Annahmen machen...

über das generative Modell für jede Bezeichnung können wir eine grobe Annäherung an das
generativen Modells für jede Klasse finden und dann mit der Bayes'schen Klassifikation fortfahren.

Verschiedene Arten von Naive-Bayes-Klassifikatoren beruhen auf unterschiedlichen naiven Annahmen über die

Daten, von denen wir in den folgenden Abschnitten einige untersuchen werden.

Wir beginnen mit den Standardimporten:

In [1]: % matplotlib inline
import numpy as np
importiere matplotlib.pyplot as plt
importiere seaborn as sns
plt.style.use('seaborn-whitegrid')

Gaußscher Naive Bayes
Der vielleicht am einfachsten zu verstehende Naive Bayes-Klassifikator ist Gaussian Naive Bayes. Mit

Bei diesem Klassifikator wird davon ausgegangen, dass die Daten für jedes Etikett aus einer einfachen Gaus-
verteilung gezogen werden. Stellen Sie sich die folgenden Daten vor, die in Abbildung 41-1 dargestellt sind:

In [2]: from sklearn.datasets import make_blobs
X, y = make_blobs(100, 2, centers=2, random_state=2, cluster_std=1.5)
plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='RdBu');

Gaussian Naive Bayes | 411
1 Eine vollfarbige Version dieser Abbildung ist auf GitHub zu finden.
2 Der Code zur Erstellung dieser Abbildung ist im Online-Anhang zu finden.
Abbildung 41-1. Daten für die Gaussian-Naive-Bayes-Klassifikation^1

Das einfachste Gaußsche Modell ist die Annahme, dass die Daten durch eine Gaußsche Kurve beschrieben werden

Verteilung ohne Kovarianz zwischen den Dimensionen. Dieses Modell lässt sich durch Kom-

den Mittelwert und die Standardabweichung der Punkte innerhalb jedes Labels, was alles ist
was alles ist, was wir brauchen, um eine solche Verteilung zu definieren. Das Ergebnis dieser naiven Gaußschen Annahme ist

in Abbildung 41-2 dargestellt.

Abbildung 41-2. Visualisierung des Gauß-Naive-Bayes-Modells^2

412 | Kapitel 41: Vertiefung: Naive Bayes-Klassifikation

Die Ellipsen stellen hier das generative Gauß-Modell für jedes Label dar, wobei größere

Wahrscheinlichkeit in Richtung des Zentrums der Ellipsen. Mit diesem generativen Modell, das für jede Klasse
jeder Klasse haben wir ein einfaches Rezept zur Berechnung der Wahrscheinlichkeit Pfeatures L 1 für jede

Datenpunkt, und so können wir schnell das posteriore Verhältnis berechnen und bestimmen, welches
Bezeichnung für einen bestimmten Punkt am wahrscheinlichsten ist.

Dieses Verfahren ist in Scikit-Learn's sklearn.naive_bayes.GaussianNB implementiert.

Schätzer:

In [3]: from sklearn.naive_bayes import GaussianNB
model = GaussianNB()
model.fit(X, y);

Erzeugen wir einige neue Daten und sagen wir das Etikett voraus:

In [4]: rng = np.random.RandomState(0)
Xneu = [-6, -14] + [14, 18] * rng.rand(2000, 2)
ynew = model.predict(Xnew)

Nun können wir diese neuen Daten aufzeichnen, um eine Vorstellung davon zu bekommen, wo die Entscheidungsgrenze liegt (siehe

Abbildung 41-3).

In [5]: plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='RdBu')
lim = plt.axis()
plt.scatter(Xneu[:, 0], Xneu[:, 1], c=ynew, s=20, cmap='RdBu', alpha=0.1)
plt.axis(lim);

Abbildung 41-3. Visualisierung der Gauß-Naive-Bayes-Klassifikation

Wir sehen eine leicht gekrümmte Grenze bei den Klassifizierungen - im Allgemeinen ist die Grenze

eines Gauß-Naive-Bayes-Modells ist quadratisch.

Gaußsche Naive Bayes | 413
Ein schöner Aspekt dieses Bayes'schen Formalismus ist, dass er auf natürliche Weise probabilistische

Klassifizierung, die wir mit der Methode predict_proba berechnen können:

In [6]: yprob = model.predict_proba(Xnew)
yprob[-8:].round(2)
Out[6]: array([[0.89, 0.11],
[1. , 0. ],
[1. , 0. ],
[1. , 0. ],
[1. , 0. ],
[1. , 0. ],
[0. , 1. ],
[0.15, 0.85]])

Die Spalten geben die posterioren Wahrscheinlichkeiten der ersten bzw. zweiten Kennzeichnung an.

tiv. Wenn Sie nach Schätzungen der Unsicherheit in Ihrer Klassifizierung suchen, sind Bayes'sche
Ansätze wie dieser ein guter Ansatzpunkt sein.

Natürlich ist die endgültige Klassifizierung nur so gut wie die Modellannahmen, die
zu ihr führen, weshalb Gaussian Naive Bayes oft keine sehr guten Ergebnisse liefert.

Ergebnisse. In vielen Fällen - insbesondere bei einer großen Anzahl von Merkmalen - ist dies jedoch

Annahme ist nicht so nachteilig, dass Gaussian Naive Bayes nicht mehr als
zuverlässige Methode zu sein.

Multinomiale Naive Bayes
Die soeben beschriebene Gauß-Annahme ist keineswegs die einzige einfache Annahme

die verwendet werden können, um die generative Verteilung für jedes Label zu spezifizieren. Ein weiteres nützliches
Beispiel ist die multinomiale naive Bayes-Verteilung, bei der angenommen wird, dass die Merkmale generiert werden

aus einer einfachen Multinomialverteilung. Die Multinomialverteilung beschreibt die

Wahrscheinlichkeit der Beobachtung von Zählungen unter einer Reihe von Kategorien, und daher ist multinomial
Naive Bayes ist am besten für Merkmale geeignet, die Zählungen oder Zählraten darstellen.

Die Idee ist genau dieselbe wie zuvor, nur dass wir die Datenverteilung nicht mit der
Gauß zu modellieren, modellieren wir sie mit einem Multinomialmodell mit bester Anpassung

Vertrieb.

Beispiel: Klassifizierung von Text
Ein Bereich, in dem Multinomial Naive Bayes häufig verwendet wird, ist die Textklassifizierung, bei der

die Merkmale beziehen sich auf die Anzahl oder Häufigkeit der Wörter in den zu untersuchenden Dokumenten

klassifiziert. Wir haben die Extraktion solcher Merkmale aus Text in Kapitel 40 besprochen; hier
werden wir die spärlichen Wortzählungsmerkmale aus dem 20 Newsgroups-Korpus verwenden, die

Scikit-Learn zur Verfügung, um zu zeigen, wie wir diese kurzen Dokumente in Kategorien einteilen
in Kategorien einteilen.

Laden wir die Daten herunter und sehen wir uns die Zielnamen an:

414 | Kapitel 41: Vertiefung: Naive Bayes-Klassifikation

In [7]: from sklearn.datasets import fetch_20newsgroups

data = fetch_20newsgroups()
data.target_names
Out[7]: ['alt.atheism',
'comp.graphics',
'comp.os.ms-windows.misc',
'comp.sys.ibm.pc.hardware',
'comp.sys.mac.hardware',
'comp.windows.x',
'misc.forsale',
'rec.autos',
'rec.motorcycles',
'rec.sport.baseball',
'rec.sport.hockey',
'sci.crypt',
'sci.electronics',
'sci.med',
'sci.space',
'soc.religion.christian',
'talk.politics.guns',
'talk.politics.mideast',
'talk.politics.misc',
'talk.religion.misc']

Der Einfachheit halber werden wir hier nur einige dieser Kategorien auswählen und die

Trainings- und Testreihen:

In [8]: categories = ['talk.religion.misc', 'soc.religion.christian',
'sci.space', 'comp.graphics']
train = fetch_20newsgroups(subset='train', categories=categories)
test = fetch_20newsgroups(subset='test', categories=categories)

Hier ist ein repräsentativer Eintrag aus den Daten:

In [9]: print(train.data[5][48:])
Out[9]: Betreff: Bundesanhörung
Absender: dmcgee@uluhe
Organisation: Fakultät für Meeres- und Geowissenschaften und Technologie
Verbreitung: usa
Zeilen: 10

Tatsache oder Gerücht....? Madalyn Murray O'Hare, eine Atheistin, die vor 15 Jahren die
die vor 15 Jahren die Bibellesung und das Gebet in öffentlichen Schulen abgeschafft hat, wird nun
mit einer Petition bei der FCC vorstellig werden, um die Verlesung des
Evangeliums in den amerikanischen Rundfunkanstalten. Und sie setzt sich auch dafür ein, dass
Weihnachtsprogramme, Lieder usw. aus den öffentlichen Schulen zu entfernen. Wenn das wahr ist
dann schreiben Sie an die Federal Communications Commission 1919 H Street Washington DC
20054 und bringen Sie Ihren Widerstand gegen ihr Ansinnen zum Ausdruck. Referenznummer der Petition

Multinomial Naive Bayes | 415
Um diese Daten für das maschinelle Lernen nutzen zu können, müssen wir in der Lage sein, die Kon-

tent jeder Zeichenkette in einen Zahlenvektor. Hierfür verwenden wir den TF-IDF-Vektor-
izer (eingeführt in Kapitel 40) und erstellen eine Pipeline, die ihn mit einem multinomialen

Naive Bayes-Klassifikator:

In [10]: from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes importiere MultinomialNB
from sklearn.pipeline import make_pipeline

model = make_pipeline(TfidfVectorizer(), MultinomialNB())

Mit dieser Pipeline können wir das Modell auf die Trainingsdaten anwenden und die Kennzeichnungen für die
die Testdaten vorhersagen:

In [11]: model.fit(train.data, train.target)
labels = model.predict(test.data)

Nachdem wir nun die Beschriftungen für die Testdaten vorhergesagt haben, können wir sie auswerten, um zu lernen

über die Leistung des Schätzers. Werfen wir zum Beispiel einen Blick auf die Konfusionsmatrix
Matrix zwischen den wahren und vorhergesagten Bezeichnungen für die Testdaten (siehe Abbildung 41-4).

In [12]: from sklearn.metrics import confusion_matrix
mat = confusion_matrix(test.target, labels)
sns.heatmap(mat.T, square= True , annot= True , fmt='d', cbar= False ,
xticklabels=Zug.target_names, yticklabels=Zug.target_names,
cmap='Blues')
plt.xlabel('true label')
plt.ylabel('vorhergesagte Beschriftung');

Es ist offensichtlich, dass sogar dieser sehr einfache Klassifikator erfolgreich Raumdiskussionen trennen kann

von Computerdiskussionen, aber sie wird mit Diskussionen über Religion verwechselt

und Diskussionen über das Christentum. Das ist vielleicht zu erwarten!

Das Tolle daran ist, dass wir jetzt die Möglichkeit haben, die Kategorie für jede

String mit Hilfe der Predict-Methode dieser Pipeline. Hier ist eine Dienstprogrammfunktion, die

gibt die Vorhersage für eine einzelne Zeichenkette zurück:

In [13]: def predict_category(s, train=train, model=model):
pred = model.predict([s])
return train.target_names[pred[0]]

Probieren wir es aus:

In [14]: predict_category('Senden einer Nutzlast zur ISS')
Out[14]: 'sci.space'

In [15]: predict_category('Diskussion über die Existenz Gottes')
Out[15]: 'soc.religion.christian'

In [16]: predict_category('Bestimmung der Bildschirmauflösung')
Out[16]: 'comp.graphics'

416 | Kapitel 41: Vertiefung: Naive Bayes-Klassifikation

Abbildung 41-4. Konfusionsmatrix für den multinomialen Naive-Bayes-Textklassifikator

Denken Sie daran, dass es sich hierbei um nichts weiter als ein einfaches Wahrscheinlichkeitsmodell handelt

für die (gewichtete) Häufigkeit jedes Wortes in der Zeichenkette; dennoch ist das Ergebnis
verblüffend. Sogar ein sehr naiver Algorithmus, wenn er sorgfältig eingesetzt und an einer großen Menge trainiert wird

von hochdimensionalen Daten, kann überraschend effektiv sein.

Wann wird Naive Bayes verwendet?
Da naive Bayes-Klassifikatoren so strenge Annahmen über die Daten treffen, werden sie
in der Regel nicht so gut ab wie kompliziertere Modelle. Dennoch haben sie sieben...

liche Vorteile:

Sie sind sowohl beim Training als auch bei der Vorhersage schnell.
Sie bieten einfache probabilistische Vorhersagen.
Sie sind oft leicht zu interpretieren.
Sie haben nur wenige (wenn überhaupt) einstellbare Parameter.
Wann sollte man Naive Bayes verwenden | 417
Diese Vorteile bedeuten, dass ein Naive-Bayes-Klassifikator oft eine gute Wahl für eine erste

Baseline-Klassifizierung. Wenn die Leistung stimmt, dann herzlichen Glückwunsch: Sie haben einen sehr
sehr schnellen, sehr interpretierbaren Klassifikator für Ihr Problem. Wenn er nicht gut abschneidet, dann

können Sie mit der Erforschung anspruchsvollerer Modelle beginnen, wenn Sie einige Grundkenntnisse über

wie gut sie funktionieren sollten.

Naive Bayes-Klassifikatoren eignen sich besonders gut für die folgenden Situationen:

Wenn die naiven Annahmen tatsächlich mit den Daten übereinstimmen (in der Praxis sehr selten)
Bei sehr gut getrennten Kategorien, wenn die Modellkomplexität weniger wichtig ist
Für sehr hochdimensionale Daten, wenn die Modellkomplexität weniger wichtig ist
Die letzten beiden Punkte scheinen unterschiedlich zu sein, aber sie hängen tatsächlich zusammen: Wenn die Dimensionalität
eines Datensatzes wächst, ist es sehr viel unwahrscheinlicher, dass zwei Punkte nahe beieinander liegen

(schließlich müssen sie in jeder einzelnen Dimension nah beieinander liegen, um insgesamt nah zu sein). Das bedeutet
dass Cluster in hohen Dimensionen im Durchschnitt stärker voneinander getrennt sind als Cluster

in niedrigen Dimensionen, vorausgesetzt, dass die neuen Dimensionen tatsächlich Informationen hinzufügen. Für diese

Grund dafür, dass einfache Klassifikatoren wie die hier diskutierten in der Regel genauso gut oder besser
als kompliziertere Klassifikatoren, wenn die Dimensionalität zunimmt: Sobald man genug

Daten, kann selbst ein einfaches Modell sehr leistungsfähig sein.

418 | Kapitel 41: Vertiefung: Naive Bayes-Klassifikation

KAPITEL 42

Vertiefung: Lineare Regression
So wie Naive Bayes (in Kapitel 41 besprochen) ein guter Ausgangspunkt für Klassifizierungs
Aufgaben ist, sind lineare Regressionsmodelle ein guter Ausgangspunkt für Regressionsaufgaben. Solche

Modelle sind beliebt, weil sie sich schnell montieren lassen und einfach zu integrieren sind.

pret. Sie sind bereits mit der einfachsten Form eines linearen Regressionsmodells vertraut (d. h.,
Anpassung einer geraden Linie an zweidimensionale Daten), aber solche Modelle können erweitert werden auf

ein komplizierteres Datenverhalten zu modellieren.

In diesem Kapitel beginnen wir mit einer kurzen Erläuterung der mathematischen Grundlagen

bekanntes Problem, bevor wir uns damit befassen, wie lineare Modelle verallgemeinert werden können

um kompliziertere Muster in den Daten zu berücksichtigen.

Wir beginnen mit den Standardimporten:

In [1]: % matplotlib inline
import matplotlib.pyplot as plt
plt.style.use('seaborn-whitegrid')
import numpy as np

Einfache lineare Regression
Wir beginnen mit der bekanntesten linearen Regression, einer geradlinigen Anpassung an die Daten. A

geradlinige Anpassung ist ein Modell der Form:

y=ax+b
wobei a allgemein als Steigung und b allgemein als Achsenabschnitt bezeichnet wird.

Betrachten Sie die folgenden Daten, die um eine Linie mit einer Steigung von 2 und einem
Schnittpunkt von -5 (siehe Abbildung 42-1).

419
In [2]: rng = np.random.RandomState(1)
x = 10 * rng.rand(50)
y = 2 * x - 5 + rng.randn(50)
plt.scatter(x, y);

Abbildung 42-1. Daten für die lineare Regression

Wir können den LinearRegression-Schätzer von Scikit-Learn verwenden, um diese Daten anzupassen und eine
die Best-Fit-Linie, wie in Abbildung 42-2 gezeigt.

In [3]: from sklearn.linear_model import LinearRegression
model = LinearRegression(fit_intercept= True )

model.fit(x[:, np.newaxis], y)

xfit = np.linspace(0, 10, 1000)
yfit = model.predict(xfit[:, np.newaxis])

plt.scatter(x, y)
plt.plot(xfit, yfit);

420 | Kapitel 42: Vertiefung: Lineare Regression

Abbildung 42-2. Ein einfaches lineares Regressionsmodell

Die Steigung und der Achsenabschnitt der Daten sind in den Anpassungsparametern des Modells enthalten, die

werden in Scikit-Learn immer durch einen nachgestellten Unterstrich gekennzeichnet. Hier ist der relevante Param-

ters sind coef_ und intercept_:

In [4]: print("Modell Steigung: ", model.coef_[0])
print("Modell-Abschnitt:", model.intercept_)
Out[4]: Modellsteigung: 2.0272088103606953
Modell-Achsenabschnitt: -4.998577085553204

Wir sehen, dass die Ergebnisse sehr nahe an den Werten liegen, die für die Erstellung der Daten verwendet wurden, da wir

hoffen könnte.

Der LinearRegression-Schätzer ist jedoch viel leistungsfähiger als dieser - zusätzlich

Neben einfachen linearen Anpassungen kann es auch mehrdimensionale lineare Modelle der
der Form:

y=a 0 +a 1 x 1 +a 2 x 2 +⋯
wobei es mehrere x-Werte gibt. Geometrisch gesehen ist dies vergleichbar mit der Anpassung einer Ebene an
Punkten in drei Dimensionen, oder der Anpassung einer Hyperebene an Punkte in höheren Dimensionen.

Die mehrdimensionale Natur solcher Regressionen macht sie schwieriger zu visualisieren.
alisieren, aber wir können eine dieser Anpassungen in Aktion sehen, indem wir einige Beispieldaten erstellen, indem wir

NumPy's Matrix-Multiplikations-Operator:

In [5]: rng = np.random.RandomState(1)
X = 10 * rng.rand(100, 3)
y = 0.5 + np.dot(X, [1.5, -2., 1.])

Einfache lineare Regression | 421
model.fit(X, y)
print(model.intercept_)
print(model.coef_)
Out[5]: 0.50000000000001
[ 1.5 -2. 1. ]

Hier werden die y-Daten aus einer linearen Kombination von drei zufälligen x-Werten gebildet,

und die lineare Regression stellt die Koeffizienten wieder her, die zur Konstruktion der Daten verwendet wurden.

Auf diese Weise können wir den einzelnen LinearRegression-Schätzer verwenden, um Linien, Ebenen oder

Hyperebenen auf unsere Daten zu übertragen. Es scheint immer noch so, als wäre dieser Ansatz auf streng
lineare Beziehungen zwischen den Variablen beschränkt, aber es stellt sich heraus, dass wir dies auch lockern können.

Basisfunktionsregression
Ein Trick, mit dem Sie die lineare Regression an nichtlineare Beziehungen zwischen

Variablen besteht darin, die Daten entsprechend den Basisfunktionen zu transformieren. Wir haben gesehen, dass eine Ver-

Die Pipeline PolynomialRegression, die in den Kapiteln 39 und

Die Idee ist, unser mehrdimensionales lineares Modell zu nehmen:
y=a 0 +a 1 x 1 +a 2 x 2 +a 3 x 3 +⋯
zu nehmen und x 1, x 2, x 3 usw. aus der eindimensionalen Eingabe x zu bilden. Das heißt, wir lassen

xn=fnx, wobei fn eine Funktion ist, die unsere Daten transformiert.

Wenn zum Beispiel fnx =xn ist, wird unser Modell zu einer polynomialen Regression:

y=a 0 +a 1 x+a 2 x^2 +a 3 x^3 +⋯
Beachten Sie, dass dies immer noch ein lineares Modell ist - die Linearität bezieht sich auf die Tatsache, dass die Koeffi-

cients an never multiply or divide each other. Was wir effektiv getan haben, ist, die

unsere eindimensionalen x-Werte und projizierte sie in eine höhere Dimension, so dass ein

Die lineare Anpassung kann auch kompliziertere Beziehungen zwischen x und y erfassen.

Polynomielle Basisfunktionen
Diese polynomiale Projektion ist so nützlich, dass sie in Scikit-Learn eingebaut ist, und zwar mit

den Transformator PolynomialFeatures:

In [6]: from sklearn.preprocessing import PolynomialFeatures
x = np.array([2, 3, 4])
poly = PolynomialFeatures(3, include_bias= False )
poly.fit_transform(x[:, None ])
Out[6]: array([[ 2., 4., 8.],

422 | Kapitel 42: Vertiefung: Lineare Regression

[ 3., 9., 27.],
[ 4., 16., 64.]])
Wir sehen hier, dass der Transformator unser eindimensionales Array in ein
dreidimensionales Array umgewandelt hat, in dem jede Spalte den potenzierten Wert enthält. Diese

Die neue, höherdimensionale Datendarstellung kann dann in eine lineare
Regression eingesetzt werden.

Wie wir in Kapitel 40 gesehen haben, ist der sauberste Weg, dies zu erreichen, die Verwendung einer Pipeline. Lassen Sie uns

auf diese Weise ein Polynommodell 7. Grades erstellen:

In [7]: from sklearn.pipeline import make_pipeline
poly_model = make_pipeline(PolynomialFeatures(7),
LinearRegression())

Mit dieser Transformation können wir das lineare Modell verwenden, um viel kompliziertere Beziehungen zwischen x und y herzustellen.
kompliziertere Beziehungen zwischen x und y. Hier ist zum Beispiel eine Sinuswelle mit Rauschen (siehe

Abbildung 42-3).

In [8]: rng = np.random.RandomState(1)
x = 10 * rng.rand(50)
y = np.sin(x) + 0.1 * rng.randn(50)

poly_model.fit(x[:, np.newaxis], y)
yfit = poly_model.predict(xfit[:, np.newaxis])

plt.scatter(x, y)
plt.plot(xfit, yfit);

Abbildung 42-3. Eine lineare Polynom-Anpassung an nichtlineare Trainingsdaten

Unser lineares Modell kann durch die Verwendung von Polynom-Basisfunktionen der siebten Ordnung

bieten eine hervorragende Anpassung an diese nichtlinearen Daten!

Basisfunktion Regression | 423
1 Der Code zur Erstellung dieser Abbildung ist im Online-Anhang zu finden.
Gaußsche Basisfunktionen
Es sind natürlich auch andere Basisfunktionen möglich. Ein nützliches Muster ist zum Beispiel die Anpassung

ein Modell, das nicht eine Summe von Polynombasen, sondern eine Summe von Gaußbasen ist. Das
Ergebnis könnte etwa wie in Abbildung 42-4 aussehen.

Abbildung 42-4. Anpassung einer Gaußschen Basisfunktion an nichtlineare Daten^1

Die schattierten Bereiche im Diagramm sind die skalierten Basisfunktionen, und wenn sie addiert werden

zusammen bilden sie die glatte Kurve durch die Daten ab. Diese Gaußschen Basisfunktionen
Funktionen sind nicht in Scikit-Learn integriert, aber wir können einen eigenen Transformator schreiben, der

erstellt, wie hier gezeigt und in Abbildung 42-5 (Scikit-Learn trans-

Formers sind als Python-Klassen implementiert; der Quelltext von Scikit-Learn ist eine gute
um zu sehen, wie sie erstellt werden können):

In [9]: from sklearn.base import BaseEstimator, TransformerMixin

class GaussianFeatures (BaseEstimator, TransformerMixin):
"""Uniformly spaced Gaussian features for one-dimensional input"""

def init(self, N, width_factor=2.0):
self.N = N
self.width_factor = width_factor

@staticmethod
def _gauss_basis(x, y, width, axis= None ):
arg = (x - y) / Breite
return np.exp(-0.5 * np.sum(arg ** 2, axis))

424 | Kapitel 42: Vertiefung: Lineare Regression

def fit(self, X, y= None ):
# N Zentren erstellen, die über den Datenbereich verteilt sind
self.centers_ = np.linspace(X.min(), X.max(), self.N)
self.width_ = self.width_factor*(self.centers_[1]-self.centers_[0])
return self

def transform(self, X):
return self.gauss_basis(X[:, :, np.newaxis], self.centers,
self.width_, axis=1)

gauss_model = make_pipeline(GaussianFeatures(20),
LinearRegression())
gauss_model.fit(x[:, np.newaxis], y)
yfit = gauss_model.predict(xfit[:, np.newaxis])

plt.scatter(x, y)
plt.plot(xfit, yfit)
plt.xlim(0, 10);

Abbildung 42-5. Eine mit einem benutzerdefinierten Transformator berechnete Gaußsche Basisfunktion

Ich habe dieses Beispiel nur angeführt, um zu verdeutlichen, dass poly- nomische Basisfunktionen nichts Magisches an sich haben.
nomischen Basisfunktionen nichts Magisches ist: Wenn Sie eine gewisse Intuition für den Erzeugungsprozess haben

Ihrer Daten, die Sie zu der Annahme veranlassen, dass die eine oder andere Grundlage angemessen sein könnte, können Sie
diese stattdessen verwenden.

Regularisierung
Die Einführung von Basisfunktionen in unsere lineare Regression macht das Modell viel

flexibler, aber es kann auch sehr schnell zu einer Überanpassung führen (siehe Kapitel 39

für eine Diskussion hierüber). Abbildung 42-6 zeigt zum Beispiel, was passiert, wenn wir eine
große Anzahl von Gaußschen Basisfunktionen:

Regularisierung | 425
In [10]: model = make_pipeline(GaussianFeatures(30),
LinearRegression())
model.fit(x[:, np.newaxis], y)

plt.scatter(x, y)
plt.plot(xfit, model.predict(xfit[:, np.newaxis]))

plt.xlim(0, 10)
plt.ylim(-1,5, 1,5);

Abbildung 42-6. Ein übermäßig komplexes Basisfunktionsmodell, das die Daten übererfüllt

Bei der Projektion der Daten auf die 30-dimensionale Basis hat das Modell eine viel zu große Flexibilität.

und erreicht Extremwerte zwischen den Stellen, an denen sie durch Daten eingeschränkt wird.
Wir können den Grund dafür sehen, wenn wir die Koeffizienten der Gaußschen Basen mit

in Bezug auf ihre Standorte, wie in Abbildung 42-7 dargestellt.

In [11]: def basis_plot(model, title= None ):
fig, ax = plt.subplots(2, sharex= True )
model.fit(x[:, np.newaxis], y)
ax[0].scatter(x, y)
ax[0].plot(xfit, model.predict(xfit[:, np.newaxis]))
ax[0].set(xlabel='x', ylabel='y', ylim=(-1.5, 1.5))

wenn Titel:
ax[0].set_title(title)

ax[1].plot(model.steps[0][1].centers_,
model.steps[1][1].coef_)
ax[1].set(xlabel='Basisort',
ylabel='Koeffizient',
xlim=(0, 10))

426 | Kapitel 42: Vertiefung: Lineare Regression

model = make_pipeline(GaussianFeatures(30), LinearRegression())
basis_plot(model)

Abbildung 42-7. Die Koeffizienten der Gauß'schen Basen in dem übermäßig komplexen Modell

Das untere Feld dieser Abbildung zeigt die Amplitude der Basisfunktion an jedem Ort.

tion. Dies ist ein typisches Overfitting-Verhalten, wenn sich Basisfunktionen überschneiden: Die Koeffizienten
benachbarter Basisfunktionen blähen sich auf und heben sich gegenseitig auf. Wir wissen, dass solche

Verhalten ist problematisch, und es wäre schön, wenn wir solche Ausschläge explizit im Modell begrenzen könnten
im Modell begrenzen könnten, indem wir große Werte der Modellparameter bestrafen. Eine solche Strafe ist

bekannt als Regularisierung, die es in verschiedenen Formen gibt.

Ridge-Regression (L 2 Regularisierung)
Die vielleicht häufigste Form der Regularisierung ist die sogenannte Ridge-Regression oder L 2

Regularisierung (manchmal auch Tikhonov-Regularisierung genannt). Dies geschieht durch
Bestrafung der Summe der Quadrate (2-Normen) der Modellkoeffizienten θn. In diesem Fall wird die

Strafe für die Anpassung des Modells wäre:

P=α ∑
n= 1
N
θn^2
wobei α ein freier Parameter ist, der die Stärke der Strafe steuert. Diese Art von

penalisierte Modell wird in Scikit-Learn mit dem Ridge-Schätzer eingebaut (siehe Abbildung 42-8).

In [12]: from sklearn.linear_model import Ridge
model = make_pipeline(GaussianFeatures(30), Ridge(alpha=0.1))
basis_plot(model, title='Ridge Regression')

Regularisierung | 427
Abbildung 42-8. Ridge (L 2 ) Regularisierung angewendet auf das zu komplexe Modell (vgl.

Abbildung 42-7)

Der Parameter α ist im Wesentlichen ein Regler, der die Komplexität der resultierenden

Modell. In der Grenze α 0 erhalten wir das Standardergebnis der linearen Regression; in der
Grenze α ∞ werden alle Modellantworten unterdrückt. Ein Vorteil der Ridge-Regression

Das Besondere an dieser Lösung ist, dass sie sehr effizient berechnet werden kann - mit kaum mehr Rechenleistung.

Kosten als das ursprüngliche lineare Regressionsmodell.

Lasso-Regression (L 1 Regularisierung)
Eine weitere gängige Art der Regularisierung ist die Lasso-Regression oder L 1-Regularisierung.

und beinhaltet die Bestrafung der Summe der absoluten Werte (1-Norm) der Regressionskoeffizienten
koeffizienten:

P=α ∑
n= 1
N
θn
Obwohl dies konzeptionell der Ridge-Regression sehr ähnlich ist, können sich die Ergebnisse sur-

prinzipiell. Zum Beispiel neigt die Lasso-Regression aufgrund ihrer Konstruktion dazu, spärliche
Modelle zu bevorzugen, d. h. sie setzt viele Modellkoeffizienten bevorzugt auf genau

Null.

Wir können dieses Verhalten sehen, wenn wir das vorherige Beispiel mit L 1 -normalisierten

Koeffizienten (siehe Abbildung 42-9).

428 | Kapitel 42: Vertiefung: Lineare Regression

In [13]: from sklearn.linear_model import Lasso
model = make_pipeline(GaussianFeatures(30),
Lasso(alpha=0.001, max_iter=2000))
basis_plot(model, title='Lasso Regression')

Abbildung 42-9. Lasso-Regularisierung (L 1 ), angewandt auf das übermäßig komplexe Modell (vgl.
Abbildung 42-8)

Bei der Lasso-Regressionsstrafe ist die Mehrzahl der Koeffizienten genau Null,

wobei das funktionale Verhalten durch eine kleine Teilmenge der verfügbaren Basisfunktionen modelliert wird.
Funktionen modelliert wird. Wie bei der Ridge-Regularisierung steuert der Parameter α die Stärke der

Strafe und sollte z. B. durch Kreuzvalidierung ermittelt werden (siehe auch
Kapitel 39 für eine Diskussion hierüber).

Beispiel: Vorhersage des Fahrradverkehrs
Schauen wir uns als Beispiel an, ob wir die Anzahl der Fahrradfahrten vorhersagen können

über die Fremont Bridge in Seattle je nach Wetter, Jahreszeit und anderen Faktoren. Wir

Wir haben diese Daten bereits in Kapitel 23 gesehen, aber hier werden wir die Fahrraddaten mit einem anderen
Datensatz und versuchen zu bestimmen, inwieweit Wetter- und saisonale Faktoren

Temperatur, Niederschlag und Tageslichtstunden - beeinflussen das Radverkehrsaufkommen
durch diesen Korridor. Glücklicherweise hat die National Oceanic and Atmospheric Adminis-

tration (NOAA) stellt ihre täglichen Wetterstationsdaten zur Verfügung - ich habe die Stations-ID

USW00024233 - und wir können Pandas ganz einfach verwenden, um die beiden Datenquellen zu verbinden. Wir werden
eine einfache lineare Regression durchführen, um das Wetter und andere Informationen mit dem Fahrrad

Zählungen, um abzuschätzen, wie sich die Änderung eines dieser Parameter auf die
Anzahl von Fahrern an einem bestimmten Tag auswirkt.

Beispiel: Vorhersage des Fahrradverkehrs | 429
Dies ist insbesondere ein Beispiel dafür, wie die Werkzeuge von Scikit-Learn in einem statistischen Projekt verwendet werden können.

Modellierungsrahmen, bei dem davon ausgegangen wird, dass die Parameter des Modells
eine interpretierbare Bedeutung haben. Wie bereits erwähnt, handelt es sich hierbei nicht um einen Standardansatz

innerhalb des maschinellen Lernens, aber eine solche Interpretation ist für einige Modelle möglich.

Beginnen wir mit dem Laden der beiden Datensätze und der Indizierung nach Datum:

In [14]: _# url = 'https://raw.githubusercontent.com/jakevdp/bicycle-data/main'

!curl -O {url}/FremontBridge.csv
!curl -O {url}/SeattleWetter.csv_
In [15]: import pandas as pd
counts = pd.read_csv('FremontBridge.csv',
index_col='Datum', parse_dates= True )
wetter = pd.read_csv('SeattleWetter.csv',
index_col='DATE', parse_dates= True )

Der Einfachheit halber betrachten wir die Daten vor 2020, um die Auswirkungen der
COVID-19-Pandemie zu vermeiden, die das Pendlerverhalten in Seattle erheblich beeinflusst hat:

In [16]: zählt = zählt[zählt.index < "2020-01-01"]
wetter = wetter[wetter.index < "2020-01-01"]

Als Nächstes werden wir den gesamten täglichen Fahrradverkehr berechnen und diesen in einen eigenen DataFrame einfügen:

In [17]: daily = counts.resample('d').sum()
daily['Total'] = daily.sum(axis=1)
daily = daily[['Gesamt']] # andere Spalten entfernen

Wir haben bereits gesehen, dass die Nutzungsmuster im Allgemeinen von Tag zu Tag variieren. Lassen Sie uns

Diesem Umstand tragen wir in unseren Daten Rechnung, indem wir binäre Spalten hinzufügen, die den Tag der Woche angeben
Woche angeben:

In [18]: days = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']
for i in range(7):
daily[days[i]] = (daily.index.dayofweek == i).astype(float)

In ähnlicher Weise könnten wir erwarten, dass sich die Fahrer an Feiertagen anders verhalten; fügen wir ein Indiz dafür hinzu
tor hinzufügen:

In [19]: from pandas.tseries.holiday import USFederalHolidayCalendar
cal = USFederalHolidayCalendar()
Feiertage = cal.holidays('2012', '2020')
daily = daily.join(pd.Series(1, index=holidays, name='holiday'))
daily['Feiertag'].fillna(0, inplace= True )

Man könnte auch vermuten, dass die Tageslichtstunden einen Einfluss darauf haben, wie viele Leute fahren.
Verwenden wir die astronomische Standardberechnung, um diese Information hinzuzufügen (siehe

Abbildung 42-10).

In [20]: def tageslichtstunden(date, axis=23.44, latitude=47.61):
"""Berechne die Tageslichtstunden für das gegebene Datum"""
Tage = (Datum - pd.datetime(2000, 12, 21)).Tage
m = (1. - np.tan(np.radians(latitude))

430 | Kapitel 42: Vertiefung: Lineare Regression

np.tan(np.radians(axis) * np.cos(days * 2 * np.pi / 365.25)))
return 24. * np.degrees(np.arccos(1 - np.clip(m, 0, 2))) / 180.
daily['daylight_hrs'] = list(map(hours_of_daylight, daily.index))
daily[['daylight_hrs']].plot()
plt.ylim(8, 17)
Out[20]: (8.0, 17.0)

Abbildung 42-10. Visualisierung der Tageslichtstunden in Seattle

Wir können auch die durchschnittliche Temperatur und die Gesamtniederschlagsmenge zu den Daten hinzufügen. Zusätzlich

der Niederschlagsmenge ein Flag hinzufügen, das anzeigt, ob ein Tag trocken ist
(ohne Niederschlag) ist:

In [21]: wetter['Temp (F)'] = 0.5 * (wetter['TMIN'] + wetter['TMAX'])
wetter['Niederschlag (in)'] = wetter['PRCP']
wetter['trockener Tag'] = (wetter['PRCP'] == 0).astype(int)

daily = daily.join(wetter[['Niederschlag (in)', 'Temperatur (F)', 'trockener Tag']])

Zum Schluss fügen wir einen Zähler hinzu, der ab Tag 1 ansteigt und misst, wie viele Jahre
vergangen sind. Damit können wir jede beobachtete jährliche Zunahme oder Abnahme von

tägliche Überfahrten:

In [22]: daily['annual'] = (daily.index - daily.index[0]).days / 365.

Beispiel: Vorhersage des Fahrradverkehrs | 431
Da unsere Daten nun in Ordnung sind, können wir sie uns ansehen:

In [23]: daily.head()
Out[23]: Gesamt Mo Di Mi Do Fr Sa So Feiertag
Datum
2012-10-03 14084.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0
2012-10-04 13900.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0
2012-10-05 12592.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0
2012-10-06 8024.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
2012-10-07 8568.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0

Tageslicht_Stunden Niederschlag ( in ) Temp (F) Trockentag jährlich
Datum
2012-10-03 11.277359 0.0 56.0 1 0.000000
2012-10-04 11.219142 0.0 56.5 1 0.002740
2012-10-05 11.161038 0.0 59.5 1 0.005479
2012-10-06 11.103056 0.0 60.5 1 0.008219
2012-10-07 11.045208 0.0 60.5 1 0.010959

Damit können wir die zu verwendenden Spalten auswählen und eine lineare Regression durchführen

Modell an unsere Daten anpassen. Wir setzen fit_intercept=False, weil die täglichen Flags essentiell sind.

als eigene tagesbezogene Abschnitte funktionieren:

In [24]: # Alle Zeilen mit Nullwerten löschen
daily.dropna(axis=0, how='any', inplace= True )

column_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun',
Feiertag', 'Tageslicht_Stunden', 'Niederschlag (in)',
'Trockentag', 'Temp (F)', 'jährlich']
X = daily[column_names]
y = daily['Total']

model = LinearRegression(fit_intercept= False )
model.fit(X, y)
daily['predicted'] = model.predict(X)

Schließlich können wir den gesamten und den prognostizierten Radverkehr visuell vergleichen (siehe
Abbildung 42-11).

In [25]: daily[['Total', 'predicted']].plot(alpha=0.5);

432 | Kapitel 42: Vertiefung: Lineare Regression

Abbildung 42-11. Die Vorhersage unseres Modells für den Radverkehr

Aus der Tatsache, dass die Daten und die Modellvorhersagen nicht genau übereinstimmen, wird deutlich
dass wir einige wichtige Merkmale übersehen haben. Entweder sind unsere Merkmale nicht vollständig (d. h., die Menschen

entscheiden, ob sie mit dem Auto zur Arbeit fahren, und zwar nicht nur aufgrund dieser Merkmale), oder es

Es gibt einige nichtlineare Beziehungen, die wir nicht berücksichtigt haben (z. B.
Vielleicht fahren die Menschen sowohl bei hohen als auch bei niedrigen Temperaturen weniger). Dennoch, unsere grobe

Annäherung reicht aus, um einige Erkenntnisse zu gewinnen, und wir können einen Blick auf die Koeffizienten des linearen Modells werfen.
des linearen Modells betrachten, um abzuschätzen, wie viel jedes Merkmal zum Ergebnis beiträgt.

tägliche Fahrradzählung:

In [26]: params = pd.Series(model.coef_, index=X.columns)
params
Out[26]: Mo -3309.953439
Di -2860.625060
Mi -2962,889892
Do -3480.656444
Fr -4836.064503
Sa -10436.802843
So -10795.195718
Feiertag -5006.995232
Tageslicht_Stunden 409.146368
Niederschlag ( in ) -2789.860745
trockener Tag 2111.069565
Temperatur (F) 179.026296
jährlich 324.437749
dtype: float64

Beispiel: Vorhersage des Fahrradverkehrs | 433
Diese Zahlen sind ohne ein gewisses Maß an Unsicherheit schwer zu interpretieren.

Mit Hilfe von Bootstrap-Resamplings der Daten können wir diese Unsicherheiten schnell berechnen:

In [27]: from sklearn.utils import resample
np.random.seed(1)
err = np.std([model.fit(*resample(X, y))).coef_
for i in range(1000)], 0)

Mit diesen geschätzten Fehlern wollen wir uns die Ergebnisse noch einmal ansehen:

In [28]: print(pd.DataFrame({'Wirkung': params.round(0),
'Unsicherheit': err.round(0)}))
Out[28]: Wirkung Unsicherheit
Mo -3310.0 265.0
Di -2861.0 274.0
Mi -2963,0 268,0
Do -3481.0 268.0
Fr -4836,0 261,0
Sa -10437,0 259,0
So -10795,0 267,0
Feiertag -5007.0 401.0
Tageslicht_Stunden 409.0 26.0
Niederschlag ( in ) -2790.0 186.0
trockener Tag 2111.0 101.0
Temperatur (F) 179,0 7,0
jährlich 324,0 22,0

Die Auswirkungsspalte zeigt, grob gesagt, wie sich die Anzahl der Fahrer
durch eine Änderung des betreffenden Merkmals beeinflusst wird. Zum Beispiel gibt es eine klare Kluft, wenn es

der Wochentag: Am Wochenende gibt es Tausende von Fahrern weniger als an
Wochentagen. Wir sehen auch, dass für jede zusätzliche Stunde Tageslicht, 409 ± 26 mehr Menschen

zu fahren; ein Temperaturanstieg von einem Grad Fahrenheit fördert 179 ± 7

Menschen zum Fahrrad greifen; ein trockener Tag bedeutet durchschnittlich 2.111 ± 101 mehr Radfahrer,
und jeder Zentimeter Niederschlag führt dazu, dass 2.790 ± 186 Personen ein anderes Verkehrsmittel wählen.

Hafen. Wenn all diese Effekte berücksichtigt werden, ergibt sich ein bescheidener Anstieg von 324 ± 22
neuen täglichen Fahrgästen pro Jahr.

In unserem einfachen Modell fehlen mit Sicherheit einige wichtige Informationen. Zum Beispiel...

wie bereits erwähnt, nichtlineare Effekte (z. B. Auswirkungen von Niederschlag und Kälte)
Temperatur) und nichtlineare Trends innerhalb der einzelnen Variablen (z. B. die Abneigung gegen das Fahren

bei sehr kalten und sehr heißen Temperaturen) kann nicht mit einem einfachen linearen
Modell berücksichtigt werden. Außerdem haben wir einige der feinkörnigeren Informationen weggeworfen

(z. B. der Unterschied zwischen einem regnerischen Morgen und einem regnerischen Nachmittag), und wir haben

Korrelationen zwischen den Tagen ignoriert (z. B. die möglichen Auswirkungen eines regnerischen Dienstags auf die
Mittwochs, oder die Auswirkungen eines unerwarteten sonnigen Tages nach einer Reihe von Regenfällen

Tage). Dies sind alles potenziell interessante Effekte, und Sie haben jetzt die Mittel, um sie zu
sie zu erforschen, wenn Sie wollen!

434 | Kapitel 42: Vertiefung: Lineare Regression

KAPITEL 43

Vertiefung: Support-Vektor-Maschinen
Support-Vektor-Maschinen (SVMs) sind eine besonders leistungsfähige und flexible Klasse von
überwachter Algorithmen für Klassifizierung und Regression. In diesem Kapitel werden wir

die Intuition hinter SVMs und ihre Verwendung bei Klassifizierungsproblemen zu erforschen.

Wir beginnen mit den Standardimporten:

In [1]: % matplotlib inline
import numpy as np
import matplotlib.pyplot as plt
plt.style.use('seaborn-whitegrid')
von scipy import stats

Farbige Abbildungen in voller Größe sind in den ergänzenden Materialien auf GitHub verfügbar.
rials auf GitHub.
Motivation für Support-Vektor-Maschinen
Im Rahmen unserer Diskussion der Bayes'schen Klassifikation (siehe Kapitel 41) haben wir etwas über

eine einfache Art von Modell, das die Verteilung jeder zugrunde liegenden Klasse beschreibt, und
experimentierte mit der Verwendung dieses Modells zur wahrscheinlichkeitsbasierten Bestimmung von Etiketten für neue Punkte. Das

war ein Beispiel für eine generative Klassifizierung; hier werden wir stattdessen die Diskriminierung

tive Klassifizierung. Das heißt, anstatt jede Klasse zu modellieren, finden wir einfach eine Linie
oder Kurve (in zwei Dimensionen) oder eine Mannigfaltigkeit (in mehreren Dimensionen), die die

Klassen voneinander zu unterscheiden.

435
Betrachten wir als Beispiel dafür den einfachen Fall einer Klassifizierungsaufgabe, bei der die

zwei Klassen von Punkten sind gut voneinander getrennt (siehe Abbildung 43-1).

In [2]: from sklearn.datasets import make_blobs
X, y = make_blobs(n_samples=50, centers=2,
random_state=0, cluster_std=0.60)
plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn');

Abbildung 43-1. Einfache Daten für die Klassifizierung

Ein linearer diskriminativer Klassifikator würde versuchen, eine gerade Linie zu ziehen, die die
zwei Datensätze zu trennen und so ein Modell für die Klassifizierung zu erstellen. Für zweidimensionale

Daten wie die hier gezeigten, könnten wir diese Aufgabe von Hand erledigen. Aber wir sehen sofort

ein Problem: Es gibt mehr als eine mögliche Trennlinie, die eine perfekte Unterscheidung
zwischen den beiden Klassen unterscheidet!

Wir können einige von ihnen wie folgt zeichnen; Abbildung 43-2 zeigt das Ergebnis:

In [3]: xfit = np.linspace(-1, 3.5)
plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')
plt.plot([0.6], [2.1], 'x', color='red', markeredgewidth=2, markersize=10)

for m, b in [(1, 0.65), (0.5, 1.6), (-0.2, 2.9)]:
plt.plot(xfit, m * xfit + b, '-k')

plt.xlim(-1, 3.5);

436 | Kapitel 43: Vertiefung: Support-Vektor-Maschinen

Abbildung 43-2. Drei perfekte lineare diskriminierende Klassifikatoren für unsere Daten

Es handelt sich um drei sehr unterschiedliche Abscheider, die dennoch eine perfekte Unterscheidung ermöglichen

zwischen diesen Stichproben. Je nachdem, was Sie wählen, wird ein neuer Datenpunkt (z. B. der

der in dieser Grafik mit einem "X" markiert ist) wird eine andere Bezeichnung zugewiesen! Offensichtlich ist unsere
einfache Intuition des "Ziehens einer Linie zwischen den Klassen" nicht gut genug, und wir müssen

ein wenig tiefer zu denken.

Support-Vektor-Maschinen: Maximierung der Gewinnspanne
Support-Vektor-Maschinen bieten eine Möglichkeit, dies zu verbessern. Die Intuition ist folgende:
Anstatt einfach eine Linie mit der Breite Null zwischen den Klassen zu ziehen, können wir

um jede Zeile einen Rand mit einer gewissen Breite, auf den nächsten Punkt genau. Hier ist ein Beispiel

wie dies aussehen könnte (Abbildung 43-3).

In [4]: xfit = np.linspace(-1, 3.5)
plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')

für m, b, d in [(1, 0,65, 0,33), (0,5, 1,6, 0,55), (-0,2, 2,9, 0,2)]:
yfit = m * xfit + b
plt.plot(xfit, yfit, '-k')
plt.fill_between(xfit, yfit - d, yfit + d, edgecolor='none',
color='lightgray', alpha=0.5)

plt.xlim(-1, 3.5);

Die Linie, die diese Marge maximiert, wird als optimales Modell ausgewählt.

Support-Vektor-Maschinen: Maximierung der Gewinnspanne | 437
Abbildung 43-3. Visualisierung von "Margen" in diskriminativen Klassifikatoren

Anpassung einer Support-Vektor-Maschine
Sehen wir uns das Ergebnis einer tatsächlichen Anpassung an diese Daten an: Wir werden die Support-Vektor-Maschine von Scikit-Learn verwenden

Klassifikator (SVC), um ein SVM-Modell auf diesen Daten zu trainieren. Vorerst werden wir einen

linearen Kernel und setzen den C-Parameter auf eine sehr große Zahl (wir werden die Mittelwerte
(auf die Mittelwerte dieser Parameter wird gleich noch näher eingegangen):

In [5]: from sklearn.svm import SVC # "Support-Vektor-Klassifikator"
model = SVC(kernel='linear', C=1E10)
model.fit(X, y)
Out[5]: SVC(C=10000000000.0, kernel='linear')

Zur besseren Veranschaulichung, was hier geschieht, erstellen wir eine schnelle Funktion
erstellen, die die SVM-Entscheidungsgrenzen für uns darstellt (Abbildung 43-4).

In [6]: def plot_svc_decision_function(model, ax= None , plot_support= True ):
"""Plotten der Entscheidungsfunktion für einen 2D-SVC"""
if ax is None :
ax = plt.gca()
xlim = ax.get_xlim()
ylim = ax.get_ylim()

# Gitter zur Auswertung des Modells erstellen
x = np.linspace(xlim[0], xlim[1], 30)
y = np.linspace(ylim[0], ylim[1], 30)
Y, X = np.meshgrid(y, x)
xy = np.vstack([X.ravel(), Y.ravel()]).T
P = model.decision_function(xy).reshape(X.shape)

# Entscheidungsgrenze und Ränder einzeichnen

438 | Kapitel 43: Vertiefung: Support-Vektor-Maschinen

ax.contour(X, Y, P, colors='k',
levels=[-1, 0, 1], alpha=0.5,
linestyles=['--', '-', '--'])

# Plotten von Unterstützungsvektoren
if plot_support:
ax.scatter(model.support_vectors_[:, 0],
model.support_vectors_[:, 1],
s=300, linewidth=1, edgecolors='black',
facecolors='none');
ax.set_xlim(xlim)
ax.set_ylim(ylim)

In [7]: plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')
plot_svc_decision_function(model);

Abbildung 43-4. Ein Support-Vector-Machine-Klassifikator, angepasst an die Daten, mit Rändern (gestrichelt)

Linien) und Stützvektoren (Kreise) dargestellt

Dies ist die Trennlinie, die die Spanne zwischen den beiden Punktesätzen maximiert.

Beachten Sie, dass einige der Trainingspunkte gerade den Rand berühren: Sie sind eingekreist in
Abbildung 43-5. Diese Punkte sind die zentralen Elemente dieser Anpassung; sie werden als die

Stützvektoren, und geben dem Algorithmus seinen Namen. In Scikit-Learn werden die Identitäten von

Diese Punkte werden im Attribut support_vectors_ des Klassifikators gespeichert:

In [8]: model.support_vectors_
Out[8]: array([[0.44359863, 3.11530945],
[2.33812285, 3.43116792],
[2.06156753, 1.96918596]])

Support-Vektor-Maschinen: Maximierung der Gewinnspanne | 439
Ein Schlüssel zum Erfolg dieses Klassifikators ist, dass für die Anpassung nur die Positionen der Stützvektoren

Punkte, die weiter vom Rand entfernt sind und auf der richtigen Seite liegen, verändern die Anpassung nicht.
verändern die Anpassung nicht. Technisch gesehen liegt das daran, dass diese Punkte nicht zum Verlust beitragen

Funktion, die zur Anpassung des Modells verwendet wird, so dass ihre Position und Anzahl keine Rolle spielen, solange

sie überschreiten nicht die Grenze.

Dies lässt sich beispielsweise erkennen, wenn wir das aus den ersten 60 Punkten gelernte Modell und

die ersten 120 Punkte dieses Datensatzes (Abbildung 43-5).

In [9]: def plot_svm(N=10, ax= None ):
X, y = make_blobs(n_samples=200, centers=2,
random_state=0, cluster_std=0.60)
X = X[:N]
y = y[:N]
model = SVC(kernel='linear', C=1E10)
model.fit(X, y)

ax = ax oder plt.gca()
ax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')
ax.set_xlim(-1, 4)
ax.set_ylim(-1, 6)
plot_svc_decision_function(model, ax)

fig, ax = plt.subplots(1, 2, figsize=(16, 6))
fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)
for axi, N in zip(ax, [60, 120]):
plot_svm(N, axi)
axi.set_title('N = {0}'.format(N))

Abbildung 43-5. Der Einfluss der neuen Trainingspunkte auf das SVM-Modell

Auf der linken Seite sehen Sie das Modell und die Support-Vektoren für 60 Trainingspunkte. Unter
rechten Feld haben wir die Anzahl der Trainingspunkte verdoppelt, aber das Modell hat

nicht geändert: Die drei Stützvektoren im linken Feld sind die gleichen wie die Stützvektoren im
Vektoren im rechten Feld. Diese Unempfindlichkeit gegenüber dem genauen Verhalten der entfernten Punkte ist

eine der Stärken des SVM-Modells.

440 | Kapitel 43: Vertiefung: Support-Vektor-Maschinen

Wenn Sie dieses Notizbuch live betreiben, können Sie die interaktiven Widgets von IPython verwenden, um

diese Funktion des SVM-Modells interaktiv betrachten:

In [10]: from ipywidgets import interact, fixed
interact(plot_svm, N=(10, 200), ax=fixed( None ));
Out[10]: interactive(children=(IntSlider(value=10, description='N', max=200, min=10),

Output()), _dom_classes=('widget-...

Jenseits linearer Grenzen: Kernel-SVM
Die SVM kann sehr leistungsfähig werden, wenn sie mit Kerneln kombiniert wird. Wir haben

Eine Version der Kernel haben wir bereits bei den Basisfunktionsregressionen in Kapitel 42 gesehen.
Dort haben wir unsere Daten in einen höherdimensionalen Raum projiziert, der durch Polynome definiert ist

und Gaußschen Basisfunktionen und konnten so nichtlineare Beziehungen mit einem
mit einem linearen Klassifikator.

Bei SVM-Modellen können wir eine Version der gleichen Idee verwenden. Zur Begründung der Notwendigkeit von ker-

nels, lassen Sie uns einige Daten betrachten, die nicht linear trennbar sind (Abbildung 43-6).

In [11]: from sklearn.datasets import make_circles
X, y = make_circles(100, factor=.1, noise=.1)

clf = SVC(kernel='linear').fit(X, y)

plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')
plot_svc_decision_function(clf, plot_support= False );

Abbildung 43-6. Ein linearer Klassifikator schneidet bei nichtlinearen Grenzen schlecht ab

Es ist klar, dass keine lineare Unterscheidung jemals in der Lage sein wird, diese Daten zu trennen. Aber wir

können Sie aus den Basisfunktionsregressionen in Kapitel 42 lernen und darüber nachdenken

Support-Vektor-Maschinen: Maximierung der Gewinnspanne | 441
wie wir die Daten in eine höhere Dimension projizieren können, so dass ein linearer Separator

ausreichen würde. Eine einfache Projektion, die wir verwenden könnten, wäre zum Beispiel
eine radiale Basisfunktion (RBF) zu berechnen, die auf den mittleren Klumpen zentriert ist:

In [12]: r = np.exp(-(X ** 2).sum(1))

Wir können diese zusätzliche Datendimension mit Hilfe einer dreidimensionalen Darstellung visualisieren, wie in

Abbildung 43-7.

In [13]: from mpl_toolkits import mplot3d

ax = plt.subplot(projection='3d')
ax.scatter3D(X[:, 0], X[:, 1], r, c=y, s=50, cmap='autumn')
ax.view_init(elev=20, azim=30)
ax.set_xlabel('x')
ax.set_ylabel('y')
ax.set_zlabel('r');

Abbildung 43-7. Eine den Daten hinzugefügte dritte Dimension ermöglicht eine lineare Trennung

Wir können sehen, dass die Daten mit dieser zusätzlichen Dimension trivialerweise linear trennbar werden
trennbar werden, indem man eine Trennebene z. B. bei r=0,7 einzeichnet.

In diesem Fall mussten wir unsere Projektion sorgfältig auswählen und abstimmen: Hätten wir unsere radiale Basisfunktion nicht
radiale Basisfunktion nicht an der richtigen Stelle platziert hätten, wären wir nicht in der Lage gewesen

saubere, linear trennbare Ergebnisse. Im Allgemeinen ist die Notwendigkeit, eine solche Wahl zu treffen, ein Prob-

lem: Wir möchten irgendwie automatisch die besten Basisfunktionen finden, die wir verwenden können.

Eine Strategie zur Erreichung dieses Ziels besteht darin, eine Basisfunktion zu berechnen, die in jedem Punkt des Bildes zentriert ist.

Datensatz und lässt den SVM-Algorithmus die Ergebnisse durchsieben. Diese Art von Basisfunktion

442 | Kapitel 43: Vertiefung: Support-Vektor-Maschinen

Transformation wird als Kernel-Transformation bezeichnet, da sie auf einer Ähnlichkeitsrela- tion basiert.

Beziehung (oder Kernel) zwischen den einzelnen Punktpaaren.

Ein mögliches Problem bei dieser Strategie - N Punkte in N Dimensionen zu projizieren - ist

dass es sehr rechenintensiv werden könnte, wenn N groß wird. Wie auch immer,

Aufgrund eines kleinen Verfahrens, das als Kernel-Trick bekannt ist, kann eine Anpassung an kernel-
transformierten Daten implizit durchgeführt werden, d. h. ohne jemals die vollständige N-

dimensionalen Darstellung der Kernelprojektion. Dieser Kernel-Trick ist in die
SVM eingebaut und ist einer der Gründe, warum die Methode so leistungsfähig ist.

In Scikit-Learn können wir die kernelisierte SVM anwenden, indem wir einfach unseren linearen Kernel in

einen RBF-Kernel unter Verwendung der Hyperparameter des Kernelmodells:

In [14]: clf = SVC(kernel='rbf', C=1E6)
clf.fit(X, y)
Out[14]: SVC(C=1000000.0)

Verwenden wir unsere zuvor definierte Funktion, um die Anpassung zu visualisieren und die Unterstützung zu identifizieren

Vektoren (Abbildung 43-8).

In [15]: plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')
plot_svc_decision_function(clf)
plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],
s=300, lw=1, facecolors='none');

Abbildung 43-8. Kernel-SVM-Anpassung an die Daten

Mit dieser kernelisierten Support-Vektor-Maschine lernen wir eine geeignete nichtlineare Entscheidungsgrenze
Grenze. Diese Kernel-Transformationsstrategie wird beim maschinellen Lernen häufig verwendet, um

schnelle lineare Methoden in schnelle nichtlineare Methoden umwandeln, insbesondere für Modelle, bei denen

kann der Kernel-Trick angewendet werden.

Support-Vektor-Maschinen: Maximierung der Gewinnspanne | 443
Abstimmung der SVM: Abschwächen der Ränder
Unsere bisherige Diskussion hat sich auf sehr saubere Datensätze konzentriert, in denen eine perfekte

Entscheidungsgrenze existiert. Was aber, wenn sich Ihre Daten in gewissem Umfang überschneiden? Unter
Sie können zum Beispiel Daten wie diese haben (siehe Abbildung 43-9).

In [16]: X, y = make_blobs(n_samples=100, centers=2,
random_state=0, cluster_std=1.2)
plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn');

Abbildung 43-9. Daten mit einem gewissen Grad an Überschneidung

Um diesen Fall zu bewältigen, hat die SVM-Implementierung einen kleinen Fudge-Faktor, der das Ergebnis "abschwächt".

die Marge: das heißt, dass einige der Punkte in die Marge hineinreichen können, wenn dies erlaubt

eine bessere Anpassung. Die Härte des Randes wird durch einen Abstimmungsparameter gesteuert, meist

Bei einem sehr großen C ist der Rand hart, und die Punkte können nicht in ihm liegen.

Bei einem kleineren C ist die Marge weicher und kann sich auf einige Punkte ausdehnen.

Das Diagramm in Abbildung 43-10 zeigt anschaulich, wie sich ein verändertes C auf die

die endgültige Anpassung durch die Aufweichung des Randes:

In [17]: X, y = make_blobs(n_samples=100, centers=2,
random_state=0, cluster_std=0.8)

fig, ax = plt.subplots(1, 2, figsize=(16, 6))
fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)

for axi, C in zip(ax, [10.0, 0.1]):
model = SVC(kernel='linear', C=C).fit(X, y)
axi.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')
plot_svc_decision_function(model, axi)

444 | Kapitel 43: Vertiefung: Support-Vektor-Maschinen

axi.scatter(model.support_vectors_[:, 0],
model.support_vectors_[:, 1],
s=300, lw=1, facecolors='none');
axi.set_title('C = {0:.1f}'.format(C), size=14)

Abbildung 43-10. Die Auswirkung des Parameters C auf die Anpassung des Stützvektors

Der optimale Wert von C hängt von Ihrem Datensatz ab, und Sie sollten diesen Parameter
Sie sollten diesen Parameter durch Kreuzvalidierung oder ein ähnliches Verfahren anpassen (siehe Kapitel 39).

Beispiel: Erkennung von Gesichtern
Als Beispiel für Support-Vektor-Maschinen in Aktion sehen wir uns die Gesichtserkennung an.

Erkennungsproblem. Wir werden den Datensatz "Labeled Faces in the Wild" verwenden, der aus

von mehreren tausend gesammelten Fotos verschiedener Persönlichkeiten des öffentlichen Lebens. Ein Abrufer für den Datensatz
ist in Scikit-Learn integriert:

In [18]: from sklearn.datasets import fetch_lfw_people
faces = fetch_lfw_people(min_faces_per_person=60)
print(faces.target_names)
print(faces.images.shape)
Out[18]: ['Ariel Sharon' 'Colin Powell' 'Donald Rumsfeld' 'George W Bush'
'Gerhard Schroeder' 'Hugo Chavez' 'Junichiro Koizumi' 'Tony Blair']
(1348, 62, 47)

Zeichnen wir ein paar dieser Flächen, um zu sehen, womit wir arbeiten (siehe Abbildung 43-11).

In [19]: fig, ax = plt.subplots(3, 5, figsize=(8, 6))
for i, axi in enumerate(ax.flat):
axi.imshow(faces.images[i], cmap='bone')
axi.set(xticks=[], yticks=[],
xlabel=faces.target_names[faces.target[i]])

Beispiel: Gesichtserkennung | 445
Abbildung 43-11. Beispiele aus dem "Labeled Faces in the Wild"-Datensatz

Jedes Bild enthält 62 × 47, also rund 3.000 Pixel. Wir könnten einfach fortfahren
jeden Pixelwert als Merkmal verwenden, aber oft ist es effektiver, eine Art von

Präprozessor, um aussagekräftigere Merkmale zu extrahieren; hier werden wir die Hauptkompo
nentenanalyse (siehe Kapitel 45), um 150 grundlegende Komponenten zu extrahieren, die in die

unseren Support-Vektor-Maschinen-Klassifikator. Wir können dies am einfachsten tun, indem wir...

den Präprozessor und den Klassifikator in einer einzigen Pipeline zusammenzufassen:

In [20]: from sklearn.svm import SVC
from sklearn.decomposition import PCA
from sklearn.pipeline import make_pipeline

pca = PCA(n_components=150, whiten= True ,
svd_solver='randomized', random_state=42)
svc = SVC(kernel='rbf', class_weight='balanced')
model = make_pipeline(pca, svc)

446 | Kapitel 43: Vertiefung: Support-Vektor-Maschinen

Um die Ergebnisse unseres Klassifikators zu testen, werden wir die Daten in einen Trainingssatz aufteilen

und einen Testsatz:

In [21]: from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(faces.data, faces.target,
random_state=42)

Schließlich können wir die Kreuzvalidierung mit der Gittersuche verwenden, um Kombinationen von Parametern zu untersuchen.

ters. Hier werden wir C (das die Randhärte steuert) und Gamma (das

steuert die Größe des Radialbasisfunktionskerns), und bestimmen Sie das beste Modell:

In [22]: from sklearn.model_selection import GridSearchCV
param_grid = {'svc__C': [1, 5, 10, 50],
'svc__gamma': [0.0001, 0.0005, 0.001, 0.005]}
grid = GridSearchCV(model, param_grid)

% Zeit grid.fit(Xtrain, ytrain)
print(grid.best_params_)
Out[22]: CPU-Zeiten: user 1min 19s, sys: 8.56 s, gesamt: 1min 27s
Wandzeit: 36.2 s
{'svc__C': 10, 'svc__gamma': 0.001}

Die optimalen Werte liegen in der Mitte unseres Gitters; würden sie an den Rändern liegen, würden wir

würde das Raster erweitern wollen, um sicherzustellen, dass wir das wahre Optimum gefunden haben.

Mit diesem kreuzvalidierten Modell können wir nun die Bezeichnungen für die Testdaten vorhersagen, die
die das Modell noch nicht gesehen hat:

In [23]: model = grid.best_estimator_
yfit = model.predict(Xtest)

Werfen wir einen Blick auf einige der Testbilder zusammen mit ihren vorhergesagten Werten (siehe

Abbildung 43-12).

In [24]: fig, ax = plt.subplots(4, 6)
for i, axi in enumerate(ax.flat):
axi.imshow(Xtest[i].reshape(62, 47), cmap='bone')
axi.set(xticks=[], yticks=[])
axi.set_ylabel(faces.target_names[yfit[i]].split()[-1],
color='schwarz' if yfit[i] == ytest[i] else 'rot')
fig.suptitle('Vorausgesagte Namen; Falsche Bezeichnungen in Rot', size=14);

Beispiel: Gesichtserkennung | 447
Abbildung 43-12. Von unserem Modell vorhergesagte Etiketten

Aus dieser kleinen Stichprobe hat unser optimaler Schätzer nur ein einziges Gesicht falsch beschriftet (Bushs

Gesicht in der unteren Reihe wurde fälschlicherweise als Blair bezeichnet). Einen besseren Eindruck von der Leistung unseres Schätzers erhält man
mator ein besseres Gefühl für die Leistung unseres Schätzers bekommen, wenn wir den Klassifizierungsbericht verwenden, der Wiederherstellungsstatistiken auflistet

Etikett für Etikett:

In [25]: from sklearn.metrics import classification_report
print(classification_report(ytest, yfit,
target_names=faces.target_names))
Out[25]: Genauigkeit Rückruf f1-Score Unterstützung

Ariel Sharon 0,65 0,73 0,69 15
Colin Powell 0,80 0,87 0,83 68
Donald Rumsfeld 0,74 0,84 0,79 31
George W. Bush 0,92 0,83 0,88 126
Gerhard Schroeder 0,86 0,83 0,84 23
Hugo Chavez 0,93 0,70 0,80 20
Junichiro Koizumi 0,92 1,00 0,96 12
Tony Blair 0,85 0,95 0,90 42

Genauigkeit 0,85 337
Makro-Durchschnittswert 0,83 0,84 0,84 337
gewichteter Durchschnitt 0.86 0.85 0.85 337

448 | Kapitel 43: Vertiefung: Support-Vektor-Maschinen

Wir können auch die Konfusionsmatrix zwischen diesen Klassen anzeigen (siehe Abbildung 43-13).

In [26]: from sklearn.metrics import confusion_matrix
import seaborn as sns
mat = confusion_matrix(ytest, yfit)
sns.heatmap(mat.T, square= True , annot= True , fmt='d',
cbar= False , cmap='Blues',
xticklabels=Gesichter.target_names,
yticklabels=faces.target_names)
plt.xlabel('wahre Bezeichnung')
plt.ylabel('vorhergesagte Beschriftung');

Abbildung 43-13. Konfusionsmatrix für die Gesichtsdaten

Dies hilft uns, ein Gefühl dafür zu bekommen, welche Bezeichnungen durch den Schätzer wahrscheinlich verwechselt werden.

Bei einer realen Gesichtserkennungsaufgabe, bei der die Fotos nicht in
Raster zugeschnitten sind, besteht der einzige Unterschied im Klassifizierungsschema für Gesichter in den

Merkmalsauswahl: Sie müssten einen ausgefeilteren Algorithmus verwenden, um die

Gesichter, und extrahieren Sie Merkmale, die unabhängig von der Pixellierung sind. Für diese Art von
Anwendung ist eine gute Option die Verwendung von OpenCV, das unter anderem,

Beispiel: Gesichtserkennung | 449
enthält vortrainierte Implementierungen von modernen Merkmalsextraktionswerkzeugen für

Bilder im Allgemeinen und Gesichter im Besonderen.

Zusammenfassung
Dies war eine kurze intuitive Einführung in die Prinzipien von Support Vector Machines
Maschinen. Diese Modelle sind eine leistungsfähige Klassifizierungsmethode für eine Reihe von

Gründe:

Ihre Abhängigkeit von relativ wenigen Stützvektoren bedeutet, dass sie kompakt sind
und benötigen nur sehr wenig Speicherplatz.
Sobald das Modell trainiert ist, ist die Vorhersagephase sehr schnell.
Da sie nur von Punkten in der Nähe des Randes beeinflusst werden, funktionieren sie gut mit
hochdimensionalen Daten - sogar mit Daten, die mehr Dimensionen als Stichproben aufweisen, was für andere
was für andere Algorithmen eine Herausforderung darstellt.
Ihre Integration mit Kernel-Methoden macht sie sehr vielseitig und ermöglicht die Anpassung an
viele Arten von Daten.
Allerdings haben SVMs auch einige Nachteile:

Die Skalierung mit der Anzahl der Stichproben N beträgt im ungünstigsten Fall 𝐀N^3 bzw. 𝐀N^2 bei effi-
zienter Implementierungen. Bei einer großen Anzahl von Trainingsproben kann dieser Rechenaufwand
tionskosten unerschwinglich sein.
Die Ergebnisse hängen stark von einer geeigneten Wahl des Weichmacherparameters C ab.
ter C ab. Dieser muss sorgfältig durch Kreuzvalidierung ausgewählt werden, was bei
wenn die Datensätze größer werden.
Die Ergebnisse haben keine direkte probabilistische Interpretation. Dies kann durch eine
Kreuzvalidierung geschätzt werden (siehe den Wahrscheinlichkeitsparameter von SVC), aber
diese zusätzliche Schätzung ist kostspielig.
In Anbetracht dieser Eigenschaften wende ich SVMs im Allgemeinen nur dann an, wenn andere, einfachere und schnellere Verfahren zum Einsatz kommen,

und weniger abstimmungsintensive Methoden haben sich für meine Bedürfnisse als unzureichend erwiesen.
Wenn Sie jedoch die CPU-Zyklen haben, die Sie für Training und Kreuzvalidierung benötigen

eine SVM auf Ihre Daten anwenden, kann die Methode zu hervorragenden Ergebnissen führen.

450 | Kapitel 43: Vertiefung: Support-Vektor-Maschinen

KAPITEL 44

Vertiefung: Entscheidungsbäume
und Zufallsforsten
Zuvor haben wir uns eingehend mit einem einfachen generativen Klassifikator (Naive Bayes; siehe
Kapitel 41) und einen leistungsfähigen diskriminierenden Klassifikator (Support Vector Machines; siehe

Kapitel 43). Hier werfen wir einen Blick auf einen weiteren leistungsstarken Algorithmus: einen nichtparametrischen

Algorithmus namens Random Forests. Zufällige Wälder sind ein Beispiel für ein Ensemble
einer Ensemble-Methode, d. h. einer Methode, die auf der Aggregation der Ergebnisse einer Reihe von einfacheren Schätzungen beruht.

toren. Das etwas überraschende Ergebnis bei solchen Ensemble-Methoden ist, dass die Summe
größer sein kann als die einzelnen Teile: Das heißt, die Vorhersagegenauigkeit einer Mehrheitsentscheidung unter einer

Anzahl von Schätzern besser sein kann als die der einzelnen Schätzer.

matoren, die die Abstimmung durchführen! Wir werden in den folgenden Abschnitten Beispiele dafür sehen.

Wir beginnen mit den Standardimporten:

In [1]: % matplotlib inline
import numpy as np
import matplotlib.pyplot as plt
plt.style.use('seaborn-whitegrid')

Motivierende Random Forests: Entscheidungsbäume
Zufallswälder sind ein Beispiel für einen Ensemble-Lerner, der auf Entscheidungsbäumen aufbaut. Für

Aus diesem Grund werden wir uns zunächst mit den Entscheidungsbäumen selbst befassen.

451
1 Der Code zur Erstellung dieser Abbildung ist im Online-Anhang zu finden.
Entscheidungsbäume sind eine äußerst intuitive Methode zur Klassifizierung oder Kennzeichnung von Objekten: Sie fragen einfach

eine Reihe von Fragen, um die Klassifizierung herauszufinden. Wenn Sie zum Beispiel
einen Entscheidungsbaum erstellen möchten, um Tiere zu klassifizieren, denen Sie auf einer Wanderung begegnen,

können Sie das in Abbildung 44-1 gezeigte Modell konstruieren.

Abbildung 44-1. Ein Beispiel für einen binären Entscheidungsbaum^1

Die binäre Aufteilung macht dies äußerst effizient: In einem gut konstruierten Baum wird jede

Frage wird die Anzahl der Optionen um etwa die Hälfte reduziert, was die Auswahl sehr schnell einschränkt.

Auch bei einer großen Anzahl von Klassen ist es möglich, die Auswahl zu treffen. Der Trick liegt natürlich darin
zu entscheiden, welche Fragen bei jedem Schritt gestellt werden sollen. In Implementierungen des maschinellen Lernens von

Bei Entscheidungsbäumen haben die Fragen im Allgemeinen die Form von achsengerechten Aufteilungen der Daten:
Das heißt, jeder Knoten im Baum teilt die Daten in zwei Gruppen auf, wobei ein Abschneidewert verwendet wird

innerhalb eines der Merkmale. Schauen wir uns nun ein Beispiel dafür an.

Erstellen eines Entscheidungsbaums
Betrachten Sie die folgenden zweidimensionalen Daten, die eine von vier Klassenbeschriftungen haben (siehe

Abbildung 44-2).

In [2]: from sklearn.datasets import make_blobs

X, y = make_blobs(n_samples=300, centers=4,
random_state=0, cluster_std=1.0)
plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='rainbow');

452 | Kapitel 44: Vertiefung: Entscheidungsbäume und Zufallsforsten

2 Der Code zur Erstellung dieser Abbildung ist im Online-Anhang zu finden.
Abbildung 44-2. Daten für den Entscheidungsbaum-Klassifikator

Ein einfacher Entscheidungsbaum, der auf diesen Daten aufgebaut wird, teilt die Daten iterativ entlang einer der beiden

der anderen Achse nach einem quantitativen Kriterium, und auf jeder Ebene wird die Bezeichnung
der neuen Region nach einem Mehrheitsvotum der darin befindlichen Punkte. Abbildung 44-3

zeigt eine Visualisierung der ersten vier Ebenen eines Entscheidungsbaum-Klassifikators für diese Daten.

Abbildung 44-3. Visualisierung, wie der Entscheidungsbaum die Daten aufteilt^2

Beachten Sie, dass nach der ersten Unterteilung jeder Punkt im oberen Zweig unverändert bleibt,
Es besteht also keine Notwendigkeit, diesen Zweig weiter zu unterteilen. Außer bei Knoten, die alle

von einer Farbe, auf jeder Ebene wird jede Region erneut entlang eines der beiden Merkmale aufgeteilt.

Dieser Prozess der Anpassung eines Entscheidungsbaums an unsere Daten kann in Scikit-Learn mit der Funktion

DecisionTreeClassifier-Schätzer:

In [3]: from sklearn.tree import DecisionTreeClassifier
tree = DecisionTreeClassifier().fit(X, y)

Motivierende Random Forests: Entscheidungsbäume | 453
Schreiben wir eine Nutzenfunktion, die uns hilft, die Ausgabe des Klassifikators zu visualisieren:

In [4]: def visualize_classifier(model, X, y, ax= None , cmap='rainbow'):
ax = ax oder plt.gca()

# Plotten der Trainingspunkte
ax.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=cmap,
clim=(y.min(), y.max()), zorder=3)
ax.axis('eng')
ax.axis('off')
xlim = ax.get_xlim()
ylim = ax.get_ylim()

# Anpassung des Schätzers
model.fit(X, y)
xx, yy = np.meshgrid(np.linspace(xlim, num=200),
np.linspace(ylim, num=200))
Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)

# Erstellen Sie ein Farbdiagramm mit den Ergebnissen.
n_classes = len(np.unique(y))
contours = ax.contourf(xx, yy, Z, alpha=0.3,
levels=np.arange(n_classes + 1) - 0.5,
cmap=cmap, zorder=1)

ax.set(xlim=xlim, ylim=ylim)

Nun können wir untersuchen, wie die Entscheidungsbaumklassifizierung aussieht (siehe Abbildung 44-4).

In [5]: visualize_classifier(DecisionTreeClassifier(), X, y)

Abbildung 44-4. Visualisierung einer Entscheidungsbaum-Klassifikation

454 | Kapitel 44: Vertiefung: Entscheidungsbäume und Zufallsforsten

3 Der Code zur Erstellung dieser Abbildung ist im Online-Anhang zu finden.
Wenn Sie dieses Notebook live ausführen, können Sie das Hilfsskript verwenden, das in der Datei

Online-Anhang, um eine interaktive Visualisierung der Erstellung des Entscheidungsbaums aufzurufen
Prozesses:

In [6]: # helpers_05_08 findet sich im Online-Anhang
importieren helpers_05_08
helpers_05_08.plot_tree_interactive(X, y);
Out[6]: interactive(children=(Dropdown(description='depth', index=1, options=(1, 5),

value=5), Output()), _dom_classes...

Beachten Sie, dass wir mit zunehmender Tiefe zu einer sehr seltsam geformten Klassifizierung neigen

Regionen; zum Beispiel gibt es bei einer Tiefe von fünf eine hohe und dünne violette Region
zwischen den gelben und blauen Regionen. Es ist klar, dass dies weniger ein Ergebnis der wahren,

Datenverteilung, sondern vielmehr ein Ergebnis der besonderen Stichproben- oder Rauscheigenschaften der Daten.
Eigenschaften der Daten. Das heißt, dass dieser Entscheidungsbaum, auch wenn er nur fünf Ebenen tief ist, eindeutig

Überanpassung unserer Daten.

Entscheidungsbäume und Overfitting
Eine solche Überanpassung ist eine allgemeine Eigenschaft von Entscheidungsbäumen: Es ist sehr einfach, die

zu tief in den Baum eindringen und somit eher den Details der einzelnen Daten als den

die allgemeinen Eigenschaften der Verteilungen, aus denen sie gezogen werden. Eine andere Möglichkeit, diese Überanpassung zu sehen
Anpassung zu erkennen, indem man sich Modelle ansieht, die auf verschiedenen Teilmengen der Daten trainiert wurden - zum Beispiel in

In Abbildung 44-5 trainieren wir zwei verschiedene Bäume, jeweils auf der Hälfte der Originaldaten.

Abbildung 44-5. Ein Beispiel für zwei randomisierte Entscheidungsbäume^3

Es ist klar, dass die beiden Bäume an einigen Stellen zu übereinstimmenden Ergebnissen führen (z. B. in den vier

Ecken), während an anderen Stellen die beiden Bäume sehr unterschiedliche Klassifizierungen ergeben (z. B. in
den Regionen zwischen zwei beliebigen Clustern). Die wichtigste Beobachtung ist, dass die Ungereimtheiten

die Klassifizierung weniger sicher ist, und daher können wir durch die Verwendung von Informationen
aus diesen beiden Bäumen könnten wir zu einem besseren Ergebnis kommen!

Motivierende Random Forests: Entscheidungsbäume | 455
Wenn Sie dieses Notebook live ausführen, können Sie mit der folgenden Funktion interagieren.

die Anpassungen von Bäumen, die auf einer zufälligen Teilmenge der Daten trainiert wurden, anschaulich darstellen:

In [7]: # helpers_05_08 findet sich im Online-Anhang
importiere Helfer_05_08
helpers_05_08.randomized_tree_interactive(X, y)
Out[7]: interactive(children=(Dropdown(description='random_state', options=(0, 100),

value=0), Output()), _dom_classes...

Genauso wie die Verwendung von Informationen aus zwei Bäumen unsere Ergebnisse verbessert, könnten wir erwarten, dass

Die Verwendung von Informationen aus vielen Bäumen würde unsere Ergebnisse noch weiter verbessern.

Ensembles von Schätzern: Random Forests
Dieser Gedanke, dass mehrere Overfitting-Schätzer kombiniert werden können, um die

Der Effekt dieser Überanpassung liegt einer Ensemble-Methode namens Bagging zugrunde. Bag-
ging verwendet ein Ensemble (vielleicht eine Wundertüte) von parallelen Schätzern, von denen jeder

der die Daten übertrifft, und mittelt die Ergebnisse, um eine bessere Klassifizierung zu finden. Ein
Ensemble von randomisierten Entscheidungsbäumen wird als Random Forest bezeichnet.

Diese Art der Bagging-Klassifizierung kann manuell mit der Scikit-Learn-Funktion "Bagging" durchgeführt werden.

Klassifikator-Meta-Schätzer, wie hier gezeigt (siehe Abbildung 44-6).

In [8]: from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import BaggingClassifier

tree = DecisionTreeClassifier()
bag = BaggingClassifier(tree, n_estimators=100, max_samples=0.8,
random_state=1)

bag.fit(X, y)
visualize_classifier(bag, X, y)

In diesem Beispiel haben wir die Daten randomisiert, indem wir jeden Schätzer mit einem Ran-

dom-Teilmenge von 80 % der Trainingspunkte. In der Praxis sind Entscheidungsbäume effizienter
randomisiert, indem eine gewisse Stochastik in die Auswahl der Splits eingebracht wird: dies

alle Daten jedes Mal zur Anpassung beitragen, aber die Ergebnisse der Anpassung haben immer noch die

gewünschte Zufälligkeit. Bei der Entscheidung, welches Merkmal aufgespalten werden soll, kann der
randomisierte Baum aus den obersten mehreren Merkmalen auswählen. Sie können mehr lesen

Technische Details zu diesen Randomisierungsstrategien finden Sie in der Scikit-Learn-Dokumenta- tion und den
tion und den darin enthaltenen Referenzen.

456 | Kapitel 44: Vertiefung: Entscheidungsbäume und Zufallsforsten

Abbildung 44-6. Entscheidungsgrenzen für ein Ensemble von Zufallsentscheidungsbäumen

In Scikit-Learn wird ein solches optimiertes Ensemble von randomisierten Entscheidungsbäumen implementiert.

in den RandomForestClassifier-Schätzer integriert, der sich um die gesamte Run-

domization automatisch. Sie müssen lediglich eine Reihe von Schätzern auswählen, und das Programm
sehr schnell - auf Wunsch auch parallel - das Ensemble von Bäumen anpassen (siehe Abbildung 44-7).

In [9]: from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier(n_estimators=100, random_state=0)
visualize_classifier(model, X, y);

Abbildung 44-7. Entscheidungsgrenzen für einen Zufallsforst, der ein optimiertes Ensemble von

Entscheidungsbäume

Ensembles von Schätzern: Random Forests | 457
Wir sehen, dass wir durch Mittelung über hundert zufällig gestörte Modelle zu folgenden Ergebnissen kommen

mit einem Gesamtmodell, das unserer Intuition über die Aufteilung des Parameterraums viel näher kommt.
Raum aufgeteilt werden sollte.

Random Forest Regression
Im vorangegangenen Abschnitt haben wir Random Forests im Kontext der Klassifizie- rung betrachtet.

tion. Zufallswälder können auch bei Regressionen eingesetzt werden (d. h. bei

kontinuierliche und nicht kategoriale Variablen). Der hierfür zu verwendende Schätzer ist der Ran

domForestRegressor, und die Syntax ist sehr ähnlich zu der, die wir zuvor gesehen haben.

Betrachten Sie die folgenden Daten, die aus der Kombination einer schnellen und einer langsamen Oszillation stammen.

tion (siehe Abbildung 44-8).

In [10]: rng = np.random.RandomState(42)
x = 10 * rng.rand(200)

def model(x, sigma=0.3):
fast_oscillation = np.sin(5 * x)
langsame_Schwingung = np.sin(0.5 * x)
Rauschen = sigma * rng.randn(len(x))

return langsame_Oszillation + schnelle_Oszillation + Rauschen

y = model(x)
plt.errorbar(x, y, 0.3, fmt='o');

Abbildung 44-8. Daten für die Random-Forest-Regression

Mit dem Random-Forest-Regressor können wir die am besten passende Kurve finden (Abbildung 44-9).

458 | Kapitel 44: Vertiefung: Entscheidungsbäume und Zufallsforsten

In [11]: from sklearn.ensemble import RandomForestRegressor
forest = RandomForestRegressor(200)
forest.fit(x[:, None ], y)

xfit = np.linspace(0, 10, 1000)
yfit = forest.predict(xfit[:, None ])
ytrue = model(xfit, sigma=0)

plt.errorbar(x, y, 0.3, fmt='o', alpha=0.5)
plt.plot(xfit, yfit, '-r');
plt.plot(xfit, ytrue, '-k', alpha=0,5);

Abbildung 44-9. Anpassung des Random-Forest-Modells an die Daten

Hier ist das wahre Modell in der glatten grauen Kurve dargestellt, während der Random Forest

Modell wird durch die gezackte rote Kurve dargestellt. Das nichtparametrische Random-Forest-Modell ist
flexibel genug, um die Mehrperiodendaten zu erfassen, ohne dass wir ein Mehrperiodenmodell spezifizieren müssen.

historisches Modell!

Beispiel: Random Forest zur Klassifizierung von Ziffern
In Kapitel 38 haben wir ein Beispiel mit dem Ziffern-Datensatz aus dem Paket

Scikit-Learn. Lassen Sie uns das hier noch einmal verwenden, um zu sehen, wie der Random Forest Classifier
in diesem Kontext angewendet werden kann:

In [12]: from sklearn.datasets import load_digits
Ziffern = load_digits()
digits.keys()
Out[12]: dict_keys(['data', 'target', 'frame', 'feature_names', 'target_names',

'Bilder', 'DESCR'])

Beispiel: Random Forest zur Klassifizierung von Ziffern | 459
Um uns daran zu erinnern, was wir betrachten, visualisieren wir die ersten paar Datenpunkte (siehe

Abbildung 44-10).

In [13]: # die Abbildung einrichten
fig = plt.figure(figsize=(6, 6)) # Größe der Abbildung in Zoll
fig.subplots_adjust(left=0, right=1, bottom=0, top=1,
hspace=0.05, wspace=0.05)

# Plotten der Ziffern: jedes Bild ist 8x8 Pixel groß
for i in range(64):
ax = fig.add_subplot(8, 8, i + 1, xticks=[], yticks=[])
ax.imshow(ziffern.bilder[i], cmap=plt.cm.binary, interpolation='nearest')

# Beschriftung des Bildes mit dem Zielwert
ax.text(0, 7, str(ziffern.ziel[i]))

Abbildung 44-10. Darstellung der Zifferndaten

460 | Kapitel 44: Vertiefung: Entscheidungsbäume und Zufallsforsten

Wir können die Ziffern mit Hilfe eines Zufallsforsts wie folgt klassifizieren:

In [14]: from sklearn.model_selection import train_test_split

Xtrain, Xtest, ytrain, ytest = train_test_split(digits.data, digits.target,
random_state=0)
model = RandomForestClassifier(n_estimators=1000)
model.fit(Xtrain, ytrain)
ypred = model.predict(Xtest)

Schauen wir uns den Klassifizierungsbericht für diesen Klassifikator an:

In [15]: from sklearn import metrics
print(metrics.classification_report(ypred, ytest))
Out[15]: Genauigkeit Rückruf f1-Score Unterstützung

0 1.00 0.97 0.99 38
1 0.98 0.98 0.98 43
2 0.95 1.00 0.98 42
3 0.98 0.96 0.97 46
4 0.97 1.00 0.99 37
5 0.98 0.96 0.97 49
6 1.00 1.00 1.00 52
7 1.00 0.96 0.98 50
8 0.94 0.98 0.96 46
9 0.98 0.98 0.98 47

Genauigkeit 0,98 450
Makro-Durchschnittswert 0,98 0,98 0,98 450
gewichteter Durchschnitt 0,98 0,98 0,98 450

Stellen Sie sicherheitshalber auch die Konfusionsmatrix dar (siehe Abbildung 44-11).

In [16]: from sklearn.metrics import confusion_matrix
import seaborn as sns
mat = confusion_matrix(ytest, ypred)
sns.heatmap(mat.T, square= True , annot= True , fmt='d',
cbar= False , cmap='Blues')
plt.xlabel('true label')
plt.ylabel('vorhergesagte Beschriftung');

Wir stellen fest, dass ein einfacher, nicht abgestimmter Random Forest zu einer recht genauen Klassifizierung führt

der Zifferndaten.

Beispiel: Random Forest zur Klassifizierung von Ziffern | 461
Abbildung 44-11. Konfusionsmatrix für die Klassifizierung von Ziffern mit Random Forest

Zusammenfassung
Dieses Kapitel bot eine kurze Einführung in das Konzept der Ensemble-Schätzer und

insbesondere der Random Forest, ein Ensemble von randomisierten Entscheidungsbäumen. Zufällige
Wälder sind eine leistungsstarke Methode mit mehreren Vorteilen:

Sowohl das Training als auch die Vorhersage sind aufgrund der Einfachheit der zugrunde liegenden Entscheidungsbäume sehr schnell.
liegenden Entscheidungsbäume. Außerdem lassen sich beide Aufgaben ohne weiteres parallelisieren,
weil die einzelnen Bäume völlig unabhängige Einheiten sind.
Die Mehrfachbäume ermöglichen eine probabilistische Klassifizierung: Eine Mehrheitsabstimmung unter den
Schätzer ergibt eine Schätzung der Wahrscheinlichkeit (in Scikit-Learn mit der
predict_proba-Methode).
Das nichtparametrische Modell ist extrem flexibel und kann daher auch für Aufgaben verwendet werden
Aufgaben, die von anderen Schätzern nur unzureichend erfasst werden.
Ein Hauptnachteil von Random Forests ist, dass die Ergebnisse nicht leicht zu interpretieren sind.

Das heißt, wenn Sie Rückschlüsse auf die Bedeutung der Klassifizierungsmerkmale ziehen möchten.

tionsmodells sind Random Forests möglicherweise nicht die beste Wahl.

462 | Kapitel 44: Vertiefung: Entscheidungsbäume und Zufallsforsten

KAPITEL 45

Vertiefung: Hauptkomponentenanalyse
Bisher haben wir uns eingehend mit Schätzern des überwachten Lernens befasst, also mit solchen
Schätzer, die Beschriftungen auf der Grundlage beschrifteter Trainingsdaten vorhersagen. Hier beginnen wir mit der Betrachtung von

mehrere unüberwachte Schätzer, die interessante Aspekte der Daten hervorheben können

ohne Bezugnahme auf irgendwelche bekannten Etiketten.

In diesem Kapitel werden wir uns mit einem der vielleicht am weitesten verbreiteten unsuper-

Algorithmus, der Hauptkomponentenanalyse (PCA). Die PCA ist im Wesentlichen ein
Dimensionalitätsreduktionsalgorithmus, kann aber auch als Werkzeug zur Visualisierung von

tion, Rauschfilterung, Merkmalsextraktion und -technik und vieles mehr. Nach einer kurzen

konzeptionellen Diskussion des PCA-Algorithmus, werden wir einige Beispiele für
dieser weiteren Anwendungen.

Wir beginnen mit den Standardimporten:

In [1]: % matplotlib inline
import numpy as np
import matplotlib.pyplot as plt
plt.style.use('seaborn-whitegrid')

Einführung in die Principal Component Analysis
Die Hauptkomponentenanalyse ist eine schnelle und flexible unüberwachte Methode zur Dimen- sionalitätsreduktion in Daten.
sionalitätsreduktion in Daten, die wir in Kapitel 38 kurz kennengelernt haben. Ihr Verhalten ist am einfachsten

zu visualisieren, indem man einen zweidimensionalen Datensatz betrachtet. Betrachten Sie diese 200 Punkte (siehe
Abbildung 45-1).

In [2]: rng = np.random.RandomState(1)
X = np.dot(rng.rand(2, 2), rng.randn(2, 200)).T
plt.scatter(X[:, 0], X[:, 1])
plt.axis('equal');

463
Abbildung 45-1. Daten zur Demonstration der PCA

Das Auge erkennt, dass zwischen den Variablen x und y eine nahezu lineare Beziehung besteht.
ablese. Dies erinnert an die linearen Regressionsdaten, die wir in Kapitel 42 untersucht haben, aber

ist die Problemstellung hier etwas anders: Anstatt zu versuchen, die y
Werte aus den x-Werten vorherzusagen, versucht das Problem des unüberwachten Lernens, etwas zu lernen über

die Beziehung zwischen den x- und y-Werten.

Bei der Hauptkomponentenanalyse wird diese Beziehung quantifiziert, indem eine Liste der
Hauptachsen in den Daten und die Verwendung dieser Achsen zur Beschreibung des Datensatzes. Mit Scikit-

Learns PCA-Schätzer können wir dies wie folgt berechnen:

In [3]: from sklearn.decomposition import PCA
pca = PCA(n_Komponenten=2)
pca.fit(X)
Out[3]: PCA(n_components=2)

Der Fit lernt einige Größen aus den Daten, vor allem die Komponenten und
erklärte Varianz:

In [4]: print(pca.components_)
Out[4]: [[-0.94446029 -0.32862557]
[-0.32862557 0.94446029]]

In [5]: print(pca.explained_variance_)
Out[5]: [0.7625315 0.0184779]

Um zu sehen, was diese Zahlen bedeuten, stellen wir sie als Vektoren über den Eingabedaten dar,

die Komponenten zur Definition der Richtung des Vektors und die erklärte Varianz
um die quadrierte Länge des Vektors zu definieren (siehe Abbildung 45-2).

464 | Kapitel 45: Vertiefung: Hauptkomponentenanalyse

In [6]: def draw_vector(v0, v1, ax= None ):
ax = ax oder plt.gca()
arrowprops=dict(arrowstyle='->', linewidth=2,
shrinkA=0, shrinkB=0)
ax.annotate('', v1, v0, arrowprops=arrowprops)

# Daten darstellen
plt.scatter(X[:, 0], X[:, 1], alpha=0.2)
for length, vector in zip(pca.explained_variance_, pca.components_):
v = vector * 3 * np.sqrt(length)
draw_vector(pca.mean_, pca.mean_ + v)
plt.axis('equal');

Abbildung 45-2. Visualisierung der Hauptachsen in den Daten

Diese Vektoren stellen die Hauptachsen der Daten dar, und die Länge der einzelnen Vektoren ist

ein Hinweis darauf, wie "wichtig" diese Achse für die Beschreibung der Verteilung der Daten ist
-Genauer gesagt ist es ein Maß für die Varianz der Daten, wenn sie auf diese Achse projiziert werden

Achse. Die Projektion jedes Datenpunktes auf die Hauptachsen sind die Hauptkomponenten der Daten.
Komponenten der Daten.

Wenn wir diese Hauptkomponenten neben den ursprünglichen Daten aufzeichnen, erhalten wir die folgenden Diagramme

in Abbildung 45-3.

Einführung in die Hauptkomponentenanalyse | 465
1 Der Code zur Erstellung dieser Abbildung ist im Online-Anhang zu finden.
Abbildung 45-3. Transformierte Hauptachsen in den Daten^1

Diese Transformation von Datenachsen zu Hauptachsen ist eine affine Transformation,

was bedeutet, dass es sich aus einer Translation, einer Rotation und einer einheitlichen Skalierung zusammensetzt.

Dieser Algorithmus zur Ermittlung der Hauptkomponenten mag zwar wie eine mathematische

Es stellt sich heraus, dass es sehr weitreichende Anwendungen in der Welt der Maschine hat.

Lernen und Datenexploration.

PCA als Dimensionalitätsreduktion
Die Verwendung der PCA zur Dimensionalitätsreduzierung beinhaltet die Eliminierung einer oder mehrerer der

kleinsten Hauptkomponenten, was zu einer niedrigdimensionalen Projektion der
Daten führt, bei der die maximale Datenvarianz erhalten bleibt.

Hier ist ein Beispiel für die Verwendung von PCA als Dimensionalitätsreduktionstransformation:

In [7]: pca = PCA(n_Komponenten=1)
pca.fit(X)
X_pca = pca.transform(X)
print("ursprüngliche Form:", X.shape)
print("transformierte Form:", X_pca.shape)
Out[7]: ursprüngliche Form: (200, 2)
transformierte Form: (200, 1)

Die transformierten Daten wurden auf eine einzige Dimension reduziert. Zum Verständnis der
Wirkung dieser Dimensionalitätsreduktion zu verstehen, können wir die inverse Transformation dieser

reduzierten Daten und stellen sie zusammen mit den Originaldaten dar (siehe Abbildung 45-4).

In [8]: X_neu = pca.inverse_transform(X_pca)
plt.scatter(X[:, 0], X[:, 1], alpha=0.2)
plt.scatter(X_Neu[:, 0], X_Neu[:, 1], alpha=0,8)
plt.axis('equal');

466 | Kapitel 45: Vertiefung: Hauptkomponentenanalyse

Abbildung 45-4. Visualisierung der PCA als Dimensionalitätsreduktion

Die hellen Punkte sind die Originaldaten, während die dunklen Punkte die projizierte Version darstellen.
Dies macht deutlich, was eine PCA-Dimensionalitätsreduktion bedeutet: Die Informationen entlang

die am wenigsten wichtige(n) Hauptachse(n) wird/werden entfernt, so dass nur die Komponente(n)
der Daten mit der höchsten Varianz übrig. Der Anteil der herausgeschnittenen Varianz (propor-

der Streuung der Punkte um die in der vorangegangenen Abbildung gebildete Linie) ist

ein Maß dafür, wie viel "Information" bei dieser Reduzierung der Dimensionalität verworfen wird.
Dimensionalität.

Dieser Datensatz mit reduzierter Dimension ist in gewisser Weise "gut genug", um die wichtigsten
Beziehungen zwischen den Punkten zu kodieren: Trotz der Reduzierung der Anzahl der Datenmerkmale

Wenn die Daten um 50 % reduziert werden, bleiben die allgemeinen Beziehungen zwischen den Datenpunkten weitgehend erhalten.

PCA für die Visualisierung: Handgeschriebene Ziffern
Die Nützlichkeit der Dimensionalitätsreduktion ist vielleicht nicht ganz offensichtlich bei nur zwei

Dimensionen, aber es wird deutlich, wenn man hochdimensionale Daten betrachtet. Um dies zu sehen,

Werfen wir einen kurzen Blick auf die Anwendung der PCA auf den Datensatz der Ziffern, mit dem wir
in Kapitel 44 gearbeitet haben.

Wir beginnen mit dem Laden der Daten:

In [9]: from sklearn.datasets import load_digits
Ziffern = load_digits()
ziffern.daten.form
Out[9]: (1797, 64)

Der Datensatz der Ziffern besteht aus 8 × 8-Pixel-Bildern, d. h. sie sind 64

dimensional. Um ein Gefühl für die Beziehungen zwischen diesen Punkten zu bekommen, können wir

Einführung in die Hauptkomponentenanalyse | 467
können mit Hilfe der PCA auf eine überschaubare Anzahl von Dimensionen projiziert werden, z. B.

zwei:

In [10]: pca = PCA(2) # Projektion von 64 auf 2 Dimensionen
projiziert = pca.fit_transform(ziffern.daten)
print(ziffern.daten.form)
print(projizierte.Form)
Out[10]: (1797, 64)
(1797, 2)

Wir können nun die ersten beiden Hauptkomponenten jedes Punktes darstellen, um etwas über die
Daten zu erfahren, wie in Abbildung 45-5 dargestellt.

In [11]: plt.scatter(projected[:, 0], projected[:, 1],
c=digits.target, edgecolor='none', alpha=0.5,
cmap=plt.cm.get_cmap('rainbow', 10))
plt.xlabel('Komponente 1')
plt.ylabel('Komponente 2')
plt.colorbar();

Abbildung 45-5. PCA angewandt auf die Daten handgeschriebener Ziffern

Erinnern Sie sich, was diese Komponenten bedeuten: Die vollständigen Daten sind eine 64-dimensionale Punktwolke,
und diese Punkte sind die Projektion jedes Datenpunktes entlang der Richtungen mit den

größte Varianz. Im Wesentlichen haben wir die optimale Streckung und Drehung im 64-
dimensionalen Raum gefunden, die es uns ermöglicht, das Layout der Daten in zwei Dimensionen zu sehen, und

Wir haben dies auf unkontrollierte Weise getan, d. h. ohne Bezugnahme auf die

Etiketten.

468 | Kapitel 45: Vertiefung: Hauptkomponentenanalyse

2 Der Code zur Erstellung dieser Abbildung ist im Online-Anhang zu finden.
Was bedeuten die Komponenten?
Wir können hier noch ein wenig weiter gehen und uns fragen, was die reduzierten Dimensionen bedeuten.

Diese Bedeutung kann in Form von Kombinationen von Basisvektoren verstanden werden. Für
wird beispielsweise jedes Bild im Trainingssatz durch eine Sammlung von 64 Pixelwerten definiert,

den wir den Vektor x nennen:

x= x 1 ,x 2 ,x 3 ⋯x 64
Eine Möglichkeit, dies zu verstehen, ist eine Pixelbasis. Das heißt, zur Konstruktion der

Bildes multiplizieren wir jedes Element des Vektors mit dem Pixel, das es beschreibt, und addieren dann

die Ergebnisse zusammen, um das Bild zu erstellen:

imagex =x 1 - Pixel 1+x 2 - Pixel 2+x 3 - Pixel 3⋯x 64 - Pixel 64
Eine Möglichkeit, die Dimensionalität dieser Daten zu reduzieren, besteht darin, alle
bis auf einige wenige dieser Basisvektoren. Wenn wir zum Beispiel nur die ersten acht Pixel verwenden, ergibt sich

eine achtdimensionale Projektion der Daten erhalten (Abbildung 45-6). Sie ist jedoch nicht sehr

Spiegelung des gesamten Bildes: Wir haben fast 90 % der Pixel weggeworfen!

Abbildung 45-6. Eine naive Dimensionalitätsreduktion durch Verwerfen von Pixeln^2

Die obere Reihe der Tafeln zeigt die einzelnen Pixel, die untere Reihe zeigt den
kumulativen Beitrag dieser Pixel zur Konstruktion des Bildes. Nur verwenden

acht der Komponenten auf Pixelbasis können wir nur einen kleinen Teil der 64-

Pixel-Bild. Würden wir diese Sequenz fortsetzen und alle 64 Pixel verwenden, würden wir
das Originalbild wiederherstellen.

Die pixelbasierte Darstellung ist jedoch nicht die einzige Grundlage, die wir wählen können. Wir können auch andere
Basisfunktionen verwenden, die jeweils einen vordefinierten Beitrag von jedem Pixel enthalten,

und schreiben Sie etwas wie:

imagex = Mittelwert +x 1 - Basis 1+x 2 - Basis 2+x 3 - Basis 3⋯
Einführung in die Principal Component Analysis | 469
3 Der Code zur Erstellung dieser Abbildung ist im Online-Anhang zu finden.
Man kann sich die PCA als einen Prozess der Wahl optimaler Basisfunktionen vorstellen, so dass

Die Addition der ersten paar Elemente reicht aus, um den Großteil des Datensatzes angemessen zu rekonstruieren
der Elemente des Datensatzes zu rekonstruieren. Die Hauptkomponenten, die als die niedrigsten

dimensionalen Darstellung unserer Daten, sind einfach die Koeffizienten, die jedes

der Elemente in dieser Reihe. Abbildung 45-7 zeigt eine ähnliche Darstellung der Rekonstruktion
derselben Ziffer unter Verwendung des Mittelwerts und der ersten acht PCA-Basisfunktionen.

Abbildung 45-7. Eine ausgefeiltere Dimensionalitätsreduktion durch Verwerfen der
weniger wichtigen Hauptkomponenten (vgl. Abbildung 45-6)^3

Im Gegensatz zur Pixelbasis erlaubt uns die PCA-Basis, die wichtigsten Merkmale des
Eingangsbildes mit nur einem Mittelwert und acht Komponenten! Der Anteil jedes Pixels in

jede Komponente ist die Konsequenz der Orientierung des Vektors in unserer Zwei-

dimensionales Beispiel. In diesem Sinne liefert die PCA eine niedrigdimensionale
Darstellung der Daten: Sie entdeckt eine Reihe von Basisfunktionen, die effizienter sind

als die native Pixelbasis der Eingabedaten.

Auswahl der Anzahl der Komponenten
Ein entscheidender Faktor bei der Anwendung der PCA in der Praxis ist die Fähigkeit, die Anzahl der Komponenten zu bestimmen

erforderlich sind, um die Daten zu beschreiben. Dies lässt sich durch Betrachtung des kumulativen
erklärten Varianzverhältnis als Funktion der Anzahl der Komponenten (siehe Abbildung 45-8).

In [12]: pca = PCA().fit(Ziffern.Daten)
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlabel('Anzahl der Komponenten')
plt.ylabel('kumulierte erklärte Varianz');

Diese Kurve gibt an, wie viel der gesamten 64-dimensionalen Varianz in den ersten N Komponenten enthalten ist.
in den ersten N Komponenten enthalten ist. Zum Beispiel sehen wir, dass bei den Zifferndaten die erste

10 Komponenten enthalten etwa 75 % der Varianz, während Sie etwa 50 % benötigen.

Komponenten, die nahezu 100 % der Varianz beschreiben.

470 | Kapitel 45: Vertiefung: Hauptkomponentenanalyse

Abbildung 45-8. Die kumulative erklärte Varianz, die misst, wie gut die PCA den Inhalt der Daten vor
den Inhalt der Daten liefert

Dies zeigt uns, dass unsere zweidimensionale Projektion eine Menge Information verliert (als Messwert).

der erklärten Varianz) und dass wir etwa 20 Komponenten benötigen, um
90% der Varianz zu erhalten. Ein Blick auf diese Darstellung für einen hochdimensionalen Datensatz kann Ihnen helfen

den Grad der Redundanz in den Merkmalen zu verstehen.

PCA als Rauschfilterung
PCA kann auch als Filterverfahren für verrauschte Daten verwendet werden. Die Idee ist folgende: Alle Kom- ponenten
Komponenten mit einer Varianz, die viel größer ist als der Effekt des Rauschens, sollten relativ

unbeeinflusst vom Rauschen. Wenn Sie also die Daten nur mit der größten Teilmenge der Daten rekonstruieren

Hauptkomponenten, sollten Sie vorzugsweise das Signal behalten und das
das Rauschen entfernen.

Schauen wir uns an, wie das mit den Zifferndaten aussieht. Zuerst werden wir einige der rauschfreien Eingangs
rauschfreien Eingangssamples (Abbildung 45-9).

In [13]: def plot_digits(data):
fig, axes = plt.subplots(4, 10, figsize=(10, 4),
subplot_kw={'xticks':[], 'yticks':[]},
gridspec_kw=dict(hspace=0.1, wspace=0.1))
for i, ax in enumerate(axes.flat):
ax.imshow(data[i].reshape(8, 8),
cmap='binary', interpolation='nearest',
clim=(0, 16))
plot_digits(ziffern.daten)

PCA als Rauschfilterung | 471
Abbildung 45-9. Ziffern ohne Rauschen

Fügen wir nun etwas zufälliges Rauschen hinzu, um einen verrauschten Datensatz zu erzeugen, und stellen ihn erneut dar
(Abbildung 45-10).

In [14]: rng = np.random.default_rng(42)
rng.normal(10, 2)
Out[14]: 10.609434159508863

In [15]: rng = np.random.default_rng(42)
noisy = rng.normal(ziffern.daten, 4)
plot_digits(noisy)

Abbildung 45-10. Ziffern mit hinzugefügtem Gaußschen Zufallsrauschen

Die Visualisierung macht das Vorhandensein dieses Zufallsrauschens deutlich. Lassen Sie uns eine PCA trainieren

Modell auf die verrauschten Daten anzuwenden und zu verlangen, dass die Projektion 50 % der Varianz beibehält:

In [16]: pca = PCA(0.50).fit(noisy)
pca.n_components_
Out[16]: 12

472 | Kapitel 45: Vertiefung: Hauptkomponentenanalyse

Hier entfallen 50% der Varianz auf 12 Hauptkomponenten, von den ursprünglich 64

Eigenschaften. Nun berechnen wir diese Komponenten und verwenden dann die Umkehrung der Trans- form, um die gefilterten Ziffern zu rekonstruieren.
Form, um die gefilterten Ziffern zu rekonstruieren; Abbildung 45-11 zeigt das Ergebnis.

In [17]: Komponenten = pca.transform(noisy)
gefiltert = pca.inverse_transform(components)
plot_digits(gefiltert)

Abbildung 45-11. Mit PCA "entrauschte" Ziffern

Diese Eigenschaft der Signalerhaltung/Rauschfilterung macht die PCA zu einem sehr nützlichen Merkmal bei der Auswahl von Merkmalen.

z.B. einen Klassifikator nicht auf sehr hochdimensionalen Daten zu trainieren, sondern
Daten zu trainieren, könnten Sie den Klassifikator stattdessen auf den niedrigdimensionalen Hauptkom-

Komponenten-Darstellung, die automatisch dazu dient, das zufällige Rauschen in

die Eingänge.

Beispiel: Eigenflächen
Zuvor haben wir ein Beispiel für die Verwendung einer PCA-Projektion als Merkmalsselektor für
Gesichtserkennung mit einer Support Vector Machine (siehe Kapitel 43). Hier werden wir eine

zurückblicken und ein wenig mehr darüber erfahren, wie es dazu kam. Erinnern Sie sich, dass wir den
Labeled Faces in the Wild (LFW)-Datensatz, der über Scikit-Learn verfügbar ist:

In [18]: from sklearn.datasets import fetch_lfw_people
faces = fetch_lfw_people(min_faces_per_person=60)
print(faces.target_names)
print(faces.images.shape)
Out[18]: ['Ariel Sharon' 'Colin Powell' 'Donald Rumsfeld' 'George W Bush'
'Gerhard Schroeder' 'Hugo Chavez' 'Junichiro Koizumi' 'Tony Blair']
(1348, 62, 47)

Werfen wir einen Blick auf die Hauptachsen, die diesen Datensatz umfassen. Da es sich um einen großen

Datensatzes werden wir den "zufälligen" Eigensolver im PCA-Schätzer verwenden: Er verwendet eine zufalls-
Methode, um die ersten N Hauptkomponenten schneller zu approximieren als mit der

Beispiel: Eigenflächen | 473
Standardansatzes, auf Kosten einer gewissen Genauigkeit. Dieser Kompromiss kann nützlich sein für

hochdimensionale Daten (hier eine Dimensionalität von fast 3.000). Wir werden einen Blick auf
die ersten 150 Komponenten:

In [19]: pca = PCA(150, svd_solver='randomized', random_state=42)
pca.fit(faces.data)
Out[19]: PCA(n_components=150, random_state=42, svd_solver='randomized')

In diesem Fall kann es interessant sein, die Bilder zu visualisieren, die mit den ersten sieben
eral Hauptkomponenten zugeordnet sind (diese Komponenten werden technisch als Eigenvektoren bezeichnet,

Daher werden diese Arten von Bildern oft als Eigenflächen bezeichnet; wie Sie in Abbildung 45-12 sehen können,
sind sie so gruselig, wie sie klingen):

In [20]: fig, axes = plt.subplots(3, 8, figsize=(9, 4),
subplot_kw={'xticks':[], 'yticks':[]},
gridspec_kw=dict(hspace=0.1, wspace=0.1))
for i, ax in enumerate(axes.flat):
ax.imshow(pca.components_[i].reshape(62, 47), cmap='bone')

Abbildung 45-12. Eine Visualisierung der aus dem LFW-Datensatz gelernten Eigenflächen

Die Ergebnisse sind sehr interessant und geben uns Aufschluss darüber, wie die Bilder variieren: für

Beispiel scheinen die ersten paar Eigenflächen (von links oben) mit dem

Winkel der Beleuchtung auf dem Gesicht, und spätere Hauptvektoren scheinen bestimmte
Merkmale wie Augen, Nasen und Lippen herauszufiltern. Werfen wir einen Blick auf die kumulative Varianz von

diese Komponenten, um zu sehen, wie viele der Dateninformationen die Projektion bewahrt.
erhält (siehe Abbildung 45-13).

In [21]: plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlabel('Anzahl der Komponenten')
plt.ylabel('kumulierte erklärte Varianz');

474 | Kapitel 45: Vertiefung: Hauptkomponentenanalyse

Abbildung 45-13. Kumulative erklärte Varianz für die LFW-Daten

Die 150 Komponenten, die wir ausgewählt haben, machen etwas mehr als 90 % der Varianz aus. Das

würde uns zu der Annahme verleiten, dass wir mit diesen 150 Komponenten den Großteil der

die wesentlichen Merkmale der Daten. Um dies zu verdeutlichen, können wir vergleichen
die Eingabebilder mit den aus diesen 150 Komponenten rekonstruierten Bildern (siehe

Abbildung 45-14).

In [22]: # Berechnung der Komponenten und projizierten Gesichter
pca = pca.fit(gesichter.daten)
komponenten = pca.transform(gesichter.daten)
projiziert = pca.inverse_transform(komponenten)

In [23]: # Plotten Sie die Ergebnisse
fig, ax = plt.subplots(2, 10, figsize=(10, 2.5),
subplot_kw={'xticks':[], 'yticks':[]},
gridspec_kw=dict(hspace=0.1, wspace=0.1))
for i in range(10):
ax[0, i].imshow(faces.data[i].reshape(62, 47), cmap='binary_r')
ax[1, i].imshow(projiziert[i].reshape(62, 47), cmap='binary_r')

ax[0, 0].set_ylabel('full-dim \n input')
ax[1, 0].set_ylabel('150-dim \n reconstruction');

Beispiel: Eigene Schnittstellen | 475
Abbildung 45-14. 150-dimensionale PCA-Rekonstruktion der LFW-Daten

Die obere Reihe zeigt die Eingabebilder, während die untere Reihe das Ergebnis der Rekonstruktion zeigt.

Konstruktion der Bilder aus nur 150 der ~3.000 ursprünglichen Merkmale. Diese Visualisierung
macht deutlich, warum die in Kapitel 43 verwendete PCA-Merkmalsauswahl so erfolgreich war:

Obwohl die Dimensionalität der Daten um fast den Faktor 20 reduziert wird, ist die Projektion

Bilder genug Informationen enthalten, dass wir die Personen auf jedem Bild mit dem Auge erkennen können.
Individuen in jedem Bild erkennen können. Das bedeutet, dass unser Klassifizierungsalgorithmus nur trainiert werden muss

auf 150-dimensionalen Daten anstatt auf 3.000-dimensionalen Daten, was je nach
je nach gewähltem Algorithmus zu einer wesentlich effizienteren Klassifizierung führen kann.

Zusammenfassung
In diesem Kapitel haben wir die Verwendung der Hauptkomponentenanalyse für dimensional-

Rauschunterdrückung, Visualisierung hochdimensionaler Daten, Rauschfilterung und

Auswahl in hochdimensionalen Daten. Aufgrund ihrer Vielseitigkeit und Interpretierbarkeit,
hat sich die PCA in einer Vielzahl von Kontexten und Disziplinen als wirksam erwiesen.

Bei jedem hochdimensionalen Datensatz neige ich dazu, mit PCA zu beginnen, um die Beziehungen zwischen den Punkten zu visualisieren
Beziehungen zwischen den Punkten zu visualisieren (wie wir es mit den Zifferndaten getan haben), um die wichtigsten

Varianz in den Daten (wie bei den Eigenflächen) und zum Verständnis der intrinsischen

Dimensionalität (durch Auftragen des erklärten Varianzverhältnisses). Sicherlich ist die PCA nicht für jeden
für jeden hochdimensionalen Datensatz, aber sie bietet einen einfachen und effizienten Weg

um Einblick in hochdimensionale Daten zu erhalten.

Die größte Schwäche der PCA ist, dass sie von Ausreißern in den Daten stark beeinflusst wird. Für

Aus diesem Grund wurden mehrere robuste Varianten der PCA entwickelt, von denen viele

um iterativ Datenpunkte zu verwerfen, die durch die Anfangskomponenten schlecht beschrieben sind.

Scikit-Learn enthält eine Reihe interessanter Varianten der PCA in der sklearn

.decomposition-Submodul; ein Beispiel ist SparsePCA, das ein Regularisierungsmodul einführt.

zationsterm (siehe Kapitel 42), der dazu dient, die Sparsamkeit der Komponenten zu erzwingen.

In den folgenden Kapiteln werden wir uns mit anderen unüberwachten Lernmethoden befassen, die

auf einigen der Ideen der PKA aufbauen.

476 | Kapitel 45: Vertiefung: Hauptkomponentenanalyse

KAPITEL 46

Vertiefung: Vielfältiges Lernen
Im vorigen Kapitel haben wir gesehen, wie die PCA zur Dimensionalitätsreduktion eingesetzt werden kann,
die Anzahl der Merkmale eines Datensatzes zu reduzieren und dabei die wesentlichen Relationen zu erhalten.

zwischen den Punkten. Die PCA ist zwar flexibel, schnell und leicht zu interpretieren, aber sie bietet

nicht so gut, wenn es nichtlineare Beziehungen in den Daten gibt, einige
Beispiele dafür werden wir gleich sehen.

Um diesen Mangel zu beheben, können wir uns an Algorithmen des vielfältigen Lernens wenden - eine Klasse von
unüberwachten Schätzern, die versuchen, Datensätze als niedrigdimensionale Mannigfaltigkeiten zu beschreiben

eingebettet in hochdimensionale Räume. Wenn Sie an eine Mannigfaltigkeit denken, würde ich vorschlagen

ein Blatt Papier vorzustellen: Es ist ein zweidimensionales Objekt, das in unserer vertrauten
dreidimensionalen Welt.

In der Sprache des Manifold Learning kann man sich dieses Blatt als eine zweidimensionale
in den dreidimensionalen Raum eingebettet. Drehen, neu ausrichten oder strecken

das Stück Papier im dreidimensionalen Raum ändert seine flache Geometrie nicht: solche

Operationen sind mit linearen Einbettungen vergleichbar. Wenn man das Papier knickt, rollt oder zerknüllt, ist es
immer noch eine zweidimensionale Mannigfaltigkeit, aber die Einbettung in die dreidimensionale

Raum ist nicht mehr linear. Vielfältige Lernalgorithmen versuchen, etwas über die grundlegende
mentale zweidimensionale Beschaffenheit des Papiers zu erfahren, auch wenn es so verformt wird, dass es den dreidimensionalen Raum ausfüllt.

dimensionalen Raum.

Hier werden wir eine Reihe vielfältiger Methoden untersuchen, wobei wir uns besonders intensiv mit einer Untergruppe
dieser Techniken eingehen: multidimensionale Skalierung (MDS), lokal lineare Einbettung

(LLE) und isometrisches Mapping (Isomap).

477
Wir beginnen mit den Standardimporten:

In [1]: % matplotlib inline
import matplotlib.pyplot as plt
plt.style.use('seaborn-whitegrid')
import numpy as np

Vielfältiges Lernen: "HALLO"
Um diese Konzepte zu verdeutlichen, beginnen wir mit der Erzeugung einiger zweidimensionaler

Daten, die wir zur Definition eines Verteilers verwenden können. Hier ist eine Funktion, die Daten in
Form des Wortes "HALLO" erzeugt:

In [2]: def make_hello(N=1000, rseed=42):
# Erstellen eines Plots mit "HELLO"-Text; Speichern als PNG
fig, ax = plt.subplots(figsize=(4, 1))
fig.subplots_adjust(left=0, right=1, bottom=0, top=1)
ax.axis('off')
ax.text(0.5, 0.4, 'HELLO', va='center', ha='center',
weight='bold', size=85)
fig.savefig('hallo.png')
plt.close(fig)

# Öffnen Sie dieses PNG und zeichnen Sie zufällige Punkte daraus
from matplotlib.image import imread
Daten = imread('hallo.png')[::-1, :, 0].T
rng = np.random.RandomState(rseed)
X = rng.rand(4 * N, 2)
i, j = (X * data.shape).astype(int).T
Maske = (Daten[i, j] < 1)
X = X[Maske]
X[:, 0] *= (daten.form[0] / daten.form[1])
X = X[:N]
return X[np.argsort(X[:, 0])]

Rufen wir die Funktion auf und visualisieren wir die resultierenden Daten (Abbildung 46-1).

In [3]: X = make_hello(1000)
colorize = dict(c=X[:, 0], cmap=plt.cm.get_cmap('rainbow', 5))
plt.scatter(X[:, 0], X[:, 1], **colorize)
plt.axis('equal');

Die Ausgabe ist zweidimensional und besteht aus Punkten, die in Form des Wortes

"HALLO". Anhand dieses Datenformulars können wir visuell erkennen, was diese Algorithmen tun.

478 | Kapitel 46: Vertiefung: Vielfältiges Lernen

Abbildung 46-1. Daten zur Verwendung mit vielfältigem Lernen

Multidimensionale Skalierung
Wenn wir uns die Daten ansehen, können wir feststellen, dass die Wahl der x- und y-Werte der

Datensatzes sind nicht die grundlegendste Beschreibung der Daten: Wir können die Daten skalieren, schrumpfen oder
die Daten drehen, und das "HELLO" wird immer noch sichtbar sein. Wenn wir zum Beispiel eine Rota-

Wenn Sie die Daten mit Hilfe einer Matrix drehen, ändern sich die x- und y-Werte, aber die Daten bleiben grundlegend gleich.
aber die Daten sind im Grunde immer noch dieselben (siehe Abbildung 46-2).

In [4]: def rotate(X, Winkel):
theta = np.deg2rad(angle)
R = [[np.cos(theta), np.sin(theta)],
[-np.sin(theta), np.cos(theta)]]
return np.dot(X, R)

X2 = rotate(X, 20) + 5
plt.scatter(X2[:, 0], X2[:, 1], **colorize)
plt.axis('equal');

Mehrdimensionale Skalierung | 479
Abbildung 46-2. Gedrehter Datensatz

Dies bestätigt, dass die x- und y-Werte nicht unbedingt grundlegend für die Beziehungen in den Daten sind.
in den Daten sind. Wesentlich ist in diesem Fall der Abstand zwischen den einzelnen Punkten

innerhalb des Datensatzes. Eine gängige Methode, dies darzustellen, ist die Verwendung einer Abstandsmatrix: für N
Punkte konstruieren wir ein N×N-Array, bei dem der Eintrag i,j den Abstand

zwischen Punkt i und Punkt j. Verwenden wir die effiziente Funktion pairwise_distances von Scikit-Learn
Funktion, um dies für unsere Originaldaten zu tun:

In [5]: from sklearn.metrics import pairwise_distances
D = pairwise_distances(X)
D.Form
Out[5]: (1000, 1000)

Wie versprochen, erhalten wir für unsere N=1.000 Punkte eine 1.000 × 1.000 Matrix, die wie folgt dargestellt werden kann
wie hier dargestellt werden kann (siehe Abbildung 46-3).

In [6]: plt.imshow(D, zorder=2, cmap='viridis', interpolation='nearest')
plt.colorbar();

480 | Kapitel 46: Vertiefung: Vielfältiges Lernen

Abbildung 46-3. Visualisierung der paarweisen Abstände zwischen Punkten

Wenn wir auf ähnliche Weise eine Abstandsmatrix für unsere gedrehten und übersetzten Daten erstellen, sehen wir

dass es dasselbe ist:

In [7]: D2 = Paarweise_Abstände(X2)
np.allclose(D, D2)
Out[7]: True

Mit dieser Abstandsmatrix erhalten wir eine Darstellung unserer Daten, die gegenüber Drehungen unveränderlich ist

und Übersetzungen, aber die Visualisierung der Matrix in Abbildung 46-3 ist nicht ganz intuitiv.
einleuchtend. In der dort gezeigten Darstellung haben wir jegliches sichtbare Zeichen für das Interesse verloren.

Struktur in den Daten: das "HELLO", das wir zuvor gesehen haben.

Die Berechnung dieser Abstandsmatrix aus den (x, y)-Koordinaten ist zwar einfach.
einfach ist, ist die Rücktransformation der Entfernungen in x- und y-Koordinaten ziemlich schwierig.

Genau darauf zielt der Algorithmus für die mehrdimensionale Skalierung ab: Aus einer Distanzmatrix zwischen
Distanzmatrix zwischen Punkten wird eine D-dimensionale Koordinatendarstellung gewonnen

der Daten. Schauen wir uns an, wie das für unsere Distanzmatrix funktioniert, indem wir die vorberechnete
Unähnlichkeit, um anzugeben, dass wir eine Abstandsmatrix übergeben (Abbildung 46-4).

In [8]: from sklearn.manifold import MDS
model = MDS(n_components=2, dissimilarity='precomputed', random_state=1701)
out = model.fit_transform(D)
plt.scatter(out[:, 0], out[:, 1], **colorize)
plt.axis('equal');

Mehrdimensionale Skalierung | 481
Abbildung 46-4. Eine MDS-Einbettung, berechnet aus den paarweisen Abständen

Der MDS-Algorithmus stellt eine der möglichen zweidimensionalen Koordinatenrepräsentationen unserer Daten wieder her.
Koordinatendarstellungen unserer Daten, wobei nur die N×N-Abstandsmatrix verwendet wird, die die Beziehung zwischen den Daten beschreibt.

zwischen den Datenpunkten.

MDS als mannigfaltiges Lernen
Die Nützlichkeit dieses Ansatzes wird noch deutlicher, wenn wir die Tatsache berücksichtigen, dass dis-

tanzmatrizen können aus Daten in jeder beliebigen Dimension berechnet werden. So können wir zum Beispiel, anstatt
die Daten in der zweidimensionalen Ebene zu drehen, können wir sie in drei Dimensionen projizieren.

Dimensionen unter Verwendung der folgenden Funktion (im Wesentlichen eine dreidimensionale Verallgemeinerung)

der zuvor verwendeten Rotationsmatrix):

In [9]: def random_projection(X, dimension=3, rseed=42):
assert dimension >= X.shape[1]
rng = np.random.RandomState(rseed)
C = rng.randn(dimension, dimension)
e, V = np.linalg.eigh(np.dot(C, C.T))
return np.dot(X, V[:X.shape[1]])

X3 = random_projection(X, 3)
X3.Form
Out[9]: (1000, 3)

Veranschaulichen wir uns diese Punkte, um zu sehen, womit wir es zu tun haben (Abbildung 46-5).

In [10]: from mpl_toolkits import mplot3d
ax = plt.axes(projection='3d')
ax.scatter3D(X3[:, 0], X3[:, 1], X3[:, 2],
**colorize);

482 | Kapitel 46: Vertiefung: Vielfältiges Lernen

Abbildung 46-5. Lineare Einbettung von Daten in drei Dimensionen

Wir können nun den MDS-Schätzer auffordern, diese dreidimensionalen Daten einzugeben, die
Distanzmatrix zu berechnen und dann die optimale zweidimensionale Einbettung für diese

Distanzmatrix. Das Ergebnis ist eine Darstellung der ursprünglichen Daten, wie in
Abbildung 46-6.

In [11]: model = MDS(n_components=2, random_state=1701)
out3 = model.fit_transform(X3)
plt.scatter(out3[:, 0], out3[:, 1], **colorize)
plt.axis('equal');

Dies ist im Wesentlichen das Ziel eines Schätzers mit vielfältigem Lernen: Bei hochdimensionalen
eingebetteten Daten sucht er nach einer niedrigdimensionalen Darstellung der Daten, die die

bestimmte Beziehungen innerhalb der Daten. Im Fall von MDS ist die erhaltene Menge die

Abstand zwischen jedem Punktpaar.

Mehrdimensionale Skalierung | 483
Abbildung 46-6. Die MDS-Einbettung der dreidimensionalen Daten stellt die Eingabe bis zu

eine Drehung und Spiegelung

Nichtlineare Einbettungen: Wo MDS versagt
Bisher haben wir uns mit linearen Einbettungen beschäftigt, die im Wesentlichen aus

von Drehungen, Übersetzungen und Skalierungen von Daten in höherdimensionale Räume. Wo
MDS versagt, wenn die Einbettung nichtlinear ist, d. h. wenn sie über

diese einfache Reihe von Operationen. Betrachten Sie die folgende Einbettung, die die

Eingabe und formt sie dreidimensional zu einem "S":

In [12]: def make_hello_s_curve(X):
t = (X[:, 0] - 2) * 0.75 * np.pi
x = np.sin(t)
y = X[:, 1]
z = np.sign(t) * (np.cos(t) - 1)
return np.vstack((x, y, z)).T

XS = make_hello_s_curve(X)

Auch hier handelt es sich um dreidimensionale Daten, aber wie wir in Abbildung 46-7 sehen können, ist die Einbettung
viel komplizierter ist.

In [13]: from mpl_toolkits import mplot3d
ax = plt.axes(projection='3d')
ax.scatter3D(XS[:, 0], XS[:, 1], XS[:, 2],
**colorize);

484 | Kapitel 46: Vertiefung: Vielfältiges Lernen

Abbildung 46-7. Nichtlinear in drei Dimensionen eingebettete Daten

Die grundlegenden Beziehungen zwischen den Datenpunkten sind nach wie vor vorhanden, aber dieses Mal

die Daten wurden auf nichtlineare Weise transformiert: Sie wurden in die Form eines
Form eines "S".

Wenn wir einen einfachen MDS-Algorithmus an diesen Daten ausprobieren, ist er nicht in der Lage, diese Nonline zu "entpacken".

Einbettung, und wir verlieren den Überblick über die grundlegenden Beziehungen in der eingebetteten
Mannigfaltigkeit (siehe Abbildung 46-8).

In [14]: from sklearn.manifold import MDS
model = MDS(n_components=2, random_state=2)
outS = model.fit_transform(XS)
plt.scatter(outS[:, 0], outS[:, 1], **colorize)
plt.axis('equal');

Mehrdimensionale Skalierung | 485
Abbildung 46-8. Der MDS-Algorithmus, angewandt auf die nichtlinearen Daten; es gelingt ihm nicht, die

Grundstruktur

Die beste zweidimensionale lineare Einbettung wickelt die S-Kurve nicht ab, sondern

verwirft stattdessen die ursprüngliche y-Achse.

Nichtlineare Mannigfaltigkeiten: Lokale lineare Einbettung
Wie können wir hier weiterkommen? Wenn wir zurückgehen, können wir sehen, dass die Quelle des
Problem darin liegt, dass MDS versucht, die Abstände zwischen weit entfernten Punkten zu bewahren, wenn die

Strukturierung der Einbettung. Was aber, wenn wir stattdessen den Algorithmus so ändern, dass er

nur die Abstände zwischen benachbarten Punkten beibehält? Die resultierende Einbettung wäre
näher an dem, was wir wollen.

Visuell kann man sich das wie in Abbildung 46-9 dargestellt vorstellen.

Hier stellt jede schwache Linie eine Entfernung dar, die bei der Einbettung erhalten bleiben soll.

Auf der linken Seite ist eine Darstellung des von MDS verwendeten Modells zu sehen: Es versucht, die Dis-

Abstände zwischen den einzelnen Punktpaaren des Datensatzes. Auf der rechten Seite ist eine Darstellung des
des Modells, das von einem Algorithmus für vielfältiges Lernen verwendet wird, der als lokal lineare Einbettung bezeichnet wird:

statt alle Abstände beizubehalten, versucht es stattdessen, nur die Abstände
zwischen benachbarten Punkten zu erhalten (in diesem Fall die nächsten 100 Nachbarn jedes Punktes).

486 | Kapitel 46: Vertiefung: Vielfältiges Lernen

1 Der Code zur Erstellung dieser Abbildung ist im Online-Anhang zu finden.
Wenn wir über das linke Feld nachdenken, können wir sehen, warum MDS fehlschlägt: Es gibt keine Möglichkeit, diese

Daten, wobei die Länge jeder zwischen den beiden Punkten gezogenen Linie adäquat erhalten
Punkte. Bei der rechten Tafel sieht es dagegen etwas optimistischer aus. Wir

könnte man sich vorstellen, die Daten so abzurollen, dass die Länge der Zeilen in etwa gleich bleibt.

die in etwa gleich sind. Genau das tut LLE, indem es eine globale Optimierung einer
Kostenfunktion, die diese Logik widerspiegelt.

Abbildung 46-9. Darstellung der Verknüpfungen zwischen Punkten innerhalb von MDS und LLE^1

LLE gibt es in einer Reihe von Varianten; hier werden wir den modifizierten LLE-Algorithmus verwenden, um
um die eingebettete zweidimensionale Mannigfaltigkeit wiederherzustellen. Im Allgemeinen ist der modifizierte LLE-

als andere Varianten des Algorithmus, wenn es darum geht, wohldefinierte Mannigfaltigkeiten mit sehr
mit sehr geringer Verzerrung (siehe Abbildung 46-10).

In [15]: from sklearn.manifold import LocallyLinearEmbedding
model = LocallyLinearEmbedding(
n_Nachbarn=100, n_Komponenten=2,
method='modified', eigen_solver='dense')
out = model.fit_transform(XS)

fig, ax = plt.subplots()
ax.scatter(out[:, 0], out[:, 1], **colorize)
ax.set_ylim(0.15, -0.15);

Das Ergebnis bleibt im Vergleich zu unserem ursprünglichen Verteiler etwas verzerrt, erfasst aber
die wesentlichen Beziehungen in den Daten!

Nichtlineare Mannigfaltigkeiten: Lokale lineare Einbettung | 487
Abbildung 46-10. Die lokal lineare Einbettung kann die zugrundeliegenden Daten aus einer nichtlinearen

nahezu eingebettete Eingabe

Einige Überlegungen zu vielfältigen Methoden
So überzeugend diese Beispiele auch sein mögen, in der Praxis sind vielfältige Lerntechniken
so heikel, dass sie nur selten für etwas anderes als einfache Qualitä- tsanalysen verwendet werden.

tive Visualisierung hochdimensionaler Daten.

Im Folgenden werden einige der besonderen Herausforderungen des vielfältigen Lernens genannt, die alle

stehen in schlechtem Kontrast zur PCA:

Beim vielfältigen Lernen gibt es keinen guten Rahmen für den Umgang mit fehlenden Daten. In
Gegensatz dazu gibt es unkomplizierte iterative Ansätze für den Umgang mit fehlenden
Daten in der PCA.
Beim Lernen von Mannigfaltigkeiten kann das Vorhandensein von Rauschen in den Daten einen "Kurzschluss" in der
und die Einbettung drastisch verändern. Im Gegensatz dazu filtert die PCA natürlich
Rauschen aus den wichtigsten Komponenten heraus.
Das Ergebnis der vielfältigen Einbettung hängt im Allgemeinen stark von der Anzahl der
Nachbarn ab, und es gibt im Allgemeinen keine solide quantitative Methode zur Auswahl einer
optimale Anzahl von Nachbarn zu wählen. Im Gegensatz dazu ist bei der PCA eine solche Wahl nicht erforderlich.
Beim Manifold Learning ist die global optimale Anzahl der Ausgabedimensionen schwer zu bestimmen.
kult zu bestimmen. Im Gegensatz dazu lässt sich bei der PCA die Anzahl der Ausgabedimensionen
die Anzahl der Ausgabedimensionen auf der Grundlage der erklärten Varianz.
Beim Manifold Learning ist die Bedeutung der eingebetteten Dimensionen nicht immer
klar. Bei der PCA haben die Hauptkomponenten eine sehr klare Bedeutung.
488 | Kapitel 46: Vertiefung: Vielfältiges Lernen

Beim vielfältigen Lernen skaliert der Rechenaufwand der vielfältigen Methoden mit
ON^2 oder ON^3. Für PCA gibt es randomisierte Ansätze, die im Allgemeinen
viel schneller sind (siehe jedoch das Megaman-Paket für einige skalierbare Implementierungen
Implementierungen des mannigfaltigen Lernens).
Nach all diesen Überlegungen ist der einzige klare Vorteil von Methoden des Manifold Learning gegenüber

PCA ist ihre Fähigkeit, nichtlineare Beziehungen in den Daten zu erhalten; aus diesem Grund neige ich
Deshalb neige ich dazu, Daten erst dann mit vielfältigen Methoden zu untersuchen, wenn ich sie zuvor mit PCA untersucht habe.

Scikit-Learn implementiert mehrere gängige Varianten des mannigfaltigen Lernens über LLE hinaus

und Isomap (die wir in einigen der vorherigen Kapitel verwendet haben und im nächsten Abschnitt betrachten werden)
nächsten Abschnitt): die Scikit-Learn Dokumentation hat eine schöne Diskussion und einen Vergleich

von ihnen. Auf der Grundlage meiner eigenen Erfahrungen würde ich die folgenden Empfehlungen aussprechen:

Für Spielzeugprobleme wie die S-Kurve, die wir zuvor gesehen haben, schneiden LLE und seine Varianten (insbesondere modifiziertes LLE) sehr gut ab.
insbesondere modifiziertes LLE) sehr gut ab. Dies ist in sklearn.mani
fold.LocallyLinearEmbedding implementiert.
Für hochdimensionale Daten aus realen Quellen liefert LLE oft schlechte
Ergebnisse, und Isomap scheint im Allgemeinen zu aussagekräftigeren Einbettungen zu führen.
Dies ist in sklearn.manifold.Isomap implementiert.
Für Daten, die stark geclustert sind, scheint die t-verteilte stochastische Nachbarschaftseinbettung (t-
SNE) scheint sehr gut zu funktionieren, obwohl es im Vergleich zu anderen Methoden sehr langsam sein kann.
Methoden. Diese Methode ist in sklearn.manifold.TSNE implementiert.
Wenn Sie daran interessiert sind, ein Gefühl dafür zu bekommen, wie diese Methoden funktionieren, würde ich vorschlagen, jede der folgenden Methoden auszuführen

die Methoden an den Daten in diesem Abschnitt.

Beispiel: Isomap auf Gesichtern
Ein Ort, an dem vielfältiges Lernen häufig eingesetzt wird, ist das Verständnis der Beziehung
zwischen hochdimensionalen Datenpunkten. Ein häufiger Fall von hochdimensionalen Daten ist

Bilder: Ein Satz von Bildern mit jeweils 1.000 Pixeln kann beispielsweise als eine Spalte betrachtet werden.

von Punkten in 1.000 Dimensionen, wobei die Helligkeit eines jeden Pixels in jedem Bild
die Koordinate in dieser Dimension definiert.

Zur Veranschaulichung wollen wir Isomap auf einige Daten aus dem Datensatz "Labeled Faces in the Wild" anwenden.
anwenden, die wir bereits in den Kapiteln 43 und 45 gesehen haben. Die Ausführung dieses Befehls bewirkt

laden Sie den Datensatz herunter und speichern Sie ihn zur späteren Verwendung in Ihrem Heimatverzeichnis:

In [16]: from sklearn.datasets import fetch_lfw_people
faces = fetch_lfw_people(min_faces_per_person=30)
faces.data.shape
Out[16]: (2370, 2914)

Beispiel: Isomap auf Gesichtern | 489
Wir haben 2.370 Bilder mit jeweils 2.914 Pixeln. Mit anderen Worten, die Bilder können sein

als Datenpunkte in einem 2.914-dimensionalen Raum gedacht!

Zeigen wir einige dieser Bilder an, um uns daran zu erinnern, womit wir arbeiten (siehe

Abbildung 46-11).

In [17]: fig, ax = plt.subplots(4, 8, subplot_kw=dict(xticks=[], yticks=[]))
for i, axi in enumerate(ax.flat):
axi.imshow(faces.images[i], cmap='gray')

Abbildung 46-11. Beispiele für die Eingabeflächen

Als wir in Kapitel 45 auf diese Daten stießen, war unser Ziel im Wesentlichen die Komprimierung:

die Komponenten zu verwenden, um die Eingaben aus der niederdimensionalen Repräsentation zu rekonstruieren.
Darstellung zu rekonstruieren.

Die PCA ist so vielseitig, dass wir sie auch in diesem Zusammenhang einsetzen können, wo wir

eine niedrigdimensionale Einbettung der 2.914-dimensionalen Daten zu erstellen, um die grundlegenden
mentalen Beziehungen zwischen den Bildern zu lernen. Betrachten wir noch einmal die erklärte Varianz

Verhältnis, das uns eine Vorstellung davon vermittelt, wie viele lineare Merkmale zur Beschreibung der Daten erforderlich sind
die Daten zu beschreiben (siehe Abbildung 46-12).

In [18]: from sklearn.decomposition import PCA
model = PCA(100, svd_solver='randomized').fit(faces.data)
plt.plot(np.cumsum(model.explained_variance_ratio_))
plt.xlabel('n Komponenten')
plt.ylabel('kumulative Varianz');

490 | Kapitel 46: Vertiefung: Vielfältiges Lernen

Abbildung 46-12. Kumulative Varianz aus der PCA-Projektion

Wir sehen, dass für diese Daten fast 100 Komponenten erforderlich sind, um 90 % der Daten zu erhalten.

Varianz. Dies zeigt uns, dass die Daten von Natur aus sehr hochdimensional sind - sie lassen sich nicht
linear mit nur wenigen Komponenten beschrieben werden.

Wenn dies der Fall ist, können nichtlineare Mannigfaltigkeitseinbettungen wie LLE und Isomap

hilfreich. Wir können eine Isomap-Einbettung auf diesen Flächen nach demselben Muster berechnen
wie zuvor gezeigt:

In [19]: from sklearn.manifold import Isomap
model = Isomap(n_components=2)
proj = model.fit_transform(faces.data)
proj.shape
Out[19]: (2370, 2)

Die Ausgabe ist eine zweidimensionale Projektion aller Eingabebilder. Um eine bessere
eine bessere Vorstellung davon zu bekommen, was die Projektion uns sagt, definieren wir eine Funktion, die das Bild

Miniaturansichten an den Stellen der Projektionen:

In [20]: from matplotlib import offsetbox

def plot_components(data, model, images= None , ax= None ,
thumb_frac=0.05, cmap='gray'):
ax = ax oder plt.gca()

proj = model.fit_transform(Daten)
ax.plot(proj[:, 0], proj[:, 1], '.k')

wenn images nicht None ist :
min_dist_2 = (thumb_frac * max(proj.max(0) - proj.min(0))) ** 2

Beispiel: Isomap auf Gesichtern | 491
shown_images = np.array([2 * proj.max(0)])
for i in range(data.shape[0]):
dist = np.sum((proj[i] - shown_images) ** 2, 1)
if np.min(dist) < min_dist_2:
# zeige keine Punkte, die zu nahe beieinander liegen
weiter
shown_images = np.vstack([shown_images, proj[i]])
imagebox = offsetbox.AnnotationBbox(
offsetbox.OffsetImage(images[i], cmap=cmap),
proj[i])
ax.add_artist(imagebox)

Wenn wir diese Funktion jetzt aufrufen, sehen wir das Ergebnis in Abbildung 46-13.

In [21]: fig, ax = plt.subplots(figsize=(10, 10))
plot_components(faces.data,
model=Isomap(n_components=2),
images=faces.images[:, ::2, ::2])

Abbildung 46-13. Isomap-Einbettung der LFW-Daten

492 | Kapitel 46: Vertiefung: Vielfältiges Lernen

Das Ergebnis ist interessant. Die ersten beiden Isomap-Dimensionen scheinen die globalen

Bildmerkmale: die Gesamthelligkeit des Bildes von links nach rechts und die allgemeine
Ausrichtung des Gesichts von unten nach oben. Dies gibt uns einen schönen visuellen Hinweis auf

einige der grundlegenden Merkmale in unseren Daten.

Von hier aus könnten wir dann mit der Klassifizierung dieser Daten fortfahren (vielleicht unter Verwendung vielfältiger Merkmale
als Eingaben für den Klassifizierungsalgorithmus), wie wir es in Kapitel 43 getan haben.

Beispiel: Visualisierung der Struktur von Ziffern
Ein weiteres Beispiel für die Verwendung von Manifold Learning für die Visualisierung ist das folgende Beispiel

der MNIST-Datensatz für handgeschriebene Ziffern. Dieser Datensatz ähnelt dem Datensatz für Ziffern, den wir in
Kapitel 44 gesehen haben, aber mit viel mehr Pixeln pro Bild. Er kann von http:// heruntergeladen werden.

openml.org mit dem Dienstprogramm Scikit-Learn:

In [22]: from sklearn.datasets import fetch_openml
mnist = fetch_openml('mnist_784')
mnist.data.shape
Out[22]: (70000, 784)

Der Datensatz besteht aus 70.000 Bildern mit jeweils 784 Pixeln (d. h. die Bilder sind 28 ×

28). Wie zuvor können wir uns die ersten Bilder ansehen (siehe Abbildung 46-14).

In [23]: mnist_data = np.asarray(mnist.data)
mnist_target = np.asarray(mnist.target, dtype=int)

fig, ax = plt.subplots(6, 8, subplot_kw=dict(xticks=[], yticks=[]))
for i, axi in enumerate(ax.flat):
axi.imshow(mnist_data[1250 * i].reshape(28, 28), cmap='gray_r')

Abbildung 46-14. Beispiele für MNIST-Ziffern

Beispiel: Visualisierung der Struktur in Ziffern | 493
Dies gibt uns einen Eindruck von der Vielfalt der Handschriften im Datensatz.

Berechnen wir nun eine vielfältige Lernprojektion über die Daten. Aus Geschwindigkeitsgründen werden wir hier
nur 1/30 der Daten verwenden, was etwa 2.000 Punkten entspricht (wegen der relativ geringen

Skalierung des mannigfaltigen Lernens, finde ich, dass ein paar tausend Stichproben eine gute Zahl sind, um

für eine relativ schnelle Erkundung, bevor man zu einer vollständigen Berechnung übergeht).
Abbildung 46-15 zeigt das Ergebnis.

In [24]: # Nur 1/30 der Daten verwenden: der gesamte Datensatz braucht viel Zeit!
Daten = mnist_data[::30]
target = mnist_target[::30]

model = Isomap(n_components=2)
proj = model.fit_transform(Daten)

plt.scatter(proj[:, 0], proj[:, 1], c=target,
cmap=plt.cm.get_cmap('jet', 10))
plt.colorbar(ticks=range(10))
plt.clim(-0.5, 9.5);

Abbildung 46-15. Isomap-Einbettung der MNIST-Zifferndaten

Das resultierende Streudiagramm zeigt einige der Beziehungen zwischen den Datenpunkten,

ist aber ein wenig überfüllt. Wir können mehr Einblick gewinnen, wenn wir jeweils nur eine einzige Zahl betrachten
betrachten (siehe Abbildung 46-16).

In [25]: # Wähle 1/4 der "1"-Ziffern für die Projektion
Daten = mnist_data[mnist_target == 1][::4]

fig, ax = plt.subplots(figsize=(10, 10))
model = Isomap(n_neighbors=5, n_components=2, eigen_solver='dense')

494 | Kapitel 46: Vertiefung: Vielfältiges Lernen

plot_components(data, model, images=data.reshape((-1, 28, 28)),
ax=ax, thumb_frac=0.05, cmap='gray_r')

Abbildung 46-16. Isomap-Einbettung nur der 1en im MNIST-Datensatz

Das Ergebnis gibt Ihnen einen Eindruck von der Vielfalt der Formen, die die Zahl 1 annehmen kann

den Datensatz. Die Daten liegen entlang einer breiten Kurve im projizierten Raum, die den Anschein erweckt
der Ausrichtung der Ziffer zu folgen scheint. Wenn Sie sich auf dem Diagramm nach oben bewegen, finden Sie 1en, die Hüte haben

und/oder Basen, obwohl diese im Datensatz sehr spärlich vorhanden sind. Die Projektion ermöglicht es uns

Ausreißer zu identifizieren, die Datenprobleme haben: z. B. Teile der benachbarten Ziffern
die sich in die extrahierten Bilder eingeschlichen haben.

Dies mag für die Klassifizierung von Ziffern an sich nicht nützlich sein, aber es hilft
aber es hilft uns, die Daten zu verstehen, und kann uns Ideen geben, wie wir...

wie wir die Daten vor der Erstellung einer Klassifizierung vorverarbeiten wollen.

tionspipeline.

Beispiel: Struktur in Ziffern visualisieren | 495
KAPITEL 47

Vertiefung: k-Means Clustering
In den vorangegangenen Kapiteln haben wir uns mit unüberwachten maschinellen Lernmodellen zur
Dimensionalitätsreduktion untersucht. Nun werden wir uns einer anderen Klasse von unüberwachten

Modelle des maschinellen Lernens: Clustering-Algorithmen. Clustering-Algorithmen versuchen zu lernen,

aus den Eigenschaften der Daten, eine optimale Aufteilung oder diskrete Kennzeichnung von Gruppen von
Punkte.

Viele Clustering-Algorithmen sind in Scikit-Learn und anderswo verfügbar, aber der vielleicht
der einfachste ist ein Algorithmus, der als k-means Clustering bekannt ist, der

implementiert in sklearn.cluster.KMeans.

Wir beginnen mit den Standardimporten:

In [1]: % matplotlib inline
import matplotlib.pyplot as plt
plt.style.use('seaborn-whitegrid')
import numpy as np

Einführung in k-Means
Der k-means-Algorithmus sucht nach einer vorgegebenen Anzahl von Clustern innerhalb einer

unmarkierten mehrdimensionalen Datensatz. Dies wird durch eine einfache Vorstellung davon erreicht
wie das optimale Clustering aussieht:

Das Clusterzentrum ist das arithmetische Mittel aller zum Cluster gehörenden Punkte.
Jeder Punkt liegt näher an seinem eigenen Clusterzentrum als an den anderen Clusterzentren.
Diese beiden Annahmen sind die Grundlage des k-means-Modells. Wir werden uns bald mit dem

wie der Algorithmus zu dieser Lösung kommt, aber schauen wir uns zunächst einen einfachen Datensatz an
Datensatz und sehen uns das k-means-Ergebnis an.

496
Erzeugen wir zunächst einen zweidimensionalen Datensatz mit vier verschiedenen Blobs. An

betonen, dass es sich um einen unüberwachten Algorithmus handelt, lassen wir die Beschriftungen aus der
Visualisierung (siehe Abbildung 47-1).

In [2]: from sklearn.datasets import make_blobs
X, y_true = make_blobs(n_samples=300, centers=4,
cluster_std=0.60, random_state=0)
plt.scatter(X[:, 0], X[:, 1], s=50);

Abbildung 47-1. Daten zur Demonstration des Clustering

Mit dem Auge ist es relativ einfach, die vier Cluster herauszufinden. Der k-means-Algorithmus leistet

Dies geschieht automatisch und in Scikit-Learn wird die typische Schätzer-API verwendet:

In [3]: from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=4)
kmeans.fit(X)
y_kmeans = kmeans.predict(X)

Veranschaulichen wir die Ergebnisse, indem wir die durch diese Beschriftungen gefärbten Daten grafisch darstellen (Abbildung 47-2). Wir

stellt auch die Clusterzentren dar, die durch den k-means-Schätzer ermittelt wurden:

In [4]: plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')

Zentren = kmeans.cluster_centers_
plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200);

Die gute Nachricht ist, dass der k-means-Algorithmus (zumindest in diesem einfachen Fall) die

weist auf Cluster hin, ganz ähnlich wie wir sie mit dem Auge zuordnen würden. Aber Sie könnten

sich fragen, wie dieser Algorithmus diese Cluster so schnell findet: Schließlich ist die Anzahl der
möglichen Kombinationen von Clusterzuordnungen exponentiell zur Anzahl der Daten

Einführung in k-Means | 497
Punkte - eine erschöpfende Suche wäre sehr, sehr kostspielig. Zum Glück für uns ist eine solche

erschöpfende Suche ist nicht notwendig: Stattdessen beinhaltet der typische Ansatz für k-means
einen intuitiven iterativen Ansatz, der als Erwartungsmaximierung bekannt ist.

Abbildung 47-2. k-means-Clusterzentren mit farblich gekennzeichneten Clustern

Erwartungs-Maximierung
Die Erwartungsmaximierung (E-M) ist ein leistungsfähiger Algorithmus, der in einer Vielzahl von Fällen zum Einsatz kommt

k-means ist eine besonders einfache und leicht zu verstehende Anwendung des Algorithmus.
Anwendung des Algorithmus, die wir hier kurz erläutern werden. Kurz gefasst,

Der Ansatz der Erwartungsmaximierung besteht hier aus folgendem Verfahren:

Schätzen Sie einige Clusterzentren.
Wiederholen Sie dies, bis Sie konvergieren:
a.E-Schritt: Weisen Sie Punkte dem nächstgelegenen Clusterzentrum zu.
b. M-Schritt: Setzen Sie die Clusterzentren auf den Mittelwert der ihnen zugewiesenen Punkte.
Hier wird der E-Schritt oder Erwartungsschritt so genannt, weil er die Aktualisierung unserer
Erwartung, zu welchem Cluster jeder Punkt gehört. Der M-Schritt oder Maximierungsschritt

ist so benannt, weil es darum geht, eine Fitnessfunktion zu maximieren, die die

der Clusterzentren - in diesem Fall wird diese Maximierung erreicht durch
einen einfachen Mittelwert der Daten in jedem Cluster.

498 | Kapitel 47: Vertiefung: k-Means Clustering

1 Der Code zur Erstellung dieser Abbildung ist im Online-Anhang zu finden.
Die Literatur über diesen Algorithmus ist sehr umfangreich, lässt sich aber wie folgt zusammenfassen: unter

Unter typischen Umständen wird jede Wiederholung des E-Schritts und des M-Schritts immer zu einer
eine bessere Schätzung der Clustereigenschaften.

Wir können den Algorithmus wie in Abbildung 47-3 veranschaulichen. Für die jeweilige Initialisierung

In der hier gezeigten Version konvergieren die Cluster in nur drei Iterationen. (Für eine interaktive Ver- sion
(Eine interaktive Version dieser Abbildung finden Sie im Code im Online-Anhang).

Abbildung 47-3. Visualisierung des E-M-Algorithmus für k-means^1

Der k-means-Algorithmus ist so einfach, dass wir ihn in ein paar Zeilen Code schreiben können.
Im Folgenden wird eine sehr einfache Implementierung gezeigt (siehe Abbildung 47-4).

In [5]: from sklearn.metrics import pairwise_distances_argmin

def find_clusters(X, n_clusters, rseed=2):
# 1. Zufällige Auswahl von Clustern
rng = np.random.RandomState(rseed)
i = rng.permutation(X.shape[0])[:n_clusters]
centers = X[i]

while True :
# 2a. Zuweisung von Etiketten basierend auf dem nächstgelegenen Zentrum
labels = pairwise_distances_argmin(X, centers)

# 2b. Neue Zentren aus den Mittelwerten der Punkte finden
new_centers = np.array([X[labels == i].mean(0)
for i in range(n_clusters)])

# 2c. Prüfung auf Konvergenz
if np.all(centers == new_centers):
break
Zentren = neue_Zentren

Rückgabezentren, Etiketten

centers, labels = find_clusters(X, 4)

Erwartungs-Maximierung | 499
plt.scatter(X[:, 0], X[:, 1], c=labels,
s=50, cmap='viridis');

Abbildung 47-4. Mit k-means beschriftete Daten

Die meisten gut getesteten Implementierungen tun unter der Haube ein bisschen mehr als das, aber

die vorangehende Funktion gibt die Grundzüge des Erwartungsmaximierungsansatzes wieder.
Bei der Verwendung der Erwartungsmaximierung gibt es einige Vorbehalte zu beachten

Algorithmus:

Das global optimale Ergebnis wird möglicherweise nicht erreicht
Erstens: Obwohl das E-M-Verfahren garantiert, dass das Ergebnis in jedem Schritt verbessert wird
Schritt zu verbessern, gibt es keine Garantie dafür, dass es zur global besten Lösung führen wird. Zum Beispiel
Beispiel: Wenn wir in unserem einfachen Verfahren einen anderen Zufallskeim verwenden, führen die einzelnen
Startannahmen zu schlechten Ergebnissen (siehe Abbildung 47-5).

In [6]: centers, labels = find_clusters(X, 4, rseed=0)
plt.scatter(X[:, 0], X[:, 1], c=labels,
s=50, cmap='viridis');
500 | Kapitel 47: Vertiefung: k-Means Clustering

Abbildung 47-5. Ein Beispiel für schlechte Konvergenz bei k-means

Hier hat der E-M-Ansatz konvergiert, aber nicht zu einer global optimalen Konfiguration geführt.
optimale Konfiguration konvergiert. Aus diesem Grund ist es üblich, den Algorithmus für mehrere
durchlaufen wird, wie es Scikit-Learn standardmäßig tut (die Anzahl wird
(die Anzahl wird durch den n_init-Parameter festgelegt, der standardmäßig auf 10 gesetzt ist).
Die Anzahl der Cluster muss im Voraus ausgewählt werden

Eine weitere häufige Herausforderung bei k-means ist, dass Sie dem System mitteilen müssen, wie viele Clus-
Es kann die Anzahl der Cluster nicht aus den Daten lernen. Zum Beispiel
Beispiel: Wenn wir den Algorithmus bitten, sechs Cluster zu identifizieren, wird er fröhlich fortfahren und
und findet die besten sechs Cluster, wie in Abbildung 40-1 dargestellt:
In [7]: labels = KMeans(6, random_state=0).fit_predict(X)
plt.scatter(X[:, 0], X[:, 1], c=labels,
s=50, cmap='viridis');
Erwartungs-Maximierung | 501
Abbildung 47-6. Ein Beispiel, bei dem die Anzahl der Cluster ungünstig gewählt ist

Ob das Ergebnis sinnvoll ist, ist eine Frage, die schwer zu beantworten ist.
Ein Ansatz, der eher intuitiv ist, auf den wir hier aber nicht weiter eingehen wollen, ist die
hier nicht weiter erörtert wird, ist die Silhouettenanalyse.
Alternativ können Sie auch einen komplizierteren Clustering-Algorithmus verwenden, der ein
besseres quantitatives Maß für die Fitness pro Anzahl von Clustern hat (z. B. Gaußsche
Mischmodelle; siehe Kapitel 48) oder der eine geeignete Anzahl von Clustern wählen kann (z. B.
wählen kann (z.B. DBSCAN, Mean-Shift oder Affinitätspropagation, alle verfügbar im
sklearn.cluster Submodul).
k-means ist auf lineare Clustergrenzen beschränkt

Die grundlegenden Modellannahmen von k-means (Punkte liegen näher an ihrem
(Punkte liegen näher an ihrem eigenen Clusterzentrum als an anderen) bedeutet, dass der Algorithmus oft ineffektiv ist
tiv ist, wenn die Cluster eine komplizierte Geometrie aufweisen.
Insbesondere werden die Grenzen zwischen k-means-Clustern immer linear sein,
was bedeutet, dass der Algorithmus bei komplizierteren Grenzen versagt. Betrachten Sie die folgenden
folgenden Daten, zusammen mit den Cluster-Labels, die mit dem typischen k-means-Ansatz
(siehe Abbildung 47-7).
502 | Kapitel 47: Vertiefung: k-Means-Clustering

In [8]: from sklearn.datasets import make_moons
X, y = make_moons(200, noise=.05, random_state=0)

In [9]: labels = KMeans(2, random_state=0).fit_predict(X)
plt.scatter(X[:, 0], X[:, 1], c=labels,
s=50, cmap='viridis');

Abbildung 47-7. Scheitern von k-means mit nichtlinearen Grenzen

Diese Situation erinnert an die Diskussion in Kapitel 43, wo wir eine
Kernel-Transformation verwendet haben, um die Daten in eine höhere Dimension zu projizieren, in der eine lineare

Trennung möglich ist. Wir könnten uns vorstellen, den gleichen Trick anzuwenden, damit k-means
nicht-lineare Grenzen zu entdecken.

Eine Version dieses kernelisierten k-means ist in Scikit-Learn in der

SpectralClustering-Schätzer. Er verwendet den Graphen der nächsten Nachbarn, um die

eine höherdimensionale Darstellung der Daten und ordnet dann Bezeichnungen
mit einem k-means-Algorithmus (siehe Abbildung 47-8).

In [10]: from sklearn.cluster import SpectralClustering
model = SpectralClustering(n_clusters=2,
affinity='nearest_neighbors',
assign_labels='kmeans')
labels = model.fit_predict(X)
plt.scatter(X[:, 0], X[:, 1], c=labels,
s=50, cmap='viridis');

Erwartungs-Maximierung | 503
Abbildung 47-8. Mit SpectralClustering gelernte nichtlineare Grenzen

Wir sehen, dass mit diesem Kernel-Transformationsansatz das kernelisierte k-means in der Lage ist
in der Lage ist, die komplizierteren nichtlinearen Grenzen zwischen Clustern zu finden.
k-means kann bei einer großen Anzahl von Stichproben langsam sein

Da jede Iteration von k-means auf jeden Punkt des Datensatzes zugreifen muss, kann der
kann der Algorithmus relativ langsam sein, wenn die Anzahl der Stichproben wächst. Sie könnten
Sie könnten sich fragen, ob diese Anforderung, alle Daten bei jeder Iteration zu verwenden, gelockert werden kann; zum Beispiel
Sie könnten beispielsweise nur eine Teilmenge der Daten verwenden, um die Clusterzentren bei
Schritt zu aktualisieren. Dies ist die Idee hinter den batch-basierten k-means-Algorithmen, von denen eine Form
die in sklearn.cluster.MiniBatchKMeans implementiert ist. Die Schnittstelle für
ist die gleiche wie für Standard-KMeans; wir werden im weiteren Verlauf der Diskussion ein Beispiel für seine Verwendung sehen.
unsere Diskussion fortsetzen.
Beispiele
Unter Beachtung dieser Einschränkungen des Algorithmus können wir k-means für unsere

Vorteil in einer Vielzahl von Situationen. Schauen wir uns nun einige Beispiele an.

Beispiel 1: k-means auf Ziffern
Lassen Sie uns zunächst die Anwendung von k-means auf die gleichen einfachen Zifferndaten betrachten, die wir

die wir in den Kapiteln 44 und 45 gesehen haben. Hier werden wir versuchen, k-means zu verwenden, um
ähnliche Ziffern zu identifizieren, ohne die ursprüngliche Beschriftungsinformation zu verwenden; dies könnte ähnlich sein wie ein

Der erste Schritt, um aus einem neuen Datensatz, über den man keine Kenntnisse hat, eine Bedeutung zu gewinnen

Vorabinformationen zum Etikett.

504 | Kapitel 47: Vertiefung: k-Means Clustering

Wir beginnen mit dem Laden des Datensatzes und finden dann die Cluster. Erinnern Sie sich, dass die Ziffern data-

Satz besteht aus 1.797 Stichproben mit 64 Merkmalen, wobei jedes der 64 Merkmale die Helligkeit eines
Helligkeit eines Pixels in einem 8 × 8-Bild darstellt:

In [11]: from sklearn.datasets import load_digits
Ziffern = load_digits()
ziffern.daten.form
Out[11]: (1797, 64)

Das Clustering kann wie zuvor durchgeführt werden:

In [12]: kmeans = KMeans(n_clusters=10, random_state=0)
clusters = kmeans.fit_predict(ziffern.daten)
kmeans.cluster_centers_.shape
Out[12]: (10, 64)

Das Ergebnis sind 10 Cluster in 64 Dimensionen. Beachten Sie, dass die Clusterzentren selbst

sind 64-dimensionale Punkte und können als Darstellung der "typischen" Ziffer
innerhalb des Clusters. Schauen wir uns an, wie diese Clusterzentren aussehen (siehe Abbildung 47-9).

In [13]: fig, ax = plt.subplots(2, 5, figsize=(8, 3))
centers = kmeans.cluster_centers_.reshape(10, 8, 8)
for axi, center in zip(ax.flat, centers):
axi.set(xticks=[], yticks=[])
axi.imshow(center, interpolation='nearest', cmap=plt.cm.binary)

Abbildung 47-9. Mit k-means gelernte Clusterzentren

Wir sehen, dass KMeans auch ohne Beschriftungen in der Lage ist, Cluster zu finden, deren Zentren

erkennbare Ziffern, vielleicht mit Ausnahme von 1 und 8.

Da k-means nichts über die Identitäten der Cluster weiß, können die 0-9-Labels
vertauscht sein. Wir können dies beheben, indem wir jedes gelernte Cluster-Label mit dem wahren

Etiketten in den Clustern gefunden:

In [14]: from scipy.stats import mode

labels = np.zeros_like(clusters)
for i in range(10):

Beispiele | 505
maske = (cluster == i)
labels[mask] = mode(ziffern.ziel[mask])[0]

Jetzt können wir überprüfen, wie genau unser unüberwachtes Clustering beim Auffinden ähnlicher
Ziffern innerhalb der Daten zu finden:

In [15]: from sklearn.metrics import accuracy_score
accuracy_score(ziffern.ziel, labels)
Out[15]: 0.7935447968836951

Mit einem einfachen k-means-Algorithmus haben wir die richtige Gruppierung für 80 % der

die eingegebenen Ziffern! Schauen wir uns dazu die Konfusionsmatrix an, die in Abbildung 47-10 dargestellt ist.

In [16]: from sklearn.metrics import confusion_matrix
import seaborn as sns
mat = confusion_matrix(ziffern.ziel, labels)
sns.heatmap(mat.T, square= True , annot= True , fmt='d',
cbar= False , cmap='Blues',
xticklabels=digits.target_names,
yticklabels=digits.target_names)
plt.xlabel('wahre Bezeichnung')
plt.ylabel('vorhergesagte Beschriftung');

Abbildung 47-10. Konfusionsmatrix für den k-means Klassifikator

506 | Kapitel 47: Vertiefung: k-Means Clustering

2 Eine Farbversion dieses und der folgenden Bilder finden Sie in der Online-Version dieses Buches.
Wie wir von den Clusterzentren, die wir zuvor visualisiert haben, erwarten können, ist der Hauptpunkt von

Die Verwechslung besteht zwischen den Achten und Einsen. Dies zeigt jedoch, dass wir mit k-means
einen Ziffernklassifikator erstellen können, ohne auf bekannte Kennzeichnungen zurückgreifen zu müssen!

Versuchen wir, das Ganze noch weiter zu treiben, nur so zum Spaß. Wir können die t-verteilte stochastische

Nachbar-Embedding-Algorithmus (erwähnt in Kapitel 46) zur Vorverarbeitung der Daten
t-SNE ist ein nichtlinearer Einbettungsalgorithmus, der insbesondere für die

besonders geschickt bei der Erhaltung von Punkten innerhalb von Clustern. Schauen wir mal, wie es funktioniert:

In [17]: from sklearn.manifold import TSNE

# Projizieren Sie die Daten: dieser Schritt wird einige Sekunden dauern
tsne = TSNE(n_components=2, init='random',
learning_rate='auto',random_state=0)
ziffern_proj = tsne.fit_transform(ziffern.daten)

# Berechnen der Cluster
kmeans = KMeans(n_clusters=10, random_state=0)
clusters = kmeans.fit_predict(ziffern_proj)

# Permutieren der Beschriftungen
labels = np.zeros_like(clusters)
for i in range(10):
mask = (clusters == i)
labels[mask] = mode(ziffern.ziel[mask])[0]

# Berechnen Sie die Genauigkeit
accuracy_score(digits.target, labels)
Out[17]: 0.9415692821368948

Das ist eine Klassifizierungsgenauigkeit von 94 % ohne Verwendung der Etiketten. Das ist die Leistung von

unüberwachtes Lernen, wenn es sorgfältig eingesetzt wird: Es kann Informationen aus den Daten extrahieren.

die nur schwer von Hand oder mit dem Auge extrahiert werden können.

Beispiel 2: k-Means für die Farbkomprimierung
Eine interessante Anwendung des Clustering ist die Farbkompression in Bildern (dieses

Beispiel ist dem Scikit-Learn-Beispiel "Color Quantization Using K-Means" entnommen). Für
Beispiel: Stellen Sie sich vor, Sie haben ein Bild mit Millionen von Farben. In den meisten Bildern ist ein großer

Viele der Farben werden nicht verwendet, und viele der Pixel im Bild haben ähnliche oder
ähnliche oder sogar identische Farben haben.

Betrachten Sie zum Beispiel das in Abbildung 47-11 gezeigte Bild, das aus dem Scikit-

Modul Datensätze lernen (damit dies funktioniert, müssen Sie das Python-Paket PIL haben

installiert):^2

Beispiele | 507
In [18]: # Hinweis: hierfür muss das PIL-Paket installiert sein
from sklearn.datasets import load_sample_image
china = load_sample_image("china.jpg")
ax = plt.axes(xticks=[], yticks=[])
ax.imshow(china);

Abbildung 47-11. Das Eingabebild

Das Bild selbst wird in einem dreidimensionalen Feld der Größe (Höhe, Breite, RGB) gespeichert,
gespeichert, das Rot/Blau/Grün-Beiträge als ganze Zahlen von 0 bis 255 enthält:

In [19]: china.shape
Out[19]: (427, 640, 3)

Eine Möglichkeit, diesen Satz von Pixeln zu betrachten, ist eine Wolke von Punkten in einer dreidimensionalen

Farbraum. Wir werden die Daten auf [n_samples, n_features] umformen und die Skalierung der

Farben, so dass sie zwischen 0 und 1 liegen:

In [20]: data = china / 255.0 # Skala 0...1 verwenden
data = data.reshape(-1, 3)
daten.form
Out[20]: (273280, 3)

Wir können diese Pixel in diesem Farbraum visualisieren, indem wir eine Teilmenge von 10.000 Pixeln für

Effizienz (siehe Abbildung 47-12).

In [21]: def plot_pixels(data, title, colors= None , N=10000):
if colors is None :
Farben = Daten

# Wählen Sie eine zufällige Teilmenge
rng = np.random.default_rng(0)

508 | Kapitel 47: Vertiefung: k-Means Clustering

3 Eine Version dieser Abbildung in voller Größe ist auf GitHub zu finden.
i = rng.permutation(data.shape[0])[:N]
colors = colors[i]
R, G, B = data[i].T

fig, ax = plt.subplots(1, 2, figsize=(16, 6))
ax[0].scatter(R, G, color=colors, marker='.')
ax[0].set(xlabel='Rot', ylabel='Grün', xlim=(0, 1), ylim=(0, 1))

ax[1].scatter(R, B, color=colors, marker='.')
ax[1].set(xlabel='Rot', ylabel='Blau', xlim=(0, 1), ylim=(0, 1))

fig.suptitle(title, size=20);

In [22]: plot_pixels(data, title='Eingabefarbraum: 16 Millionen mögliche Farben')

Abbildung 47-12. Die Verteilung der Pixel im RGB-Farbraum^3

Reduzieren wir nun diese 16 Millionen Farben auf nur 16 Farben, indem wir ein k-means Clustering durchführen

über den gesamten Pixelraum. Da wir es mit einem sehr großen Datensatz zu tun haben, werden wir Folgendes verwenden

der Mini-Batch-k-means-Algorithmus, der mit Teilmengen der Daten arbeitet, um das Ergebnis zu berechnen
(siehe Abbildung 47-13) sehr viel schneller als der standardmäßige k-means-Algorithmus:

In [23]: from sklearn.cluster import MiniBatchKMeans
kmeans = MiniBatchKMeans(16)
kmeans.fit(Daten)
new_colors = kmeans.cluster_centers_[kmeans.predict(data)]

plot_pixels(data, colors=new_colors,
title="Reduzierter Farbraum: 16 Farben")

Beispiele | 509
4 Eine Version dieser Abbildung in voller Größe ist auf GitHub zu finden.
Abbildung 47-13. 16 Cluster im RGB-Farbraum^4

Das Ergebnis ist eine Neueinfärbung der ursprünglichen Pixel, wobei jedem Pixel die Farbe
des nächstgelegenen Clusterzentrums zugewiesen wird. Die Darstellung dieser neuen Farben im Bildraum anstelle von

Der Pixelraum zeigt uns die Auswirkungen dieser Vorgehensweise (siehe Abbildung 47-14).

In [24]: china_recolored = new_colors.reshape(china.shape)

fig, ax = plt.subplots(1, 2, figsize=(16, 6),
subplot_kw=dict(xticks=[], yticks=[]))
fig.subplots_adjust(wspace=0.05)
ax[0].imshow(china)
ax[0].set_title('Originalbild', size=16)
ax[1].imshow(china_umgefärbt)
ax[1].set_title('16-farbiges Bild', size=16);

Abbildung 47-14. Ein Vergleich des Vollfarbbildes (links) und des 16-Farben-Bildes (rechts)

510 | Kapitel 47: Vertiefung: k-Means Clustering

Ganz rechts im Bild sind zwar einige Details verloren gegangen, aber das Gesamtbild ist immer noch gut zu erkennen.

wiedererkennbar. Gemessen an den Bytes, die zur Speicherung der Rohdaten benötigt werden, erreicht das nebenstehende Bild
rechts einen Kompressionsfaktor von etwa 1 Million! Nun, diese Art von Ansatz

nicht an die Genauigkeit von speziell entwickelten Bildkomprimierungsverfahren wie

JPEG, aber das Beispiel zeigt, wie wichtig es ist, über den Tellerrand hinauszublicken und unsuper-
visierten Methoden wie k-means.

Beispiele | 511
KAPITEL 48

Vertiefung: Gaußsche Mischungsmodelle
Das im vorigen Kapitel untersuchte k-means-Clustermodell ist einfach und relativ
leicht zu verstehen, aber seine Einfachheit führt zu praktischen Herausforderungen bei seiner Anwendung.

tion. Insbesondere die nicht-probabilistische Natur von k-means und die Verwendung von einfachen

Entfernung vom Clusterzentrum zur Zuweisung der Clustermitgliedschaft führt in vielen
für viele Situationen in der Praxis. In diesem Kapitel werden wir einen Blick auf die Gaußsche Mischung werfen

Modelle, die als eine Erweiterung der Ideen hinter k-means angesehen werden können, aber auch
auch ein leistungsfähiges Instrument für Schätzungen sein, die über die einfache Clusterbildung hinausgehen.

Wir beginnen mit den Standardimporten:

In [1]: % matplotlib inline
import matplotlib.pyplot as plt
plt.style.use('seaborn-whitegrid')
import numpy as np

Motivierende Gaußsche Mischungen: Schwachstellen von k-Means
Werfen wir einen Blick auf einige der Schwächen von k-means und überlegen wir, wie wir
das Clustermodell verbessern können. Wie wir im vorigen Kapitel gesehen haben, kann man bei einfachen, gut

getrennte Daten, findet k-means geeignete Clustering-Ergebnisse.

Wenn wir zum Beispiel einfache Datenblöcke haben, kann der k-means-Algorithmus diese schnell beschriften
diese Cluster auf eine Weise beschriften, die dem entspricht, was wir mit dem Auge tun könnten (siehe

Abbildung 48-1).

In [2]: # Einige Daten generieren
from sklearn.datasets import make_blobs
X, y_true = make_blobs(n_samples=400, centers=4,
cluster_std=0.60, random_state=0)
X = X[:, ::-1] # Achsen zur besseren Darstellung umdrehen

512
In [3]: # Plotten der Daten mit k-means Labels
from sklearn.cluster import KMeans
kmeans = KMeans(4, random_state=0)
labels = kmeans.fit(X).predict(X)
plt.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis');

Abbildung 48-1. k-means Labels für einfache Daten

Vom intuitiven Standpunkt aus könnte man erwarten, dass die Clusterzuordnung für

Einige Punkte sind sicherer als andere: Zum Beispiel scheint es eine sehr geringe
Überschneidungen zwischen den beiden mittleren Clustern zu geben, so dass wir nicht ganz sicher sein können

digkeit in der Clusterzuordnung von Punkten zwischen ihnen. Leider hat das k-means
Modell kein intrinsisches Maß für die Wahrscheinlichkeit oder Unsicherheit von Clusterzuweisungen

(obwohl es möglich sein könnte, einen Bootstrap-Ansatz zur Schätzung dieser Unsicherheit zu verwenden).

Hierfür müssen wir über eine Verallgemeinerung des Modells nachdenken.

Eine Möglichkeit, sich das k-means-Modell vorzustellen, ist, dass es einen Kreis (oder, in höheren

Dimensionen, eine Hypersphäre) im Zentrum jedes Clusters, mit einem Radius, der durch den
am weitesten entfernten Punkt des Clusters. Dieser Radius dient als harter Grenzwert für die Clusterzuordnung.

der Trainingsmenge: Jeder Punkt außerhalb dieses Kreises wird nicht als Mitglied betrachtet.

des Clusters. Wir können dieses Clustermodell mit der folgenden Funktion visualisieren (siehe
Abbildung 48-2).

Motivierende Gaußsche Mischungen: Schwachstellen von k-Means | 513
In [4]: from sklearn.cluster import KMeans
aus scipy.spatial.distance import cdist

def plot_kmeans(kmeans, X, n_clusters=4, rseed=0, ax= None ):
labels = kmeans.fit_predict(X)

# Plotten der Eingabedaten
ax = ax oder plt.gca()
ax.axis('gleich')
ax.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis', zorder=2)

# Plotten der Darstellung des KMeans-Modells
Zentren = kmeans.cluster_centers_
radii = [cdist(X[labels == i], [center]).max()
for i, center in enumerate(centers)]
for c, r in zip(centers, radii):
ax.add_patch(plt.Circle(c, r, ec='black', fc='lightgray',
lw=3, alpha=0.5, zorder=1))

In [5]: kmeans = KMeans(n_clusters=4, random_state=0)
plot_kmeans(kmeans, X)

Abbildung 48-2. Die kreisförmigen Cluster, die durch das k-means-Modell impliziert werden

Eine wichtige Beobachtung für k-means ist, dass diese Clustermodelle zirkulär sein müssen: k-

hat keine eingebaute Möglichkeit, längliche oder elliptische Haufen zu berücksichtigen. Wenn wir also zum Beispiel
Wenn wir beispielsweise dieselben Daten nehmen und sie transformieren, werden die Clusterzuordnungen am Ende

wie in Abbildung 48-3 zu sehen ist, durcheinander geraten.

514 | Kapitel 48: Vertiefung: Gaußsche Mischungsmodelle

In [6]: rng = np.random.RandomState(13)
X_stretched = np.dot(X, rng.randn(2, 2))

kmeans = KMeans(n_clusters=4, random_state=0)
plot_kmeans(kmeans, X_stretched)

Abbildung 48-3. Schlechte Leistung von k-means bei nicht kreisförmigen Clustern

Mit dem Auge erkennen wir, dass diese transformierten Cluster nicht kreisförmig sind und daher kreisförmige
len Clustern schlecht passen würden. Dennoch ist k-means nicht flexibel genug, um

berücksichtigt dies und versucht, die Daten in vier kreisförmige Cluster einzuteilen. Das Ergebnis

in einer Vermischung von Clusterzuordnungen, bei der sich die resultierenden Kreise überschneiden: siehe insbesondere
die untere rechte Seite dieser Grafik. Man könnte sich vorstellen, diese besondere Situation zu behandeln

durch Vorverarbeitung der Daten mit PCA (siehe Kapitel 45), aber in der Praxis gibt es keine Garantie dafür, dass eine solche
aber in der Praxis gibt es keine Garantie, dass eine solche globale Operation die einzelnen Gruppen zirkularisiert.

Diese beiden Nachteile von k-means - die mangelnde Flexibilität bei der Clusterbildung und der Mangel an

der probabilistischen Clusterzuweisung - bedeuten, dass sie bei vielen Datensätzen (insbesondere bei niedrig
dimensionalen Datensätzen) nicht so gut abschneidet, wie Sie vielleicht hoffen.

Sie könnten sich vorstellen, diese Schwächen durch eine Verallgemeinerung des k-means-Modells zu beheben:
Man könnte zum Beispiel die Unsicherheit bei der Clusterzuordnung messen, indem man die

Entfernungen der einzelnen Punkte zu allen Clusterzentren, anstatt sich nur auf die nächstgelegenen zu konzentrieren.

Sie können sich auch vorstellen, die Clustergrenzen als Ellipsen statt als Kreise zu
Kreisen, um nicht kreisförmige Cluster zu berücksichtigen. Es stellt sich heraus, dass dies zwei wesentliche Kom- ponenten sind.

Komponenten eines anderen Typs von Clustermodellen, den Gaußschen Mischmodellen.

Motivierende Gaußsche Mischungen: Schwachstellen von k-Means | 515
Verallgemeinerung von E-M: Gaußsche Mischungsmodelle
Ein Gaußsches Mischungsmodell (GMM) versucht, eine Mischung aus mehrdimensionalen

Gaußsche Wahrscheinlichkeitsverteilungen, die jeden Eingabedatensatz am besten modellieren. Im einfachsten
Fall können GMMs zum Auffinden von Clustern auf dieselbe Weise wie k-means verwendet werden (siehe

Abbildung 48-4).

In [7]: from sklearn.mixture import GaussianMixture
gmm = GaussianMixture(n_components=4).fit(X)
labels = gmm.predict(X)
plt.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis');

Abbildung 48-4. Beschriftungen des Gaußschen Mischmodells für die Daten

Da ein GMM jedoch ein probabilistisches Modell enthält, ist es auch möglich

um probabilistische Cluster-Zuordnungen zu finden - in Scikit-Learn geschieht dies mit der

predict_proba-Methode. Sie liefert eine Matrix der Größe [n_Stichproben, n_Cluster]
zurück, die die Wahrscheinlichkeit misst, dass ein beliebiger Punkt zu einem bestimmten Cluster gehört:

In [8]: probs = gmm.predict_proba(X)
print(probs[:5].round(3))
Out[8]: [[0. 0.531 0.469 0. ]
[0. 0. 0. 1. ]
[0. 0. 0. 1. ]
[0. 1. 0. 0. ]
[0. 0. 0. 1. ]]

Wir können diese Unsicherheit visualisieren, indem wir zum Beispiel die Größe jedes Punktes pro- duzieren.

der Gewissheit seiner Vorhersage; ein Blick auf Abbildung 48-5 zeigt, dass es sich

516 | Kapitel 48: Vertiefung: Gaußsche Mischungsmodelle

genau die Punkte an den Grenzen zwischen den Clustern, die diese Unsicherheit widerspiegeln

Cluster-Zuordnung:

In [9]: size = 50 * probs.max(1) ** 2 # das Quadrat betont die Unterschiede
plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=size);

Abbildung 48-5. GMM probabilistische Etiketten: Wahrscheinlichkeiten werden durch die Größe der Punkte dargestellt

Unter der Haube ist ein Gaußsches Mischungsmodell dem k-means-Modell sehr ähnlich: Es verwendet einen
Erwartungsmaximierungsansatz, der qualitativ Folgendes leistet:

Wählen Sie Anfangsschätzungen für den Ort und die Form.
Wiederholen Sie dies, bis Sie konvergieren:
a.E-Schritt: Für jeden Punkt Gewichte finden, die die Wahrscheinlichkeit der Zugehörigkeit
in jedem Cluster kodiert.
b. M-Schritt: Aktualisieren Sie für jeden Cluster seine Lage, Normalisierung und Form auf der Grundlage
auf der Grundlage aller Datenpunkte und unter Verwendung der Gewichte.
Das Ergebnis ist, dass jeder Cluster nicht mit einer scharfkantigen Kugel assoziiert ist, sondern

mit einem glatten Gaußschen Modell. Genau wie bei der k-means-Erwartungsmaximierung

Ansatzes kann dieser Algorithmus manchmal die global optimale Lösung verfehlen, so dass
werden in der Praxis mehrere zufällige Initialisierungen verwendet.

Verallgemeinerung von E-M: Gaussian Mixture Models | 517
Lassen Sie uns eine Funktion erstellen, die uns hilft, die Positionen und Formen des GMM zu visualisieren

Clustern durch Zeichnen von Ellipsen auf der Grundlage der GMM-Ausgabe:

In [10]: from matplotlib.patches import Ellipse

def draw_ellipse(position, covariance, ax= None , **kwargs):
"""Zeichne eine Ellipse mit einer gegebenen Position und Kovarianz"""
ax = ax oder plt.gca()

# Kovarianz in Hauptachsen umwandeln
if covariance.shape == (2, 2):
U, s, Vt = np.linalg.svd(covariance)
Winkel = np.Grad(np.arctan2(U[1, 0], U[0, 0]))
Breite, Höhe = 2 * np.sqrt(s)
sonst :
Winkel = 0
Breite, Höhe = 2 * np.sqrt(Kovarianz)

# Zeichnen der Ellipse
for nsig in range(1, 4):
ax.add_patch(Ellipse(position, nsig * width, nsig * height,
angle, **kwargs))

def plot_gmm(gmm, X, label= True , ax= None ):
ax = ax oder plt.gca()
labels = gmm.fit(X).predict(X)
if label:
ax.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis',
zorder=2)
sonst :
ax.scatter(X[:, 0], X[:, 1], s=40, zorder=2)
ax.axis('equal')

w_factor = 0.2 / gmm.gewichte_.max()
for pos, covar, w in zip(gmm.means_, gmm.covariances_, gmm.weights_):
draw_ellipse(pos, covar, alpha=w * w_factor)

Vor diesem Hintergrund können wir einen Blick darauf werfen, was das Vier-Komponenten-GMM uns für

unsere ursprünglichen Daten (siehe Abbildung 48-6).

In [11]: gmm = GaussianMixture(n_components=4, random_state=42)
plot_gmm(gmm, X)

518 | Kapitel 48: Vertiefung: Gaußsche Mischungsmodelle

Abbildung 48-6. Das Vier-Komponenten-GMM bei Vorhandensein von Kreisclustern

In ähnlicher Weise können wir den GMM-Ansatz zur Anpassung unseres gestreckten Datensatzes verwenden, wobei eine

vollen Kovarianz passt das Modell auch auf sehr längliche, langgestreckte Cluster, wie wir in
in Abbildung 48-7 zu sehen ist.

In [12]: gmm = GaussianMixture(n_components=4, covariance_type='full',
random_state=42)
plot_gmm(gmm, X_stretched)

Abbildung 48-7. Das Vier-Komponenten-GMM bei Vorhandensein von nicht kreisförmigen Clustern

Verallgemeinerung von E-M: Gaußsche Mischungsmodelle | 519
1 Der Code zur Erstellung dieser Abbildung ist im Online-Anhang zu finden.
Dies macht deutlich, dass GMMs die beiden wichtigsten praktischen Probleme mit k-means lösen

begegnet sind.

Auswahl des Kovarianztyps
Wenn Sie sich die Details der vorangegangenen Anpassungen ansehen, werden Sie feststellen, dass der covariance_type

Option wurde in jedem Fall anders gesetzt. Dieser Hyperparameter steuert den Grad der

Freiheit in der Form jedes Clusters; es ist wichtig, dies für jeden einzelnen Cluster sorgfältig festzulegen

Problem. Die Voreinstellung ist covariance_type="diag", was bedeutet, dass die Größe der
Größe des Clusters entlang jeder Dimension unabhängig eingestellt werden kann, wobei die resultierende Ellipse kon-

an den Achsen ausgerichtet. covariance_type="spherical" ist ein etwas einfacheres
und schnelleres Modell, das die Form des Clusters so einschränkt, dass alle Dimensionen

gleich sind. Das resultierende Clustering hat ähnliche Eigenschaften wie das von k-means,
obwohl es nicht völlig gleichwertig ist. Eine kompliziertere und rechenaufwendigere

sive Modell (insbesondere wenn die Anzahl der Dimensionen zunimmt) ist die Verwendung von Cova

riance_type="full", wodurch jeder Cluster als Ellipse mit beliebiger Ausrichtung modelliert werden kann.
beliebiger Ausrichtung. Abbildung 48-8 zeigt diese drei Möglichkeiten für einen einzelnen Cluster.

Abbildung 48-8. Visualisierung der GMM-Kovarianztypen^1

Gaußsche Mischungsmodelle als Dichteschätzung
Obwohl das GMM oft als Clustering-Algorithmus kategorisiert wird, ist es im Grunde ein
ein Algorithmus zur Dichteschätzung. Das bedeutet, dass das Ergebnis einer GMM-Anpassung an einige

Daten ist technisch gesehen kein Clustermodell, sondern ein generatives probabilistisches Modell
das die Verteilung der Daten beschreibt.

Betrachten Sie als Beispiel einige Daten, die mit der Funktion make_moons von Scikit-Learn
erzeugt wurden, die in Kapitel 47 vorgestellt wurde (siehe Abbildung 48-9).

520 | Kapitel 48: Vertiefung: Gaußsche Mischungsmodelle

In [13]: from sklearn.datasets import make_moons
Xmoon, ymoon = make_moons(200, noise=.05, random_state=0)
plt.scatter(XMond[:, 0], XMond[:, 1]);

Abbildung 48-9. GMM angewendet auf Cluster mit nichtlinearen Grenzen

Wenn wir versuchen, dies mit einem Zweikomponenten-GMM, das als Clustermodell betrachtet wird, zu erfassen, wird die

sind die Ergebnisse nicht besonders nützlich (siehe Abbildung 48-10).

In [14]: gmm2 = GaussianMixture(n_components=2, covariance_type='full',
random_state=0)
plot_gmm(gmm2, Xmoon)

Abbildung 48-10. Zweikomponenten-GMM-Anpassung an nichtlineare Cluster

Gaußsche Mischungsmodelle als Dichteschätzung | 521
Wenn wir jedoch stattdessen viel mehr Komponenten verwenden und die Clusterbeschriftungen ignorieren, finden wir eine

Fit, der viel näher an den Eingabedaten liegt (siehe Abbildung 48-11).

In [15]: gmm16 = GaussianMixture(n_components=16, covariance_type='full',
random_state=0)
plot_gmm(gmm16, Xmoon, label= False )

Abbildung 48-11. Verwendung von vielen GMM-Clustern zur Modellierung der Verteilung von Punkten

Hier dient die Mischung aus 16 Gauß-Komponenten nicht dazu, einzelne Datencluster zu finden
Daten zu finden, sondern vielmehr die Gesamtverteilung der Eingabedaten zu modellieren. Dies ist eine generative

Modell der Verteilung, d. h. das GMM gibt uns das Rezept zur Erzeugung neuer
Zufallsdaten zu generieren, die ähnlich wie unsere Eingabe verteilt sind. Zum Beispiel, hier sind 400 neue Punkte

aus dieser 16-Komponenten-GMM-Anpassung an unsere Originaldaten (siehe Abbildung 48-12).

In [16]: Xnew, ynew = gmm16.sample(400)
plt.scatter(Xnew[:, 0], Xnew[:, 1]);

522 | Kapitel 48: Vertiefung: Gaußsche Mischungsmodelle

Abbildung 48-12. Neue Daten aus dem 16-Komponenten-GMM

Ein GMM eignet sich als flexibles Mittel zur Modellierung einer beliebigen mehrdimensionalen
Verteilung von Daten.

Die Tatsache, dass ein GMM ein generatives Modell ist, gibt uns ein natürliches Mittel zur Bestimmung

die optimale Anzahl von Komponenten für einen bestimmten Datensatz. Ein generatives Modell ist inhärent-
inhärent eine Wahrscheinlichkeitsverteilung für den Datensatz, und so können wir einfach die wie-

Likelihood der Daten unter dem Modell, unter Verwendung von Kreuzvalidierung, um eine Überanpassung zu vermeiden.
Eine weitere Möglichkeit zur Korrektur der Überanpassung ist die Anpassung der Modellwahrscheinlichkeiten mit

ein analytisches Kriterium wie das Akaike-Informationskriterium (AIC) oder das Baye-

sianisches Informationskriterium (BIC). Der GaussianMixture-Schätzer von Scikit-Learn ist eigentlich

enthält integrierte Methoden, die beides berechnen, so dass es sehr einfach ist, mit diesem Ansatz zu arbeiten.
diesen Ansatz zu verwenden.

Betrachten wir den AIC und BIC in Abhängigkeit von der Anzahl der GMM-Komponenten für unsere Monde

Datensatz (siehe Abbildung 48-13).

In [17]: n_components = np.arange(1, 21)
models = [GaussianMixture(n, covariance_type='full',
random_state=0).fit(Xmoon)
for n in n_components]

plt.plot(n_components, [m.bic(Xmoon) for m in models], label='BIC')
plt.plot(n_components, [m.aic(Xmoon) for m in models], label='AIC')
plt.legend(loc='best')
plt.xlabel('n_components');

Gaußsche Mischungsmodelle als Dichteschätzung | 523
Abbildung 48-13. Visualisierung von AIC und BIC für die Auswahl der Anzahl von GMM

Komponenten

Die optimale Anzahl von Clustern ist der Wert, der den AIC oder BIC minimiert, je nachdem, welche Approximation wir verwenden wollen.
je nachdem, welche Annäherung wir verwenden möchten. Der AIC sagt uns, dass unsere Wahl von 16

Komponenten war wahrscheinlich zu viel: etwa 8-12 Komponenten wären besser gewesen.
eine bessere Wahl gewesen. Wie bei dieser Art von Problemen üblich, empfiehlt das BIC eine

einfacheres Modell.

Beachten Sie den wichtigen Punkt: Diese Wahl der Anzahl der Komponenten misst, wie gut
ein GMM als Dichteschätzer funktioniert, nicht wie gut es als Clusteralgorithmus funktioniert.

Ich möchte Sie ermutigen, das GMM in erster Linie als Dichteschätzer zu betrachten und es für
Clustering nur dann zu verwenden, wenn dies bei einfachen Datensätzen gerechtfertigt ist.

Beispiel: GMMs zur Generierung neuer Daten
Wir haben soeben ein einfaches Beispiel für die Verwendung eines GMMs als generatives Modell gesehen, um neue Daten zu erstellen.

neue Stichproben aus der durch die Eingabedaten definierten Verteilung ziehen. Hier werden wir ausführen

mit dieser Idee und generieren neue handgeschriebene Ziffern aus dem Standard-Ziffernkorpus
die wir zuvor verwendet haben.

Zunächst laden wir die Zifferndaten mit den Datenwerkzeugen von Scikit-Learn:

In [18]: from sklearn.datasets import load_digits
Ziffern = load_digits()
ziffern.daten.form
Out[18]: (1797, 64)

524 | Kapitel 48: Vertiefung: Gaußsche Mischungsmodelle

Als Nächstes zeichnen wir die ersten 50 davon auf, um uns genau zu vergegenwärtigen, was wir hier betrachten (siehe

Abbildung 48-14).

In [19]: def plot_digits(data):
fig, ax = plt.subplots(5, 10, figsize=(8, 4),
subplot_kw=dict(xticks=[], yticks=[]))
fig.subplots_adjust(hspace=0.05, wspace=0.05)
for i, axi in enumerate(ax.flat):
im = axi.imshow(data[i].reshape(8, 8), cmap='binary')
im.set_clim(0, 16)
plot_digits(ziffern.daten)

Abbildung 48-14. Eingabe von handgeschriebenen Ziffern

Wir haben fast 1.800 Ziffern in 64 Dimensionen, und wir können ein GMM darauf aufbauen
aufbauen, um mehr zu generieren. GMMs können bei einer so hohen Anzahl von Ziffern nur schwer konvergieren.

dimensionalen Raum, so dass wir mit einem Algorithmus zur invertierbaren Dimensionalitätsreduktion
rithmus für die Daten. Hier werden wir eine einfache PCA verwenden, die 99% erhalten soll

der Varianz in den projizierten Daten:

In [20]: from sklearn.decomposition import PCA
pca = PCA(0.99, whiten= True )
Daten = pca.fit_transform(Ziffern.Daten)
daten.form
Out[20]: (1797, 41)

Das Ergebnis sind 41 Dimensionen, eine Reduzierung um fast 1/3 ohne Informationsverlust.
Verlust. In Anbetracht dieser hochgerechneten Daten können wir den AIC verwenden, um ein Maß für die Anzahl der

GMM-Komponenten, die wir verwenden sollten (siehe Abbildung 48-15).

In [21]: n_components = np.arange(50, 210, 10)
models = [GaussianMixture(n, covariance_type='full', random_state=0)
for n in n_components]

Beispiel: GMMs zur Generierung neuer Daten | 525
aics = [model.fit(data).aic(data) for model in models]
plt.plot(n_components, aics);

Abbildung 48-15. AIC-Kurve für die Auswahl der geeigneten Anzahl von GMM-Komponenten

Es scheint, dass etwa 140 Komponenten das AIC minimieren; wir werden dieses Modell verwenden.

Passen wir dies schnell an die Daten an und bestätigen wir, dass es konvergiert hat:

In [22]: gmm = GaussianMixture(140, covariance_type='full', random_state=0)
gmm.fit(Daten)
print(gmm.converged_)
Out[22]: True

Jetzt können wir Stichproben von 100 neuen Punkten innerhalb dieser 41-dimensionalen Projektion ziehen

Raum, unter Verwendung des GMM als generatives Modell:

In [23]: data_new, label_new = gmm.sample(100)
daten_neu.form
Out[23]: (100, 41)

Schließlich können wir die inverse Transformation des PCA-Objekts verwenden, um das neue dig-

sein (siehe Abbildung 48-16).

In [24]: digits_new = pca.inverse_transform(data_new)
plot_digits(ziffern_neu)

526 | Kapitel 48: Vertiefung: Gaußsche Mischungsmodelle

Abbildung 48-16. "Neue" Ziffern, die nach dem Zufallsprinzip aus dem zugrunde liegenden Modell des GMM gezogen werden

Schätzer

Die Ergebnisse sehen größtenteils nach plausiblen Zahlen aus dem Datensatz aus!

Betrachten wir, was wir hier getan haben: Bei einer Stichprobe von handgeschriebenen Ziffern haben wir

die Verteilung dieser Daten so modelliert, dass wir ganz neue Ziffernproben aus den Daten erzeugen können
Ziffernstichproben aus den Daten generieren können: Es handelt sich um "handgeschriebene Ziffern", die nicht individualisiert sind.

im Originaldatensatz vorkommen, sondern vielmehr die allgemeinen Merkmale der Eingabedaten
Daten, wie sie durch das Mischungsmodell modelliert werden. Ein solches generatives Modell von Ziffern kann beweisen

sehr nützlich als Komponente eines generativen Bayes'schen Klassifikators, wie wir in den folgenden Abschnitten sehen werden

nächstes Kapitel.

Beispiel: GMMs zur Generierung neuer Daten | 527
KAPITEL 49

Vertiefung: Kernel-Dichte-Schätzung
In Kapitel 48 haben wir uns mit Gaußschen Mischmodellen beschäftigt, die eine Art Hybrid
zwischen einem Clustering-Schätzer und einem Dichte-Schätzer sind. Erinnern Sie sich, dass ein Dichte-Schätzer

ist ein Algorithmus, der aus einem D-dimensionalen Datensatz eine Schätzung der D-

dimensionalen Wahrscheinlichkeitsverteilung, aus der die Daten gezogen werden. Der GMM-Algorithmus
erreicht dies, indem er die Dichte als eine gewichtete Summe von Gauß-Verteilungen darstellt.

tionen. Die Kernel-Dichte-Schätzung (KDE) ist in gewissem Sinne ein Algorithmus, der die
Idee der Gaußschen Mischung auf die Spitze treibt: Er verwendet eine Mischung, die aus einer

Gaußkomponente pro Punkt, was zu einem im Wesentlichen nichtparametrischen Schätzer für

Dichte. In diesem Kapitel werden wir die Motivation und den Nutzen von KDE untersuchen.

Wir beginnen mit den Standardimporten:

In [1]: % matplotlib inline
import matplotlib.pyplot as plt
plt.style.use('seaborn-whitegrid')
import numpy as np

Motivierende Kernel-Dichte-Schätzung: Histogramme
Wie bereits erwähnt, ist ein Dichte-Schätzer ein Algorithmus, der versucht, die

Wahrscheinlichkeitsverteilung, die einen Datensatz erzeugt hat. Bei eindimensionalen Daten sind Sie
wahrscheinlich bereits mit einem einfachen Dichteschätzer vertraut: dem Histogramm. Ein Histo-

gram unterteilt die Daten in diskrete Bins, zählt die Anzahl der Punkte, die in jedes Bin fallen
Bins fallen, und visualisiert die Ergebnisse auf intuitive Weise.

528
Erstellen wir zum Beispiel Daten, die aus zwei Normalverteilungen stammen:

In [2]: def make_data(N, f=0.3, rseed=1):
rand = np.random.RandomState(rseed)
x = rand.randn(N)
x[int(f * N):] += 5
Rückgabe x

x = make_data(1000)

Wir haben bereits gesehen, dass das standardmäßige zählungsbasierte Histogramm mit folgender Methode erstellt werden kann

die Funktion plt.hist. Durch die Angabe des Dichteparameters des Histogramms beenden wir

mit einem normalisierten Histogramm, bei dem die Höhe der Bins nicht die Zählungen widerspiegelt,

sondern spiegelt stattdessen die Wahrscheinlichkeitsdichte wider (siehe Abbildung 49-1).

In [3]: hist = plt.hist(x, bins=30, density= True )

Abbildung 49-1. Daten, die aus einer Kombination von Normalverteilungen stammen

Beachten Sie, dass diese Normalisierung bei gleichem Binning lediglich die Skala auf der y-
Achse, wobei die relativen Höhen im Wesentlichen dieselben bleiben wie in einem Histogramm, das aus

zählt. Diese Normalisierung wird so gewählt, dass die Gesamtfläche unter dem Histogramm

gleich 1, wie wir durch einen Blick auf die Ausgabe der Histogramm-Funktion bestätigen können:

In [4]: Dichte, Bins, Patches = hist
widths = bins[1:] - bins[:-1]
(density * widths).sum()
Out[4]: 1.0

Motivierende Kernel-Dichte-Schätzung: Histogramme | 529
Eines der Probleme bei der Verwendung eines Histogramms als Dichteschätzer ist, dass die Wahl der

Größe und Lage des Platzes können zu Darstellungen führen, die qualitativ unterschiedliche Merkmale aufweisen.
turen haben. Betrachten wir zum Beispiel eine Version dieser Daten mit nur 20 Punkten, so ist die Wahl

wie die Bins zu zeichnen sind, kann zu einer völlig anderen Interpretation der Daten führen!

Betrachten Sie dieses Beispiel, das in Abbildung 49-2 dargestellt ist.

In [5]: x = make_data(20)
bins = np.linspace(-5, 10, 10)

In [6]: fig, ax = plt.subplots(1, 2, figsize=(12, 4),
sharex= True , sharey= True ,
subplot_kw={'xlim':(-4, 9),
'ylim':(-0.02, 0.3)})
fig.subplots_adjust(wspace=0.05)
for i, offset in enumerate([0.0, 0.6]):
ax[i].hist(x, bins=bins + offset, density= True )
ax[i].plot(x, np.full_like(x, -0.01), '|k',
markeredgewidth=1)

Abbildung 49-2. Das Problem mit Histogrammen: Die Lage der Felder kann die Interpretation beeinflussen

Das Histogramm auf der linken Seite verdeutlicht, dass es sich um eine bimodale Verteilung handelt. Auf der

rechts, sehen wir eine unimodale Verteilung mit einem langen Schwanz. Ohne das vorhergehende zu sehen

Code, würden Sie wahrscheinlich nicht vermuten, dass diese beiden Histogramme aus denselben
gleichen Daten erstellt wurden. Wie können Sie also der Intuition vertrauen, die Ihnen Histogramme vermitteln?

Und wie können wir das verbessern?

Wir können uns ein Histogramm als einen Stapel von Blöcken vorstellen, bei dem wir einen

Block in jedem Bin über jedem Punkt des Datensatzes. Lassen Sie uns dies direkt betrachten (siehe

Abbildung 49-3).

In [7]: fig, ax = plt.subplots()
bins = np.arange(-3, 8)
ax.plot(x, np.full_like(x, -0.1), '|k',
markeredgewidth=1)
for count, edge in zip(*np.histogram(x, bins)):
for i in range(count):
ax.add_patch(plt.Rectangle(

530 | Kapitel 49: Vertiefung: Kernel-Dichte-Schätzung

(edge, i), 1, 1, ec='black', alpha=0.5))
ax.set_xlim(-4, 8)
ax.set_ylim(-0.2, 8)
Out[7]: (-0.2, 8.0)

Abbildung 49-3. Histogramm als Stapel von Blöcken

Das Problem mit unseren beiden Binnings ergibt sich aus der Tatsache, dass die Höhe des Blocks

Stapel spiegelt oft nicht die tatsächliche Dichte der Punkte in der Nähe wider, sondern die Übereinstimmung der
wie die Bins mit den Datenpunkten ausgerichtet sind. Diese Fehlausrichtung zwischen Punkten und ihren

Blöcke sind eine mögliche Ursache für die schlechten Histogrammergebnisse, die hier zu sehen sind. Was aber, wenn stattdessen

wir die Blöcke nicht nach den Fächern stapeln, sondern nach den
den Punkten, die sie darstellen? Wenn wir dies tun, werden die Blöcke nicht ausgerichtet, aber wir können Folgendes hinzufügen

ihre Beiträge an jeder Stelle entlang der x-Achse, um das Ergebnis zu ermitteln. Versuchen wir dies
(siehe Abbildung 49-4).

In [8]: x_d = np.linspace(-4, 8, 2000)
Dichte = Summe((abs(xi - x_d) < 0.5) for xi in x)

plt.fill_between(x_d, density, alpha=0.5)
plt.plot(x, np.full_like(x, -0.1), '|k', markeredgewidth=1)

plt.axis([-4, 8, -0.2, 8]);

Motivierende Kernel-Dichte-Schätzung: Histogramme | 531
Abbildung 49-4. Ein "Histogramm", bei dem die Blöcke auf jeden einzelnen Punkt zentriert sind; dies ist ein
Beispiel für eine Kernel-Dichte-Schätzung

Das Ergebnis sieht etwas unübersichtlich aus, spiegelt aber die tatsächlichen Daten wesentlich besser wider

Eigenschaften als das Standardhistogramm. Dennoch sind die rauen Kanten weder ästhetisch
Sie sind weder ästhetisch ansprechend noch spiegeln sie die tatsächlichen Eigenschaften der Daten wider. Um die

zu glätten, könnten wir beschließen, die Blöcke an jeder Stelle durch eine
glatte Funktion zu ersetzen, z. B. eine Gauß-Kurve. Verwenden wir eine Standard-Normalkurve an jedem Punkt

anstelle eines Blocks (siehe Abbildung 49-5).

In [9]: from scipy.stats import norm
x_d = np.linspace(-4, 8, 1000)
density = sum(norm(xi).pdf(x_d) for xi in x)

plt.fill_between(x_d, density, alpha=0.5)
plt.plot(x, np.full_like(x, -0.1), '|k', markeredgewidth=1)

plt.axis([-4, 8, -0,2, 5]);

532 | Kapitel 49: Vertiefung: Kernel-Dichte-Schätzung

Abbildung 49-5. Eine Kernel-Dichte-Schätzung mit einem Gauß-Kernel

Diese geglättete Darstellung mit einer Gaußschen Verteilung an der Stelle

jedem Eingabepunkt, gibt eine viel genauere Vorstellung von der Form der Datenverteilung.

und eine, die eine viel geringere Varianz aufweist (d. h., die sich als Reaktion auf Unterschiede in der Probenahme viel weniger verändert).
Unterschiede bei der Probenahme).

In den letzten beiden Diagrammen sind wir bei der so genannten Kernel-Dichte-Schätzung in einer Dimension gelandet.
einer Dimension: Wir haben einen "Kernel" - einen quadratischen oder hutförmigen Kernel - in die

einen Gauß-Kernel an der Stelle eines jeden Punktes, und verwendeten ihre

Summe als eine Schätzung der Dichte. Mit dieser Intuition im Hinterkopf werden wir nun die Kernel
Dichteschätzung im Detail untersuchen.

Kernel-Dichte-Schätzung in der Praxis
Die freien Parameter der Kernel-Dichte-Schätzung sind der Kernel, der die

Form der Verteilung an jedem Punkt und die Kernelbandbreite, die die Größe des
die die Größe des Kernels an jedem Punkt festlegt. In der Praxis gibt es viele Kernel, die man

für die Kernel-Dichte-Schätzung: insbesondere die KDE-Implementierung von Scikit-Learn

unterstützt sechs Kernel, über die Sie im Abschnitt "Dichteschätzung" der
der Dokumentation nachlesen können.

Es gibt zwar mehrere Versionen von KDE, die in Python implementiert sind (vor allem in der SciPy

und statsmodels-Pakete), bevorzuge ich die Version von Scikit-Learn wegen ihrer effi-

und Flexibilität. Sie ist in der Sklearn.neighbors.KernelDensity implementiert

Schätzer, der die KDE in mehreren Dimensionen mit einem von sechs Kerneln und
einer von mehreren Dutzend Abstandsmetriken. Da KDE ziemlich rechenaufwendig sein kann

Kernel-Dichte-Schätzung in der Praxis | 533
intensiv, verwendet der Scikit-Learn-Schätzer einen baumbasierten Algorithmus unter der Haube und

kann die Berechnungszeit gegen die Genauigkeit abgewogen werden, indem die Parameter atol (absolute Toleranz) und

rtol (relative Toleranz) Parameter. Die Kernelbandbreite kann mit den
Scikit-Learn's Standard-Kreuzvalidierungstools bestimmt werden, wie wir gleich sehen werden.

Lassen Sie uns zunächst ein einfaches Beispiel für die Replikation des vorherigen Plots unter Verwendung der Scikit-

Lernen Sie den KernelDensity-Schätzer (siehe Abbildung 49-6).

In [10]: from sklearn.neighbors import KernelDensity

# Instanziierung und Anpassung des KDE-Modells
kde = KernelDensity(bandwidth=1.0, kernel='gaussian')
kde.fit(x[:, None ])

# score_samples liefert den Logarithmus der Wahrscheinlichkeitsdichte
logprob = kde.score_samples(x_d[:, None ])

plt.fill_between(x_d, np.exp(logprob), alpha=0.5)
plt.plot(x, np.full_like(x, -0.01), '|k', markeredgewidth=1)
plt.ylim(-0.02, 0.22);

Abbildung 49-6. Eine mit Scikit-Learn berechnete Kernel-Dichte-Schätzung

Das Ergebnis wird hier so normiert, dass die Fläche unter der Kurve gleich 1 ist.

534 | Kapitel 49: Vertiefung: Kernel-Dichte-Schätzung

Auswahl der Bandbreite durch Kreuzvalidierung
Die durch ein KDE-Verfahren erzeugte endgültige Schätzung kann sehr empfindlich auf die Wahl der

der Bandbreite, die den Kompromiss zwischen Vorspannung und Varianz bei der Schätzung der Dichte steuert.
mate der Dichte. Eine zu enge Bandbreite führt zu einer Schätzung mit hoher Varianz (d. h. zu viel

Anpassung), wo das Vorhandensein oder Fehlen eines einzigen Punktes einen großen Unterschied macht. Eine zu
eine zu große Bandbreite führt zu einer Schätzung mit hoher Verzerrung (d. h. Underfitting), bei der die Struktur

in den Daten wird durch den breiten Kernel verwischt.

In der Statistik gibt es eine lange Geschichte von Methoden zur schnellen Schätzung der besten Bandbreite
basierend auf ziemlich strengen Annahmen über die Daten: Wenn Sie die KDE-Implementierung nachschlagen

In den SciPy- und Statsmodels-Paketen finden Sie zum Beispiel Implementierungen

auf der Grundlage einiger dieser Regeln.

Im Zusammenhang mit maschinellem Lernen haben wir gesehen, dass eine solche Abstimmung der Hyperparameter oft
empirisch über einen Kreuzvalidierungsansatz erfolgt. Vor diesem Hintergrund ist die Scikit-Learn

KernelDensity-Schätzer ist so konzipiert, dass er direkt in der

Standardwerkzeuge für die Rastersuche des Pakets. Hier werden wir GridSearchCV verwenden, um die

Bandbreite für den vorangegangenen Datensatz. Weil es sich um einen so kleinen Datensatz handelt,

verwenden wir die Leave-One-Out-Kreuzvalidierung, die die Reduzierung der Trainingsmenge
Größe der Trainingsmenge für jeden Kreuzvalidierungsversuch minimiert:

In [11]: from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import LeaveOneOut

Bandbreiten = 10 ** np.linspace(-1, 1, 100)
grid = GridSearchCV(KernelDensity(kernel='gaussian'),
{'bandwidth': bandwidths},
cv=LeaveOneOut())
grid.fit(x[:, None ]);

Jetzt können wir die Bandbreite wählen, die die Punktzahl maximiert (die in diesem Fall

Fall wird standardmäßig die Log-Wahrscheinlichkeit verwendet):

In [12]: grid.best_params_
Out[12]: {'bandwidth': 1.1233240329780276}

Die optimale Bandbreite liegt sehr nahe an der Bandbreite, die wir in der Beispielgrafik verwendet haben

früher, wo die Bandbreite 1,0 war (d. h. die Standardbreite von scipy.stats.norm).

Beispiel: Nicht ganz so Naive Bayes
Dieses Beispiel befasst sich mit der generativen Bayes'schen Klassifikation mit KDE und demonstriert

wie man die Scikit-Learn-Architektur nutzt, um einen benutzerdefinierten Schätzer zu erstellen.

In Kapitel 41 haben wir uns mit der naiven Bayes'schen Klassifizierung beschäftigt, bei der wir eine einfache

generatives Modell für jede Klasse und verwenden diese Modelle, um einen schnellen Klassifikator zu erstellen. Für

Gaussian naive Bayes ist das generative Modell ein einfacher achsenorientierter Gaussian. Mit einer

Auswahl der Bandbreite durch Cross-Validierung | 535
Dichte-Schätzalgorithmus wie KDE, können wir das "naive" Element entfernen und per-

die gleiche Klassifizierung mit einem anspruchsvolleren generativen Modell für jede
Klasse. Es handelt sich immer noch um eine Bayes'sche Klassifizierung, aber nicht mehr um eine naive.

Der allgemeine Ansatz für die generative Klassifikation ist der folgende:

Teilen Sie die Trainingsdaten nach Label auf.
Passen Sie für jeden Satz eine KDE an, um ein generatives Modell der Daten zu erhalten. Dies ermöglicht Ihnen,
für jede Beobachtung x und jedes Label y eine Likelihood Px y zu berechnen.
Berechnen Sie aus der Anzahl der Beispiele für jede Klasse im Trainingssatz den Klassen
prior, Py.
Für einen unbekannten Punkt x ist die Posteriorwahrscheinlichkeit für jede Klasse
Py x ∝Px yPy. Die Klasse, die diesen Posteriorwert maximiert, ist das Label
die dem Punkt zugewiesen wird.
Der Algorithmus ist einfach und intuitiv zu verstehen; der schwierigere Teil

ist die Einbettung in den Scikit-Learn-Rahmen, um das Raster zu nutzen

Such- und Kreuzvalidierungsarchitektur.

Dies ist der Code, der den Algorithmus im Rahmen von Scikit-Learn implementiert;

werden wir im Anschluss an den Codeblock Schritt für Schritt durchgehen:

In [13]: from sklearn.base import BaseEstimator, ClassifierMixin

class KDEClassifier (BaseEstimator, ClassifierMixin):
"""Bayessche generative Klassifikation basierend auf KDE

_Parameter
bandwidth : float
die Kernelbandbreite innerhalb jeder Klasse
kernel : str
der Kernel-Name, der an KernelDensity übergeben wird
"""_
def init(self, bandwidth=1.0, kernel='gaussian'):
self.bandwidth = bandwidth
self.kernel = kernel

def fit(self, X, y):
self.classes_ = np.sort(np.unique(y))
training_sets = [X[y == yi] for yi in self.classes_]
self.models_ = [KernelDensity(bandwidth=self.bandwidth,
kernel=self.kernel).fit(Xi)
for Xi in training_sets]
self.logpriors_ = [np.log(Xi.shape[0] / X.shape[0])
for Xi in training_sets]
return self

536 | Kapitel 49: Vertiefung: Kernel-Dichte-Schätzung

def predict_proba(self, X):
logprobs = np.array([model.score_samples(X)
for model in self.models_]).T
Ergebnis = np.exp(logprobs + self.logpriors_)
return result / result.sum(axis=1, keepdims= True )

def predict(self, X):
return self.classes_[np.argmax(self.predict_proba(X), 1)]

Anatomie eines benutzerdefinierten Estimators
Gehen wir diesen Code Schritt für Schritt durch und besprechen wir die wichtigsten Funktionen:

from sklearn.base import BaseEstimator, ClassifierMixin
class KDEClassifier (BaseEstimator, ClassifierMixin):
"""Bayessche generative Klassifikation basierend auf KDE
Parameter
----------
bandwidth : float
die Kernelbandbreite innerhalb jeder Klasse
kernel : str
der Kernel-Name, der an KernelDensity übergeben wird
"""
Jeder Schätzer in Scikit-Learn ist eine Klasse, und es ist am praktischsten, wenn diese Klasse

von der Klasse BaseEstimator sowie dem entsprechenden Mixin erben, das die

bietet Standardfunktionen. Hier enthält der BaseEstimator zum Beispiel (neben

(u.a.) die Logik, die zum Klonen/Kopieren eines Schätzers zur Verwendung in einem Cross-

Validierungsverfahren, und ClassifierMixin definiert eine Standardbewertungsmethode, die von

solche Routinen. Wir stellen auch einen Docstring zur Verfügung, der von der Hilfefunktion von IPython
Funktionalität erfasst wird (siehe Kapitel 1).

Als Nächstes folgt die Initialisierungsmethode der Klasse:

def __init__(self, bandwidth=1.0, kernel='gaussian'):
self.bandwidth = bandwidth
self.kernel = kernel
Dies ist der eigentliche Code, der ausgeführt wird, wenn das Objekt mit

KDEClassifier. In Scikit-Learn ist es wichtig, dass die Initialisierung keine Operationen enthält.

tionen als die Zuweisung der übergebenen Werte an self mit Namen. Dies ist auf die Logik zurückzuführen

die in BaseEstimator enthalten sind und für das Klonen und Ändern von Schätzern für Cross-

Validierung, Gittersuche und andere Funktionen. In ähnlicher Weise werden alle Argumente für init

sollten explizit sein: d.h. *args oder **kwargs sollten vermieden werden, da sie nicht kor-

im Rahmen von Kreuzvalidierungsroutinen korrekt gehandhabt werden.

Beispiel: Not-so-Naive Bayes | 537
Als Nächstes folgt die Fit-Methode, bei der wir Trainingsdaten verarbeiten:

def fit(self, X, y):
self.classes_ = np.sort(np.unique(y))
training_sets = [X[y == yi] for yi in self.classes_]
self.models_ = [KernelDensity(bandwidth=self.bandwidth,
kernel=self.kernel).fit(Xi)
for Xi in training_sets]
self.logpriors_ = [np.log(Xi.shape[0] / X.shape[0])
for Xi in training_sets]
return self
Hier finden wir die eindeutigen Klassen in den Trainingsdaten, trainieren ein KernelDensity-Modell für
jede Klasse und berechnen die Klassenprioritäten auf der Grundlage der Anzahl der Eingabeproben.

Schließlich sollte fit immer self zurückgeben, damit wir Befehle verketten können. Zum Beispiel:

label = model.fit(X, y).predict(X)
Beachten Sie, dass jedes dauerhafte Ergebnis der Anpassung mit einem Unterstrich am Ende gespeichert wird (z. B.,

self.logpriors_). Dies ist eine Konvention, die in Scikit-Learn verwendet wird, damit Sie schnell
die Mitglieder eines Schätzers (mit IPythons Tabulatorvervollständigung) schnell durchsuchen und genau sehen

welche Mitglieder an die Trainingsdaten angepasst werden.

Schließlich haben wir die Logik für die Vorhersage von Etiketten auf neuen Daten:

def predict_proba(self, X):
logprobs = np.vstack([model.score_samples(X)
for model in self.models_]).T
Ergebnis = np.exp(logprobs + self.logpriors_)
return result / result.sum(axis=1, keepdims= True )
def predict(self, X):
return self.classes_[np.argmax(self.predict_proba(X), 1)]
Da es sich um einen probabilistischen Klassifikator handelt, implementieren wir zunächst predict_proba, das

gibt ein Array von Klassenwahrscheinlichkeiten der Form [n_samples, n_classes] zurück. Eintrag [i,

j] dieses Arrays ist die posteriore Wahrscheinlichkeit, dass die Probe i zur Klasse j gehört.
die durch Multiplikation der Wahrscheinlichkeit mit dem Klassenprior und Normalisierung ermittelt wird.

Die Vorhersagemethode verwendet diese Wahrscheinlichkeiten und gibt einfach die Klasse mit der

größte Wahrscheinlichkeit.

538 | Kapitel 49: Vertiefung: Kernel-Dichte-Schätzung

Verwendung unseres benutzerdefinierten Schätzers
Probieren wir diesen benutzerdefinierten Schätzer an einem Problem aus, das wir schon einmal gesehen haben: die Klassifizierung von

handgeschriebene Ziffern. Hier werden wir die Ziffern laden und die Kreuzvalidierung berechnen

Bewertung für eine Reihe von Bandbreitenkandidaten unter Verwendung des GridSearchCV-Meta-Schätzers

(siehe auch Kapitel 39):

In [14]: from sklearn.datasets import load_digits
from sklearn.model_selection import GridSearchCV

Ziffern = load_digits()

grid = GridSearchCV(KDEClassifier(),
{'bandwidth': np.logspace(0, 2, 100)})
grid.fit(ziffern.daten, ziffern.ziel);

Als Nächstes können wir die Ergebnisse der Kreuzvalidierung als Funktion der Bandbreite darstellen (siehe
Abbildung 49-7).

In [15]: fig, ax = plt.subplots()
ax.semilogx(np.array(grid.cv_results_['param_bandwidth']),
grid.cv_results_['mean_test_score'])
ax.set(title='KDE Model Performance', ylim=(0, 1),
xlabel='Bandbreite', ylabel='Genauigkeit')
print(f'bester Parameter: {grid.best_params_}')
print(f'Genauigkeit = {grid.best_score_}')
Out[15]: bester Parameter: {'bandwidth': 6.135907273413174}
Genauigkeit = 0.9677298050139276

Abbildung 49-7. Validierungskurve für den KDE-basierten Bayes'schen Klassifikator

Beispiel: Nicht-so-Naive Bayes | 539
Dies zeigt, dass unser KDE-Klassifikator eine Kreuzvalidierungsgenauigkeit von über

96 %, verglichen mit etwa 80 % für den Naive-Bayes-Klassifikator:

In [16]: from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
cross_val_score(GaussianNB(), ziffern.daten, ziffern.ziel).mean()
Out[16]: 0.8069281956050759

Ein Vorteil eines solchen generativen Klassifikators ist die Interpretierbarkeit der Ergebnisse: für jede

unbekannten Probe erhalten wir nicht nur eine probabilistische Klassifizierung, sondern ein vollständiges Modell der
Verteilung der Punkte, mit denen wir sie vergleichen! Falls erwünscht, bietet dies eine intuitive Win-

die Gründe für eine bestimmte Klassifizierung zu ergründen, die Algorithmen wie SVMs und ran-

Domwälder neigen dazu, zu verdunkeln.

Wenn Sie dies weiter ausbauen möchten, finden Sie hier einige Ideen für Verbesserungen, die

an unserem KDE-Klassifikatormodell vorgenommen werden können:

Sie könnten die Bandbreite in jeder Klasse unabhängig voneinander variieren lassen.
Sie könnten diese Bandbreiten nicht auf der Grundlage ihrer Vorhersageergebnisse optimieren, sondern auf der Grundlage
der Wahrscheinlichkeit der Trainingsdaten unter dem generativen Modell innerhalb jeder Klasse
(d. h. Sie verwenden die Werte von KernelDensity selbst und nicht die globale Vorhersage
Genauigkeit).
Wenn Sie schließlich etwas Übung beim Erstellen eines eigenen Schätzers haben möchten, können Sie Folgendes in Angriff nehmen

Erstellung eines ähnlichen Bayes'schen Klassifikators unter Verwendung von Gauß'schen Mischmodellen anstelle von KDE.

540 | Kapitel 49: Vertiefung: Kernel-Dichte-Schätzung

KAPITEL 50

Anwendung: Eine Pipeline zur Gesichtserkennung
In diesem Teil des Buches wurden einige der zentralen Konzepte und Algorithmen
des maschinellen Lernens. Doch der Übergang von diesen Konzepten zu einer realen Anwendung kann

eine Herausforderung sein. Reale Datensätze sind verrauscht und heterogen; sie können Fehler aufweisen.

und die Daten können in einer Form vorliegen, die sich nur schwer auf eine saubere [n_samples] abbilden lässt,

n_features]-Matrix. Bevor Sie eine der hier beschriebenen Methoden anwenden, müssen Sie

Zunächst müssen Sie diese Merkmale aus Ihren Daten extrahieren: Es gibt keine Formel dafür, wie Sie dies tun können.

gilt für alle Bereiche, und deshalb müssen Sie als Datenwissenschaftler
Ihre eigene Intuition und Ihr Fachwissen einsetzen.

Eine interessante und überzeugende Anwendung des maschinellen Lernens sind Bilder, und wir
Wir haben bereits einige Beispiele gesehen, bei denen Merkmale auf Pixelebene zur Klassifizierung von Bildern verwendet werden.

fikation. Auch hier gilt, dass die Daten in der realen Welt selten so einheitlich sind, und einfache Pixel werden nicht

geeignet: Dies hat zu einer umfangreichen Literatur über Methoden der Merkmalsextraktion für Bilddaten geführt
(siehe Kapitel 40).

In diesem Kapitel werfen wir einen Blick auf eine solche Technik der Merkmalsextraktion: die Histo-
gram der orientierten Gradienten (HOG), das Bildpixel in einen Vektor umwandelt, der die

Darstellung, die unabhängig von der Art des Bildes auf breit angelegte informative Bildmerkmale anspricht

störende Faktoren wie die Beleuchtung. Wir werden diese Merkmale nutzen, um eine einfache
eine einfache Gesichtserkennungspipeline entwickeln, die Algorithmen des maschinellen Lernens und Konzepte verwendet, die wir

in diesem Teil des Buches.

Wir beginnen mit den Standardimporten:

In [1]: % matplotlib inline
import matplotlib.pyplot as plt
plt.style.use('seaborn-whitegrid')
import numpy as np

541
HOG-Merkmale
HOG ist ein einfaches Verfahren zur Merkmalsextraktion, das in der

Kontext der Identifizierung von Fußgängern in Bildern. Es umfasst die folgenden Schritte:

Optional können Sie die Bilder pränormalisieren. Dies führt zu Merkmalen, die nicht von
Abhängigkeit von Beleuchtungsschwankungen widerstehen.
Falten Sie das Bild mit zwei Filtern, die empfindlich auf horizontale und vertikale
Helligkeitsgradienten reagieren. Dadurch werden Kanten-, Kontur- und Texturinformationen erfasst.
Unterteilen Sie das Bild in Zellen einer bestimmten Größe, und berechnen Sie ein Histogramm
der Gradientenausrichtungen innerhalb jeder Zelle.
Normalisieren Sie die Histogramme in jeder Zelle durch Vergleich mit dem Block der benachbarten
Zellen. Auf diese Weise werden die Auswirkungen der Beleuchtung auf das Bild weiter unterdrückt.
Konstruieren Sie einen eindimensionalen Merkmalsvektor aus den Informationen in jeder Zelle.
Ein schneller HOG-Extraktor ist in das Scikit-Image-Projekt integriert, und wir können ihn relativ schnell ausprobieren.
Wir können ihn relativ schnell ausprobieren und die orientierten Gradienten in jeder Zelle visualisieren (siehe Abbildung 50-1).

In [2]: from skimage import data, color, feature
import skimage.data

image = color.rgb2gray(data.chelsea())
hog_vec, hog_vis = feature.hog(image, visualize= True )

fig, ax = plt.subplots(1, 2, figsize=(12, 6),
subplot_kw=dict(xticks=[], yticks=[]))
ax[0].imshow(bild, cmap='grau')
ax[0].set_title('Eingabebild')

ax[1].imshow(hog_vis)
ax[1].set_title('Visualisierung von HOG-Merkmalen');

Abbildung 50-1. Visualisierung der aus einem Bild berechneten HOG-Merkmale

542 | Kapitel 50: Anwendung: Eine Gesichtserkennungs-Pipeline

HOG in Aktion: Ein einfacher Gesichtsdetektor
Mit diesen HOG-Merkmalen können wir einen einfachen Algorithmus zur Gesichtserkennung entwickeln, der

jeden Scikit-Learn-Schätzer; hier werden wir eine lineare Support-Vektor-Maschine verwenden (siehe
zurück zu Kapitel 43, wenn Sie eine Auffrischung benötigen). Die Schritte sind wie folgt:

Erhalten Sie eine Reihe von Miniaturbildern von Gesichtern, die als "positive" Trainings
Proben.
Erhalten Sie eine Reihe von Miniaturbildern von Nicht-Gesichtern, um "negative" Trainingsmuster
Proben.
Extrahieren Sie HOG-Merkmale aus diesen Trainingsmustern.
Trainieren Sie einen linearen SVM-Klassifikator auf diesen Proben.
Für ein "unbekanntes" Bild wird ein gleitendes Fenster über das Bild gelegt, wobei das
Modell zu bewerten, ob dieses Fenster ein Gesicht enthält oder nicht.
Wenn sich die Erkennungen überschneiden, kombinieren Sie sie zu einem einzigen Fenster.
Gehen wir diese Schritte durch und probieren sie aus.

1. Beschaffung einer Reihe von positiven Trainingsbeispielen
Wir beginnen damit, einige positive Trainingsbeispiele zu finden, die eine Vielzahl von Gesichtern zeigen. Wir
haben einen einfachen Datensatz, mit dem wir arbeiten können - den Datensatz "Labeled Faces in the Wild", der

kann von Scikit-Learn heruntergeladen werden:

In [3]: from sklearn.datasets import fetch_lfw_people
Gesichter = fetch_lfw_people()
positive_patches = faces.images
positive_patches.shape
Out[3]: (13233, 62, 47)

So erhalten wir eine Stichprobe von 13.000 Gesichtsbildern, die wir für das Training verwenden können.

2. Einen Satz negativer Trainingsbeispiele erhalten
Als Nächstes benötigen wir einen Satz ähnlich großer Miniaturbilder, die kein Gesicht enthalten. Eine
Weg, dies zu erhalten, ist, einen beliebigen Korpus von Eingabebildern zu nehmen und Thumbnails aus

in einer Vielzahl von Maßstäben. Hier werden wir einige der Bilder verwenden, die mit Scikit-

Image, zusammen mit dem PatchExtractor von Scikit-Learn:

In [4]: data.camera().shape
Out[4]: (512, 512)

In [5]: aus skimage Daten importieren, transformieren

imgs_to_use = ['Kamera', 'Text', 'Münzen', 'Mond',
'Seite', 'Uhr', 'Immunhistochemie',

HOG in Aktion: Ein einfacher Gesichtsdetektor | 543
'chelsea', 'coffee', 'hubble_deep_field']
raw_images = (getattr(data, name)() for name in imgs_to_use)
images = [color.rgb2gray(image) if image.ndim == 3 else image
for image in raw_images]

In [6]: from sklearn.feature_extraction.image import PatchExtractor

def extract_patches(img, N, scale=1.0, patch_size=positive_patches[0].shape):
extracted_patch_size = tuple((scale * np.array(patch_size)).astype(int))
extractor = PatchExtractor(patch_size=extracted_patch_size,
max_patches=N, random_state=0)
patches = extractor.transform(img[np.newaxis])
if scale != 1:
patches = np.array([transform.resize(patch, patch_size)
for patch in patches])
return patches

negative_patches = np.vstack([extract_patches(im, 1000, scale)
for im in images for scale in [0.5, 1.0, 2.0]])
negative_patches.shape
Out[6]: (30000, 62, 47)

Wir haben nun 30.000 geeignete Bildfelder, die keine Gesichter enthalten. Visualisieren wir eine

einige von ihnen, um eine Vorstellung davon zu bekommen, wie sie aussehen (siehe Abbildung 50-2).

In [7]: fig, ax = plt.subplots(6, 10)
for i, axi in enumerate(ax.flat):
axi.imshow(negative_patches[500 * i], cmap='gray')
axi.axis('off')

Wir hoffen, dass diese den Bereich der "Nicht-Gesichter" ausreichend abdecken, so dass unser Algo-

rithmus zu sehen sein wird.

544 | Kapitel 50: Anwendung: Eine Gesichtserkennungs-Pipeline

Abbildung 50-2. Negative Bildausschnitte, die keine Gesichter enthalten

3. Kombinieren von Gruppen und Extrahieren von HOG-Merkmalen
Nun, da wir diese positiven und negativen Proben haben, können wir sie kombinieren

und berechnen die HOG-Merkmale. Dieser Schritt nimmt etwas Zeit in Anspruch, da er eine nicht
nicht triviale Berechnung für jedes Bild:

In [8]: from itertools import chain
X_train = np.array([feature.hog(im)
for im in chain(positive_patches,
negative_Flecken)])
y_train = np.zeros(X_train.shape[0])
y_train[:positive_Flecken.shape[0]] = 1

In [9]: X_train.shape
Out[9]: (43233, 1215)

Es verbleiben 43.000 Trainingsmuster in 1.215 Dimensionen, und wir haben nun unsere
Daten in einer Form, die wir in Scikit-Learn einspeisen können!

HOG in Aktion: Ein einfacher Gesichtsdetektor | 545
4. Trainieren einer Support-Vektor-Maschine
Als Nächstes verwenden wir die Werkzeuge, die wir hier erforscht haben, um einen Klassifikator für Miniaturbilder zu erstellen

Flecken. Für eine solche hochdimensionale binäre Klassifizierungsaufgabe ist eine lineare Support-Vec-

tor-Maschine ist eine gute Wahl. Wir werden die LinearSVC von Scikit-Learn verwenden, weil sie im Kom-

Im Vergleich zu SVC hat es oft eine bessere Skalierung für eine große Anzahl von Proben.

Zunächst wollen wir jedoch einen einfachen Gauß-Naive-Bayes-Schätzer verwenden, um eine schnelle Grundlage zu erhalten:

In [10]: from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score

cross_val_score(GaussianNB(), X_train, y_train)
Out[10]: array([0.94795883, 0.97143518, 0.97224471, 0.97501735, 0.97374508])

Wir sehen, dass selbst ein einfacher naiver Bayes-Algorithmus bei unseren Trainingsdaten zu folgenden Ergebnissen führt

eine Genauigkeit von über 95 %. Versuchen wir es mit der Support-Vektor-Maschine, mit einer Rastersuche

über einige Auswahlmöglichkeiten für den Parameter C:

In [11]: from sklearn.svm import LinearSVC
from sklearn.model_selection import GridSearchCV
grid = GridSearchCV(LinearSVC(), {'C': [1.0, 2.0, 4.0, 8.0]})
grid.fit(X_train, y_train)
grid.best_score_
Out[11]: 0.9885272620319941

In [12]: grid.best_params_
Out[12]: {'C': 1.0}

Damit erreichen wir eine Genauigkeit von fast 99 %. Wir nehmen den besten Schätzer und trainieren ihn erneut auf

den vollständigen Datensatz:

In [13]: model = grid.best_estimator_
model.fit(X_train, y_train)
Out[13]: LinearSVC()

5. Gesichter in einem neuen Bild finden
Nachdem wir nun dieses Modell erstellt haben, nehmen wir ein neues Bild und sehen, wie das Modell

tut. Zur Vereinfachung wird ein Teil des in Abbildung 50-3 gezeigten Astronautenbildes verwendet.
der Einfachheit halber einen Teil des Astronautenbildes aus Abbildung 50-3 (siehe Diskussion im folgenden Abschnitt) und lassen ein gleitendes Fenster über

und bewerten jeden Patch:

In [14]: test_image = skimage.data.astronaut()
test_image = skimage.color.rgb2gray(test_image)
test_image = skimage.transform.rescale(test_image, 0.5)
test_image = test_image[:160, 40:180]

plt.imshow(test_image, cmap='gray')
plt.axis('off');

546 | Kapitel 50: Anwendung: Eine Gesichtserkennungs-Pipeline

Abbildung 50-3. Ein Bild, in dem wir versuchen werden, ein Gesicht zu lokalisieren

Als Nächstes erstellen wir ein Fenster, das über Bereiche dieses Bildes iteriert, und berechnen die HOG

Merkmale für jeden Patch:

In [15]: def sliding_window(img, patch_size=positive_patches[0].shape,
istep=2, jstep=2, scale=1.0):
Ni, Nj = (int(scale * s) for s in patch_size)
for i in range(0, img.shape[0] - Ni, istep):
for j in range(0, img.shape[1] - Ni, jstep):
patch = img[i:i + Ni, j:j + Nj]
if scale != 1:
patch = transform.resize(patch, patch_size)
yield (i, j), patch

indizes, patches = zip(*sliding_window(test_image))
patches_hog = np.array([feature.hog(patch) for patch in patches])
patches_hog.shape
Out[15]: (1911, 1215)

Schließlich können wir diese mit HOG-Merkmalen versehenen Flecken nehmen und unser Modell verwenden, um zu bewerten

ob jedes Feld ein Gesicht enthält:

In [16]: labels = model.predict(patches_hog)
labels.sum()
Out[16]: 48.0

Wir sehen, dass wir von fast 2.000 Patches 48 Entdeckungen gemacht haben. Verwenden wir die

Informationen, die wir über diese Flecken haben, um zu zeigen, wo sie in unserem Testbild liegen,
indem wir sie als Rechtecke zeichnen (siehe Abbildung 50-4).

In [17]: fig, ax = plt.subplots()
ax.imshow(test_image, cmap='gray')
ax.axis('off')

HOG in Aktion: Ein einfacher Gesichtsdetektor | 547
Ni, Nj = positive_Flächen[0].form
Indizes = np.array(Indizes)

for i, j in indices[labels == 1]:
ax.add_patch(plt.Rectangle((j, i), Nj, Ni, edgecolor='red',
alpha=0.3, lw=2, facecolor='none'))

Abbildung 50-4. Fenster, für die eine Fläche ermittelt wurde

Alle erkannten Flecken überschneiden sich und haben das Gesicht im Bild gefunden! Nicht schlecht für ein paar

Zeilen von Python.

Vorbehalte und Verbesserungen
Wenn Sie den vorangegangenen Code und die Beispiele etwas genauer betrachten, werden Sie feststellen, dass wir immer noch

haben noch ein wenig Arbeit vor uns, bevor wir einen produktionsreifen Gesichtsdetektor vorweisen können. Es gibt
gibt es einige Probleme mit dem, was wir gemacht haben, und einige Verbesserungen, die

gemacht. Im Besonderen:

Unser Trainingssatz, insbesondere für negative Merkmale, ist nicht sehr vollständig

Das Hauptproblem ist, dass es viele gesichtsähnliche Texturen gibt, die nicht in der Trainingsmenge enthalten sind.
Unser aktuelles Modell ist daher sehr anfällig für falsch-positive Ergebnisse. Sie können dies sehen
wenn Sie den Algorithmus auf dem kompletten Astronautenbild ausprobieren: Das aktuelle Modell führt
Das aktuelle Modell führt zu vielen falschen Erkennungen in anderen Regionen des Bildes.
Wir könnten uns vorstellen, dieses Problem zu beheben, indem wir eine größere Anzahl von Bildern zur negativen Trainingsmenge hinzufügen.
Trainingsmenge hinzufügen, was wahrscheinlich zu einer gewissen Verbesserung führen würde. Eine weitere
Option wäre die Verwendung eines gezielteren Ansatzes, wie z. B. das Hard Negative Mining,
bei dem wir einen neuen Satz von Bildern nehmen, die unser Klassifikator noch nicht gesehen hat, alle
Patches, die falsch-positive Ergebnisse darstellen, und fügen sie explizit als negative Instanzen
in die Trainingsmenge ein, bevor der Klassifikator neu trainiert wird.
548 | Kapitel 50: Anwendung: Eine Pipeline zur Gesichtserkennung

Unsere derzeitige Pipeline sucht nur in einem Maßstab

In der derzeitigen Fassung wird unser Algorithmus Gesichter übersehen, die nicht ungefähr 62
× 47 Pixel groß sind. Dies lässt sich leicht beheben, indem man gleitende Fenster mit
verschiedener Größen und die Größenanpassung jedes Patches mit skimage.transform.resize
angepasst wird, bevor es in das Modell eingespeist wird. Das hier verwendete Dienstprogramm sliding_window ist
bereits mit diesem Ziel vor Augen gebaut.
Wir sollten überlappende Erkennungsfelder kombinieren

Für eine produktionsreife Pipeline würden wir es vorziehen, nicht 30 Erkennungen desselben Gesichts zu haben
Gesicht zu haben, sondern überlappende Gruppen von Erkennungen irgendwie auf eine
einzelne Erkennung zu reduzieren. Dies könnte durch einen unüberwachten Clustering-Ansatz geschehen
(Mean Shift Clustering ist ein guter Kandidat dafür) oder über einen verfahrenstechnischen
Ansatz wie die nicht-maximale Unterdrückung, ein in der maschinellen
Bildverarbeitung.
Die Pipeline sollte gestrafft werden

Sobald wir die oben genannten Probleme angegangen sind, wäre es auch schön, eine
rationellere Pipeline für die Aufnahme von Trainingsbildern und die Vorhersage von Schiebefenstern
Ausgaben. Hier kann Python als Datenwissenschaftstool wirklich glänzen: Mit ein bisschen
Arbeit könnten wir unseren Prototyp-Code mit einer gut durchdachten objektorientierten
objektorientierten API verpacken, die dem Benutzer die Möglichkeit gibt, ihn einfach zu verwenden. Ich werde dies
als eine sprichwörtliche "Übung für den Leser".
Neuere Fortschritte: Deep Learning

Abschließend möchte ich noch hinzufügen, dass im Kontext des maschinellen Lernens HOG und andere Verfahren zur
Methoden zur Extraktion von Merkmalen nicht immer verwendet werden. Stattdessen verwenden viele moderne
Objektdetektionspipelines Varianten von tiefen neuronalen Netzen (oft als
Deep Learning): Man kann sich neuronale Netze als Schätzer vorstellen, die optimale
optimale Strategien zur Merkmalsextraktion aus den Daten ermitteln, statt sich auf die
die Intuition des Benutzers verlassen.
Obwohl das Feld in den letzten Jahren fantastische Ergebnisse hervorgebracht hat, ist Deep Learning
sich konzeptionell nicht allzu sehr von den in den vorangegangenen Kapiteln untersuchten
den vorangegangenen Kapiteln. Der wichtigste Fortschritt ist die Fähigkeit, moderne Computerhardware (oft große Cluster) zu nutzen.
Computerhardware (oft große Cluster leistungsstarker Maschinen) zu nutzen, um viel
flexiblere Modelle auf viel größeren Datenbeständen zu trainieren. Auch wenn der Umfang
unterschiedlich ist, ist das Endziel dasselbe: die Erstellung von Modellen aus Daten.
Wenn Sie an weiteren Informationen interessiert sind, finden Sie im folgenden Abschnitt eine Liste mit Referenzen

sollte ein nützlicher Startpunkt sein!

Vorbehalte und Verbesserungen | 549
Weitere Ressourcen zum maschinellen Lernen
Dieser Teil des Buches war ein kurzer Rundgang durch das maschinelle Lernen in Python, hauptsächlich

die Verwendung der Werkzeuge der Scikit-Learn-Bibliothek. So lang diese Kapitel auch sind, sie sind
noch zu kurz, um viele interessante und wichtige Algorithmen, Ansätze und

Diskussionen. Hier möchte ich einige Ressourcen empfehlen, um mehr über maschinelles
maschinelles Lernen in Python zu erfahren, für alle Interessierten:

Die Scikit-Learn-Website

Die Scikit-Learn-Website bietet eine beeindruckende Fülle von Dokumentationen und Beispielen
ples, die einige der hier besprochenen Modelle abdecken, und vieles, vieles mehr. Wenn Sie
einen kurzen Überblick über die wichtigsten und am häufigsten verwendeten Algorithmen für maschinelles Lernen
rithmen, ist dies ein guter Ausgangspunkt.
SciPy-, PyCon- und PyData-Lernvideos

Scikit-Learn und andere Themen des maschinellen Lernens sind Dauerbrenner in den
Tutorial-Tracks vieler auf Python ausgerichteter Konferenzreihen, insbesondere der
PyCon-, SciPy- und PyData-Konferenzen. Die meisten dieser Konferenzen veröffentlichen Videos
ihrer Keynotes, Vorträge und Tutorials kostenlos online, und Sie sollten in der Lage sein
Sie sollten diese leicht über eine geeignete Websuche finden können (z. B. "PyCon 2022 videos").
Einführung in maschinelles Lernen mit Python, von Andreas C. Müller und Sarah Guido
(O'Reilly)

Dieses Buch behandelt viele der Grundlagen des maschinellen Lernens, die in diesen
besprochenen Grundlagen des maschinellen Lernens ab, ist aber vor allem wegen der fortgeschrittenen Funktionen von
Scikit-Learn, einschließlich zusätzlicher Schätzer, Modellvalidierungsansätze und
Pipelining.
Maschinelles Lernen mit PyTorch und Scikit-Learn, von Sebastian Raschka (Packt)

Sebastian Raschkas neuestes Buch beginnt mit einigen der grundlegenden Themen
die in diesen Kapiteln behandelt werden, geht aber tiefer und zeigt, wie diese Konzepte
Konzepte auf anspruchsvollere und rechenintensive Deep-Learning- und Reinforcement-
und Reinforcement-Learning-Modelle unter Verwendung der bekannten PyTorch-Bibliothek.
550 | Kapitel 50: Anwendung: Eine Gesichtserkennungs-Pipeline

Index.
Symbole
& (kaufmännisches Und), 78

(Sternchen), 8
: (Doppelpunkt), 45
? (Fragezeichen), 5
?? (doppeltes Fragezeichen), 6
_ (Unterstrich) Abkürzung, 16
| (Operator), 78
A
Absolutwertfunktion, 55
aggregate()-Methode, 172
Aggregate
direkt vom Objekt berechnet, 58
mehrdimensional, 61
Zusammenfassen einer Menge von Werten mit, 63
Aggregation (NumPy), 60 -64
Minimum und Maximum, 61
Mehrdimensionale Aggregate, 61
Beispiel für die durchschnittliche Höhe eines Präsidenten, 63
Summierung der Werte in einem Array, 60
verschiedene Funktionen, 62
Aggregation (Pandas), 164 -175
groupby()-Operation, 167 -175
Planetendatensatz für, 165
einfache Aggregation, 165 -175
Akaike-Informationskriterium (AIC), 523 -524
Algorithmische Effizienz, Datensatzgröße und, 87
Altair, 351
kaufmännisches Und (&), 78
Anaconda, xxii
und Schlüsselwort, 78
Beschriftung von Diagrammen, 294 -301
Pfeile, 298 -301

Beispiel Feiertage/US-Geburten, 294 -296
Transformationen und Textposition, 296-298
APIs (siehe Estimator API)
append()-Methode, Pandas gegenüber Python, 150
apply()-Methode, 173
arithmetische Operatoren, 53
Arrays
Zugriff auf einzelne Zeilen/Spalten, 46
arithmetische Operatoren, 53
Attribute, 44
Grundlagen, 43 -50
Boolesche, 74 -78
Übertragen, 65 -71
Zentrierung, 70
Berechnung auf, 51 -59
Verkettung, 49 -50, 146
Kopien erstellen, 47
Erstellen aus Python-Listen, 39
von Grund auf neu erstellen, 40
Daten als, 33
DataFrame-Objekt als, 105
DataFrame-Objekt konstruiert aus, 107
fester Typ, 39
Index-Objekt als unveränderliches Array, 108
Index-Objekt gegenüber, 108
Indexierung: Zugriff auf einzelne Elemente, 44
Umformung, 48
Reihenobjekt gegenüber, 102
Slicing, 45 -48
Slicing von mehrdimensionalen Subarrays, 46
Slicing von eindimensionalen Subarrays, 45
Festlegen der Ausgabe an, 57
Aufteilen, 50
Standard-Datentypen, 41
551
strukturiert, 94 -98
Subarrays als kopierfreie Ansichten, 47
Summieren von Werten in, 60
Universelle Funktionen, 51 -59
Pfeile, 298 -301
asfreq()-Methode, 203 -205
Sternchen (*), 8
%automagic, 21
Automagische Funktion, 21
Achsengrenzen, 238 -240

B
Bagging, 456
Bandbreite (siehe Kernelbandbreite)
Balken (|) Operator, 78
Balkendiagramme, 340
Basisfunktionsregression, 407 , 422-425
Gaußsche Basisfunktionen, 424 -425
Polynomielle Basisfunktionen, 422
Bayes-Theorem, 410
Bayes'sche Klassifizierung, 410 , 535-540
Bayessches Informationskriterium (BIC), 523
Stylesheet "Bayes'sche Methoden für Hacker", 318
Bias-Varianz-Ausgleich
Kernel-Bandbreite und, 535
Modellauswahl und, 389 -391
Vorhersage des Fahrradverkehrs
lineare Regression, 429 -434
Zeitreihen, 208 -214
binäre ufuncs, 53
Binnings, 264
Bitweise logische Operatoren, 76
Bokeh, 351
Boolesche Arrays
Boolesche Operatoren und, 76
Zählen von Einträgen in, 75
Boolesche Masken, 72-79
Boolesche Arrays als, 77
Niederschlagsstatistik, 72
Arbeiten mit booleschen Arrays, 77
Boolesche Operatoren, 76
Übertragen, 65 -71
Hinzufügen von zweidimensionalen Arrays zu eindimensionalen
eindimensionalen Array, 68
Grundlagen, 65 -66
Zentrieren eines Arrays, 70
Definieren, 59 , 65
Zeichnen einer zweidimensionalen Funktion, 71
in der Praxis, 70

Regeln, 67 -70
zwei kompatible Arrays, 68
zwei inkompatible Felder, 69
C
kategorische Daten, 402
kategorische Diagramme, 338
Klassenbeschriftungen (für Datenpunkte), 356
Klassifizierungsaufgabe
definiert, 356
Maschinelles Lernen, 356 -359
Clustering, 356
Grundlagen, 363 -364
Gaußsche Mischmodelle, 377 , 512-527
k-means, 363 , 496-511
Code
magische Befehle zur Bestimmung der Ausführungs-
Ausführungszeit, 14
magische Befehle für die Ausführung externer, 13
Profilerstellung und Zeitmessung, 26 -31
Zeitmessung von Snippets, 27
Bestimmungskoeffizient, 390
Doppelpunkt (:), 45
Farbkomprimierung, 507 -511
Farbbalken,
Farbgrenzen und Erweiterungen, 280
Farbkartenauswahl, 277 -282
Anpassen, 276 -284
diskret, 281
Beispiel für handgeschriebene Ziffern, 282 -284
Farbkarte, 277 -282
Spalte(n)
Zugriff auf einzelne, 46
Indizierung, 169
MultiIndex für, 138
Sortieren von Arrays entlang, 89
Suffixe Schlüsselwort und überlappende Namen,
158
Spaltenweise Operationen, 218 -220
Befehlshistorienkürzel, 10
Vergleichsoperatoren, 73-74
Verkettung
Arrays, 49 -50, 146
mit pd.concat(), 147 -150
Konfusionsmatrix, 381
Konturdiagramme, 255 -266
Dichte und, 255 -266
dreidimensionale Funktion, 255 -260
dreidimensionale Darstellung, 323 -324
552 | Index

Conway, Drew, xix
Kreuzvalidierung, 386 -401

D
Stylesheet für dunklen Hintergrund, 319
Daten
als Arrays, 33
fehlende Daten (siehe fehlende Daten)
Datendarstellung (Scikit-Learn-Paket),
367 -370
Daten als Tabelle, 367
Merkmalsmatrix, 368
Ziel-Array, 368 -370
Datenwissenschaft, Definition, xix
Datentypen, 35 -41
Arrays vom festen Typ, 39
Ganzzahlen, 36
Listen, 37 -39
NumPy, 41
DataFrame-Objekt (Pandas), 104 -107
Konstruieren, 106
Datenauswahl in, 113
definiert, 99
als Wörterbuch, 113
als verallgemeinertes NumPy-Array, 105
Indexausrichtung in, 120
Maskierung, 117
mehrfach indiziert, 140
Operationen zwischen Serienobjekt und, 121
Zerlegen, 116
als spezialisiertes Wörterbuch, 106
als zweidimensionales Array, 115 -116
DataFrame.eval()-Methode, 218 -220
Zuweisung in, 219
lokale Variablen in, 219
DataFrame.query()-Methode, 220
Datensätze
Anhängen, 150
Kombinieren, 145 -150
Zusammenführen/Verbinden, 151 -163
datetime-Modul, 195
datetime64 dtype, 196 -197
datutil-Baustein, 195
%debug, 24
Fehlersuche, 22 -26
Entscheidungsbäume, 451 -456
Erstellen, 452 -455
Überanpassung, 455
Tiefes Lernen, 549

Dichte-Schätzer
Gaußsche Mischungsmodelle, 520 -527
Histogramm als, 528
KDE (siehe Kernel-Dichte-Schätzung)
describe()-Methode, 170
Entwicklung, IPython
Profilerstellung und Zeitmessung von Code, 26 -31
Profilerstellung vollständiger Skripte, 28
Zeitmessung von Codeschnipseln, 27
Wörterbuch(-ies)
DataFrame als Spezialisierung von, 106
DataFrame-Objekt, konstruiert aus einer Liste von,
106
Pandas-Reihenobjekt gegenüber, 103
Ziffern, Erkennung von Ziffern (siehe optische Zeichenerken- nung)
ognition)
Dimensionalitätsreduktion, 283
Maschinelles Lernen, 364-365
Hauptkomponentenanalyse, 466
diskriminierende Klassifizierung, 435 -437
Dokumentation, Zugriff auf
IPython, 4, 100
Pandas, 100
doppeltes Fragezeichen (??), 6
dropna()-Methode, 129
dynamische Typisierung, 35
E
Eigenflächen, 473 -476
Ensemble-Schätzer/Methode, 451
Ensemble-Lernprogramm, 451
Fehlerbalken, 251
Fehler (siehe Ungewissheiten, Visualisierung)
Schätzer-API, 370 -378
Grundlagen, 371
Beispiel für Iris-Klassifizierung, 375
Beispiel für Iris-Clustering, 377
Beispiel für die Dimensionalität von Iris, 376
Beispiel für einfache lineare Regression, 372 -375
eval()-Funktion, 216 -220
DataFrame.eval()-Methode und, 218 -220
pd.eval()-Funktion und, 217 -220
wann zu verwenden, 220
Ausnahmen, Kontrolle, 22
Erwartungs-Maximierungs-Algorithmus (E-M)
Gaußsches Mischungsmodell als Verallgemeinerung
von, 516 -520
k-means Clustering und, 498 -504
Einschränkungen, 500 -504
Index | 553
Erklärtes Varianzverhältnis, 470
Exponentiale, 56
Externer Code, magische Befehle zum Ausführen,
13

F
Gesichtserkennung
Histogramm der orientierten Gradienten, 541 -549
Isomap, 489 -493
Hauptkomponentenanalyse, 473 -476
Support-Vektor-Maschinen, 445 -450
facettierte Histogramme, 336
Phantasievolle Indizierung, 80 -87
Grundlagen, 80
Binning von Daten, 85
kombiniert mit anderen Indizierungsschemata, 81
Ändern von Werten mit, 84
Auswahl von Zufallspunkten, 82
Merkmalstechnik, 402 -409
kategorische Merkmale, 402
abgeleitete Merkmale, 405 -408
Bildmerkmale, 405
Imputation von fehlenden Daten, 408
Verarbeitungspipeline, 409
Textmerkmale, 404 -405
Merkmal, Datenpunkt, 356
Merkmalsmatrix, 368
fillna()-Methode, 129 , 130
filter()-Methode, 172
FiveThirtyEight-Stylesheet, 317
Arrays vom festen Typ, 39

G
Gaußsche Basisfunktionen, 424 -425
Gaußsche Mischmodelle (GMMs), 512 -527
Auswahl des Kovarianztyps, 520
Clustering mit, 377
Korrektur der Überanpassung mit, 523
Algorithmus zur Dichteschätzung, 520 -524
E-M-Verallgemeinerung, 516 -520
Beispiel für die Erzeugung handschriftlicher Daten,
524 -527
Schwächen von k-means, 512 -515
Kernel-Dichte-Schätzung und, 528
Gaußsche Naive Bayes-Klassifikation, 375 , 381,
411 -414, 546
Gaußsche Prozessregression (GPR), 253
generative Modelle, 411
get()-Operation, 189

get_dummies()-Methode, 189
ggplot-Stylesheet, 318
GMMs (siehe Gaußsche Mischmodelle)
GPR (Gaußsche Prozessregression), 253
Graustufen-Stylesheet, 319
GroupBy-Aggregation, 176
GroupBy-Objekt, 169 -171
aggregate()-Methode, 172
apply()-Methode, 173
Spaltenindizierung, 169
Dispatch-Methoden, 170
filter()-Methode, 172
Iteration über Gruppen, 170
transform()-Methode, 173
groupby()-Operation (Pandas), 167 -175
GroupBy-Objekt und, 169 -171
Gruppierungsbeispiel, 175
Pivot-Tabellen im Vergleich, 176
Split-Schlüssel-Spezifikation, 174
split-apply-combine Beispiel, 167 -169
H
Handgeschriebene Ziffern, Erkennung von (siehe optische
Zeichenerkennung)
Harter Negativbergbau, 548
Hilfe
IPython, 4
magische Funktionen, 15
help()-Funktion, 5
hexagonale Binnings, 264
hierarchische Indizierung, 132 -144
(siehe auch MultiIndex-Typ)
in eindimensionalen Reihen, 132 -135
mit Python-Tupeln als Schlüssel, 133
Neuordnung von Multi-Indizes, 141 -144
unstack()-Methode, 134
Histogramm der orientierten Gradienten (HOG)
Vorbehalte und Verbesserungen, 548 -549
für die Gesichtserkennungspipeline, 541 -549
Merkmale, 542
einfacher Gesichtsdetektor, 543 -548
Histogramme, 260 - 266
Binning von Daten zur Erstellung, 85
facettiert, 336
Kernel-Dichte-Schätzung und, 264 , 528-533
manuelle Anpassung, 312 -314
plt.hexbin()-Funktion, 264
plt.hist2d()-Funktion, 263
Seaborn, 333 -335
554 | Index

einfach, 260 -262
zweidimensional, 263 -266
%Geschichte, 17
HOG (siehe Histogramm der orientierten Verläufe)
Überbrückungssätze, 385
Holo-Ansichten, 351
Jäger, John, 223
Hyperparameter, 373
(siehe auch Modellvalidierung)

I
iloc-Attribut (Pandas), 112
Bilder, Kodierung für maschinelle Lernanalyse
sis, 405
Unveränderliches Array, Index-Objekt als, 108
Importieren, Tabulatorvervollständigung für, 8
In Objekten, IPython, 15
Index-Ausrichtung
in DataFrame, 120
in Reihen, 119
Index-Objekt (Pandas), 108
als unveränderliches Array, 108
als geordnete Menge, 108
Indizierung
fancy (siehe fancy indexing)
hierarchisch (siehe hierarchische Indizierung)
NumPy-Arrays: Zugriff auf einzelne Elemente, 44
Pandas, 110 -117
IndexSlice-Objekt, 141
Indikator-Variablen, 189
innere Verknüpfung, 157
Eingabe-/Ausgabeverlauf, IPython, 15 -18
In und Out Objekte, 15
Shell-bezogene magische Befehle, 20
Unterdrückung von Ausgaben, 17
Unterstrich-Abkürzungen und vorherige Ausgaben,
16
Installation, Python, xxii
Ganzzahlen, Python, 36
IPython
Zugriff auf die Dokumentation mit ?, 5
Zugriff auf den Quellcode mit ??, 6
Kommandozeilenbefehle in der Shell, 18
Ausnahmen kontrollieren, 22
Fehlersuche, 24 -26
Dokumentation, 4
Erweiterte interaktive Funktionen, 13 -21
Fehlerbehandlung, 22 -26
Erkunden von Modulen mit Tabulatorvervollständigung, 7-9

Hilfe und Dokumentation, 4
Eingabe-/Ausgabeverlauf, 15 -18
Tastaturkürzel in der Shell, 9-12
Jupyter-Notizbuch starten, 4
Starten der Shell, 3
Lernressourcen, 31
Magische Befehle, 13 -15
Notizbuch (siehe Jupyter-Notizbuch)
Plotten aus der Shell, 227
Profilierung und Zeitmessung von Code, 26 -31
Shell-Befehle, 18 -21
Shell-bezogene magische Befehle, 20
Web-Ressourcen, 31
Wildcard-Abgleich, 8
Iris-Datensatz
Klassifizierung, 375
Clustering, 377
Dimensionalität, 376
Paar-Diagramme, 335
Streudiagramme, 249
als Tabelle, 367
Visualisierung von, 369
isnull()-Methode, 128
Isomap
Dimensionalitätsreduktion, 365 , 380
Gesichtsdaten, 489 -493
J
Jet-Farbkarte, 278
Verknüpfungen, 149
(siehe auch Verschmelzung)
Kategorien von, 152 -154
Datensätze, 151 -163
viele-zu-eins, 153
eins-zu-eins, 152
Mengenarithmetik für, 157
Gemeinsame Verteilungen, 339
Jupyter-Notizbuch, 1
Fehlersuche, 24 -26
Starten, 4
Plotten von, 227
K
k-means-Clustering, 363 , 496-511
Grundlagen, 496 -498
Beispiel für Farbkomprimierung, 507 -511
Erwartungs-Maximierungs-Algorithmus,
498 -504
Stichwortverzeichnis | 555
Gaußsches Mischungsmodell als Mittel zur
Behebung von Schwächen, 512 -515
Anwendung auf einfache Zifferndaten, 504 -507
K-Nächste-Nachbarn-Klassifikator, 90 -93
Kernel (definiert), 533
Kernel-Bandbreite
definiert, 533
Auswahl durch Kreuzvalidierung, 535
Kernel-Dichte-Schätzung (KDE), 528 -540
Bandbreitenauswahl mittels Kreuzvalidierung,
535
Bayessche generative Klassifikation mit,
535 -540
Benutzerdefinierter Schätzer, 537 -540
Histogramme und, 528 -533
Matplotlib, 264
in der Praxis, 533 -534
Seaborn, 334
Kernel-SVM, 441 -443
Kernel-Transformation, 442
Kernel-Trick, 443
Tastaturkürzel, IPython-Shell, 9-12
Befehlsverlauf, 10
Navigation, 10
Texteingabe, 10
Knuth, Donald, 26

L
Etiketten/Etikettierung
Klassifizierungsaufgabe, 356 -359
Clustering, 363 -364
Dimensionalitätsreduktion und, 364 -365
Ausblenden von Häkchen oder Beschriftungen, 304 -305
Regressionsaufgabe, 359 -363
einfache Liniendiagramme, 240
Lasso-Regularisierung (L1-Regularisierung), 428
Lernkurven
Berechnen, 395 -399
Scikit-Learn, 398
linke Verknüpfung, 158
left_index Schlüsselwort, 155 -157
Legenden, Plot
Auswahl der Elemente für, 270
Anpassen, 267 -274
Mehrere Legenden auf denselben Achsen, 274
Punktgröße, 272
Ebenen, Benennung, 137
Liniendiagramme
Achsengrenzen für, 238 -240

Beschriftung, 240
Linienfarben und -stile, 235 -238
Matplotlib, 232 -242
zeilenweise Profilerstellung, 29
lineare Regression (beim maschinellen Lernen),
419 -434
Basisfunktionsregression, 422 -425
Regularisierung, 425 -429
Beispiel für die Vorhersage des Fahrradverkehrs in Seattle,
429 -434
einfach, 419 -422
Listen, Python
über, 37 -39
Erstellen von Arrays aus, 39
loc-Attribut (Pandas), 112
lokal lineare Einbettung (LLE), 486 -487
Logarithmen, 56
%lprun, 29
%lsmagic, 15
M
Maschinelles Lernen, 353
Grundlagen, 355 -366
Kategorien von, 355
Klassifizierungsaufgabe, 356 -359
Clustering, 363 -364
Entscheidungsbäume und Zufallswälder, 451 -462
definiert, 355
Dimensionalitätsreduktion, 364 -365
Bildungsressourcen, 550
Gesichtserkennungs-Pipeline, 541 -549
Merkmalstechnik, 402 -409
Gaußsche Mischmodelle, 512 -527
Hyperparameter und Modellvalidierung,
384 -401
k-means-Clustering, 496 -511
Kernel-Dichte-Schätzung, 528 -540
Lineare Regression, 419 -434
Vielfältiges Lernen, 477 -495
Naive Bayes-Klassifikation, 410 -418
Hauptkomponentenanalyse, 463 -476
qualitative Beispiele, 356 -365
Regressionsaufgabe, 359 -363
Scikit-Learn Grundlagen, 367 -383
Überwacht, 356
Support-Vektor-Maschinen, 435 -450
Unüberwacht, 356
%magic, 15
magische Befehle, 13 -15
556 | Index

Zeitsteuerung der Code-Ausführung, 14
Hilfe-Befehle, 15
IPython-Ein-/Ausgabeverlauf, 15 -18
Ausführen von externem Code, 13
Shell-bezogen, 20
Vielfältiges Lernen, 477 -495
Vorteile/Nachteile, 488
Anwendung von Isomap auf Gesichtsdaten, 489 -493
definiert, 477
HELLO-Funktion, 478
k-means Clustering (siehe k-means Clustering)
Mehrdimensionale Skalierung, 479 -486
Visualisierung von Strukturen in Ziffern, 493 -495
Viele-zu-Eins-Verknüpfungen, 153
Ränder, Maximierung, 437 -445
Maskierung, 117
(siehe auch Boolesche Masken)
Boolesche Arrays, 77
Boolesche Masken, 72 -79
MATLAB-ähnliche Schnittstelle, 230
Matplotlib, 223
Achsengrenzen für Liniendiagramme, 238 -240
Ändern von Standardeinstellungen über rcParams, 314
Farbbalkenanpassung, 276 -284
Konfigurationen und Stylesheets, 312 -320
Dichte- und Konturdiagramme, 255 -266
Allgemeine Tipps, 225 -231
Stolpersteine, 242
Histogramme, Binnings und Dichte, 260 -266
Importieren, 225
Schnittstellen, 230 -231
Beschriftung einfacher Liniendiagramme, 240
Linienfarben und -stile, 235 -238
MATLAB-ähnliche Schnittstellen, 230
Mehrere Teilplots, 285 -293
Objekthierarchie von Plots, 302
Objektorientierte Schnittstellen, 231
Plot-Anpassung, 312 -314
Anzeigekontexte für Plots, 226 -231
Anpassung der Plotlegende, 267 -274
Plotten aus einem Skript, 226
Plotten aus dem IPython-Notebook, 227
Plotten aus der IPython-Shell, 227
Ressourcen und Dokumentation für, 350
Speichern von Zahlen in einer Datei, 228
Seaborn und, 332 -351
Einstellen von Stilen, 225
einfache Liniendiagramme, 232 -242
einfache Streudiagramme, 244 -254

Stylesheets, 316 -320
Text und Beschriftung, 294 -301
Dreidimensionale Funktionsvisualisierung,
255 -260
Dreidimensionales Plotten, 321 -331
Zeckenanpassung, 302 -311
Visualisierung von Unsicherheiten, 251 -254
Visualisierung mit Seaborn, 332 -351
%matplotlib, 227
max()-Funktion, 61
MDS (siehe multidimensionale Skalierung)
%memit, 30
Speichernutzung, Profiling, 30
Merge-Schlüssel
auf Schlüsselwort, 154
Spezifikation von, 154 -157
Zusammenführen, 151 -163
(siehe auch Zusammenführungen)
Schlüsselangabe, 154 -157
relationale Algebra und, 151
Beispiel für Bevölkerungsdaten der US-Bundesstaaten, 159 -163
min()-Funktion, 61
Miniconda, xxii
fehlende Daten
Merkmalstechnik und, 409
Behandlung, 123 -131
NaN, 125
Keine, 125
Arbeiten mit Nullwerten in Pandas, 128 -131
Abwägungen bei Konventionen, 123
Möbiusband, 330
%Modus, 22
Modell (definiert), 357
Modellparameter (definiert), 357
Modellauswahl, 388 -395
Bias-Varianz-Abgleich, 389 -391
Überprüfungskurven in Scikit-Learn, 391 -395
Modellvalidierung, 384 -401
Bias-Varianz-Abgleich, 389 -391
Kreuzvalidierung, 386 -388
Beispiel für Rastersuche, 400 -401
Überbrückungssätze, 385
Lernkurven, 395 -399
Modellauswahl, 388 -395
Naiver Ansatz, 385
Überprüfungskurven, 391 -395
Module, IPython, 7-9
%mprun, 30
Mehrfachindizierung (siehe hierarchische Indizierung)
Index | 557
Multidimensionale Skalierung (MDS), 479 -486
Grundlagen, 479 -482
lokal lineare Einbettung und, 486 -487
als mannigfaltiges Lernen, 482
nichtlineare Einbettungen, 484 -486
MultiIndex-Typ, 132 -135
für Spalten, 138
Erstellungsmethoden, 136 -138
explizite Konstruktoren für, 136
zusätzliche Dimension der Daten mit, 134
Index setzen/zurücksetzen, 143
Indizierung und Slicing, 138 -141
Option Schlüssel, 149
Ebenennamen, 137
mehrfach indizierte DataFrames, 140
Mehrfach indizierte Reihen, 139
Umgruppieren, 141 -144
sortierte/unsortierte Indizes mit, 141
Stapeln/Entstapeln von Indizes, 143
multinomiale naive Bayes-Klassifizierung, 414 -417

N
Naive Bayes-Klassifizierung, 410 -418
Vorteile/Nachteile, 417
Bayes'sche Klassifizierung und, 410
Gauß, 411 -414
multinomial, 414 -417
Beispiel für Textklassifizierung, 414 -417
NaN-Wert (Nicht eine Zahl), 107 , 120, 125
Navigationsabkürzungen, 10
Neuronale Netze, 549
Rauschfilter, PCA als, 471 -473
Kein Objekt, 125
nichtlineare Einbettungen, MDS und, 484 -486
notnull()-Methode, 128
np.argsort() Funktion, 89
np.concatenate()-Funktion, 49 , 146
np.sort()-Funktion, 89
Nullwerte, 128 -131
Erkennen, 128
Fallenlassen, 129 -130
Auffüllen, 130
nullbare D-Typen, 127
NumPy, 33
Aggregationen, 60 -64
Array-Attribute, 44
Array-Grundlagen, 43 -50
Array-Indizierung: Zugriff auf einzelne Elemente, 44
Array-Slicing: Zugriff auf Subarrays, 45

Boolesche Masken, 72 -79
Rundsenden, 65 -71
Vergleichsoperatoren als ufuncs, 73 -74
Berechnungen auf Arrays, 51 -59
Datentypen in Python, 35 -41
datetime64 dtype, 196 -197
Dokumentation, 34
Phantasievolle Indizierung, 80 -87
Schlüsselwörter und/oder versus Operatoren &/|, 78
Arrays sortieren, 88 -93
Standard-Datentypen, 41
strukturierte Arrays, 94 -98
Universelle Funktionen, 51 -59
O
Objektorientierte Schnittstelle, 231
Offsets, Zeitreihen, 201 -202
on-Schlüsselwort, 154
Ein-Hot-Codierung, 403
Eins-zu-eins-Verknüpfungen, 152
Optische Zeichenerkennung
Klassifizierung von Ziffern, 381 -382
Gaußsche Mischmodelle, 524 -526
k-means-Clustering, 504 -507
Laden/Visualisieren von Zifferndaten, 378 -380
Hauptkomponentenanalyse als Rauschfilter
Filterung, 471 -473
Hauptkomponentenanalyse zur Visualisierung
tion, 467 -473
Zufallswälder zur Klassifizierung von Ziffern,
459 -461
Scikit-Learn-Anwendung, 378 -382
Visualisierung von Strukturen in Ziffern, 493 -495
oder Schlüsselwort, 78
geordnete Menge, Index-Objekt als, 108
Out-Objekte, IPython, 15
äußere Verknüpfung, 157
äußere Produkte, 59
Ausreißer, PCA und, 476
Ausgabe, Unterdrückung, 17
Überanpassung, 397 , 455
P
Paar-Diagramme, 335
Pandas, 100
(siehe auch universelle Funktionen)
Aggregation und Gruppierung, 164 -175
Anhängen von Datensätzen, 150
eingebaute Dokumentation, 100
558 | Index

Kombinieren von Datensätzen, 145 -150
Zusammengesetzte Ausdrücke, 215
Datenindexierung und -auswahl, 110 -117
Datenauswahl in DataFrame, 113 -117
Datenauswahl in Serien, 110 -113
DataFrame-Objekt, 104 -107
eval() und query(), 215 -221
Umgang mit fehlenden Daten, 123 -131
Hierarchische Indizierung, 132 -144
Index-Objekt, 108
Installation, 100
Zusammenführen/Verbinden von Datensätzen, 151 -163
NaN und Keine in, 126
Nullwerte, 128 -131
nullbare D-Typen, 127
Objekte, 101 -108
Bearbeitung von Daten in, 118 -122
Pivot-Tabellen, 176 -184
Serien-Objekt, 101 -104
Zeitreihen, 194 -214
Vektorisierte String-Operationen, 185 -193
pandas.eval()-Funktion, 216 -220
Partielles Slicing, 139
Partitionierung (partielle Sortierung), 90
PCA (siehe Hauptkomponentenanalyse)
pd.concat()-Funktion
Abfangen von Wiederholungen als Fehler, 148
Verkettung mit, 147 -150
Verkettung mit Joins, 149
doppelte Indizes, 148
Ignorieren des Index, 148
MultiIndex-Schlüssel, 149
pd.date_range()-Funktion, 200
pd.eval()-Funktion, 217 -218
pd.merge()-Funktion, 151 -163
Kategorien von Verknüpfungen, 152 -154
Schlüsselwörter, 154 -157
left_index/right_index Schlüsselwörter, 155 -157
Spezifikation des Zusammenführungsschlüssels, 154 -157
relationale Algebra und, 151
Festlegen der Mengenarithmetik für Joins, 157
pdb (Python-Debugger), 24
Perez, Fernando, 1, 223
Zeitraum-Typ, 199
Pipelines, 392 , 409
Pivot-Tabellen, 176 -184
groupby()-Operation gegenüber, 176
mehrstufig, 178
Syntax, 178 -179

Beispiel Titanic-Passagiere, 176
Beispiel für US-Geburtenraten, 180 -184
Planetendatensatz
Aggregation und Gruppierung, 165
Balkendiagramme, 340
Legenden für Diagramme
Auswahl von Elementen für, 270
Anpassen, 267-274
Mehrere Legenden auf denselben Achsen, 274
Punktgröße, 272
Plotly, 351
Plotten
Achsengrenzen für einfache Liniendiagramme, 238 -240
Balkendiagramme, 340
kategorische Plots, 338
Standardeinstellungen über rcParams ändern, 314
Farbbalken, 276 -284
Dichte- und Konturdiagramme, 255 -266
Anzeigekontexte, 226 -231
Histogramme, Binnings und Dichte, 260 -266
IPython-Notizbuch, 227
aus einer IPython-Shell, 227
Gemeinsame Verteilungen, 339
Beschriftung einfacher Liniendiagramme, 240
Linienfarben und -stile, 235 -238
manuelle Anpassung, 312 -314
Matplotlib, 223
mehrere Teilplots, 285 -293
von Unsicherheiten, 251 -254
Paarplots, 335
Plot-Legenden, 267 -274
aus Skript, 226
Seaborn, 333 -350
einfache Matplotlib-Liniendiagramme, 232 -242
einfache Streudiagramme, 242
Stylesheets für, 316 -320
Text und Beschriftung für, 294 -301
dreidimensional, 321 -331
dreidimensionale Funktion, 255 -260
Zecken, 302 -311
zweidimensionale Funktion, 71
verschiedene Python-Grafikbibliotheken, 351
plt.axes()-Funktion, 285 -286
plt.contour()-Funktion, 255 -260
plt.GridSpec()-Funktion, 291 -293
plt.hist2d() Funktion, 263
plt.imshow() Funktion, 258 -260
Befehl plt.legend(), 272 -274
plt.plot()-Funktion
Index | 559
Farbargumente, 235
plt.scatter gegen, 248 , 250
Streuungsdiagramme mit, 244 -247
plt.scatter()-Funktion
plt.plot versus, 248 , 250
einfache Streuungsdiagramme mit, 242
plt.subplot()-Funktion, 287 -288
plt.subplots()-Funktion, 289 -290
Polynomielle Basisfunktionen, 422
Polynomielles Regressionsmodell, 391
pop()-Methode, 114
Bevölkerungsdaten, US, Merge- und Join-Operationen
mit, 159 -163
Hauptachsen, 464 -467
Hauptkomponentenanalyse (PCA), 463 -476
Grundlagen, 463 -473
Auswahl der Anzahl der Komponenten, 470
Dimensionalitätsreduktion, 466
Beispiel Eigengesichter, 473 -476
Beispiel Gesichtserkennung, 473 -476
Beispiel für handgeschriebene Ziffern, 467 -471
Bedeutung der Komponenten, 469 -470
Rauschfilterung, 471 -473
Stärken/Schwächen, 476
Visualisierung mit, 467
Profilerstellung
Vollständige Skripte, 28
Zeile für Zeile, 29
Speichernutzung, 30
%prun, 28
Python (allgemein)
Überlegungen zur Installation, xxii
Gründe für die Verwendung, xxi

Q
query()-Methode
DataFrame.query()-Methode, 220
wann zu verwenden, 220
Fragezeichen (?), Zugriff auf IPython-Dokumen- tation
tation mit, 5

R
Radiale Basisfunktion (RBF), 442
Niederschlagsstatistik, 72
Zufallswälder, 456 -462
Vorteile/Nachteile, 462
Grundlagen, 456 -458
Klassifizierung von Ziffern mit, 459 -461
definiert, 456

Ensembles von Schätzern, 456 -462
Motivierung durch Entscheidungsbäume, 451 -456
Regression, 458 -459
RBF (Radiale Basisfunktion), 442
rcParams-Wörterbuch, Ändern von Standardwerten über, 314
RdBu-Farbkarte, 280
Datensatz-Arrays, 97
reduce()-Methode, 58
Regression, 458 -459
(siehe auch spezielle Formen, z. B. lineare Regres- sion)
sion)
Regressionsaufgabe
definiert, 356
Maschinelles Lernen, 359 -363
Reguläre Ausdrücke, 187
Regularisierung, 425 -429
Lasso-Regularisierung, 428
Ridge-Regression, 427
relationale Algebra, 151
resample()-Methode, 203 -205
reset_index()-Methode, 143
Umformung, 48
Ridge-Regression (L2-Regularisierung), 427
rechte Verknüpfung, 158
right_index Schlüsselwort, 155 -157
Rollierende Statistik, 206
Zeilen, Sortieren von Arrays entlang, 89
%run, 13
Laufzeitkonfiguration (rc), 314
S
Streuungsdiagramme (siehe einfache Streuungsdiagramme)
Scikit-Learn-Paket, 353 , 367-383
Grundlagen, 367 -383
Daten als Tabelle, 367
Datendarstellung in, 367 -370
Schätzer-API, 370 -378
Merkmalsmatrix, 368
Anwendung für handgeschriebene Ziffern, 378 -382
Lernkurven in, 398
Stützvektor-Klassifikator, 438 -441
Ziel-Array, 368 -370
scipy.special Untermodul, 57
Skript
Plotten von, 226
Profilerstellung, 28
Seaborn, 332 -351
Balkendiagramme, 340
kategorische Diagramme, 338
560 | Index

Datensätze und Diagrammtypen, 333 -350
facettierte Histogramme, 336
Histogramme, Kernel-Dichte-Schätzung und
Dichten, 333 -335
Gemeinsame Verteilungen, 339
Beispiel für Marathon-Endzeiten, 342 -350
Matplotlib gegenüber, 333 -350
Paar-Diagramme, 335
Stylesheet, 320
Visualisierung mit, 332 -351
Seattle, Vorhersage des Fahrradverkehrs in
lineare Regression, 429 -434
Zeitreihen, 208 -214
Seattle, Niederschlagsstatistik in, 72
Semi-überwachtes Lernen, 356
Sentinel-Wert, 123 , 125
Reihenobjekt (Pandas), 101 -104
Konstruieren, 104
Datenindizierung/-auswahl in, 110 -113
DataFrame als Wörterbuch von, 113
DataFrame-Objekt konstruiert aus, 106
DataFrame-Objekt konstruiert aus Diktio-
nary von, 107
als Wörterbuch, 103 , 110
verallgemeinertes NumPy-Array, 102
hierarchische Indizierung in, 132 -144
Indexausrichtung in, 119
Indexer-Attribute, 112
Vielfach indiziert, 139
eindimensionales Array, 111
Operationen zwischen DataFrame und, 121
Shell, IPython
Grundlagen, 18
Kommandozeilenbefehle, 18
Befehle, 18 -21
Tastaturkürzel in, 9-12
Starten, 3
Magische Befehle, 21
Übergabe von Werten an und von, 20
shift()-Funktion, 205
Tastenkombinationen
Zugriff auf vorherige Ausgabe, 16
Befehlsverlauf, 10
IPython-Shell, 9-12
Navigation, 10
Texteingabe, 10
Einfache Histogramme, 260 -262
einfache Liniendiagramme
Achsengrenzen für, 238 -240

Beschriftung, 240
Linienfarben und -stile, 235 -238
Matplotlib, 232 -242
einfache lineare Regression, 419 -422
einfache Streudiagramme, 244 -254
Kalifornische Stadtbevölkerung, 272 -274
Matplotlib, 242
plt.plot, 244 -247
plt.plot versus plt.scatter, 248 , 250
plt.scatter, 242
slice()-Operation, 189
Slicing
MultiIndex mit sortierten/unsortierten Indizes,
141
mehrfach indizierte DataFrames, 141
NumPy-Felder, 45 -48
NumPy-Arrays: mehrdimensionale Subarrays,
46
NumPy-Arrays: eindimensionale Unterarrays,
45
NumPy gegenüber Python, 47
Pandas-Konventionen, 116
Vektorisierter Elementzugriff und, 189
Sortieren von Arrays, 88 -93
entlang von Zeilen oder Spalten, 89
Grundlagen, 88
schnelles Sortieren mit np.sort und np.argsort, 89
Beispiel für k-nächste Nachbarn, 90 -93
Partitionierung, 90
Quellcode, Zugriff auf, 6
Arrays aufteilen, 50
String-Operationen (siehe vektorisierte String-Operationen
tionen)
strukturierte Arrays, 94 -98
erweiterte zusammengesetzte Typen, 97
Erstellen, 96
Datensatz-Arrays, 97
Stylesheets
Bayes'sche Methoden für Hacker, 318
dunkler Hintergrund, 319
Standard-Stil, 317
FiveThirtyEight-Stil, 317
ggplot, 318
Graustufen, 319
Matplotlib, 316 -320
Seaborn, 320
Subarrays
Kopien erstellen, 47
Ansichten ohne Kopien, 47
Index | 561
Slicing mehrdimensional, 46
Slicing eindimensional, 45
Teilflächen
manuelle Anpassung, 285 -286
mehrere, 285 -293
plt.axes() für, 285 -286
plt.GridSpec() für, 291 -293
plt.subplot() für, 287 -288
plt.subplots() für, 289 -290
Schlüsselwort suffixes, 158
Überwachtes Lernen, 356
Klassifizierungsaufgabe, 356 -359
Regressionsaufgabe, 359 -363
Unterstützungsvektor (definiert), 439
Unterstützungsvektor-Klassifikator, 438 -441
Unterstützungsvektormaschinen (SVMs), 435 -450
Vorteile/Nachteile, 450
Beispiel Gesichtserkennung, 445 -450
Anpassung, 438 -441
Kernel und, 441 -443
Maximierung der Marge, 437
motivierend, 435 -445
einfacher Gesichtsdetektor, 543
Abschwächung der Ränder, 444 -445
Oberflächenplots, dreidimensional, 325 -327

T
t-verteilte stochastische Nachbarschaftseinbettung (t-
SNE), 489 , 507
Tabulator-Vervollständigung
Erkunden von IPython-Modulen mit, 7-9
von Objektinhalten, 7
beim Importieren, 8
Tabelle, Daten als, 367
Ziel-Array, 368 -370
Begriffshäufigkeit - inverse Dokumentenhäufigkeit
(TF-IDF), 404
Text, 294 -301
(siehe auch Annotation von Plots)
Merkmalstechnik, 404 -405
Transformationen und Position von, 296 -298
Tastenkombinationen für die Texteingabe, 10
Dreidimensionales Plotten
Konturdiagramme, 323 -324
mit Matplotlib, 321 -331
Visualisierung des Mobiusbandes, 330
Punkte und Linien, 322
Oberflächenplots, 325 -327
Oberflächen-Triangulationen, 328

Drahtgitter, 325 -329
Häkchen (Häkchenmarkierungen)
Anpassen, 302 -311
Ausgefallene Formate, 307 -309
Optionen für Formatierer/Locator, 310
Ausblenden von Häkchen oder Beschriftungen, 304 -305
Dur und Moll, 302
Verringern/Erhöhen der Anzahl von, 306
Tichonow-Regularisierung, 427
%Zeit, 27
Zeitreihen
Balkendiagramme, 340
Datums- und Zeitangaben in Pandas, 197
datetime64, 196 -197
Häufigkeitscodes, 201 -202
Indizierung von Daten nach Zeitstempeln, 198
Native Python-Daten und -Zeiten, 195
Offsets, 201 -202
Pandas, 194 -214
Pandas-Datenstrukturen für, 199
pd.date_range(), 200
Python gegenüber Pandas, 195 -198
Resampling und Konvertierung von Frequenzen,
203 -205
Rollierende Statistik, 206
Beispiel für Fahrradzählungen in Seattle, 208 -214
Zeitverschiebungen, 205
Typisierte Arrays, 196 -197
Typ Timedelta, 199
%timeit, 14 , 27
Zeitstempel-Typ, 199
Zeitstempel, Indizierung von Daten durch, 198
Zeitmessung, von Code, 14 , 26-28
transform()-Methode, 173
Transformationen
Ändern, 296 -298
Textposition und, 296 - 298
Triangulierte Oberflächenplots, 328
Trigonometrische Funktionen, 55
Zweifache Kreuzvalidierung, 386
U
ufuncs (siehe universelle Funktionen)
unäre ufuncs, 53
Ungewissheiten, Visualisierung
grundlegende Fehlerbalken, 251
Kontinuierliche Fehler, 253
Matplotlib, 251 -254
Unteranpassung, 389 , 397
562 | Index

Unterstrich (_) als Abkürzung, 16
Universelle Funktionen (ufuncs), 51 -59
absoluter Wert, 55
Erweiterte Funktionen, 57 -59
Aggregate, 58
Array-Arithmetik, 53
Grundlagen, 52 -57
Vergleichsoperatoren als, 73 -74
Exponentiale, 56
Indexausrichtung, 119 -121
Indexerhaltung, 118
Logarithmen, 56
Operationen auf Daten in Pandas, 118 -122
Operationen zwischen DataFrame und Series,
121
äußere Produkte, 59
Langsamkeit von Python-Schleifen, 51
spezialisierte ufuncs, 56
Festlegen der Ausgabe, 57
Trigonometrische Funktionen, 55
unstack()-Methode, 134
Unüberwachtes Lernen
Clustering, 363 -364, 377
definiert, 356
Dimensionalitätsreduktion, 283 , 364-365,
376 , 380
PCA (siehe Hauptkomponentenanalyse)

V
Validierung (siehe Modellvalidierung)
Validierungskurven, 391 -395
Variablen
dynamische Typisierung, 35
Übergabe an und von Shell, 20
Varianz, in Bias-Varianz-Abwägung, 389 -391
Vektorisierte String-Operationen, 185 -193
Grundlagen, 185
Indikator-Variablen, 189
Methoden ähnlich den Python-String-Methoden,
186
Methoden mit regulären Ausdrücken, 187
Beispiel einer Rezeptdatenbank, 190 -193
Tabellen von, 186 -190
Vektorisierter Elementzugriff und Slicing, 189
Vega/Vega-Lite, 351
Geigenplot, 347
Viridis-Farbkarte, 279
Visualisierungssoftware (siehe Matplotlib; Seaborn)
W
Wickham, Hadley, 167
Wildcard-Matching, 8
Drahtgitterplot, 325 -329
Wortzählungen, 404
Index | 563
Über den Autor
Jake VanderPlas ist Software-Ingenieur bei Google Research und arbeitet an Tools, die

datenintensive Forschung zu unterstützen. Jake erstellt und entwickelt Python-Tools für die Verwendung in

datenintensive Wissenschaft, einschließlich Pakete wie Scikit-Learn, SciPy, AstroPy, Altair,
JAX, und viele andere. Er beteiligt sich an der breiteren Data-Science-Community, entwickelt...

Vorträge und Tutorials zu Themen des wissenschaftlichen Rechnens auf verschiedenen
Konferenzen in der Welt der Datenwissenschaft.

Kolophon
Das Tier auf dem Cover des Python Data Science Handbook ist eine mexikanische Perleidechse

(Heloderma horridum), ein Reptil, das in Mexiko und Teilen von Guatemala vorkommt. Das griechische
Wort "Heloderma" bedeutet übersetzt "genoppte Haut" und bezieht sich auf die charakteristische, wulstige Textur.

der Haut der Eidechse. Diese Beulen sind Osteoderme, die jeweils eine kleine

Knochenstück und dienen als Schutzpanzer.

Die mexikanische Perleidechse ist schwarz mit gelben Flecken und Bändern. Sie hat einen breiten

Kopf und einen dicken Schwanz, der Fett speichert, um die heißen Sommermonate zu überleben, wenn er
inaktiv ist. Im Durchschnitt sind diese Eidechsen 22-36 cm lang und wiegen etwa 1,8

Pfunde. Wie bei den meisten Schlangen und Eidechsen ist die Zunge der mexikanischen Perleidechse

sein wichtigstes Sinnesorgan. Er schnippt es wiederholt aus, um Duftpartikel aus der Umgebung zu sammeln
aus der Umgebung aufzunehmen und Beute (oder während der Paarungszeit einen potenziellen Partner) zu entdecken.

Sie und das Gila-Monster (ein naher Verwandter) sind die einzigen giftigen Eidechsen der Welt.
Wenn sie sich bedroht fühlt, beißt die mexikanische Perleidechse zu und klammert sich fest, um zu kauen,

weil er nicht in der Lage ist, eine große Menge an Gift auf einmal freizusetzen. Dieser Biss und der anschließende...

Die Auswirkungen des Giftes sind äußerst schmerzhaft, wenn auch selten tödlich für den Menschen. Das Gift der Perleidechse
Das Gift der Sickereidechse enthält Enzyme, die zur Behandlung von Diabetes synthetisiert wurden,

und weitere pharmakologische Forschungen sind im Gange. Die Art ist durch den Verlust von Lebensräumen
Lebensraumes, durch Wilderei für den Heimtierhandel und durch Einheimische, die ihn aus Angst töten. Dieses Tier ist pro-

in den beiden Ländern, in denen sie leben, gesetzlich geschützt. Viele der Tiere auf O'Reilly

sind gefährdet; sie alle sind wichtig für die Welt.

Die Illustration des Umschlags stammt von Karen Montgomery und basiert auf einem Schwarz-Weiß-Stich

von Woods Animate Creation. Die Schriftarten für das Cover sind Gilroy Semibold und Guardian
Sans. Die Textschrift ist Adobe Minion Pro; die Überschriftenschrift ist Adobe Myriad Con-

densed; und die Code-Schriftart ist Ubuntu Mono von Dalton Maag.

Lernen Sie von Experten.

Werden Sie selbst einer.

Bücher | Live-Online-Kurse

Sofortige Antworten | Virtuelle Veranstaltungen

Videos | Interaktives Lernen

Beginnen Sie auf oreilly.com.

©2022 O'Reilly Media, Inc. O'Reilly ist eine eingetragene Marke von O'Reilly Media, Inc. | 175
Dies ist ein Offline-Tool, Ihre Daten bleiben lokal und werden nicht an einen Server gesendet!
Feedback & Fehlerberichte