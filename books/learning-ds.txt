 PDF To Markdown Converter
Debug View
Result View
Learning Data Science
With Early Release ebooks, you get books in their earliest

form—the author’s raw and unedited content as they write—

so you can take advantage of these technologies long before

the official release of these titles.

Sam Lau, Deborah Nolan, and Joseph Gonzalez
Learning Data Science
Programming and Statistics Fundamentals
Using Python
BeijingBeijing BostonBoston FarnhamFarnham SebastopolSebastopol TokyoTokyo

978-1-098-11293-
Learning Data Science

by Sam Lau , Deborah Nolan , and Joseph Gonzalez

Copyright © 2023 O’Reilly Media. All rights reserved.

Printed in the United States of America.

Published by O’Reilly Media, Inc. , 1005 Gravenstein Highway North, Sebastopol, CA 95472.

O’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are
also available for most titles ( http://oreilly.com ). For more information, contact our corporate/institu‐
tional sales department: 800-998-9938 or corporate@oreilly.com.

Editors: Melissa Potter and Jessica Haberman
Production Editor: Katherine Tozer
Interior Designer: David Futato

Cover Designer: Karen Montgomery
Illustrator: Kate Dullea
May 2023: First Edition

Revision History for the Early Release

2022-02-09: First Release
2022-05-11: Second Release
2022-09-20: Third Release
2022-11-09: Fourth Release

See http://oreilly.com/catalog/errata.csp?isbn=9781098113001 for release details.

The O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Learning Data Science, the cover image,
and related trade dress are trademarks of O’Reilly Media, Inc.

The views expressed in this work are those of the authors, and do not represent the publisher’s views.
While the publisher and the authors have used good faith efforts to ensure that the information and
instructions contained in this work are accurate, the publisher and the authors disclaim all responsibility
for errors or omissions, including without limitation responsibility for damages resulting from the use of
or reliance on this work. Use of the information and instructions contained in this work is at your own
risk. If any code samples or other technology this work contains or describes is subject to open source
licenses or the intellectual property rights of others, it is your responsibility to ensure that your use
thereof complies with such licenses and/or rights.

Table of Contents
Questions and Data Scope.
Big Data and New Opportunities
Example: Google Flu Trends
Target Population, Access Frame, Sample
Instruments and Protocols
Measuring Natural Phenomenon
Accuracy
Types of Bias
Types of Variation
Summary
Exercises
Simulation and Data Design.
The Urn Model
Sampling Designs
Sampling Distribution of a Statistic
Simulating the Sampling Distribution
The Hypergeometric
Example: Simulating Election Poll Bias and Variance
The Pennsylvania Urn Model
An Urn Model with Bias
Conducting Larger Polls
Example: Simulating a Randomized Trial for a Vaccine
Scope
The Urn Model for Random Assignment
Example: Measurement Error in Air Quality
Summary
Exercises
Modeling with Summary Statistics.
The Constant Model
Loss Functions
Mean Absolute Error
Mean Squared Error
Choosing Loss Functions
Summary
Exercises
Working With Dataframes Using pandas.
Subsetting
About the Data
DataFrames and Indices
Slicing
Filtering Rows
Example: How recently has Luna become a popular name?
Takeaways
Aggregating
Basic Group-Aggregate
Grouping on Multiple Columns
Custom Aggregation Functions
Example: Have People Become More Creative With Baby Names?
Pivoting
Takeaways
Joining
Inner Joins
Left, Right, and Outer Joins
Example: Popularity of NYT Name Categories
Takeaways
Transforming
Apply
Example: Popularity of “L” Names
The Price of Apply
Takeaways
How are Dataframes Different from Other Data Representations?
Dataframes and Spreadsheets
Dataframes and Matrices
Dataframes and Relations
Summary
Exercises
Working With Relations Using SQL.
Subsetting
About the Data
What’s a Relation?
Slicing
Filtering Rows
Example: How recently has Luna become a popular name?
Takeaways
Aggregating
Basic Group-Aggregate
Grouping on Multiple Columns
Other Aggregation Functions
Example: Have People Become More Creative With Baby Names?
Takeaways
Joining
Inner Joins
Left, Right, and Outer Joins
Example: Popularity of NYT Name Categories
Takeaways
Transforming
SQL Functions
Multistep Queries Using a WITH Clause
Example: Popularity of “L” Names
Takeaways
How are Relations Different from Other Data Representations?
Relations and Spreadsheets
Relations and Matrices
Relations and Dataframes
Conclusion
Exercises
Wrangling Files.
Data Source Examples
Drug Abuse Warning Network (DAWN) Survey
San Francisco Restaurant Food Safety
File Formats
Delimited format
Fixed-width Format
Hierarchical Formats
Loosely Structured Formats
File Encoding
File Size
Working With Large Datasets
The Shell and Command Line Tools
Table Shape and Granularity
Granularity of Restaurant Inspections and Violations
DAWN Survey Shape and Granularity
Summary
CHAPTER 1

Questions and Data Scope

A Note for Early Release Readers

With Early Release ebooks, you get books in their earliest form—the author’s raw and
unedited content as they write—so you can take advantage of these technologies long
before the official release of these titles.
This will be the 2nd chapter of the final book. Please note that the GitHub repo will be
made active later on.
If you have comments about how we might improve the content and/or examples in
this book, or if you notice missing material within this chapter, please reach out to the
author at mpotter@oreilly.com.
As data scientists we use data to answer questions, and the quality of the data collec‐

tion process can significantly impact the validity and accuracy of the data, the

strength of the conclusions we draw from an analysis, and the decisions we make. In

this chapter, we describe a general approach for understanding data collection and

evaluating the usefulness of the data in addressing the question of interest. Ideally, we

aim for data to be representative of the phenomenon that we are studying, whether

that phenomenon is a population characteristic, a physical model, or some type of

social behavior. Typically, our data do not contain complete information (the scope is

restricted in some way), yet we want to use the data to accurately describe a popula‐

tion, estimate a scientific quantity, infer the form of a relationship between features,

or predict future outcomes. In all of these situations, if our data are not representative

of the object of our study, then our conclusions can be limited, possibly misleading,

or even wrong.

7
1 The notion of “scope” has been adapted from Hellerstein’s course notes on scope, temporality, and faithfull‐
ness.
To motivate the need to think about these issues, we begin with an example of the

power of big data and what can go wrong (“Big Data and New Opportunities” on

page 8). We then provide a framework that can help you connect the goal of your

study (your question) with the data collection process. We refer to this as the data

scope 1 , and provide terminology to help describe data scope, along with examples

from surveys, government data, scientific instruments, and online resources. Later in

this chapter, we consider what it means for data to be accurate. There, we introduce

different forms of bias and variation, and describe conditions where they can arise.

Throughout, the examples cover the spectrum of the sorts of data that you may be

using as a data scientists; these examples are from science, political elections, public

health, and online communities.

Big Data and New Opportunities
The tremendous increase in openly available data has created new roles and opportu‐

nities in data science. For example, data journalists look for interesting stories in data

much like how traditional beat reporters hunt for news stories. The data life cycle for

the data journalist begins with the search for existing data that might have an inter‐

esting story, rather than beginning with a research question and looking for how to

collect new or use existing data to address the question.

Citizen science projects are another example. They engage many people (and instru‐

ments) in data collection. Collectively, these data are often made available to

researchers who organize the project and often they are made available in repositories

for the general public to further investigate.

The availability of administrative/organizational data creates other opportunities.

Researchers can link data collected from scientific studies with, say medical data that

have been collected for healthcare purposes; in other words, administrative data col‐

lected for reasons that don’t directly stem from the question of interest can be useful

in other settings. Such linkages can help data scientists expand the possibilities of

their analyses and cross-check the quality of their data. In addition, found data can

include digital traces, such as your web-browsing activity, posts on social media, and

online network of friends and acquaintances, and can be quite complex.

When we have large amounts of administrative data or expansive digital traces, it can

be tempting to treat them as more definitive than data collected from traditional

smaller research studies. We might even consider these large datasets as a replace‐

ment for scientific studies or essentially a census. This over-reach is referred to as the

8 | Chapter 1: Questions and Data Scope

2 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5754279/
“big data hubris” (Lazer et al. 2014). Data with a large scope does not mean that we

can ignore foundational issues of how representative the data are, nor can we ignore

issues with measurement, dependency, and reliability. One well-known example is the

Google Flu Trends tracking system.

Example: Google Flu Trends
Digital epidemiology, a new subfield of epidemiology, leverages data generated out‐

side the public health system to study patterns of disease and health dynamics in pop‐

ulations^2 The Google Flu Trends (GFT) tracking system was one of the earliest

examples of digital epidemiology. In 2007, researchers found that counting the

searches people made for flu-related terms could accurately estimate the number of

flu cases. It made headlines, and helped make researchers excited about the possibili‐

ties of big data. However, GFT did not live up to expectations and was abandoned in

What went wrong with GFT? After all, it used millions of digital traces from online

queries for terms related to influenza to predict flu activity. Despite initial success, in

the 2011–2012 flu season, Google’s data scientists found that GFT was not a substitute

for more traditionally collected data from the Centers for Disease Control (CDC)

surveillance reports, collected from laboratories across the United States. In compari‐

son, GFT overestimated the CDC numbers for 100 out of 108 weeks (see Figure 1-1).

Week after week, GFT came in too high for the cases of influenza, even though it was

based on big data.

Big Data and New Opportunities | 9
Figure 1-1. Google Flu Trend (GFT) weekly estimates for influenza-like illness. For 108

weeks, GFT (solid line) over estimated the actual CDC reports (dashed line) 100 times.

Also plotted are predictions from a model based on 3-week old CDC data and seasonal

trends (dotted line).

Data scientists found that the GFT was not a substitute for more traditionally collec‐

ted data from the CDC. A simple model built from past CDC reports that used 3-

week-old CDC data and seasonal trends did a better job of predicting flu prevalence

than GFT. That is, the GFT overlooks considerable information that could be extrac‐

ted by basic statistical methods. This does not mean that big data captured from

online activity is useless. In fact, researchers have shown that the combination of GFT

data with CDC data can substantially improve on both GFT predictions and the

CDC-based model (Lazer 2015). It is often the case that combining different

approaches leads to improvements over individual methods.

The GFT example shows us that even when we have tremendous amounts of infor‐

mation, the connections between the data, the topic of investigation, and the question

being asked are paramount. Understanding this framework can help us avoid answer‐

ing the wrong question, applying inappropriate methods to the data, and overstating

our findings.

In the age of big data, we are tempted to collect more and more data. After all, a cen‐

sus gives us perfect information, so shouldn’t big data be nearly perfect? A key factor

10 | Chapter 1: Questions and Data Scope

to keep in mind is the scope of the data. What population do we want to study? How

can we access information about that population? Who or what are we actually study‐

ing? Answers to these questions help us see potential gaps in our approach. This is

the topic of the next section.

Target Population, Access Frame, Sample
An important initial step in the data life cycle is to express the question of interest in

the context of the subject area and consider the connection between the question and

the data collected to answer that question. It’s good practice to do this before even

thinking about the analysis or modeling steps because it may uncover a disconnect

where the question of interest cannot be directly addressed with these data. As part of

making the connection between the data collection process and the topic of investiga‐

tion, we identify the population, the means of accessing the population, instruments

of measurement, and additional protocols used in the collection process. These con‐

cepts help us understand the scope of the data, whether we aim to gain knowledge

about a population, scientific quantity, physical model, social behavior, etc.

The target population consists of the collection of elements that you ultimately intend

to describe and draw conclusions about. By element we mean those individuals that

make up our population. The element may be a person in a group of people, a voter

in an election, a tweet from a collection of tweets, or a county in a state. We some‐

times call an element a unit or an atom.

The access frame is the collection of elements that are accessible to you for measure‐

ment and observation. These are the units by which you can access the target popula‐

tion. Ideally, the frame and population are perfectly aligned; meaning they consist of

the exact same elements. However, the units in an access frame may be only a subset

of the target population; additionally, the frame may include units that don’t belong

to the population. For example, to find out how a voter intends to vote in an election,

you might call people by phone. Someone you call, may not be a voter so they are in

your frame but not in the population. On the other hand, a voter who never answers

a call from an unknown number can’t be reached so they are in the population but

not in your frame.

The sample is the subset of units taken from the access frame to measure, observe,

and analyze. The sample gives you the data to analyze to make predictions or general‐

izations about the population of interest.

The contents of the access frame, in comparison to the target population, and the

method used to select units from the frame to be in the sample are important factors

in determining whether or not the data can be considered representative of the target

population. If the access frame is not representative of the target population, then the

Target Population, Access Frame, Sample | 11
data from the sample is most likely not representative either. And, if the units are

sampled in a biased manner, problems with representativeness also arise.

You will also want to consider time and place in the data scope. For example, the

effectiveness of a drug trial tested in one part of the world where a disease is raging

might not compare as favorably with a trial in a different part of the world where

background infection rates are lower (see Chapter %s). Additionally, data collected

for the purpose of studying changes over time, like with the monthly measurements

of CO2 in the atmosphere (see ???) and the weekly reporting of Google searches for

predicting flu trends have a temporal structure that we need to be mindful of as we

examine the data. At other times, there might be spatial patterns in the data. For

example, the environmental heath data, described later in this section, are reported

for each census tract in the State of California, and we might, say, make maps to look

for spatial correlations.

And, if you didn’t collect the data, you will want to consider who did and for what

purpose. This is especially relevant now since more data is passively collected instead

of collected with a specific goal in mind. Taking a hard look at found data and asking

yourself whether and how these data might be used to address your question can save

you from making a fruitless analysis or drawing inappropriate conclusions.

For each of the following examples, we begin with a general question, narrow it to

one that can be answered with data, and in doing so, we identify the target popula‐

tion, access frame, and sample. These concepts are represented by circles in a diagram

of data scope, and the configuration of their overlap helps reveal key aspects the

scope. Also in each example, we describe relevant temporal and spatial features to the

data scope.

What makes members of an online community active? Content on Wikipedia is written

and edited by volunteers who belong to the Wikipedia community. This online com‐

munity is crucial to the success and vitality of Wikipedia. In trying to understand

how to incentivize members of online communities, researchers carried out an

experiment with Wikipedia contributors as subjects (Restivo and van de Rijt 2012). A

narrowed version of the general question asks: do informal awards increase the activ‐

ity of Wikipedia contributors? For this experiment, the target population is the collec‐

tion of active contributors—those who made at least one contribution to Wikipedia

in the month before the start of the study. Additionally, the target population was fur‐

ther restricted to the top 1% of contributors. The access frame eliminated anyone in

the population who had received an informal incentive that month. The access frame

purposely excluded some of the contributors in the population because the research‐

ers want to measure the impact of an incentive and those who had already received

one incentive might behave differently. (See Figure 1-2).

12 | Chapter 1: Questions and Data Scope

Figure 1-2. The access frame does not include the entire population because the experi‐

ment included only those contributers who had not already received an incentive. The

sample is a randomly selected subset from the frame.

The sample is a randomly selected set of 200 contributors from the frame. The sample

of contributors were observed for 90 days and digital traces of their activities on

Wikipedia were collected. Notice that the contributor population is not static; there is

regular turnover. In the month prior to the start of the study more than 144,000 vol‐

unteers produced content for Wikipedia. Selecting top contributors from among this

group limits the generalizability of the findings, but given the size of the group of top

contributors, if they can be influenced by an informal reward to maintain or increase

their contributions that is a valuable finding.

In many experiments and studies, we don’t have the ability to include all population

units in the frame. It is often the case that the access frame consists of volunteers who

are willing to join the study/experiment.

Who will win the election? The outcome of the US presidential election in 2016 took

many people and many pollsters by surprise. Most pre-election polls predicted Clin‐

ton would beat Trump by a wide margin. Political polling is a type of public opinion

survey held prior to an election that attempts to gauge who people will vote for. Since

Target Population, Access Frame, Sample | 13
3 https://projects.fivethirtyeight.com/pollster-ratings/
opinions change over time, the focus is reduced to a “horse-race” question, where

respondents are asked for whom would they vote in a head-to-head race if the elec‐

tion were tomorrow: Candidate A or Candidate B.

Polls are conducted regularly throughout the presidential campaign, and the closer to

election day, we expect the polls to get better at predicting the outcome, as preferen‐

ces stabilize. Polls are also typically conducted statewide, and later combined to make

predictions for the overall winner. For these reasons, the timing and location of a poll

matters. The pollster matters too; some have consistently been closer to the mark

than others^3.

In these pre-election surveys, the target population consists of those who will vote in

the election, which in this example was the 2016 US presidential election. However,

pollsters can only guess at whether someone will vote in the election so the access

frame consists of those deemed to be likely voters (this is usually based on their past

voting record, but other factors may be used to determine this), and since people are

contacted by phone, the access frame is limited to those who have a landline or

mobile phone. The sample consists of those people in the frame who are chosen

according to a random dialing scheme. (See Figure 1-3).

Figure 1-3. This representation is typical of many surveys. The access frame does not

cover all of the population and includes some who are not in the population.

Later in Chapter 2, we discuss the impact on the election predictions of people’s

unwillingness to answer their phone or participate in the poll.

14 | Chapter 1: Questions and Data Scope

How do environmental hazards impact an individual’s health? To address this question,

the California Environmental Protection Agency (CalEPA), the California Office of

Health Hazard Assessment (OEHHA), and the public developed the CalEnviroScreen

project. The project studies connections between population health and environmen‐

tal pollution in California communities using data collected from several sources that

includes demographic summaries from the U.S. census, health statistics from the Cal‐

ifornia Office of Statewide Health Planning and Development, and pollution meas‐

urements from air monitoring stations around the state maintained by the California

Air.

Ideally, we want to study the people of California, and assess the impact of these envi‐

ronmental hazards on an individual’s health. However, in this situation, the data can

only be obtained at the level of a census tract. The access frame consists of groups of

residents living in the same census tract. So, the units in the frame are census tracts

and the sample is a census–all of the tracts–since data are provided for all of the tracts

in the state. (See Figure 1-4).

Figure 1-4. The grid in the access frame represents the census tracts. The population,

frame, and sample cover all Californians, but the grid limits measurments to the level of

census tract.

Unfortunately, we cannot deaggregate the information in a tract to examine what

happens to an individual person. This aggregation impacts the question we can

address and the conclusions that we can draw.

Target Population, Access Frame, Sample | 15
These examples have demonstrated some of the configurations a target, access frame,

and sample might have, and the exercises at the end of this chapter provide a few

more examples. When a frame doesn’t reach everyone, we should consider how this

missing information might impact our findings. Similarly we ask what might happen

when a frame includes those not in the population. Additionally, the techniques for

drawing the sample can affect how representative the sample is of the population.

When you think about the generalizability of your data findings, you also want to

consider the quality of the instruments and procedures used to collect the data. If

your sample is a census that matches your target, but the information is poorly collec‐

ted, then your findings will be of little value. This is the topic of the next section.

Instruments and Protocols
When we consider the scope of the data, we also consider the instrument being used

to take the measurements and the procedure for taking measurements, which we call

the protocol. For a survey, the instrument is typically a questionnaire that an individ‐

ual in the sample answers. The protocol for a survey includes how the sample is

chosen, how nonrespondents are followed up, interviewer training, protections for

confidentiality, etc.

Good instruments and protocols are important to all kinds of data collection. If we

want to measure a natural phenomenon, such as the speed of light, we need to quan‐

tify the accuracy of the instrument. The protocol for calibrating the instrument and

taking measurements is vital to obtaining accurate measurements. Instruments can

go out of alignment and measurements can drift over time leading to poor, highly

inaccurate measurements (see ???).

Protocols are also critical in experiments. Ideally, any factor that can influence the

outcome of the experiment is controlled. For example, temperature, time of day, con‐

fidentiality of a medical record, and even the order of taking measurements need to

be consistent to rule out potential effects from these factors getting in the way.

With digital traces, the algorithms used to support online activity are dynamic and

continually re-engineered. For example, Google’s search algorithms are continually

tweaked to improve user service and advertising revenue. Changes to the search algo‐

rithms can impact the data generated from the searches, which in turn impact sys‐

tems built from these data, such as the Google Flu Trend tracking system. This

changing environment can make it untenable to maintain data collection protocols

and difficult to replicate findings.

Many data science projects involve linking data together from multiple sources. Each

source should be examined through this data-scope construct and any difference

across sources considered. Additionally, matching algorithms used to combine data

16 | Chapter 1: Questions and Data Scope

from multiple sources need to be clearly understood so that populations and frames

from the sources can be compared.

Measurements from an instrument taken to study a natural phenomenon can also be

cast in the scope-diagram of a target, access frame, and sample (see “Target Popula‐

tion, Access Frame, Sample” on page 11 ). This approach is helpful in understanding

their accuracy.

Measuring Natural Phenomenon
The scope-diagram introduced for observing a target population can be extended to

the situation where we want to measure a quantity such as the count of particles in

the air, the age of a fossil, the speed of light, etc. In these cases we consider the quan‐

tity we want to measure as an unknown value. (This unknown value referred is often

referred to as a parameter.) In our diagram, we shrink the target to a point that repre‐

sents this unknown. The instrument’s accuracy acts as the frame, and the sample con‐

sists of the measurements taken by the instrument within the frame. You might think

of the frame as a dart board, where the instrument is the person throwing the darts. If

they are reasonably good, the darts land within the circle, scattered around the bulls‐

eye. The scatter of darts correspond to the measurements taken by the instrument.

The target point is not seen by the dart thrower, but ideally it coincides with the bulls‐

eye.

To illustrate the concepts of measurement error and the connection to sampling

error, we examine the problem of calibrating air quality sensors.

How accurate is my air quality monitor? Across the US, sensors to measure air pollu‐

tion are widely used by individuals, community groups, and state and local air moni‐

toring agencies (Owyang 2020). For example, on two days in September, 2020,

approximately 600,000 Californians and 500,000 Oregonians viewed PurpleAir’s map

as fire spread through their states and evacuations were planned. PurpleAir creates

air quality maps from crowdsourced data that streams in from their sensors.

Measuring Natural Phenomenon | 17
Figure 1-5. This representation is typical of many measurement processes. The access

frame represents the measurement process which reflects the accuracy of the instrument.

We can think of the data scope as follows: at any location and point in time, there is a

true particle composition in the air surrounding the sensor, this is our target. Our

instrument, the sensor, takes many measurements, in some cases a reading every sec‐

ond. These form a sample contained in the access frame, the dart board. If the instru‐

ment is working properly, the measurements are centered around the bullseye, and

the target coincides with the bullseye. Researchers have found that low humidity can

distort the readings so that they are too high (Hug 2020). In ???, we address how to

use data science to calibrate these instruments to improve their accuracy.

We continue the dart board analogy in the next section to introduce the concepts of

bias and variation, describe common ways in which a sample might not be represen‐

tative of the population, and draw connections between accuracy and the protocol.

Accuracy
In a census, the access frame matches the population, and the sample captures the

entire population. In this situation, if we administer a well-designed questionnaire,

then we have complete and accurate knowledge of the population and the scope is

complete. Similarly in measuring air quality, if our instrument has perfect accuracy

and is properly used, then we can measure the exact value of the air quality. These

situations are rare, if not impossible. In most settings, we need to quantify the accu‐

racy of our measurements in order to generalize our findings to the unobserved. For

example, we often use the sample to estimate an average value for a population, infer

18 | Chapter 1: Questions and Data Scope

the value of a scientific unknown from measurements, or predict the behavior of a

new individual. In each of these settings, we also want a quantifiable degree of accu‐

racy. We want to know how close our estimates, inferences, and predictions are to the

truth.

The analogy of darts thrown at a dart board that was introduced earlier can be useful

in understanding accuracy. We divide accuracy into two basic parts: bias and variance

(also known as precision). Our goal is for the darts to hit the bullseye on the dart

board and for the bullseye to line up with the unseen target. The spray of the darts on

the board represents the variance in our measurements, and the gap from the bullseye

to the unknown value that we are targeting represents the bias. Figure Figure 1-6

shows combinations of low and high bias and variance.

Figure 1-6. In each of these diagrams, the dots represent the measurements taken. They

form a scattershot within the access frame represented by the dart board. When the

bullseye of the access frame is roughly centered on the targetted value (top row), the

measurements are scattered around it and bias is low. The larger dart boards (right col‐

umn) indicate a wider spread in the measurements.

Representative data puts us in the top row of the diagram, where there is low bias,

meaning that the bullseye and the unseen target are in alignment. Ideally our instru‐

ments and protocols put us in the upper left part of the diagram, where the variance

Accuracy | 19
is also low. The pattern of points in the bottom row systematically miss the targeted

value. Taking larger samples will not correct this bias.

Types of Bias
Bias comes in many forms. We describe some classic types here and connect them to

our target-access-sample framework.

Coverage bias occurs when the access frame does not include everyone in the tar‐
get population. For example, a survey based on cell-phone calls cannot reach
those with only a landline or no phone. In this situation, those who cannot be
reached may differ in important ways from those in the access frame.
Selection bias arises when the mechanism used to choose units for the sample
tends to select certain units more often than they should. As an example, a con‐
venience sample chooses the units most easily available. Problems can arise when
those who are easy to reach differ in important ways from those harder to reach.
Another example of selection bias can happen with observational studies and
experiments. These studies often rely on volunteers (people who choose to par‐
ticipate), and this self-selection has the potential for bias, if the volunteers differ
from the target population in important ways.
Non-response bias comes in two forms: unit and item. Unit non-response hap‐
pens when someone selected for a sample is unwilling to participate, and item
non-response occurs when, say, someone in the sample refuses to answer a par‐
ticular survey question. Non-response can lead to bias if those who choose not to
participate or to not answer a particular question are systematically different
from those who respond.
Measurement bias happens when an instrument systematically misses the target
in one direction. For example, low humidity can systematically give us incor‐
rectly high measurements of air pollution. In addition, measurement devices can
become unstable and drift over time and so produce systematic errors. In sur‐
veys, measurement bias can arise when questions are confusingly worded or
leading, or when respondents may not be comfortable answering honestly.
Each of these types of bias can lead to situations where the data are not centered on

the unknown targeted value. Often we cannot assess the potential magnitude of the

bias, since little to no information is available on those who are outside of the access

frame, less likely to be selected for the sample, or disinclined to respond. Protocols

are key to reducing these sources of bias. Chance mechanisms to select a sample from

the frame or to assign units to experimental conditions can eliminate selection bias. A

non-response follow-up protocol to encourage participation can reduce non-

response bias. A pilot survey can improve question wording and so reduce measure‐

20 | Chapter 1: Questions and Data Scope

ment bias. Procedures to calibrate instruments and protocols to take measurements

in, say, random order can reduce measurement bias.

In the 2016 US Presidential Election, non-response bias and measurement bias were

key factors in the inaccurate predictions of the winner. Nearly all voter polls leading

up to the election predicted Clinton a winner over Trump. Clinton’s upset victory

came as a surprise. After the election, many polling experts attempted to diagnose

where things went wrong in the polls. The American Association for Public Opinion

Research (Kennedy et al. 2017), found predictions made were flawed for two key rea‐

sons:

Over-representation of college-educated voters. College-educated voters are
more likely to participate in surveys than those with less education, and in 2016
they were more likely to support Clinton. Non-response biased the sample and
over-estimated support for Clinton (Pew Research Center 2012).
Voters were undecided or changed their preferences a few days before the elec‐
tion. Since a poll is static and can only directly measure current beliefs, it cannot
reflect a shift in attitudes.
It’s difficult to figure out whether people held back their preference or changed their

preference and how large a bias this created. However, exit polls have helped polling

experts understand what happened, after the fact. They indicate that in battleground

states, such as Michigan, many voters made their choice in the final week of the cam‐

paign, and that group went for Trump by a wide margin.

Bias does not need to be avoided under all circumstances. If an instrument is highly

precise (low variance) and has a small bias, then that instrument might be preferable

to another with higher variance and no bias. As an example, biased studies are poten‐

tially useful to pilot a survey instrument or to capture useful information for the

design of a larger study. Many times we can at best recruit volunteers for a study.

Given this limitation, it can still be useful to enroll these volunteers in the study and

use random assignment to split them into treatment groups. That’s the idea behind

randomized controlled experiments.

Whether or not bias is present, data typically also exhibit variation. Variation can be

introduced purposely by using a chance mechanism to select a sample, and it can

occur naturally through an instrument’s precision. In the next section, we identify

three common sources of variation.

Types of Variation
Variation that results from a chance mechanism has the advantage of being quantifia‐

ble.

Accuracy | 21
Sampling variation results from using chance to take a sample. We can in princi‐
ple compute the chance a particular sample is selected.
Assignment variation of units to treatment groups in a controlled experiment
produces variation. If we split the units up differently, then we can get different
results from the experiment. This randomness allows us to compute the chance
of a particular group assignment.
Measurement error for instruments result from the measurement process; if the
instrument has no drift and a reliable distribution of errors, then when we take
multiple measurements on the same object, we get variations in measurements
that are centered on the truth.
The Urn Model is a simple abstraction that can be helpful for understanding varia‐

tion. This model examines a container (an urn) full of identical marbles that have

been labeled, and we use the simple action of drawing balls from the urn to reason

about sampling schemes, randomized controlled experiments, and measurement

error. For each of these types of variation, the urn model helps us estimate the size of

the variation using either probability or simulation (see Chapter 2). The example of

selecting Wikipedia contributors to receive an informal award provides two examples

of the urn model.

Recall that for the Wikipedia experiment, a group of 200 contributors was selected at

random from 1,440 top contributors. These 200 contributors were then split, again at

random, into two groups of 100 each. One group received an informal award and the

other didn’t. Here’s how we use the urn model to characterize this process of selection

and splitting:

Imagine an urn filled with 1,440 marbles that are identical in shape and size, and
written on each marble is one of the 1,440 Wikipedia usernames. (This is the
access frame.)
Mix the marbles in the urn really well, select one marble and set it aside.
Repeat the mixing and selecting of the marbles to obtain 200 marbles.
The marbles drawn form the sample. Then, to determine which of the 200 contribu‐

tors receives the informal award, we work with another urn.

In a second urn, put in the 200 marbles from the above sample.
Mix these marbles well and select one marble and set it aside.
Repeat. That is, choose 100 marbles, one at a time, mixing in between, and set‐
ting the chosen marble aside.
22 | Chapter 1: Questions and Data Scope

The 100 drawn marbles are assigned to the treatment group and correspond to the

contributors who receive the award. The 100 left in the urn form the control group

and receive no award.

Both the selection of the sample and the choice of award recipients use a chance

mechanism. If we were to repeat the first sampling activity again, returning all 1,440

the marbles to the original urn, then we would most likely get a different sample. This

variation is the source of sampling variation. Likewise, if we were to repeat the ran‐

dom assignment process again (keeping the sample of 200 from the first step

unchanged), then we would get a different treatment group. Assignment variation ari‐

ses from this second chance process.

The Wikipedia experiment provided an example of both sampling and assignment

variation. In both cases, the researcher imposed a chance mechanism on the data col‐

lection process. Measurement error can at times also be considered a chance process

that follows an urn model. We characterize the measurement error in the air quality

sensors in this way in the following example.

If we can draw an accurate analogy between variation in the data and the urn model,

the urn model provides us the tools to estimate the size of the variation. (See Chap‐

ter 2). This is highly desirable because way we can give concrete values for the varia‐

tion in our data. However, it’s vital to confirm that the urn model is a reasonable

depiction of the source of variation. Otherwise, our claims of accuracy can be seri‐

ously flawed. Knowing as much as possible about data scope, including instruments

and protocols and chance mechanism used in data collection, are needed to apply urn

models.

Summary
No matter the kind of data you are working with, before diving into cleaning, explo‐

ration, and analysis, take a moment to look into the data’s source. If you didn’t collect

the data, ask yourself:

Who collected the data?
Why were the data collected?
Answers to these questions can help determine whether these found data can be used

to address the question of interest to you.

Consider the scope of the data. Questions about the temporal and spatial aspects of

data collection can provide valuable insights:

When were the data collected?
Where were the data collected?
Summary | 23
Answers to these questions help you determine whether your findings are relevant to

the situation that interests you, or whether your situation that may not be comparable

to this other place and time.

Core to the notion of scope are answers to the following questions:

What is the target population (or unknown parameter value)?
How was the target accessed?
What methods were used to select samples/take measurements?
What instruments were used and how were they calibrated?
Answering as many of these questions as possible can give you valuable insights as to

how much trust you can place in your findings and how far you can generalize your

findings.

This chapter has provided you with a terminology and framework for thinking about

and answering these questions. The chapter has also outlined ways to identify possi‐

ble sources of bias and variance that can impact the accuracy of your findings. To

help you reason about bias and variance, we have introduced the following diagrams

and notions:

Scope diagram to indicate the overlap between target population, access frame,
and sample;
Dart board to describe an instrument’s bias and variance; and
Urn model for situations when a chance mechanism has been used to select a
sample from an access frame, divide a group into experimental treatment groups,
or take measurements from a well calibrated instrument.
These diagrams and models attempt to boil down key concepts to understanding how

to identify limitations and judge the usefulness of your data in answering your ques‐

tion. Chapter 2 continues the development of the urn model to more formally quan‐

tify accuracy and design simulation studies.

Exercises
24 | Chapter 1: Questions and Data Scope

CHAPTER 2

Simulation and Data Design

A Note for Early Release Readers

With Early Release ebooks, you get books in their earliest form—the author’s raw and
unedited content as they write—so you can take advantage of these technologies long
before the official release of these titles.
This will be the 3rd chapter of the final book. Please note that the GitHub repo will be
made active later on.
If you have comments about how we might improve the content and/or examples in
this book, or if you notice missing material within this chapter, please reach out to the
author at mpotter@oreilly.com.
In this chapter, we develop the theory behind the chance processes introduced in

Chapter 1. This theory makes the concepts of bias and variation more precise. We

continue to motivate the accuracy of our data through the abstraction of an urn

model that was first introduced in Chapter 1, and we use simulation studies to help us

understand and make decisions based on the data.

The urn model gives us a technical framework to design and run simulation studies

to understand larger and more complex situations. For example, we can dive deeper

into understanding how the pollsters might have gotten the 2016 Presidential Elec‐

tion predictions wrong (Chapter 1). To do this, we use the actual votes cast in Penn‐

sylvania and simulate the sampling variation for a poll of the six million voters. This

simulation helps us uncover how response bias can skew polls, and convince us that

collecting a lot more data would not have helped the situation (another example of

big data hubris).

25
In a second simulation study, we examine the efficacy of a COVID-19 vaccine. A

designed experiment for the vaccine was carried out on over 50,000 volunteers.

Abstracting the experiment to an urn model gives us a tool for studying assignment

variation in randomized controlled experiments. Through simulation, we find the

expected outcome of the clinical trial. Our simulation, along with careful examination

of the data scope, debunks claims of vaccine ineffectiveness.

In addition to sampling variation and assignment variation, we also cast measure‐

ment error in terms of an urn model. We use multiple measurements from different

times of the day to estimate the accuracy of an air quality sensor. Later in ???, we pro‐

vide a more comprehensive treatment of measurement error and instrument calibra‐

tion for air quality sensors.

We begin with an artificial example of a small population; it’s so small that we can list

all the possible samples that can be drawn from the population.

The Urn Model
The urn model is a simple abstraction of a chance mechanism. But, we can extend the

randomness in the selection process of drawing marbles from an urn to many chance

processes in real-life examples, and we can simulate this random behavior and use

our findings to better understand the accuracy of our data. To explain the urn model,

we use a small example with seven marbles. The urn is small enough that we can list

all possible outcomes that might result from drawing marbles from the urn.

We use the SpaceX Starship prototypes as a made-up example. The prototypes are

called SN 1 , SN 2 , ..., where SN stands for “serial number”. In the first half of 2020,

seven of these prototypes were built. Before deploying them a few were pressure

tested. Suppose we want to select three of the seven Starship prototypes for pressure

testing. (While this example is made up, the context is based on the actual SpaceX

program; pressure tests were made on the Starship prototypes.)

We can set up the urn model to represent the selection process as follows: write a

unique label on each of seven marbles, place all the marbles in the urn, mix them

well, and draw three without looking and without replacing marbles between draws.

The urn is small enough that we can list all possible samples of three marbles that can

be drawn:

ABC ABD ABE ABF ABG ACD ACE ACF ACG ADE ADF ADG
AEF AEG AFG BCD BCE BCF BCG BDE BDF BDG BEF BEG
BFG CDE CDF CDG CEF CEG CFG DEF DEG DFG EFG
We use the labels A, B, etc. rather than SN 1 , SN 2 , etc. because they are shorter and

easier to distinguish. Our list shows that we could wind up with any one of the ( 35 )

unique sets of three of the seven marbles.

26 | Chapter 2: Simulation and Data Design

We can draw an analogy to data scope from Chapter 1: a set of marbles drawn from

the urn is a sample, and the collection of all marbles placed in the urn is the popula‐

tion. This particular urn model prescribes a particular selection method, called the

Simple Random Sample (SRS). We describe the SRS and other sampling techniques

based on the SRS in the next section.

Sampling Designs
The urn model for the SpaceX prototypes reduced to a few basics. We specified:

the number of marbles in the urn;
what is written on each marble;
the number of marbles drawn from the urn; and
whether or not marbles are replaced between draws.
This process is equivalent to a Simple Random Sample. Our example is a SRS of three

draws from a population of seven, and by design, each of the ( 35 ) samples is

equally likely to be chosen because the marbles are indistinguishable and well mixed.

This means the chance of any one particular sample must be 1/35,

ℙABC =ℙABD =⋯=ℙEFG =

1
35
.
We use the special symbol ℙ to stand for “probability” or “chance”, and we read the

statement ℙABC as the “chance the sample contains the marbles labeled A, B, and C

in any order.”

The urn model gives us a more formal definition of “representative data” that is very

useful:

In a *Simple Random Sample*, every sample has the same chance of being selected.
Many people mistakingly think that the defining property of a SRS
is that every unit has an equal chance of being in the sample. How‐
ever, this is not the case. A SRS of n units from a population of N
means that every possible subset of n units has the same chance of
being selected.
We can use the enumeration of all of the possible samples from the urn to answer

additional questions about this chance process. For example, to find the chance that

marble A is in the sample, we can add up the chance of all samples that contain A.

There are 15 of them so the chance is:

ℙA is in the sam ple =

15
35
=
3
7
.
The Urn Model | 27
When it’s too difficult to list and count all of the possible samples, we can use simula‐

tion to help understand this chance process.

The SRS (and its corresponding urn) is the main building block for more complex

survey designs. We briefly describe two of the more widely used designs.

Stratified Sampling Divide the population into non-overlapping groups, called
strata (one group is called a stratum and more than one are strata), and then take
a simple random sample from each. This is like having a separate urn for each
stratum and drawing marbles from each urn, independently. The strata do not
have to be the same size, and we need not take the same number of units from
each.
Cluster Sampling Divide the population into non-overlapping subgroups, take a
simple random sample of the clusters, and include all of the units in a cluster in
the sample. We can think on this as a SRS from one urn that contains large mar‐
bles that are themselves containers of small marbles. When opened, the sample of
large marbles turns into the sample of small marbles. (Clusters tend to be smaller
than strata.)
Often, we are interested in a summary of the sample; that is, some statistic. For any

sample, we can calculate the statistic, and the urn model helps us find the distribution

of possible values that statistic may take on. In the next section, we examine the dis‐

tribution of a statistic for our example.

Sampling Distribution of a Statistic
Suppose we are interested in whether or not the prototypes can pass a pressure test.

It’s expensive to carry out the pressure test, so we first test only a sample of proto‐

types. We can use the urn model to choose the protoypes to be pressure tested, and

then, we can summarize our test results by, say, the proportion of prototypes that fail

the test. The urn model provides us the knowledge that each of the 35 possible sam‐

ples has the same chance of being selected and so the pressure test results are repre‐

sentative of the population.

For concreteness, suppose prototypes A, B, D, and F would fail the pressure test, if

chosen. For each sample of three marbles, we can find the proportion of failures

according to how many of these four defective prototypes are in the sample. We give

a few examples of this calculation.

Sample ABC BCE BDF CEG
Proportion 2/3 1/3 1 0
28 | Chapter 2: Simulation and Data Design

Since we are drawing three marbles from the urn, the only possible sample propor‐

tions are ( 0 ), 1/3, 2/3 and ( 1 ), and for each triple, we can calculate its corre‐

sponding proportion. There are 4 samples that give us all failed tests (a sample

proportion of 1): ABD , ABF, ADF, BDF, so the chance of observing a sample pro‐

portion of ( 1 ) is 4/35. We can summarize the distribution of values for the sample

proportion into a table.

Proportion of Fails No. of Samples Fraction of Samples
1 4 4/35
2/3 18 18/35
1/3 12 12/35
0 1 1/35
Total 35 1
While these calculations are relatively straight forward, we can approximate them

through a simulation study. To do this, we take samples of three from our population

over and over, say 10,000 times. For each sample, we calculate the proportion of fail‐

ures. That gives us 10,000 simulated sample proportions. The table of the simulated

proportions should come close to our distribution table. We confirm this with a sim‐

ulation study.

Simulating the Sampling Distribution
Our original urn had seven marbles marked A through G. However, since we care

only whether the prototype fails or passes the test, we can re-label each marble as ‘fail’

or ‘pass’. We create this revised urn as an array.

urn = ['fail', 'fail', 'fail', 'fail', 'pass', 'pass', 'pass']
Then, we simulate the draw of three marbles from our urn without replacement

between draws using numpy’s ‘random.choice’ method.

np.random.choice(urn, size=3, replace=False)
array(['pass', 'fail', 'pass'], dtype='<U4')
Let’s take a few more sample from our urn to see what the results might look like.

[np.random.choice(urn, size = 3, replace = False) for i in range(10)]
[array(['fail', 'fail', 'pass'], dtype='<U4'),
array(['fail', 'fail', 'pass'], dtype='<U4'),
array(['fail', 'pass', 'pass'], dtype='<U4'),
array(['fail', 'pass', 'pass'], dtype='<U4'),
array(['fail', 'pass', 'fail'], dtype='<U4'),
array(['fail', 'fail', 'pass'], dtype='<U4'),
array(['fail', 'fail', 'fail'], dtype='<U4'),
array(['fail', 'fail', 'pass'], dtype='<U4'),
The Urn Model | 29
array(['fail', 'fail', 'pass'], dtype='<U4'),
array(['fail', 'fail', 'pass'], dtype='<U4')]
Since we simply want to count the number of failures in the sample, it’s easier if the

marbles are labeled 1 for fail and 0 for pass. This way, we can sum the results of the

three draws to get the number of failures in the sample. We re-label the marbles in the

urn again, and compute the fraction of fails in a sample.

urn = [1, 1, 1, 1, 0, 0, 0]
sum(np.random.choice(urn, size=3, replace=False))/3
0.6666666666666666
We have stream-lined the process and we’re now ready to carry out the simulation

study. Let’s repeat the process 100,000 times.

simulations = [sum(np.random.choice(urn, size=3, replace=False)) / 3
for i in range(10000)]
We can study these 100,000 sample proportions and match our findings against what

we calculated already using the complete enumeration of all 35 possible samples. We

expect the simulation results to be close to our earlier calculations because we have

repeated the sampling process many many times. That is, we want to compare the

fraction of the 100,000 sample proportion that are ( 0 ), 1/3, 2/3, and ( 1 ) to those

in the table. These fractions should be, approximately, 1/35, 12/35, 18/35, and 4/35,

or about 0. 03, 0. 34, 0. 51, and 0. 11.

unique_els, counts_els = np.unique(np.array(simulations), return_counts=True)
np.array((unique_els, counts_els/10000))
array([[0. , 0.33, 0.67, 1. ],
[0.03, 0.34, 0.52, 0.11]])
The simulation results closely match the table.

This simulation study does not prove, say, that we expect 18/35 samples to have two

fails, but it does give us excellent approximations to our earlier calculations, which is

reassuring. More importantly, when we have a more complex setting where it might

be difficult to list all possibilities, a simulation study can offer valuable insights.

A simulation study repeats a random process many many times. A
summary of the patterns that result from the simulation can
approximate the theoretical properties of the chance process. This
summary is not the same as proving the theoretical properties, but
often the guidance we get from the simulation is adequate for our
purposes.
Drawing marbles from an urn with 0s and 1s is such a popular framework for under‐

standing randomness that this chance process has been given a formal name, the

30 | Chapter 2: Simulation and Data Design

hypergeometric. And, most software provide functionality to rapidly carry out simu‐

lations of the hypergeomtric. We can redo our simulation using the hypergeometric.

The Hypergeometric
The version of the urn model where we count the number of marbles of a certain type

(in our case ‘fail’ marbles) is so common that there is a random chance process

named for it: the hypergeometric. Instead of using random.choice, we can use num‐

py’s random.hypergeometric to simulate drawing marbles from the urn and counting

the number of fails. The random.hypergeometric method is optimzed for the 0-1

urn and allows us to ask for 10,000 simulations in one call. For completeness, we

repeat our simulation study and calculate the empirical proportions.

simulations_fast = np.random.hypergeometric(ngood=4, nbad=3, nsample=3, size=10000)
Note: we don’t think that a pass is “bad”; it’s just a naming convention to call the type

you want to count “good” and the other “bad”.

unique_els, counts_els = np.unique(np.array( simulations_fast ), return_counts=True)
np.array((unique_els, counts_els/10000))
array([[0. , 1. , 2. , 3. ],
[0.03, 0.34, 0.52, 0.11]])
You might have asked yourself already - since the hypergeometric is so popular, why

not provide the exact distribution of the possible values. In fact, these are available,

and we show how to calculate them below.

from scipy.stats import hypergeom
x = np.arange(0, 4)
hypergeom.pmf(x, 7, 4, 3)
array([0.03, 0.34, 0.51, 0.11])
Perhaps the two most common chance processes are those that arise from counting

the number of 1s drawn from a 0-1 urn: drawing without replacement is the hyper‐

geometric and drawing with replacement is the binomial.

Whenever possible, it’s a good idea to use the functionality pro‐
vided in a third party package for simulating from a named distri‐
bution, rather than writing your own function, such as the random
number generators offered in numpy. It’s best to take advanatge of
efficient and accurate code that others have devloped.
While this simulation was simple, so simple that we could have used hypergeom.pmf

to complete our distribution, we wanted to demonstrate the intuition that a simula‐

tion study can reveal. The approach we take in this book is to develop understanding

The Urn Model | 31
1 https://www.aapor.org/Education-Resources/Reports/An-Evaluation-of-2016-Election-Polls-in-the-U-S.aspx
about chance processes based on simulation studies. However, we do formalize the

notion of a probability distribution of a statistic (like the proportion of fails in a sam‐

ple) in ???.

Now that we have simulation as a tool for understanding accuracy, we can revisit the

election example from Chapter 1 and carry out a post-election study of what might

have gone wrong with the voter polls. This simulation study imitates drawing more

than a thousand marbles (voters who participate in the poll) from an urn of six mil‐

lion. We can examine potential sources of bias and the variation in the polling results,

and carry out a what-if analysis, where we examine how the predictions might have

gone if even a larger number of draws from the urn were taken.

Example: Simulating Election Poll Bias and Variance
The President of the US is chosen by the Electoral College, and not solely by popular

vote. Each state is alotted a certan number of votes to cast in the Electoral College,

according to the size of their population. Typically, whomever wins the popular vote

in a state receives all of the electoral college votes for that state. With the aid of polls

conducted in advance of the election, pundits identify “battleground” states where the

election is expected to be close and the electoral college votes might swing the elec‐

tion.

In 2016, nearly every prediction for the outcome of the election was wrong. Pollsters

correctly predicted the election outcome in 46 of the 50 states. For those 46 states,

Trump received 231 and Clinton received 232 electoral college votes. The remaining 4

states, Florida, Michigan, Pennsylvania, and Wisconsin, were identified as battle‐

ground states and accounted for a total of 75 votes. The margins of the popular vote

in these four states were narrow. For example, in Pennsylvania, Trump received

48.18% and Clinton received 47.46% of the 6,165,478 votes cast. Such narrow mar‐

gins can make it hard to predict the outcome given the sample sizes that the polls

used.

Many experts have studied the 2016 election results to dissect and identify what went

wrong. According to the American Association for Public Opinion Research^1 , one

online, opt-in poll adjusted their polling results for the education of the respondents

but used only three broad categories (high school or less, some college, and college

graduate). They found that if they had separated out those with advanced degrees

from those with college degrees, then they would have reduced Clinton’s margin by

0.5 percentage points. In other words, after the fact they were able to identify an edu‐

cation bias where highly educated voters tended to be more willing to participate in

32 | Chapter 2: Simulation and Data Design

2 https://blogs.lse.ac.uk/usappblog/2018/02/01/better-poll-sampling-would-have-cast-more-doubt-on-the-
potential-for-hillary-clinton-to-win-the-2016-election/
polls. This bias matters because these voters also tended to prefer Clinton over

Trump.

Now that we know how people actually voted, we can carry out a simulation study

that imitates election polling under different scenarios to help develop intuition for

accuracy, bias, and variance^2. We will simulate the polls for Pennsylvania under two

scenarios:

People surveyed didn’t change their minds, didn’t hide who they voted for, and
were representative of those who voted on election day.
People with a higher education were more likely to respond, which led to a bias
for Clinton.
Our ultimate goal is to understand the frequency that a poll incorrectly calls the elec‐

tion for Hillary Clinton when a sample is collected with absolutely no bias and also

when there is a small amount of non-response bias. We begin by setting up the urn

model for the first scenario.

The Pennsylvania Urn Model
Our urn model for carrying out a poll of Pennsylvania voters is an after-the-fact sit‐

uation where we use the outcome of the election. The urn has 6,165,478 marbles in it,

one for each voter. Like with our tiny population, we write on each marble the candi‐

date that they voted for, draw 1500 marbles from the urn (1500 is about the typical

size of the polls), and tally up the votes for Trump, Clinton, and any other candidate.

From the tally, we can calculate Trump’s lead over Clinton.

To set up our simulation, we figure out what the urn looks like. We need to know the

number of votes cast for each of the candidates. Since we care only about Trump’s

lead over Clinton, we can lump together all votes for other candidates together. This

way each marble has one of three possible votes: Trump, Clinton, and Other. We can’t

ignore the “Other” category, because it impacts the size of the lead.

proportions = np.array([0.4818, 0.4746, 1 - (0.4818 + 0.4746)])
n = 1_500
N = 6_165_478
votes = np.trunc(N * proportions).astype(int)
votes
array([2970527, 2926135, 268814])
This version of the urn model has three types of marbles in it. It is a bit more complex

than the hypergeometric, but still common enough to have a named distribution: the

Example: Simulating Election Poll Bias and Variance | 33
multivariate hypergeometric. In Python, the urn model with more than two types of

marbles is implemented as the scipy.stats.multivariate_hypergeom.rvs method.

The function returns the number of each type of marbel drawn from the urn. We call

the function as follows.

from scipy.stats import multivariate_hypergeom
multivariate_hypergeom.rvs(votes, n)
array([747, 681, 72])
As before, each time we call multivariate_hypergeom.rvs we get a different sample

and counts.

multivariate_hypergeom.rvs(votes, n)
array([749, 697, 54])
We need to compute Trump’s lead for each sample: nT−nC/n, where nT are the

number of Trump votes in the sample and nC the number for Clinton. If the lead is

positive, then the sample shows a win for Trump.

We know the actual lead was, 0.4818 - 0.4746 = 0.0072. To get a sense of the variation

in the poll, we can simulate the chance process of drawing from the urn over and over

and examine the values that we get in return. Below we simulate 100,000 polls of 1500

voters from the votes cast in Pennsylvania.

def trump_advantage(votes, n):
sample_votes = multivariate_hypergeom.rvs(votes, n)
return (sample_votes[0] - sample_votes[1]) / n
simulations = [trump_advantage(votes, n) for _ in range(100000)]
On average, the polling results show Trump with close to a 0.7% lead, as expected

given the composition of the six-plus million votes cast.

np.mean(simulations)
0.007210106666666665
However, many times the lead in the sample was negative, meaning Clinton was the

winner for that sample of voters. The histogram below shows the sampling distribu‐

tion of Trump’s advantage in Pennsylvania for a sample of 1500 voters. The vertical

dashed line at 0 shows that more often than not, Trump is called, but there are many

times when the poll of 1,500 shows Clinton in the lead.

Text(0.5, 0, 'Trump Lead in the Sample')
34 | Chapter 2: Simulation and Data Design

3 https://blogs.lse.ac.uk/usappblog/2018/02/01/better-poll-sampling-would-have-cast-more-doubt-on-the-
potential-for-hillary-clinton-to-win-the-2016-election/
In the 100,0000 simulated polls, we find Trump a victor about 60% of the time:

np.count_nonzero(np.array(simulations) > 0) / 100000
0.60797
In other words, a sample will correctly predict Trump’s victory even if the sample was

collected with absoutely no bias about 60% of the time. And, this unbiased sample will

be wrong about 40% of the time.

We have used the urn model to study the variation in a simple poll, and we found

how a poll’s prediction might look if there was no bias in our selection process (the

marbles are indistinguishable and every possible collection of 1500 marbles of the

six-plus million marbles is equally likely). Next, we see what happens when a little

bias enters into the mix.

An Urn Model with Bias
“In a perfect world, polls sample from the population of voters, who would state their

political preference perfectly clearly and then vote accordingly.”^3 That’s the simulation

Example: Simulating Election Poll Bias and Variance | 35
study that we just performed. In reality, it is often difficult to control for every source

of bias.

We investigate here the effect of a small, education bias on the polling results. Specifi‐

cally, we examine the impacts of 0.5 percent bias in favor of Clinton. This bias essen‐

tially means that we see a distorted picture of voter preferences in our poll. Instead of

47.46 percent votes for Clinton, we have 47.96, and we have 48.18 - 0.5 = 47.68 per‐

cent for Trump. We adjust the proportions of marbles in the urn to reflect this bias.

proportions_bias = np.array([0.4818 - 0.005, 0.4747 + 0.005, 1 - (0.4818 + 0.4746) ])
proportions_bias
array([0.48, 0.48, 0.04])
votes_bias = np.trunc(N * proportions_bias).astype(int)
votes_bias
array([2939699, 2957579, 268814])
When we carry out the simulation study again, this time with the biased urn, we find

a different result.

simulations_bias = [trump_advantage(votes_bias, n) for i in range(100000)]
Text(0.5, 0, 'Trump Lead in the Sample')
np.count_nonzero(np.array(simulations_bias) > 0) / 100000
0.44742
Now, Trump would have a positive lead in about 45% of the polls. Notice that the his‐

tograms from the two simulations are similar in shape. They are symmetric with rea‐

36 | Chapter 2: Simulation and Data Design

sonable length tails. That is, they appear to roughly follow the normal curve. The

second histogram is shifted slightly to the left, which reflects the non-response bias

we introduced. Would increasing the sample size have helped? This is the topic of the

next section.

Conducting Larger Polls
With our simulation study we can gain insight on the impact of a larger poll on the

sample lead. For example, we can try a sample size of 12,000, eight times the size of

the actual poll, and run 100,000 simulations for both scenarios: the unbiased and the

biased.

simulations_big = [trump_advantage(votes, 12000) for i in range(100000)]
simulations_bias_big = [trump_advantage(votes_bias, 12000) for i in range(100000)]
scenario_no_bias = np.count_nonzero(np.array(simulations_big) > 0) / 100000
scenario_bias = np.count_nonzero(np.array(simulations_bias_big) > 0) / 100000
print(scenario_no_bias, scenario_bias)
0.78664 0.37131
The simulation shows that Trump’s lead is detected in only about one-third of the

simulated biased scenario. The spread of the histogram of these results is narrower

than the spread when only 1,500 voters were polled. Unfortunately, it’s narrowing in

on the wrong value. We haven’t overcome the bias; we just have a more accurate pic‐

ture of the biased situation. Big data has not come to the rescue. Additionally, larger

polls have other problems. They are often harder to conduct because pollsters are

working with limited resources and efforts that go into improving the data scope are

being redirected to expanding the poll.

Text(0.5, 0, 'Trump Lead in the Sample')
Example: Simulating Election Poll Bias and Variance | 37
After the fact, with multiple polls for the same election, we can detect bias. In a post-

election analysis of over 4,000 polls for 600 state-level, gubernatorial, senatorial, and

presidential elections, (Shirani-Mehr et al. 2018) found that on average election polls

exhibit a bias of about 1.5 percentage points.

When the margin of victory is relatively small as it was in 2016, a larger sample size

reduces the sampling error, but unfortunately, if there is bias, then the predictions are

close to the biased estimate. If the bias pushes the prediction from one candidate

(Trump) to another (Clinton), then we have a “surprise” upset. Pollsters develop voter

selection schemes that attempt to reduce bias, like the separation of voters preference

by education level, but, as in this case, it can be difficult, even impossible, to account

for new, unexpected sources of bias. Polls are still useful, but we need to acknowledge

the issues with bias and do a better job at reducing bias.

In this example we used the urn model to study a SRS in polling. Another common

use of the urn is in randomized controlled experiments.

Example: Simulating a Randomized Trial for a Vaccine
In a drug trial, volunteers for the trial either receive the new treatment or a placebo (a

fake treatment), and researchers control the assignment of volunteers to groups. In a

randomized controlled experiment, they use a chance process to make the assign‐

ment. Scientists essentially use an urn model model to select the subjects for the treat‐

ment and control (those given the placebo). We can simulate the chance mechanism

of the urn to better understand variation in the outcome of an experiment and the

meaning of efficacy in clinical trials.

38 | Chapter 2: Simulation and Data Design

In March 2021, Detroit Mayor Mike Duggan made national news when he turned

down a shipment of over 6,000 Johnson & Johnson vaccine doses stating that the citi‐

zens of his city should “get the best”. The mayor was referring to the efficacy rate of

the vaccine, which was reported to be about 66%. In comparison, Moderna and Pfizer

both reported efficacy rates of about 95% for their vaccines.

On the surface, Duggan’s reasoning seems valid, but the scope of the three clinical tri‐

als are not comparable, meaning direct comparisons of the experimental results is

problematic (Irfan 2021). Moreover, the Centers for Disease Control (CDC) consid‐

ers a 66% efficacy rate quite good, which is why it was given emergency approval

(Centers for Disease Control 2021).

We consider these points in turn, beginning with scope and then efficacy.

Scope
Recall that when we evaluate the scope of the data, we consider the who, when, and

where of the study. For the Johnson & Johnson clinical trial, the participants:

included adults 18 and over, where roughly 40% had conditions, called comor‐
bidities, associated with an increased risk for getting severe COVID-19;
enrolled in the study from October to November, 2020; and
came from 8 countries across 3 continents, including the US and South Africa.
The participants in the Moderna and Pfizer trials were primarily from the US,

roughly 40% had comorbidities for severe COVID-19, and the trial took place earlier,

over summer 2020. The timing and location of the trials make them difficult to com‐

pare. Cases of COVID-19 were at a low point in the summer in the US, but they rose

rapidly in the late fall. Also, a variant of the virus that is more contagious was spread‐

ing rapidly in South Africa at the time of the J&J trial.

Each clinical trial was designed to test a vaccine against the situation of no vaccine

under similar circumstances through the random assignment of subjects to treatment

and control groups. While the scope from one trial to the next are quite different, the

randomization within a trial keeps the scope of the treatment and control groups

roughly the same. This enables meaningful comparisons between groups in the same

trial. The scope was different enough across the three vaccine trials to make direct

comparisons of the three trials problematic.

How was the trial carried out for the Johnson & Johnson vaccine? To begin, 43,738

people enrolled in the trial (Janssen Biotech, Inc. 2021). These participants were split

into two groups at random. Half received the new vaccine, and the other half received

a placebo, such as a saline solution. Then, everyone was followed for 28 days to see

whether they contracted COVID-19.

Example: Simulating a Randomized Trial for a Vaccine | 39
A lot of information was recorded on each patient, such as their age, race, and sex,

and in addition, whether they caught COVID, including the severity of the disease. At

the end of 28 days, they found 468 cases of COVID-19, 117 of these were in the treat‐

ment group, and 351 in the control group.

The random assignment of patients to treatment and control, gives the scientists a

framework to assess the effectiveness of the vaccine. The typical reasoning goes as fol‐

lows:

Begin with the assumption that the vaccine is ineffective.
So, the 468 who caught COVID-19 would have caught it whether or not they
received the vaccine.
And, the remaining 43,270 people in the trial who did not get sick would have
remained healthy whether or not they received the vaccine.
The split of 117 sick people in treatment and 351 in control was solely due to the
chance process in assigning participants to treatment or control.
We can set up an urn model that reflects this scenario and then study, via simulation,

the behavior of the experimental results.

The Urn Model for Random Assignment
Our urn has 43,738 marbles, one for each person in the clinical trial. Since there were

468 cases of COVID-19 among them, we label 468 marbles with a 1 and the remain‐

ing 43,270 with 0. We draw half the marbles (21,869) from the urn to receive the

treatment, and the remaining half receive the placebo. The results of the experiment

are simply the count of the number of marbles marked 1 that were randomly drawn

from the urn.

We can simulate this process to get a sense of how likely it would be under these

assumptions to draw at most 117 marbles marked 1 from the urn. Since we draw half

of the marbles from the urn, we would expect about half of the 468, or 234, to be

drawn. The simulation study gives us a sense of the variation that might result from

the random assignment process. That is, the simulation can give us an approximate

chance that the trials would result in so few cases of the virus in the treatment group.

There are several key assumptions that enter into this urn model,
such as the assumption that the vaccine is ineffective. It’s important
to keep track of the reliance on these assumptions because our sim‐
ulation study gives us an approximation of the rarity of an outcome
like the one observed only under these key assumptions.
40 | Chapter 2: Simulation and Data Design

As before, we can simulate the urn model using the hypergeometric probability dis‐

tribution, rather than having to program the chance process from scratch.

simulations_fast = np.random.hypergeometric(ngood=468, nbad=43270, nsample=21869, size=500000)
Text(0.5, 0, 'Cases in the Treatment Group')
In our simulation, we repeated the process of random assignment to the treatment

group 500,000 times. Indeed, we found not one of the 500,000 simulations had 117 or

fewer cases. It would be an extremely rare event to see so few cases of COVID-19, if

in fact the vaccine was not effective.

After the problems with comparing drug trials that have different scopes and the effi‐

cacy for preventing severe cases of COVID-19 was explained, the Mayor of Detroit

retracted his original statement, saying “I have full confidence that the Johnson &

Johnson vaccine is both safe and effective.”

This example has shown that:

Using a chance process in the assignment of subjects to treatments in clinical tri‐
als can help us answer what-if scenarios;
Considering data scope can help us determine whether it is reasonable to com‐
pare figures from different datasets.
Another example of the usefulness of the urn model is with measurement error.

Example: Simulating a Randomized Trial for a Vaccine | 41
Example: Measurement Error in Air Quality
Simulating the draw of marbles from an urn is a useful abstraction for studying the

possible outcomes from survey samples and controlled experiments. The simulation

works because it imitates the chance mechanism used to select a sample or to assign

people to a treatment. In many settings, measurement error also follows a similar

chance process. As mentioned in Chapter 1, instruments typically have an error asso‐

ciated with them, and by taking repeated measurements on the same object, we can

quantify the variability associated with the instrument.

As an example, let’s look at data from a PurpleAir sensor that measures air quality.

PurpleAir provides a data download tool so anyone can access air quality measure‐

ments by interacting with their map. These data are available in 2-minute intervals for

any sensor appearing on their map. To get a sense of the size of the variations in

measurements for a sensor, we downloaded data for one sensor from a 24-hour

period and selected five hours spread throughout the day. This gives us five sets of

thirty consecutive measurements for a total of 150 measurements.

pm = pd.read_csv('data/purpleAir2minsample.csv')
pm
aq2.5 time hour diffs meds
0 6.14 2022-04-01 00:01:10 UTC^0 0.77 5.38
1 5.00 2022-04-01 00:03:10 UTC^0 -0.38 5.38
2 5.29 2022-04-01 00:05:10 UTC^0 -0.09 5.38
... ... ... ... ... ...
147 8.08 2022-04-01 19:57:20 UTC^19 -0.47 8.55
148 7.38 2022-04-01 19:59:20 UTC^19 -1.18 8.55
149 7.26 2022-04-01 20:01:20 UTC^19 -1.29 8.55
150 rows × 5 columns

The feature aq2.5 in the data table refers to the amount of particulate matter meas‐

ured in the air that has a diameter smaller than 2.5 micrometers (the unit of measure‐

ment is micrograms per cubic meter: μg/m3). These measurements are 2-minute

averages. A scatter plot can gives us a sense of variation in the instrument. Within an

hour, we expect the measurements to be roughly the same, so we can get a sense of

the variability in the instrument.

42 | Chapter 2: Simulation and Data Design

At 11 in the morning, we see the measurements clump around 7 μg/m3, and five

hours later, they cluster around 10 μg/m3 or so. While the level of particulate matter

changes over the course of a day, in any one hour interval most measurements are

within about +/-1 μg/m3 of the median. To get a better sense of this variation, we can

examine the differences of each individual measurement from the median for the

hour.

Text(0.5, 0, 'Deviation from Hourly Median')
Example: Measurement Error in Air Quality | 43
The histogram shows us the typical error in measurements from this instrument are

about 1 μg/m3. Given the hourly measurements range from 4 to 13 μg/m3, we find

the instrument is reasonably accurate. Unfortunately, what we don’t know is whether

the measurements are close to the true air quality. To detect bias in the instrument,

we need to make comparisons against a more accurate instrument or take measure‐

ments in a protected environment where the air has a known quantity of PM2.5. A

more comprehensive analysis of the PurpleAir sensor measurement error, including

bias, is addressed in ???.

Summary
In this chapter, we used the analogy of drawing marbles for an urn to model random

sampling from populations, random assignment of subjects to treatments in experi‐

ments, and measurement error. This framework enables us to run simulation studies

for hypothetical surveys, experiments, or other chance processes in order to study

their behavior. We found the chance of observing particular results from a clinical

trial under the assumption that the treatment was not effective; and, we studied the

support for Clinton and Trump with samples based on actual votes cast in the elec‐

tion. These simulation studies enabled us to quantify the typical deviations in the

chance process and to approximate the distribution of summary statistics, lie Trump’s

lead. These simulation studies revealed the sampling distribution of a statistic, and

44 | Chapter 2: Simulation and Data Design

helped us answer questions about the likelihood of observing results like ours under

the urn model.

The urn model reduces to a few basics: the number of marbles in the urn; what is

written on each marble; the number of marbles to draw from the urn; and whether or

not they are replaced between draws. From there we can simulate more and more

complex data designs. However, the crux of the urn’s usefulness is the mapping from

the data design to the urn. If samples are not randomly drawn, subjects are not ran‐

domly assigned to treatments, or measurements are not made on well-calibrated

equipment, then this framework falls short in helping us understand our data and

making decisions. On the other hand, we also need to remember that the urn is a

simplification of the actual data collection process. If in reality, there is bias in data

collection, then the randomness we observe in the simulation doesn’t capture the

complete picture. Too often, data scientists wave these annoyances aside and address

only the variability described by the urn model. That was one of the main problems

in the surveys predicting the outcome of the 2016 US Presidential election.

In each of these examples, the summary statistics that we have studied was given to us

as part of the example. In the next chapter, we address the question of how to choose

a summary statistic to represent your data.

Exercises
In cluster sampling, the population is divided into non-overlapping subgroups,
which tend to be smaller than strata. The sampling method is to take a simple
random sample of the clusters and include all of the units in the chosen clusters
in the sample. Use the urn analogy, to represent cluster sampling. As a simple
example, suppose our population of (7) starship prototypes is placed into (4)
clusters as follows: A,B C,D E,F G. And, imagine we take a SRS of (2)
clusters.
—List all of the possible collections of starships that might result.

—What is the chance that A is in the sample?

—What is the chance that A, C and E are in the sample?

Cluster sampling has the advantage of making sample collection easier. For
example, it is much easier to poll 100 homes of 2-4 people each than to poll 300
individuals. But, since people in a cluster tend to be similar to each other, we
need to keep the sampling procedure in mind as we generalize from sample to
population. Continue with the starship clusters from the previous exercise, and
suppose additionally that prototypes, A, B, E and F are defective.
Exercises | 45
—Using the list of all possible samples that might result that you found in the
previous exercise, calculate the fraction of defective prototypes for each sam‐
ple.
—Create the sampling distribution of the fraction of defective prototypes. Rep‐
resent it in a table that contains the possible fractions and the chance of
observing each fraction.
—Compare this sampling distribution to one obtained for a SRS of three proto‐
types.
—Notice that the clusters contain similar prototypes (in terms of whether they
are defective). How does this impact the shape of the sampling distribution?
Systematic sampling is another popular technique. To start, the population is
ordered, and the first unit is selected at random from the first k elements. Then,
every kth unit after that is placed in the sample. As an example, suppose our pop‐
ulation of (7) prototypes is ordered alphabetically and we select one from the
first three A, B, C at random, and then every third element after that.
—List all of the possible samples that might result.

—What is the chance that A is in the sample?

—What is the chance that A and B are in the sample? A and G?

An example of an online intercept survey is when a popup window asks you to
complete a brief questionnaire. If every k
th
visitor to a website is asked to com‐
plete a survey, then this intercept survey behaves like a systematic sample.
—Describe the population, access frame, and sample for an intercept survey.

—Consider a stream of visits to a website on one day, V 1 , V 2 , ... Suppose you
choose the 20th visit, V 20 , to your site as the first to receive the popup survey.
Then, you administer the popup survey to every 20th visit after that. What’s
the chance that the 17th and 27th visits are in the sample? What’s the chance
that the 17th and 37th visits are in the sample?
—Why don’t you need to know the size of the population to calculate the above
chances?
—Does it seem reasonable to imagine that this sample doesn’t introduce a selec‐
tion bias in the sampling process? Why or why not?
A recent report found that half of the prisoners released in any given year in the
United States will end up back in prison within three years. Yet, another report
that follows individual prisoners released from prison over 20 years found that
about 2/3 of the prisoners released will never end up back in prison, over their
whole lifetime. How is this possible?
46 | Chapter 2: Simulation and Data Design

—For each report, carefully describe the population of interest and the access

frame.

—Some have described this apparent paradox as the difference between an

“event-based” sample and an “offender-based” sample. What might this mean?

—The notion of size-biased sampling and length-biased sampling occur when a

unit that is larger/longer is more likely to be part of the sample than another
smaller/shorter unit. Explain how a length bias may be responsible for these
apparent conflicting findings.
Exercises | 47
CHAPTER 3

Modeling with Summary Statistics

A Note for Early Release Readers

With Early Release ebooks, you get books in their earliest form—the author’s raw and
unedited content as they write—so you can take advantage of these technologies long
before the official release of these titles.
This will be the 4th chapter of the final book. Please note that the GitHub repo will be
made active later on.
If you have comments about how we might improve the content and/or examples in
this book, or if you notice missing material within this chapter, please reach out to the
author at mpotter@oreilly.com.
A model is an idealized representation of a system. You probably use models all the

time. For instance, a weather forecast is a model. A weather forecast uses past

weather, current conditions, and the physics of the atmosphere to make predictions

about the future. Models don’t always match reality, as you’ve experienced if you’ve

been surprised by rain or snow. And even the most complicated models of weather

can’t make precise predictions more than a few weeks into the future. Still, weather

forecasts are useful enough that we check the forecast before heading outside each

day.

We’ve previously introduced a model called the urn model in Chapter 2. The urn

model likens the underlying chance process in data generation like draws of marbles

from an urn, and we use it to study variation. In this chapter, we introduce another

kind of model that describes the pattern/signal in the data rather than the random

variation. This process is called fitting a model to data. We focus on the simplest of

49
1 We (the authors) first learned of the bus arrival time data from an analysis by a data scientist named Jake
VanderPlas. We’ve named the protagonist of this section in his honor.
these sorts of models, called the constant model. It serves as a useful building block

towards the more complex models appearing later in the book.

The constant model lets us introduce model fitting from the perspective of loss mini‐

mization, which connects summary statistics like the mean and median to more com‐

plex models. We begin with an example that uses data about the wait times for a bus

to introduce the constant model.

The Constant Model
A transit rider, Jake, often takes the northbound C bus at the 3rd & Pike bus stop in

downtown Seattle^1. The bus is supposed to arrive every 10 minutes, but Jake notices

that he sometimes waits a long time for the bus. He wants to know how late the bus

usually is. Jake was able to acquire the scheduled arrival and actual arrival times, from

the Washington State Transportation Center so that he can calculate the minutes that

each bus is late for his stop. We read in these data.

route direction scheduled actual minutes_late
0 C northbound 2016-03-26 06:30:28 2016-03-26 06:26:04 -4.40
1 C northbound 2016-03-26 01:05:25 2016-03-26 01:10:15 4.83
2 C northbound 2016-03-26 21:00:25 2016-03-26 21:05:00 4.58
... ... ... ... ... ...
1431 C northbound 2016-04-10 06:15:28 2016-04-10 06:11:37 -3.85
1432 C northbound 2016-04-10 17:00:28 2016-04-10 16:56:54 -3.57
1433 C northbound 2016-04-10 20:15:25 2016-04-10 20:18:21 2.93
1434 rows × 5 columns

The minutes_late column in the data table records how late each bus was. Notice

that some of the times are negative, which means that the bus arrived early. Let’s plot

a histogram of the minutes each bus is late.

50 | Chapter 3: Modeling with Summary Statistics

There are already some interesting patterns in the data. For example, many buses

arrive earlier than scheduled and some are well over 20 minutes late. We also see a

clear mode (high point) at 0, meaning many buses arrive roughly on time.

To understand how late the bus usually is, we’d like to summarize the lateness by a

constant—this is a statistic, a single number, like the mean, median, or mode. Let’s

find each of these summary statistics for the minutes_late data.

From the histogram, we estimate the mode of the data to be 0. We use Python to

compute the mean and median.

mean: 1.92 mins late
median: 0.74 mins late
mode: 0.00 mins late
Naturally, we want to know which of these numbers is best as a summary of lateness.

Rather than relying on rules of thumb, we take a more formal approach. We make a

constant model for bus lateness. Let’s call this constant θ (in modeling, θ is often

referred to as a parameter). For example, if we say that θ= 5, our model’s best guess is

that the bus will typically be 5 minutes late.

Now, θ= 5 isn’t a particularly good guess. From the histogram of arrival times, we

saw that there are a lot more points closer to 0 than 5. But it isn’t clear that θ= 0 (the

mode) is a better choice than θ= 0. 74 (the median), θ= 1. 92 (the mean), or some‐

thing in between. To make precise choices between different values of θ, we would

like to assign each value of θ a score that measures how well the model fits the data.

Using more formal language, we say that we use a loss function to pick the best

parameter, θ, for a constant model of our data. A loss function takes as input a value

of θ and the points in our dataset. It outputs a single number that we can use to select

The Constant Model | 51
the best θ. In the next section, we examine how to define and use loss functions to fit

this constant model.

Loss Functions
We’re modeling how late the northbound C bus is by a constant called θ, and we want

to use the data of actual arrival times to figure out a good value for θ. To find θ, we

use a loss function—a function that measures how far away our model is from the

actual data.

A loss function is a mathematical function that takes in θ and a data value y. It out‐

puts a single number, the loss, that measures how far away θ is from y. We write the

loss function as 𝐀 θ,y.

By convention, the loss function outputs lower values for better values of θ and larger

values for worse θ. To fit a model to our data, we select the particular θ that produces

a lower average loss across all other choices; in other words we find the θ that minimi‐

zes the average loss for our data, y 1 , ...,yn. We write the average loss as

Lθ,y 1 ,y 2 , ...,yn, where

Lθ,y 1 ,y 2 , ...,yn = mean𝐀 θ,y 1 ,𝐀θ,y 2 , ...,𝐀θ,yn

=
1
n
∑
i= 1
n
𝐀θ,yi
As a shorthand, we often define the vector 𝐀= y 1 ,y 2 , ...,yn. Then, we can write the

average loss as:

Lθ,𝐀 =
1
n
∑
i= 1

n
𝐀θ,yi
Notice that 𝐀θ,y tells us the model’s loss on a single data point
while Lθ,𝐀 gives the model’s average loss for all of our data
points. The capital L helps us remember that the average loss com‐
bines multiple smaller 𝐀 values.
Once we define a loss function, we can find the value of θ that produces the smallest

average loss. We call this minimizing value, θ. In other words, of all the possible θ

values, θ is the one that produces the smallest average loss for our data. We call this

process fitting a model to our data, since it finds the best constant model for a given

loss function.

Next, we look at two specific loss functions: absolute error and square error. Our goal

is to fit our model and find θ for each of these loss functions.

52 | Chapter 3: Modeling with Summary Statistics

Mean Absolute Error
We start with the absolute error loss function. Here’s the idea behind absolute loss. For

some value of θ and data value y,

Find the error: y−θ, and
Take the absolute value of the error: y−θ.
So, our loss function is 𝐀θ,y = y−θ.

Taking the absolute value of the error is a simple way to convert negative errors into

positive ones. For instance, the point, y= 4, is equally far away from θ= 2 and θ= 6,

so the errors are equally “bad”.

The average loss is called the mean absolute error, or MAE for short. That is, the

MAE is the average of each of the individual absolute errors:

Lθ,𝐀 =

1
n
∑
i= 1
n
yi−θ
Notice that the name MAE, tells you how to compute it–take the mean of the abso‐

lute errors.

We can write a simple Python function to compute this loss:

def mae_loss(theta, y_vals):
return np.mean(np.abs(y_vals - theta))
Let’s see how this loss function behaves when we have just five points − 1, 0, 2, 5, 10.

We can try different values of θ and see what the loss function outputs for each value.

Loss Functions | 53
We suggest verifying some of these loss values by hand to check that you understand

how the MAE is computed.

Of the values of θ that we tried, we found that θ= 2 has the lowest mean absolute

error. For this simple example, (2) is the median of the data values. This isn’t a coin‐

cidence—let’s now see what the loss function outputs on the original dataset of bus

times. We find the MAE when we set θ to the mode, median, and mean of the arrival

times.

We see again that the median (middle plot) gives a smaller loss than the mode and

mean (left and right plots). In fact, we can prove that for absolute loss, the minimiz‐

ing θ is the mediany 1 ,y 2 , ...,yn. (The proof appears in the Exercises.)

So far, we have found the best value of θ by simply trying out a few values and then

picking the one with the smallest loss. To get a better sense of the MAE as a function

of θ, we can try many more values of θ to plot a complete curve that shows how

Lθ,𝐀 changes as θ changes. We draw the curve for the example from above with the

five data values, − 1, 0, 2, 5, 10.

The plot above shows that in fact, θ= 2 is the best choice for the small dataset of five

values. Notice the shape of curve. It is piecewise linear, where the line segments con‐

54 | Chapter 3: Modeling with Summary Statistics

nect at the location of the data values (-1, 0, 2, and 5). This is a property of the abso‐

lute value function. With a lot of data, the flat pieces are less obvious. Our bus data

have over 1400 points and its MAE curve appears below.

We can use this plot to help confirm that the median of the data is the minimizing

value, in other words θ= 0. 74.

Next, let’s look at another loss function, the squared error.

Mean Squared Error
We have used a constant model for our data and found that with mean absolute error,

the minimizer is the median. Now, we keep our model the same but switch to a differ‐

ent loss function: squared error. Instead of taking the absolute difference between our

data value y and the constant θ, we square the error. That is, for some value of θ and

data value y,

Find the error: y−θ and
Take the squared value of the error: y−θ
2
.
This gives the loss function 𝐀θ,y = y−θ^2.

As before, we want to use all of our data to find the best θ so we compute the mean

squared error, or MSE for short:

Lθ,𝐀 =Lθ,y 1 ,y 2 , ...,yn =

1
n
∑
i= 1
n
yi−θ
2
We can write a simple Python function to compute the MSE:

Loss Functions | 55
def mse_loss(theta, y_vals):
return np.mean((y_vals - theta) ** 2)
Let’s again try the mean, median, and mode as potential minimizers of the MSE.

Now, when we fit the constant model using MSE loss, we find that the mean (right

plot) has a smaller loss than the mode and the median (left and middle plot).

Let’s plot the MSE curve for different values of θ given our data. We can see that the

minimizing value θ is close to 2.

One feature of this curve that is quite noticeable is how rapidly large the MSE grows

compared to the MAE (note the range on the vertical axis). This growth has to do

with the nature of squaring errors; it places a much higher loss on data values further

away. If θ= 10 and y= 110, the squared loss is 10 − 110
2
= 10000 whereas the abso‐

lute loss is 10 − 110 = 100. For this reason, MSE is more sensitive to unusually large

values than MAE.

56 | Chapter 3: Modeling with Summary Statistics

When we use MSE loss, we saw that θ appears to be the mean𝐀. Again, this no mere

coincidence; the mean of the data always produces the θ that minimizes the MSE. We

can show how this comes about using the quadratic nature of MSE. To begin, we add

and subtract y in the loss function and expand the square as follows:

Lθ,𝐀 =

1
n
∑
i= 1
n
yi−θ^2
=
1
n
∑
i= 1
n
yi−y + y−θ
2
=
1
n
∑
i= 1
n
yi−y
2
+ 2yi−y y−θ + y−θ
2
Next, we split the MSE into the sum of these three terms, and note that the middle

term is 0, due to the simple property of the average: ∑yi−y = 0.

1
n
∑
i= 1
n
yi−y
2
+
1
n
∑
i= 1
n
2 yi−y y−θ +
1
n
∑
i= 1
n
y−θ
2
=
1
ni∑= 1
n
yi−y
2
+ 2y−θ
1
ni∑= 1
n
yi−y +
1
ni∑= 1
n
y−θ
2
=
1
n
∑
i= 1
n
yi−y^2 + y−θ^2
Of the remaining two terms, the first does not involve θ and the second is always

non-negative:

Lθ,𝐀 =

1
n
∑
i= 1
n
yi−y
2
+
1
n
∑
i= 1
n
y−θ
2
≥
1
n
∑
i= 1
n
yi−y^2
This inequality shows us that the MSE is minimized when θ is y. That is, there is a

single value of θ that gives the smallest possible MSE for any dataset and that value is

θ=y.

We have seen that for absolute loss, the best constant is the median, but for square

error, it’s the mean. The choice of the loss function is an important aspect of model

fitting.

Choosing Loss Functions
Now that we’ve worked with two loss functions, we can return to our original ques‐

tion: why would we choose the median, mean, or mode over the others? Since the

Loss Functions | 57
2 The mode minimizes a loss function called 0-1 loss. Although we haven’t covered this specific loss, the proce‐
dure is identical: pick the loss function, then find that minimizes the loss.
statistics minimize different loss functions^2 , we can equivalently ask: what is the most

appropriate loss function for our problem? To answer this question, we look at the

context of our problem.

Compared to the MAE, the MSE gives especially large loss values when the bus is

much later (or earlier) than expected. So, a bus rider who wants to understand the

typical wait times would use MAE and the median (0.74 minutes late), and a rider

who despises unexpected wait times would summarize the wait time using MSE and

the mean (1.92 minutes late).

If we want to refine the model even more, we can use a more specialized loss func‐

tion. For example, if when a bus arrives early, it waits at the stop until the scheduled

time of departure, then we might want to assign an early arrival 0 loss. And, if a really

late bus is a larger aggravation than a moderately late one, we might choose an asym‐

metric loss function that penalizes super late arrivals more.

In essence, context matters when choosing a loss function. By thinking carefully

about how we plan to use the fitted model, we can pick a loss function that helps us

make good decisions using data.

Summary
We’ve introduced the constant model: a model that summarizes all of the data by a

single value. To fit the constant model, we chose a loss function that measured how

well a given constant fits a data value, and we computed the average loss over all of

the data values. We saw that depending on the choice of loss function, we get a differ‐

ent minimizing value. We found that the mean minimizes the average squared error

(MSE) and the median minimizes the average absolute error (MAE). We also dis‐

cussed how we can incorporate context and knowledge of our problem to pick loss

functions.

The idea of fitting models through loss minimization ties simple summary statistics—

like the mean, median, and mode—to more complex modeling situations. The steps

we took to model our data apply to many modeling scenarios:

Select the form of a model (such as the constant model)
Select a loss function (such as absolute error)
Fit the model by minimizing the average loss for the data
58 | Chapter 3: Modeling with Summary Statistics

For the rest of this book, all of our modeling techniques expand upon one or more of

these steps. We introduce new models (1), new loss functions (2), and new techniques

for minimizing loss (3).

The next chapter revisits the study of a bus arriving late at its stop. This time, it steps

back to visit all stages of the data science lifecycle as a case study.

Exercises
Another loss function called the Huber loss combines the absolute and squared
loss to create a loss function that is both smooth and robust to outliers. The
Huber loss accomplishes this by behaving like the squared loss for θ values close
to the minimum and switching to absolute loss for θ values far from the mini‐
mum. Below is a formula for a simplified version of Huber loss. Use this defini‐
tion of Huber loss to
—Write a function called mhe to compute the mean Huber error.

—Plot the smooth mhe curve for the bus times data where θ ranges from -2 to 8.

—Use trial and error to find the minimizing θ for bus times.

lθ,y =

1
2
y−θ
2
for y−θ ≤ 2
= 2 y−θ − 1 otherwise.
Continue with Huber loss and the function mhe in the previous problem:
—Plot the smooth mhe for the five data points − 2, 0, 1, 5, 10.

—Describe the curve.

—For these five points, what is the minimizing θ?

—What happens when the data point 10 is swapped for 100? Compare the mini‐
mizer to the mean and median of the five points.
Consider a loss function that has 0 loss for negative values of y and quadratic loss
for positive y.
—Write a function, called m0e that computes the average loss for this function.

—Plot the m0e curve for many θs given the data 𝐀= − 2, 0, 1, 5, 10

—Use trial and error to find the minimizing θ.

—Intuitively, what should the minimizing value be? What if we use linear loss
instead?
Exercises | 59
In this exercise, we again show that the mean minimizes the mean square error,
but we will use calculus instead.
—Take the derivative of the average loss with respect to θ.

—Set the derivative to 0 and solve for θ.

—To be thorough, take a second derivative to confirm that y is a minimizer.
(Recall that if the second derivative is positive than the quadratic is concave.)
Follow the steps below to establish that MAE is minimized for the median.
—Split the summation,
1
n∑i= 1
n y
i−θ into three terms for when yi−θ is nega‐
tive, 0, and positive.
—Set the middle term to 0 so that the equations are easier to work with. Use the
fact that the derivative of the absolute value is -1 or +1 to differentiate the
remaining two terms with respect to θ.
—Set the derivative to 0 and simplify terms. Explain why when there are an odd
number of points, the solution is the median.
—Explain why when there are an even number of points, the minimizing θ is
not uniquely defined (just as with the median).
60 | Chapter 3: Modeling with Summary Statistics

CHAPTER 4

Working With Dataframes Using pandas

A Note for Early Release Readers

With Early Release ebooks, you get books in their earliest form—the author’s raw and
unedited content as they write—so you can take advantage of these technologies long
before the official release of these titles.
This will be the 6th chapter of the final book. Please note that the GitHub repo will be
made active later on.
If you have comments about how we might improve the content and/or examples in
this book, or if you notice missing material within this chapter, please reach out to the
author at mpotter@oreilly.com.
Data scientists work with data stored in tables. This chapter introduces dataframes,

one of the most widely used ways to represent data tables. We’ll also introduce pan

das, the standard Python package for working with dataframes. Here’s an example of

a dataframe that holds information about popular dog breeds:

grooming food_cost kids size
breed
Labrador Retriever weekly 466.0 high medium
German Shepherd weekly 466.0 medium large
Beagle daily 324.0 high small
Golden Retriever weekly 466.0 high medium
Yorkshire Terrier daily 324.0 low small
Bulldog weekly 466.0 medium medium
Boxer weekly 466.0 high medium
61
In a dataframe, each row represents a single record—in this case, a single dog breed.

Each column represents a feature about the record—for example, the grooming col‐

umn represents how often each dog breed needs to be groomed.

Dataframes have labels for both columns and rows. For instance, this dataframe has a

column labeled grooming and a row labeled German Shepherd. The columns and

rows of a dataframe are ordered—we can refer to the Labrador Retriever row as the

first row of the dataframe.

Within a column, data have the same type. For instance, the food_cost column con‐

tains numbers, and the size column contains categories. But data types can be differ‐

ent within a row.

Because of these properties, dataframes enable all sorts of useful operations.

As a data scientist, you’ll often find yourself working with people
from different backgrounds who use different terms. For instance,
computer scientists say that the columns of a dataframe represent
features of the data, while statisticians call them variables instead.
Other times, people will use the same term to refer to slightly dif‐
ferent things. Data types in a programming sense refers to how a
computer stores data internally. For instance, the size column has
a string data type in Python. But from a statistical point of view, the
size column stores ordered categorical data (ordinal data). We talk
more about this specific distinction in the ??? chapter.
In this chapter, we’ll show you how to do common dataframe operations using pan

das. Data scientists use the pandas library when working with dataframes in Python.

First, we’ll explain the main objects that pandas provides: the DataFrame and Series

classes. Then, we’ll show you how to use pandas to perform common data manipula‐

tion tasks, like slicing, filtering, sorting, grouping, and joining.

Subsetting
This section introduces operations for taking subsets of dataframes. When data scien‐

tists first read in a dataframe, they often want to subset the specific data that they plan

to use. For example, a data scientist can slice out the ten relevant features from a data‐

frame with hundreds of columns. Or, they can filter a dataframe to remove rows with

incomplete data. For the rest of this chapter, we’ll introduce dataframe operations

using a dataframe of baby names.

62 | Chapter 4: Working With Dataframes Using pandas

About the Data
There’s a 2021 New York Times article that talks about Prince Harry and Meghan’s

unique choice for their new baby daughter’s name: Lilibet (Williams 2021). The arti‐

cle has an interview with Pamela Redmond, an expert on baby names, who talks

about interesting trends in how people name their kids. For example, she says that

names that start with the letter “L” have become very popular in recent years, while

names that start with the letter “J” were most popular in the 1970s and 1980s. Are

these claims reflected in data? We can use pandas to find out.

First, import the package as pd, the canonical abbreviation:

import pandas as pd
We have a dataset of baby names stored in a comma-separated values (CSV) file

called babynames.csv. Use the pd.read_csv function to read the file as a pan

das.DataFrame object.

baby = pd.read_csv('babynames.csv')
baby
Name Sex Count Year
0 Liam M^196592020
1 Noah M^182522020
2 Oliver M^141472020
... ... ... ... ...
2020719 Verona F^51880
2020720 Vertie F^51880
2020721 Wilma F^51880
2020722 rows × 4 columns

The data in the baby table comes from the US Social Security department, which

records the baby name and birth sex for birth certificate purposes. They make the

baby names data available on their website (Department 2021). We’ve loaded this data

into the baby table.

The Social Security website has a page that describes the data in more detail (link).

We won’t go in-depth in this chapter about the data’s limitations, but we’ll point out

this relevant quote from the website:

All names are from Social Security card applications for births that occurred in the
United States after 1879. Note that many people born before 1937 never applied for a
Social Security card, so their names are not included in our data. For others who did
apply, our records may not show the place of birth, and again their names are not
included in our data.
Subsetting | 63
All data are from a 100% sample of our records on Social Security card applications as
of March 2021.
DataFrames and Indices
Let’s examine the baby dataframe in more detail. A dataframe has rows and columns.

Every row and column has a label, as highlighted in Figure 4-1.

Figure 4-1. The baby dataframe has labels for both rows and columns (boxed).

By default, pandas assigns row labels as incrementing numbers starting from 0. In

this case, the data at the row labeled 0 and column labeled Name has the data 'Liam'.

64 | Chapter 4: Working With Dataframes Using pandas

Dataframes can also have strings as row labels. Figure 4-2 shows a dataframe of dog

data where the row labels are strings.

Figure 4-2. Row labels in dataframes can also be strings. In this example, each row is

labeled using the dog breed name.

The row labels have a special name. We call them the index of a dataframe, and pan

das stores the row labels in a special pandas.Index object. We won’t discuss the pan

das.Index object since it’s not very common to manipulate the index itself. But, it’s

important to remember that even though the index looks like a column of data, the

index really represents row labels, not data. For instance, the dataframe of dog breeds

has 4 columns of data, not 5, since the index doesn’t count as a column.

Slicing
Slicing is an operation that creates a new dataframe by taking a subset of rows or col‐

umns out of another dataframe. Think about slicing a tomato—slices can go both

vertially and horizontally. To take slices of a dataframe in pandas, we use the .loc

and .iloc properties. Let’s start with .loc.

Here’s the full baby dataframe:

baby
Name Sex Count Year
0 Liam M^196592020
Subsetting | 65
Name Sex Count Year
1 Noah M^182522020
2 Oliver M^141472020
... ... ... ... ...
2020719 Verona F^51880
2020720 Vertie F^51880
2020721 Wilma F^51880
2020722 rows × 4 columns

.loc lets you select rows and columns using their labels. For example, to get the data

in the row labeled 1 and column labeled Name:

# The first argument is the row label
# ↓
baby.loc[1, 'Name']
# ↓
# The second argument is the column label
'Noah'
Notice that .loc needs brackets; running baby.loc(1, 'Name')
will error.
To slice out multiple rows or column, you can use Python slice syntax instead of indi‐

vidual values:

baby.loc[0:3, 'Name':'Count']
Name Sex Count
0 Liam M^19659
1 Noah M^18252
2 Oliver M^14147
3 Elijah M^13034
To get an entire column of data, pass an empty slice as the first argument:

baby.loc[:, 'Count']
0 19659
1 18252
2 14147
...
2020719 5
66 | Chapter 4: Working With Dataframes Using pandas

2020720 5
2020721 5
Name: Count, Length: 2020722, dtype: int64
Notice that the output of this doesn’t look like a dataframe, and it’s not. Selecting out

a single row or column of a dataframe produces a pd.Series object.

counts = baby.loc[:, 'Count']
counts.__class__.__name__
'Series'
What’s the difference between a pd.Series and pd.DataFrame object? Essentially, a

pd.DataFrame is two-dimensional—it has rows and columns and represents a table of

data. A pd.Series is one-dimensional—it represents a list of data. pd.Series and

pd.DataFrame objects have many methods in common, but they really represent two

different things. Confusing the two can cause bugs and confusion.

To select specific columns of a dataframe, pass a list into .loc:

# Here's the original dataframe
baby
Name Sex Count Year
0 Liam M^196592020
1 Noah M^182522020
2 Oliver M^141472020
... ... ... ... ...
2020719 Verona F^51880
2020720 Vertie F^51880
2020721 Wilma F^51880
2020722 rows × 4 columns

# And here's the dataframe with only Name and Year columns
baby.loc[:, ['Name', 'Year']]
# └──────┬───────┘
# list of column labels
Name Year
0 Liam^2020
1 Noah^2020
2 Oliver^2020
... ... ...
2020719 Verona^1880
2020720 Vertie^1880
Subsetting | 67
Name Year
2020721 Wilma^1880
2020722 rows × 2 columns

Selecting columns is very common, so there’s a shorthand.

# Shorthand for baby.loc[:, 'Name']
baby['Name']
0 Liam
1 Noah
2 Oliver
...
2020719 Verona
2020720 Vertie
2020721 Wilma
Name: Name, Length: 2020722, dtype: object
# Shorthand for baby.loc[:, ['Name', 'Count']]
baby[['Name', 'Count']]
Name Count
0 Liam^19659
1 Noah^18252
2 Oliver^14147
... ... ...
2020719 Verona^5
2020720 Vertie^5
2020721 Wilma^5
2020722 rows × 2 columns

Slicing using .iloc works similarly to .loc, except that .iloc uses the positions of

rows and columns rather than labels. It’s easiest to show the difference between .iloc

and .loc when the dataframe index has strings, so for demonstration purposes let’s

look at a dataframe with information on dog breeds:

dogs = pd.read_csv('dogs.csv', index_col='breed')
dogs
grooming food_cost kids size
breed
Labrador Retriever weekly 466.0 high medium
German Shepherd weekly 466.0 medium large
Beagle daily 324.0 high small
68 | Chapter 4: Working With Dataframes Using pandas

grooming food_cost kids size
breed
Golden Retriever weekly 466.0 high medium
Yorkshire Terrier daily 324.0 low small
Bulldog weekly 466.0 medium medium
Boxer weekly 466.0 high medium
To get the first three rows and first two columns by position, use .iloc:

dogs.iloc[0:3, 0:2]
grooming food_cost
breed
Labrador Retriever weekly 466.0
German Shepherd weekly 466.0
Beagle daily 324.0
The same operation using .loc requires you to use the dataframe labels:

dogs.loc['Labrador Retriever':'Beagle', 'grooming':'food_cost']
grooming food_cost
breed
Labrador Retriever weekly 466.0
German Shepherd weekly 466.0
Beagle daily 324.0
Next, we’ll look at filtering rows.

Filtering Rows
So far, we’ve shown how to use .loc and .iloc to slice a dataframe using labels and

positions.

However, data scientists often want to filter rows—they want to take subsets of rows

using some criteria. Let’s say you want to find the most popular baby names in 2020.

To do this, you can filter rows to keep only the rows where the Year is 2020.

To filter, you can 1) check whether each value in the Year column is equal to 1970,

then 2) keep only those rows.

To compare each value in Year, slice out the column and make a boolean compari‐

son. (This is similar to what you would do with a numpy array.)

Subsetting | 69
# Here's the dataframe for reference
baby
Name Sex Count Year
0 Liam M^196592020
1 Noah M^182522020
2 Oliver M^141472020
... ... ... ... ...
2020719 Verona F^51880
2020720 Vertie F^51880
2020721 Wilma F^51880
2020722 rows × 4 columns

# Get a Series with the Year data
baby['Year']
0 2020
1 2020
2 2020
...
2020719 1880
2020720 1880
2020721 1880
Name: Year, Length: 2020722, dtype: int64
# Compare with 2020
baby['Year'] == 2020
0 True
1 True
2 True
...
2020719 False
2020720 False
2020721 False
Name: Year, Length: 2020722, dtype: bool
Notice that a boolean comparison on a Series gives a Series of booleans. This is nearly

equivalent to writing:

is_2020 = []
for value in baby['Year']:
is_2020.append(value == 2020)
But the boolean comparison is easier to write and much faster to execute than a for

loop.

Now, we tell pandas to keep only the rows where the comparison evaluated to True:

70 | Chapter 4: Working With Dataframes Using pandas

# Passing a Series of booleans into .loc only keeps rows where the Series has
# a True value.
# ↓
baby.loc[baby['Year'] == 2020, :]
Name Sex Count Year
0 Liam M^196592020
1 Noah M^182522020
2 Oliver M^141472020
... ... ... ... ...
31267 Zylynn F^52020
31268 Zynique F^52020
31269 Zynlee F^52020
31270 rows × 4 columns

# Filtering has a shorthand. This computes the same table as the snippet above
# without using .loc
baby[baby['Year'] == 2020]
Name Sex Count Year
0 Liam M^196592020
1 Noah M^182522020
2 Oliver M^141472020
... ... ... ... ...
31267 Zylynn F^52020
31268 Zynique F^52020
31269 Zynlee F^52020
31270 rows × 4 columns

Finally, to find the most common names in 2020, sort the dataframe by Count in

descending order.

# When you have a long expression, you can wrap it in parentheses, then add
# line breaks to make it more readable.
(baby[baby['Year'] == 2020]
.sort_values('Count', ascending=False)
.head(7) # take the first seven rows
)
Name Sex Count Year
0 Liam M^196592020
1 Noah M^182522020
Subsetting | 71
Name Sex Count Year
13911 Emma F^155812020
2 Oliver M^141472020
13912 Ava F^130842020
3 Elijah M^130342020
13913 Charlotte F^130032020
We see that Liam, Noah, and Emma were the most popular baby names in 2020.

Example: How recently has Luna become a popular name?
The New York Times article mentions that the name “Luna” was almost nonexistent

before 2000 but has since grown to become a very popular name for girls. When

exactly did Luna become popular? We can check this using slicing and filtering.

When approaching a data manipulation task, we recommend breaking the problem

down into smaller steps. For example, we could think:

Filter: keep only rows with 'Luna' in the Name column.
Filter: keep only rows with 'F' in the Sex column.
Slice: keep the Count and Year columns.
Now, it’s a matter of translating each step into code.

luna = baby[baby['Name'] == 'Luna'] # [1]
luna = luna[luna['Sex'] == 'F'] # [2]
luna = luna[['Count', 'Year']] # [3]
luna
Count Year
13923^77702020
45366^77722019
77393^69292018
... ... ...
2014083^171883
2018187^181881
2020223^151880
128 rows × 2 columns

In this book we use a library called plotly for plotting. We won’t go in-depth in plot‐

ting here since we talk more about plotting in ???. For now, remember that you can

use px.line() to make a simple line plot.

72 | Chapter 4: Working With Dataframes Using pandas

px.line(luna, x='Year', y='Count', width=350, height=250)
It’s just as the article says. Luna wasn’t popular at all until the year 2000 or so. Think

about that—if someone tells you that their name is Luna, you can take a pretty good

guess at their age even without any other information about them!

Just for fun, here’s the same plot for the name Siri.

# Using .query is similar to using .loc with a boolean series. query() has more
# restrictions on what kinds of filtering you can do but can be convenient as a
# shorthand.
siri = (baby.query('Name == "Siri"')
.query('Sex == "F"'))
px.line(siri, x='Year', y='Count', width=350, height=250)
Subsetting | 73
Why might the popularity have dropped so suddenly after 2010? Well, Siri happens to

be the name of Apple’s voice assistant and was introduced in 2011. Let’s draw a line

for the year 2011 and take a look...

fig = px.line(siri, x='Year', y='Count', width=350, height=250)
fig.add_vline(x=2011, line_color='red', line_width=4)
It looks like parents don’t want their kids to be confused when other people say “Hey

Siri” to their phones.

Takeaways
This section introduces dataframes in pandas. We’ve covered the common ways that

data scientists subset dataframes—slicing with labels and filtering using a boolean

condition. In the next section, we explain how to aggregate rows together.

Aggregating
This section introduces operations for aggregating rows in a dataframe. Data scien‐

tists aggregate rows together to make summaries of data. For instance, a dataset con‐

taining daily sales can be aggregated to show monthly sales instead. Specifically, we’ll

introduce grouping and pivoting, two common operations for aggregating data.

We’ll work with the baby names data, as introduced in the previous section:

baby = pd.read_csv('babynames.csv')
baby
Name Sex Count Year
0 Liam M^196592020
74 | Chapter 4: Working With Dataframes Using pandas

Name Sex Count Year
1 Noah M^182522020
2 Oliver M^141472020
2020719 Verona F^51880
2020720 Vertie F^51880
2020721 Wilma F^51880
2020722 rows × 4 columns

Basic Group-Aggregate
Let’s say we want to find out the total number of babies born as recorded in this data.

This is simply the sum of the Count column:

baby['Count'].sum()
352554503
Summing up the name counts is one simple way to aggregate the data—it combines

data from multiple rows.

But let’s say we instead want to answer a more interesting question: are U.S. births

trending upwards over time? To answer this question, we can sum the Count column

within each year rather than taking the sum over the entire dataset. In other words,

we split the data into groups based on Year, then sum up the Count values within

each group. This process is depicted in Figure 4-3.

Figure 4-3. A depiction of grouping then aggregating for example data.

We call this operation grouping followed by aggregating. In pandas, we write:

baby.groupby('Year')['Count'].sum()
Aggregating | 75
Year
1880 194419
1881 185772
1882 213385
...
2018 3487193
2019 3437438
2020 3287724
Name: Count, Length: 141, dtype: int64
Notice that the code is nearly the same as the non-grouped version, except that it

starts with a call to .groupby('Year').

The result is a pd.Series with the total babies born for each year in the data. Notice

that the index of this series contains the unique Year values. Now we can plot the

counts over time:

counts_by_year = baby.groupby('Year')['Count'].sum().reset_index()
px.line(counts_by_year, x='Year', y='Count', width=350, height=250)
What do we see in this plot? First, we notice that there seem to be suspiciously few

babies born before 1920. One likely explanation is that the Social Security Adminis‐

tration was created in 1935, so its data for prior births could be less complete.

76 | Chapter 4: Working With Dataframes Using pandas

We also notice the dip when World War II began in 1939, and the post-war Baby

Boomer era from 1946-1964.

Here’s the basic recipe for grouping in pandas:

(baby # the dataframe
.groupby('Year') # column(s) to group
['Count'] # column(s) to aggregate
.sum() # how to aggregate
)
Grouping on Multiple Columns
We pass multiple columns into .groupby as a list to group by multiple columns at

once. This is useful when we need to further subdivide our groups. For example, we

can group by both year and sex to see how many male and female babies were born

over time.

counts_by_year_and_sex = (baby
.groupby(['Year', 'Sex']) # Arg to groupby is a list of column names
['Count']
.sum()
)
counts_by_year_and_sex
Year Sex
1880 F 83929
M 110490
1881 F 85034
...
2019 M 1785527
2020 F 1581301
M 1706423
Name: Count, Length: 282, dtype: int64
Notice how the code closely follows the grouping recipe.

The counts_by_year_and_sex series has what we call a multi-level index with two

levels, one for each column that was grouped. It’s a bit easier to see if we convert the

series to a dataframe:

# The result only has one column
counts_by_year_and_sex.to_frame()
Count
Year Sex
1880 F^83929
M^110490
1881 F^85034
... ... ...
Aggregating | 77
Count
Year Sex
2019 M^1785527
2020 F^1581301
M^1706423
282 rows × 1 columns

There are two levels to the index because we grouped by two columns. It can be a bit

tricky to work with multilevel indices, so we can reset the index to go back to a data‐

frame with a single index.

counts_by_year_and_sex.reset_index()
Year Sex Count
0^1880 F^83929
1^1880 M^110490
2^1881 F^85034
279^2019 M^1785527
280^2020 F^1581301
281^2020 M^1706423
282 rows × 3 columns

Custom Aggregation Functions
After grouping, pandas gives us flexible ways to aggregate the data. So far, we’ve seen

how to use .sum() after grouping:

(baby
.groupby('Year')
['Count']
.sum() # aggregate by summing
)
Year
1880 194419
1881 185772
1882 213385
...
2018 3487193
2019 3437438
2020 3287724
Name: Count, Length: 141, dtype: int64
78 | Chapter 4: Working With Dataframes Using pandas

pandas also supplies other aggregation functions, like .mean(), .size(),

and .first(). Here’s the same grouping using .max():

(baby
.groupby('Year')
['Count']
.max() # aggregate by taking the max within each group
)
Year
1880 9655
1881 8769
1882 9557
...
2018 19924
2019 20555
2020 19659
Name: Count, Length: 141, dtype: int64
But sometimes pandas doesn’t have the exact aggregation function we want to use. In

these cases, we can define and use a custom aggregation function. pandas lets us do

this through .agg(fn), where fn is a function that we define.

For instance, if we want to find the difference between the largest and smallest values

within each group (the range of the data), we could first define a function called

data_range, then pass that function into .agg().

# The input to this function is a pd.Series object containing a single column
# of data. It gets called once for each group.
def data_range(counts):
return counts.max() - counts.min()
(baby
.groupby('Year')
['Count']
.agg(data_range) # aggregate using custom function
)
Year
1880 9650
1881 8764
1882 9552
...
2018 19919
2019 20550
2020 19654
Name: Count, Length: 141, dtype: int64
Aggregating | 79
Example: Have People Become More Creative With Baby Names?
Have people become more creative with baby names over time? One way to measure

this is to see whether the number of unique baby names per year has increased over

time.

We start by defining a count_unique function that counts the number of unique val‐

ues in a series. Then, we pass that function into .agg().

def count_unique(s):
return len(s.unique())
unique_names_by_year = (baby
.groupby('Year')
['Name']
.agg(count_unique) # aggregate using the custom count_unique function
)
unique_names_by_year
Year
1880 1889
1881 1829
1882 2012
...
2018 29619
2019 29417
2020 28613
Name: Name, Length: 141, dtype: int64
px.line(unique_names_by_year.reset_index(),
x='Year', y='Name',
labels={'Name': '# unique names'},
width=350, height=250)
80 | Chapter 4: Working With Dataframes Using pandas

We see that the number of unique names has generally increased over time, even

though the number of babies born has mostly stabilized since the 1960s.

Pivoting
Pivoting is essentially a convenient way to arrange the results of a group and aggrega‐

tion when grouping with two columns. Earlier in this section we grouped the baby

names data by year and sex:

counts_by_year_and_sex = (baby
.groupby(['Year', 'Sex'])
['Count']
.sum()
)
counts_by_year_and_sex.to_frame()
Count
Year Sex
1880 F^83929
M^110490
1881 F^85034
... ... ...
2019 M^1785527
Aggregating | 81
Count
Year Sex
2020 F^1581301
M^1706423
282 rows × 1 columns

This produces a pd.Series with the counts. We can also imagine the same data with

the Sex index level “pivoted” to the columns of a dataframe. It’s easier to see with an

example:

mf_pivot = pd.pivot_table(
baby,
index='Year', # Column to turn into new index
columns='Sex', # Column to turn into new columns
values='Count', # Column to aggregate for values
aggfunc=sum) # Aggregation function
mf_pivot
Sex F M
Year
1880^83929110490
1881^85034100738
1882^99699113686
... ... ...
2018^16768841810309
2019^16519111785527
2020^15813011706423
141 rows × 2 columns

Notice that the data values are identical in the pivot table and the table produced

with .groupby(); the values are just arranged differently. Pivot tables are useful for

quickly summarizing data using two attributes and are often seen in articles and

papers.

The px.line() function also happens to work well with pivot tables, since the func‐

tion draws one line for each column of data in the table:

px.line(mf_pivot, width=350, height=250)
82 | Chapter 4: Working With Dataframes Using pandas

Takeaways
This section covers common ways to aggregate data in pandas using the .groupby()

function with one or more columns, or by using the pd.pivot_table() function. In

the next section, we’ll explain how to join dataframes together.

Joining
Data scientists very frequently want to join two or more dataframes together in order

to connect data values across dataframes. For instance, an online bookstore might

have one dataframe with the books each user has ordered and a second dataframe

with the genres of each book. By joining the two dataframes together, the data scien‐

tist can see what genres each user prefers.

We’ll continue looking at the baby names data. We’ll use joins to check some trends

mentioned in the New York Times article about baby names (Williams 2021). The

article talks about how certain categories of names have become more or less popular

over time. For instance, it mentions that mythological names like Julius and Cassius

have become popular, while baby boomer names like Susan and Debbie have become

less popular. How has the popularity of these categories changed over time?

Joining | 83
We’ve taken the names and categories in the NYT article and put them in a small

dataframe:

nyt = pd.read_csv('nyt_names.csv')
nyt
nyt_name category
0 Lucifer forbidden
1 Lilith forbidden
2 Danger forbidden
20 Venus celestial
21 Celestia celestial
22 Skye celestial
23 rows × 2 columns

To see how popular the categories of names are, we join the nyt dataframe with the

baby dataframe to get the name counts from baby.

baby = pd.read_csv('babynames.csv')
baby
Name Sex Count Year
0 Liam M^196592020
1 Noah M^182522020
2 Oliver M^141472020
2020719 Verona F^51880
2020720 Vertie F^51880
2020721 Wilma F^51880
2020722 rows × 4 columns

Imagine going down each row in baby and asking, is this name in the nyt table? If so,

then add the value in the category column to the row. That’s the basic idea behind a

join. Let’s look at a few simpler examples first.

Inner Joins
We’ll make smaller versions of the baby and nyt tables so it’s easier to see what hap‐

pens when we join tables together.

nyt_small = nyt.iloc[[11, 12, 14]].reset_index(drop=True)
nyt_small
84 | Chapter 4: Working With Dataframes Using pandas

nyt_name category
0 Karen boomer
1 Julius mythology
2 Freya mythology
names_to_keep = ['Julius', 'Karen', 'Noah']
baby_small = (baby
.query("Year == 2020 and Name in @names_to_keep")
.reset_index(drop=True)
)
baby_small
Name Sex Count Year
0 Noah M^182522020
1 Julius M^9602020
2 Karen M^62020
3 Karen F^3252020
4 Noah F^3052020
To join tables in pandas, we’ll use the .merge() method:

baby_small.merge(nyt_small,
left_on='Name', # column in left table to match
right_on='nyt_name') # column in right table to match
Name Sex Count Year nyt_name category
0 Julius M^9602020 Julius mythology
1 Karen M^62020 Karen boomer
2 Karen F^3252020 Karen boomer
Notice that the new table has the columns of both baby_small and nyt_small tables.

The rows with the name Noah are gone. And the remaining rows have their matching

category from nyt_small.

When we join two tables together, we tell pandas the column(s) from each table that

we want to use to join (the left_on and right_on arguments). pandas matches rows

together when the values in the joining columns match, as shown in Figure 4-4.

Joining | 85
Figure 4-4. To join, pandas matches rows using the values in the Name and nyt_name

columns. For inner joins (the default), rows that don’t have matching values are dropped.

By default, pandas does an inner join. If either table has rows that don’t have matches

in the other table, pandas drops those rows from the result. In this case, the Noah

rows in baby_small don’t have matches in nyt_small, so they are dropped. Also, the

Freya row in nyt_small doesn’t have matches in baby_small, so it’s dropped as well.

Only the rows with a match in both tables stay in the final result.

Left, Right, and Outer Joins
We will sometimes want to keep rows without a match instead of dropping them

entirely. There are other types of joins—left, right, and outer—that keep rows even

when they don’t have a match.

86 | Chapter 4: Working With Dataframes Using pandas

In a left join, rows in the left table without a match are kept in the final result, as

shown in Figure 4-5.

Figure 4-5. In a left join, rows in the left table that don’t have matching values are kept.

To do a left join in pandas, use how='left' in the call to .merge():

baby_small.merge(nyt_small,
left_on='Name',
right_on='nyt_name',
how='left') # left join instead of inner
Name Sex Count Year nyt_name category
0 Noah M^182522020 NaN NaN
1 Julius M^9602020 Julius mythology
2 Karen M^62020 Karen boomer
3 Karen F^3252020 Karen boomer
4 Noah F^3052020 NaN NaN
Notice that the Noah rows are kept in the final table. Since those rows didn’t have a

match in the nyt_small dataframe, the join leaves NaN values in the nyt_name and

category columns. Also, notice that the Freya row in nyt_small is still dropped.

A right join works similarly to the left join, except that non-matching rows in the

right table are kept instead of the left table:

baby_small.merge(nyt_small,
left_on='Name',
right_on='nyt_name',
how='right')
Joining | 87
Name Sex Count Year nyt_name category
0 Karen M 6.0 2020.0 Karen boomer
1 Karen F 325.0 2020.0 Karen boomer
2 Julius M 960.0 2020.0 Julius mythology
3 NaN NaN NaN NaN Freya mythology
Finally, an outer join keeps rows from both tables even when they don’t have a match.

baby_small.merge(nyt_small,
left_on='Name',
right_on='nyt_name',
how='outer')
Name Sex Count Year nyt_name category
0 Noah M 18252.0 2020.0 NaN NaN
1 Noah F 305.0 2020.0 NaN NaN
2 Julius M 960.0 2020.0 Julius mythology
3 Karen M 6.0 2020.0 Karen boomer
4 Karen F 325.0 2020.0 Karen boomer
5 NaN NaN NaN NaN Freya mythology
Example: Popularity of NYT Name Categories
Now, let’s return to the full dataframes baby and nyt.

baby
Name Sex Count Year
0 Liam M^196592020
1 Noah M^182522020
2 Oliver M^141472020
... ... ... ... ...
2020719 Verona F^51880
2020720 Vertie F^51880
2020721 Wilma F^51880
2020722 rows × 4 columns

nyt
nyt_name category
0 Lucifer forbidden
1 Lilith forbidden
88 | Chapter 4: Working With Dataframes Using pandas

nyt_name category
2 Danger forbidden
... ... ...
20 Venus celestial
21 Celestia celestial
22 Skye celestial
23 rows × 2 columns

We want to know how the popularity of name categories in nyt have changed over

time. To answer this question:

Inner join baby with nyt.
Group the table by category and Year
Aggregate the counts using a sum.
cate_counts = (
baby.merge(nyt, left_on='Name', right_on='nyt_name') # [1]
.groupby(['category', 'Year']) # [2]
['Count'] # [3]
.sum() # [3]
.reset_index()
)
cate_counts
category Year Count
0 boomer^1880292
1 boomer^1881298
2 boomer^1882326
... ... ... ...
647 mythology^20182944
648 mythology^20193320
649 mythology^20203489
650 rows × 3 columns

Now, we can plot the popularity of boomer names and mythology names:

Joining | 89
As the NYT article claims, “baby boomer” names have become less popular after

2000, while mythological names have become more popular.

We can also plot the popularities of all the categories at once. Take a look at the plots

below and see whether they support the claims made in the New York Times article.

90 | Chapter 4: Working With Dataframes Using pandas

Takeaways
When joining dataframes together, we match rows using the .merge() function. It’s

important to consider the type of join (inner, left, right, or outer) when joining data‐

frames. In the next section, we’ll explain how to transform values in a dataframe.

Transforming
Data scientists transform dataframe columns when they need to change each value in

a feature in the same way. For example, if a feature contains heights of people in feet,

a data scientist might want to transform the heights to centimeters. In this section,

we’ll introduce apply, an operation that transforms columns of data using a user-

defined function.

baby = pd.read_csv('babynames.csv')
baby
Name Sex Count Year
0 Liam M^196592020
1 Noah M^182522020
2 Oliver M^141472020
... ... ... ... ...
2020719 Verona F^51880
Transforming | 91
Name Sex Count Year
2020720 Vertie F^51880
2020721 Wilma F^51880
2020722 rows × 4 columns

In the baby names New York Times article (Williams 2021), Pamela mentions that

names starting with the letter “L” and “K” became popular after 2000. On the other

hand, names starting with the letter “J” peaked in popularity in the 1970s and 1980s

and have dropped off in popularity since. We can verify these claims using the baby

dataset.

We approach this problem using the following steps:

Transform the Name column into a new column that contains the first letters of
each value in Name.
Group the dataframe by the first letter and year.
Aggregate the name counts by summing.
To complete the first step, we’ll apply a function to the Name column.

Apply
pd.Series objects contain an .apply() method that takes in a function and applies it

to each value in the series. For instance, to find the lengths of each name, we apply

the len function.

names = baby['Name']
names.apply(len)
0 4
1 4
2 6
..
2020719 6
2020720 6
2020721 5
Name: Name, Length: 2020722, dtype: int64
To extract the first letter of each name, define a custom function and pass it

into .apply().

# The argument to the function is an individual value in the series.
def first_letter(string):
return string[0]
names.apply(first_letter)
92 | Chapter 4: Working With Dataframes Using pandas

0 L
1 N
2 O
2020719 V
2020720 V
2020721 W
Name: Name, Length: 2020722, dtype: object
Using .apply() is similar to using a for loop. The code above is roughly equivalent

to writing:

result = []
for name in names:
result.append(first_letter(name))
Now, we can assign the first letters to a new column in the dataframe:

letters = baby.assign(Firsts=names.apply(first_letter))
letters
Name Sex Count Year Firsts
0 Liam M^196592020 L
1 Noah M^182522020 N
2 Oliver M^141472020 O
2020719 Verona F^51880 V
2020720 Vertie F^51880 V
2020721 Wilma F^51880 W
2020722 rows × 5 columns

To create a new column in a dataframe, you might also encounter
this syntax:
baby['Firsts'] = names.apply(first_letter)
This mutates the baby table by adding a new column called Firsts.
In the code above, we use .assign() which doesn’t mutate the baby
table itself; it creates a new dataframe instead. Mutating dataframes
isn’t wrong but can be a common source of bugs. Because of this,
we’ll mostly use .assign() in this book.
Example: Popularity of “L” Names
Now, we can use the letters dataframe to see the popularity of first letters over time.

letter_counts = (letters
.groupby(['Firsts', 'Year'])
Transforming | 93
['Count']
.sum()
.reset_index()
)
letter_counts
Firsts Year Count
0 A^188016740
1 A^188116257
2 A^188218790
... ... ... ...
3638 Z^201855996
3639 Z^201955293
3640 Z^202054011
3641 rows × 3 columns

fig = px.line(letter_counts.loc[letter_counts['Firsts'] == 'L'],
x='Year', y='Count', title='Popularity of "L" names',
width=350, height=250)
margin(fig, t=30)
94 | Chapter 4: Working With Dataframes Using pandas

The plot shows that “L” names were popular in the 1960s, dipped in the decades after,

but have indeed resurged in popularity after 2000.

What about “J” names?

fig = px.line(letter_counts.loc[letter_counts['Firsts'] == 'J'],
x='Year', y='Count', title='Popularity of "J" names',
width=350, height=250)
margin(fig, t=30)
The NYT article says that “J” names were popular in the 1970s and 80s. The plot

agrees, and also shows that they have become less popular after 2000.

The Price of Apply
The power of .apply() is its flexibility—you can call it with any function that takes in

a single data value and outputs a single data value.

Its flexibility has a price, though. Using .apply() can be slow, since pandas can’t opti‐

mize arbitrary functions. For example, using .apply() for numeric calculations is

much slower than using vectorized operations directly on pd.Series objects:

%%timeit
# Calculate the decade using vectorized operators
baby['Year'] // 10 * 10
Transforming | 95
20.5 ms ± 442 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)
%%timeit
def decade(yr):
return yr // 10 * 10
# Calculate the decade using apply
baby['Year'].apply(decade)
549 ms ± 35.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
The version using .apply() is 30 times slower! For numeric operations in particular,

we recommend operating on pd.Series objects directly.

Takeaways
To transform values in a dataframe, we commonly use the .apply() and .assign()

functions. In the next section, we’ll compare dataframes with other ways to represent

and manipulate data tables.

How are Dataframes Different from Other Data Representations?
Dataframes are just one way to represent data stored in a table. In practice, data sci‐

entists encounter many other types of data tables, like spreadsheets, matrices, and

relations. In this section, we’ll compare and contrast the dataframe with other repre‐

sentations to explain why dataframes have become so widely used for data analysis.

We’ll also point out scenarios where other representations might be more appropri‐

ate.

Dataframes and Spreadsheets
Spreadsheets are computer applications where users can enter data in a grid and use

formulas to perform calcuations. One famous example today is Microsoft Excel,

although spreadsheets date back to at least 1979 with VisiCalc (Grad 2007). Spread‐

sheets make it easy to see and directly manipulate data. These properties make

spreadsheets highly popular—by a 2005 estimate, there are over 55 million spread‐

sheet users compared to 3 million professional programmers in industry (Scaffidi et

al. 2005).

Dataframes have several key advantages over spreadsheets. Writing dataframe code in

a computational notebook like Jupyter naturally produces a data lineage. Someone

who opens the notebook can see the input files for the notebook and how the data

were changed. Spreadsheets do not make a data lineage visible; if a person manually

edits data values in a cell, it is difficult for future users to see which values were man‐

96 | Chapter 4: Working With Dataframes Using pandas

ually edited or how they were edited. Dataframes can also handle larger datasets than

spreadsheets, and users can also use distributed programming tools to work with

huge datasets that would be very hard to load into a spreadsheet.

Dataframes and Matrices
A matrix is a two-dimensional array of data used primarily for linear algebra opera‐

tions. In the example below, 𝐀 is a matrix with three rows and two columns.

𝐀=

1 0
0 4
0 0
Matrices are mathematical objects defined by the operators that they allow. For

instance, matrices can be added or multiplied together. Matrices also have a trans‐

pose. These operators have very useful properties which data scientists rely on for

statistical modeling.

One important difference between a matrix and a dataframe: when treated as a math‐

ematical object, matrices can only contain numbers. Dataframes, on the other hand,

can also have other types of data like text. This makes dataframes more useful for

loading and processing raw data which may contain all kinds of data types. In fact,

data scientists often use dataframes to create matrices. In this book, we’ll generally use

dataframes for exploratory data analysis and data cleaning, then process the data into

matrices for machine learning models.

Data scientists refer to matrices not only as mathematical objects,
but also as program objects as well. For instance, the R program‐
ming language has a matrix object, while in Python we could repre‐
sent a matrix using a two-dimensional numpy array. Matrices as
implemented in Python and R can contain other data types besides
numbers, but lose mathematical properties when doing so. This is
yet another example of how domains can refer to different things
with the same term.
Dataframes and Relations
The relation is a data table representation used in database systems, especially SQL

systems like SQLite and PostgreSQL. (We cover relations and SQL in the Chapter 5

chapter of this book.) Relations share many similarities with dataframes; both use

rows to represent records and columns to represent features. Both have column

names, and data within a column have the same type.

One key advantage of dataframes is that they don’t require rows to represent records

and columns to represent features. Many times, raw data don’t come in a convenient

format that can directly be put into a relation. In these scenarios, data scientists use

How are Dataframes Different from Other Data Representations? | 97
1 https://www.postgresql.org/about/
the dataframe to load and process data since dataframes are more flexible in this

regard. Often, data scientists will load raw data into a dataframe, then process the

data into a format that can easily stored into a relation.

One key advantage that relations have over dataframes is that relations are used by

relational database systems like PostgreSQL^1 that have highly useful features for data

storage and management. Consider a data scientist at a company that runs a large

social media website. The database might hold data that is far too large to read into a

pandas dataframe all at once; instead, data scientists use SQL queries to subset and

aggregate data since database systems are more capable of handling large datasets.

Also, website users constantly make updates to their data by making posts, uploading

pictures, and editing their profile. Here, database systems let data scientists reuse

their existing SQL queries to update their analyses with the latest data rather than

having to repeatedly download large CSV files.

For a more rigourous description of the difference between dataframes and relations,

see (Petersohn et al. 2020).

Summary
In this chapter, we explain what dataframes are, why they’re useful, and how to work

with them using pandas code. Subsetting, aggregating, joining, and transforming are

useful in nearly every data analysis. We’ll rely on these operations often in the rest of

the book, especially in the Chapter 6, ???, and ??? chapters.

Exercises
Use the baby data to create a plot of how popular your name was over time. If
you used that plot to make a guess at your age, what would you guess? Is that
close to your actual age? Think of a potential reason.
In this chapter we talked about how to use .loc and .iloc for slicing. We’ve also
shown a few shorthands. For each of these shorthand code snippets, convert
them to the equivalent code that uses .loc or .iloc.
baby['Name']
baby[0:5]
baby[['Name', 'Count']]
baby[baby['Count'] < 10]
98 | Chapter 4: Working With Dataframes Using pandas

What’s the difference between running:
baby['Name']
and:
baby[['Name']]
And, why does this code work:
baby[['Name']].iloc[0:5, 0]
but this code errors?
baby['Name'].iloc[0:5, 0]
The first code snippet below makes a dataframe with 6 rows, but the second
makes a dataframe with 5 rows. Why?
baby.loc[0:5]
baby.iloc[0:5]
When plotting male and female baby names over time, you might notice that
after 1950 there are generally more male babies. Is this trend reflected in the U.S.
census data? Go to the Census website (https://data.census.gov/cedsci/) and
check.
Find the five names with the highest standard deviation of yearly counts. What
might a large standard deviation tell you about the popularity of these names
over time?
Find the five names with the highest interquartile range of yearly counts. The
interquartile range is the 75th percentile minus the 25th percentile of the data.
You may find the pd.Series.quantile() function useful (link to documenta‐
tion). Are these names different than the names with the highest standard devia‐
tion? Why might this happen?
We’ve shown this syntax for grouping:
baby.groupby('Year')['Count'].sum()
This code also does the same thing:
baby.groupby(baby['Year'])['Count'].sum()
The second syntax passes a pd.Series into .groupby(). It’s a bit more verbose
but also gives more flexibility. Why is this syntax more flexible?
Hint: What does this code do?
baby.groupby(baby['Year'] // 10 * 10)['Count'].sum()
Exercises | 99
Let’s say you want to find the most popular male and female baby name each
year. You might write this:
(baby
.groupby([['Year', 'Sex']])
[['Count', 'Name']]
.max()
)
But this code doesn’t produce the right result. Why?

Now, write code to produce the most popular male and female name each year,
along with its count. Hint: you can make use of the fact that within each year and
birth sex, the names are sorted in descending order of popularity.
Come up with a realistic data example where a data scientist would prefer an
inner join to a left join, and an example where a data scientist would prefer a left
join to an inner join.
In the section on Joins, the nyt table doesn’t have any duplicate names. But a
name could feasibly belong to multiple categories—for instance, Elizabeth is a
name from the Bible and a name for royalty. Let’s say we have a dataframe called
multi_cat that can list a name multiple times—once for each category it belongs
to:
multi_cat = pd.DataFrame([
['Elizabeth', 'bible'],
['Elizabeth', 'royal'],
['Arjun', 'hindu'],
['Arjun', 'mythological'],
], columns=nyt_small.columns)
multi_cat
nyt_name category
0 Elizabeth bible
1 Elizabeth royal
2 Arjun hindu
3 Arjun mythological
What happens when we join baby with this table? In general, what happens when

there are multiple rows that match in both left and right tables?

In a self-join, we take a table and join it with itself. For example, the friends table
contains pairs of people who are friends with each other.
100 | Chapter 4: Working With Dataframes Using pandas

friends = pd.DataFrame([
['Jim', 'Scott'],
['Scott', 'Philip'],
['Philip', 'Tricia'],
['Philip', 'Ailie'],
], columns=['self', 'other'])
friends
self other
0 Jim Scott
1 Scott Philip
2 Philip Tricia
3 Philip Ailie
Why might a data scientist find the following self-join useful?

friends.merge(friends, left_on='other', right_on='self')
self_x other_x self_y other_y
0 Jim Scott Scott Philip
1 Scott Philip Philip Tricia
2 Scott Philip Philip Ailie
Have names become longer on average over time? Produce a plot to answer this
question.
In this chapter we found that you could make reasonable guesses at a person’s age
just by knowing their name. For instance, the name “Luna” has sharply risen in
popularity after 2000, so you could guess that a person named “Luna” was born
around after 2000. Can you make reasonable guesses at a person’s age just from
the first letter of their name? Write code to see whether this is possible, and which
first letters provide the most information about a person’s age.
Exercises | 101
CHAPTER 5

Working With Relations Using SQL

A Note for Early Release Readers

With Early Release ebooks, you get books in their earliest form—the author’s raw and
unedited content as they write—so you can take advantage of these technologies long
before the official release of these titles.
This will be the 7th chapter of the final book. Please note that the GitHub repo will be
made active later on.
If you have comments about how we might improve the content and/or examples in
this book, or if you notice missing material within this chapter, please reach out to the
author at mpotter@oreilly.com.
This chapter repeats the data analyses in the Chapter 4 chapter
using relations and SQL instead of dataframes and Python. The
datasets, data manipulations, and conclusions are nearly identical
across the two chapters so that it’s easier for the reader to see how
the same data manipulations are performed in both pandas and
SQL.
If you’ve already read the dataframe chapter, you can focus your
attention on this section where we introduce the relation, and the
specific SQL code examples in the sections that follow.
Data scientists work with data stored in tables. This chapter introduces relations, one

of the most widely used ways to represent data tables. We’ll also introduce SQL, the

standard programming language for working with relations. Here’s an example of a

relation that holds information about popular dog breeds:

103
breed grooming food_cost kids size
Labrador Retriever weekly 466.0 high medium
German Shepherd weekly 466.0 medium large
Beagle daily 324.0 high small
Golden Retriever weekly 466.0 high medium
Yorkshire Terrier daily 324.0 low small
Bulldog weekly 466.0 medium medium
Boxer weekly 466.0 high medium
In a relation, each row represents a single record—in this case, a single dog breed.

Each column represents a feature about the record—for example, the grooming col‐

umn represents how often each dog breed needs to be groomed.

Relations have labels for columns. For instance, this relation has a column labeled

grooming. Within a column, data have the same type. For instance, the food_cost

column contains numbers, and the size column contains categories. But data types

can be different within a row.

Because of these properties, relations enable all sorts of useful operations.

As a data scientist, you’ll often find yourself working with people
from different backgrounds who use different terms. For instance,
computer scientists say that the columns of a relation represent fea‐
tures of the data, while statisticians call them variables instead.
Other times, people will use the same term to refer to slightly dif‐
ferent things. Data types in a programming sense refers to how a
computer stores data internally. For instance, the size column has
a string data type in Python. But from a statistical point of view, the
size column stores ordered categorical data (ordinal data). We talk
more about this specific distinction in the ??? chapter.
In this chapter, we’ll show you how to do common relation operations using SQL.

First, we’ll explain the structure of SQL queries. Then, we’ll show how to use SQL to

perform common data manipulation tasks, like slicing, filtering, sorting, grouping,

and joining.

Subsetting
This section introduces operations for taking subsets of relations. When data scien‐

tists begin working with a relation, they often want to subset the specific data that

they plan to use. For example, a data scientist can slice out the ten relevant features

from a relation with hundreds of columns. Or, they can filter a relation to remove

104 | Chapter 5: Working With Relations Using SQL

rows with incomplete data. For the rest of this chapter, we’ll introduce relation opera‐

tions using a dataset of baby names.

To work with relations, we’ll introduce a domain-specific programming language

called SQL (Structured Query Language). We commonly pronounce “SQL” like

“sequel” instead of spelling out the acronym. SQL is a specialized language for work‐

ing with relations—as such, SQL has its own syntax that makes it easier to write pro‐

grams that operate on relational data.

About the Data
There’s a 2021 New York Times article that talks about Prince Harry and Meghan’s

unique choice for their new baby daughter’s name: Lilibet (Williams 2021). The arti‐

cle has an interview with Pamela Redmond, an expert on baby names, who talks

about interesting trends in how people name their kids. For example, she says that

names that start with the letter “L” have become very popular in recent years, while

names that start with the letter “J” were most popular in the 1970s and 1980s. Are

these claims reflected in data? We can use SQL to find out.

In this chapter, we’ll use SQL queries within Python programs. This illustrates a com‐

mon workflow—data scientists often process and subset data in SQL before loading

the data into Python for further analysis. SQL databases make it easier to work with

large amounts of data compared to pandas programs. However, loading data into pan

das makes it easier to visualize the data and build statistical models.

So, in this chapter we’ll use the pandas.read_sql function which runs a SQL query

and stores the output in a dataframe. Using this function requires some setup. We

start by importing the pandas and sqlalchemy Python packages.

import pandas as pd
import sqlalchemy
Our database is stored in a file called babynames.db. This file is a SQLite database

(2021), so we’ll set up a sqlalchemy object that can process this format.

db = sqlalchemy.create_engine('sqlite:///babynames.db')
Subsetting | 105
SQL is a programming language that is implemented differently
across database systems. In this book, we use SQLite, one popular
database system. Other systems make different tradeoffs that are
useful for different domains. For instance, PostgreSQL and MySQL
are systems that are useful for large web applications where many
end users are writing data at the same time.
To make matters more complicated, each SQL system has slight
differences. In this book, we’ll rely on core parts of SQL syntax that
are unlikely to change across implementations. While we won’t go
over other systems in detail, we’ll point out where different SQL
systems may differ in capabilities.
Now, we can use pd.read_sql to run SQL queries on this database. This database has

two relations: baby and nyt. Here’s a simple example that reads in the entire baby

relation.

# SQL query saved in a Python string
query = '''
SELECT *
FROM baby;
'''
pd.read_sql(query, db)
Name Sex Count Year
0 Liam M^196592020
1 Noah M^182522020
2 Oliver M^141472020
... ... ... ... ...
2020719 Verona F^51880
2020720 Vertie F^51880
2020721 Wilma F^51880
2020722 rows × 4 columns

The text inside the query variable contains SQL code. SELECT and FROM are SQL key‐

words. We read the query above like:

SELECT * -- Get all the columns...
FROM baby; -- ...from the baby relation
The data in the baby relation comes from the US Social Security department, which

records the baby name and birth sex for birth certificate purposes. They make the

baby names data available on their website (Department 2021).

106 | Chapter 5: Working With Relations Using SQL

The Social Security website has a page that describes the data in more detail (link).

We won’t go in-depth in this chapter about the data’s limitations, but we’ll point out

this relevant quote from the website:

All names are from Social Security card applications for births that occurred in the
United States after 1879. Note that many people born before 1937 never applied for a
Social Security card, so their names are not included in our data. For others who did
apply, our records may not show the place of birth, and again their names are not
included in our data.
All data are from a 100% sample of our records on Social Security card applications as
of March 2021.
What’s a Relation?
Let’s examine the baby relation in more detail. A relation has rows and columns.

Every column has a label, as illustrated in Figure 5-1. Unlike dataframes, however,

individual rows in a relation don’t have labels. Also, unlike dataframes, rows of a rela‐

tion aren’t ordered.

Figure 5-1. The baby relation has labels for columns (boxed).

Relations have a long history. More formal treatments of relations use the term

“tuple” to refer to the rows of a relation, and “attribute” to refer to the columns. There

is also a rigorous way to define data operations using relational algebra, which is

derived from mathematical set algebra. Interested data scientists can find a more in-

depth treatment of relations in books on database systems such as the one by Garcia-

Molina, Ullman, and Widom (Garcia-Molina et al. 2008.

Subsetting | 107
Slicing
Slicing is an operation that creates a new relation by taking a subset of rows or col‐

umns out of another relation. Think about slicing a tomato—slices can go both verti‐

ally and horizontally. To slice columns of a relation, we give the SELECT statement the

columns we want.

query = '''
SELECT Name
FROM baby;
'''
pd.read_sql(query, db)
Name
0 Liam
1 Noah
2 Oliver
... ...
2020719 Verona
2020720 Vertie
2020721 Wilma
2020722 rows × 1 columns

query = '''
SELECT Name, Count
FROM baby;
'''
pd.read_sql(query, db)
Name Count
0 Liam^19659
1 Noah^18252
2 Oliver^14147
... ... ...
2020719 Verona^5
2020720 Vertie^5
2020721 Wilma^5
2020722 rows × 2 columns

To slice out a specific number of rows, use the LIMIT keyword:

108 | Chapter 5: Working With Relations Using SQL

query = '''
SELECT Name
FROM baby
LIMIT 10;
'''
pd.read_sql(query, db)
Name
0 Liam
1 Noah
2 Oliver
7 Lucas
8 Henry
9 Alexander
10 rows × 1 columns

In sum, we use the SELECT and LIMIT keywords to slice columns and rows of a rela‐

tion.

Filtering Rows
So far, we’ve shown how to use SELECT and LIMIT to slice columns and rows of a rela‐

tion.

However, data scientists often want to filter rows—they want to take subsets of rows

using some criteria. Let’s say you want to find the most popular baby names in 2020.

To do this, you can filter rows to keep only the rows where the Year is 2020.

To filter a relation, use the WHERE keyword with a predicate:

query = '''
SELECT *
FROM baby
WHERE Year = 2020;
'''
pd.read_sql(query, db)
Name Sex Count Year
0 Liam M^196592020
1 Noah M^182522020
2 Oliver M^141472020
... ... ... ... ...
Subsetting | 109
Name Sex Count Year
31267 Zylynn F^52020
31268 Zynique F^52020
31269 Zynlee F^52020
31270 rows × 4 columns

Note that when comparing for equality, SQL uses a single equals
sign:
SELECT *
FROM baby
WHERE Year = 2020;
-- ↓
-- Single equals sign
In Python, however, single equals signs are used for variable assign‐
ment. The statement Year = 2020 will assign the value 2020 to the
variable Year. To compare for equality, Python code uses double
equals signs:
# Assignment
my_year = 2021
# Comparison, which evaluates to False
my_year == 2020
To add more predicates to the filter, use the AND and OR keywords. For instance, to

find the names that have more than 10000 babies in either 2020 or 2019, we write:

query = '''
SELECT *
FROM baby
WHERE Count > 10000
AND (Year = 2020
OR Year = 2019);
'''
pd.read_sql(query, db)
Name Sex Count Year
0 Liam M^196592020
1 Noah M^182522020
2 Oliver M^141472020
... ... ... ... ...
41 Mia F^124522019
42 Harper F^104642019
110 | Chapter 5: Working With Relations Using SQL

Name Sex Count Year
43 Evelyn F^104122019
44 rows × 4 columns

Finally, to find the ten most common names in 2020, we can sort the dataframe by

Count in descending order using the ORDER BY keyword with the DESC option (short

for DESCending).

query = '''
SELECT *
FROM baby
WHERE Year = 2020
ORDER BY Count DESC
LIMIT 10;
'''
pd.read_sql(query, db)
Name Sex Count Year
0 Liam M^196592020
1 Noah M^182522020
2 Emma F^155812020
7 Sophia F^129762020
8 Amelia F^127042020
9 William M^125412020
10 rows × 4 columns

We see that Liam, Noah, and Emma were the most popular baby names in 2020.

Example: How recently has Luna become a popular name?
The New York Times article mentions that the name “Luna” was almost nonexistent

before 2000 but has since grown to become a very popular name for girls. When

exactly did Luna become popular? We can check this using slicing and filtering.

When approaching a data manipulation task, we recommend breaking the problem

down into smaller steps. For example, we could think:

Filter: keep only rows with 'Luna' in the Name column, and 'F' in the Sex col‐
umn.
Slice: keep the Count and Year columns.
Now, it’s a matter of translating each step into code.

Subsetting | 111
query = '''
SELECT *
FROM baby
WHERE Name = "Luna"
AND Sex = "F";
'''
luna = pd.read_sql(query, db)
luna
Name Sex Count Year
0 Luna F^77702020
1 Luna F^77722019
2 Luna F^69292018
... ... ... ... ...
125 Luna F^171883
126 Luna F^181881
127 Luna F^151880
128 rows × 4 columns

pd.read_sql returns a pandas.DataFrame object, which we can use to make a plot.

This illustrates a common workflow: process the data using SQL, load it into a pandas

dataframe, then visualize the results.

px.line(luna, x='Year', y='Count', width=350, height=250)
It’s just as the article says. Luna wasn’t popular at all until the year 2000 or so. Think

about that—if someone tells you that their name is Luna, you can take a pretty good

guess at their age even without any other information about them!

112 | Chapter 5: Working With Relations Using SQL

Just for fun, here’s the same plot for the name Siri.

query = '''
SELECT *
FROM baby
WHERE Name = "Siri"
AND Sex = "F";
'''
siri = pd.read_sql(query, db)
px.line(siri, x='Year', y='Count', width=350, height=250)
Why might the popularity have dropped so suddenly after 2010? Well, Siri happens to

be the name of Apple’s voice assistant and was introduced in 2011. Let’s draw a line

for the year 2011 and take a look...

fig = px.line(siri, x='Year', y='Count', width=350, height=250)
fig.add_vline(x=2011, line_color='red', line_width=4)
Subsetting | 113
It looks like parents don’t want their kids to be confused when other people say “Hey

Siri” to their phones.

Takeaways
This section introduces relations in SQL. We’ve covered the common ways that data

scientists subset relations—slicing with column labels and filtering using a boolean

condition. In the next section, we explain how to aggregate rows together.

Aggregating
This section introduces operations for aggregating rows in a relation. Data scientists

aggregate rows together to make summaries of data. For instance, a dataset contain‐

ing daily sales can be aggregated to show monthly sales instead. Specifically, we’ll

introduce grouping, a common operation for aggregating data.

We’ll work with the baby names data, as introduced in the previous section:

# Set up connection to database
import sqlalchemy
db = sqlalchemy.create_engine('sqlite:///babynames.db')
query = '''
SELECT *
FROM baby
LIMIT 10
'''
pd.read_sql(query, db)
114 | Chapter 5: Working With Relations Using SQL

Name Sex Count Year
0 Liam M^196592020
1 Noah M^182522020
2 Oliver M^141472020
7 Lucas M^112812020
8 Henry M^107052020
9 Alexander M^101512020
10 rows × 4 columns

Basic Group-Aggregate
Let’s say we want to find out the total number of babies born as recorded in this data.

This is simply the sum of the Count column. SQL provides functions that we use in

the SELECT statement, like SUM:

query = '''
SELECT SUM(Count)
FROM baby
'''
pd.read_sql(query, db)
SUM(Count)
0^352554503
Summing up the name counts is one simple way to aggregate the data—it combines

data from multiple rows.

But let’s say we instead want to answer a more interesting question: are U.S. births

trending upwards over time? To answer this question, we can sum the Count column

within each year rather than taking the sum over the entire dataset. In other words,

we split the data into groups based on Year, then sum up the Count values within

each group. This process is depicted in Figure 5-2.

Aggregating | 115
Figure 5-2. A depiction of grouping then aggregating for example data.

We call this operation grouping followed by aggregating. In SQL, we specify what

column we wish to use for grouping through the GROUP BY clause, then using aggre‐

gation functions in SELECT:

query = '''
SELECT Year, SUM(Count)
FROM baby
GROUP BY Year
'''
pd.read_sql(query, db)
Year SUM(Count)
0^1880194419
1^1881185772
2^1882213385
... ... ...
138^20183487193
139^20193437438
140^20203287724
141 rows × 2 columns

Notice that the code is nearly the same as the non-grouped version, except that it

contains a GROUP BY clause using the Year column. We also add the Year column to

the SELECT clause so that each row of the result also contains the year.

The result is a relation with the total babies born for each year in the data. Notice that

the Year column contains the unique Year values—there are no duplicate Year values

anymore since we grouped them together. Now we can plot the counts over time:

116 | Chapter 5: Working With Relations Using SQL

counts_by_year = pd.read_sql(query, db)
px.line(counts_by_year, x='Year', y='SUM(Count)', width=350, height=250)
What do we see in this plot? First, we might notice that there seem to be suspiciously

few babies born before 1920. One likely explanation is that the Social Security

Administration was created in 1935, so its data for prior births could be less complete.

We might also notice the dip when World War II began in 1939, and the post-war

Baby Boomer era from 1946-1964.

Here’s the basic recipe for grouping in SQL:

SELECT
col1, -- column used for grouping
SUM(col2) -- aggregation of another column
FROM table_name -- relation to use
GROUP BY col1 -- the column(s) to group by
Grouping on Multiple Columns
We pass multiple columns into GROUP BY to group by multiple columns at once. This

is useful when we need to further subdivide our groups. For example, we can group

by both year and sex to see how many male and female babies were born over time.

query = '''
SELECT Year, Sex, SUM(Count)
FROM baby
GROUP BY Year, Sex
'''
pd.read_sql(query, db)
Aggregating | 117
1 https://www.sqlite.org/lang_aggfunc.html
Year Sex SUM(Count)
0^1880 F^83929
1^1880 M^110490
2^1881 F^85034
279^2019 M^1785527
280^2020 F^1581301
281^2020 M^1706423
282 rows × 3 columns

Notice how the code closely follows the grouping recipe.

Other Aggregation Functions
The SQLite database has several other built-in aggregation functions, such as COUNT,

AVG, MIN, and MAX. The full list of functions is available on the SQLite website^1.

We’ve already seen the SUM function:

query = '''
SELECT Year, SUM(Count)
FROM baby
GROUP BY Year
'''
pd.read_sql(query, db)
Year SUM(Count)
0^1880194419
1^1881185772
2^1882213385
... ... ...
138^20183487193
139^20193437438
140^20203287724
141 rows × 2 columns

To use another aggregation function, we call it in the SELECT clause. For instance, we

can use MAX instead of SUM:

118 | Chapter 5: Working With Relations Using SQL

2 https://www.postgresql.org/docs/current/functions-aggregate.html
query = '''
SELECT Year, MAX(Count)
FROM baby
GROUP BY Year
'''
pd.read_sql(query, db)
Year MAX(Count)
0^18809655
1^18818769
2^18829557
138^201819924
139^201920555
140^202019659
141 rows × 2 columns

The available aggregation functions are one of the first places a data
scientist may encounter differences in SQL implementations. For
instance, SQLite has a relatively minimal set of aggregation func‐
tions, while PostgreSQL has many more^2. Most SQL implementa‐
tions provide SUM, COUNT, MIN, MAX, and AVG.
Example: Have People Become More Creative With Baby Names?
Have people become more creative with baby names over time? One way to measure

this is to see whether the number of unique baby names per year has increased over

time.

To do this aggregation in SQL, we use the COUNT function and the DISTINCT keyword.

The DISTINCT keyword tells SQL to only keep the unique values within a set of col‐

umns.

# Finds the unique baby names
query = '''
SELECT DISTINCT Name
FROM baby
'''
pd.read_sql(query, db)
Aggregating | 119
Name
0 Liam
1 Noah
2 Oliver
... ...
100361 Crete
100362 Roll
100363 Zilpah
100364 rows × 1 columns

To count the number of distinct names, we can aggregate using the COUNT function.

We’ll also use the AS keyword to rename the resulting column.

# Finds the number of unique baby names
query = '''
SELECT COUNT(DISTINCT Name) AS n_names
FROM baby
'''
pd.read_sql(query, db)
n_names
0^100364
Finally, we group by the Year column to aggregate over each year rather than over the

entire dataset:

query = '''
SELECT Year, COUNT(DISTINCT Name) AS n_names
FROM baby
GROUP BY Year
'''
unique_names_by_year = pd.read_sql(query, db)
unique_names_by_year
Year n_names
0^18801889
1^18811829
2^18822012
... ... ...
138^201829619
139^201929417
140^202028613
120 | Chapter 5: Working With Relations Using SQL

3 https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.unstack.html
141 rows × 2 columns

Now, we can plot the number of unique names over time:

px.line(unique_names_by_year,
x='Year', y='n_names',
labels={'n_names': '# unique names'},
width=350, height=250)
We see that the number of unique names has generally increased over time, even

though the number of babies born has mostly stabilized since the 1960s.

Unlike pandas, SQLite doesn’t provide a simple way to pivot a rela‐
tion. Instead, we use GROUP BY on two columns in SQL, read the
result into a dataframe, and then use the unstack() dataframe
method^3.
Takeaways
This section covers common ways to aggregate data in SQL using the GROUP BY key‐

word with one or more columns. In the next section, we’ll explain how to join rela‐

tions together.

Aggregating | 121
Joining
Data scientists very frequently want to join two or more relations together in order to

connect records between relations. For instance, an online bookstore might have one

relation with the books each user has ordered and a second relation with the genres of

each book. By joining the two relations together, the data scientist can see what genres

each user prefers.

We’ll continue looking at the baby names data. We’ll use joins to check some trends

mentioned in the New York Times article about baby names (Williams 2021). The

article talks about how certain categories of names have become more or less popular

over time. For instance, it mentions that mythological names like Julius and Cassius

have become popular, while baby boomer names like Susan and Debbie have become

less popular. How has the popularity of these categories changed over time?

We’ve taken the names and categories in the NYT article and put them in a small rela‐

tion named nyt:

# Set up connection to database
import sqlalchemy
db = sqlalchemy.create_engine('sqlite:///babynames.db')
query = '''
SELECT *
FROM nyt;
'''
pd.read_sql(query, db)
nyt_name category
0 Lucifer forbidden
1 Lilith forbidden
2 Danger forbidden
... ... ...
20 Venus celestial
21 Celestia celestial
22 Skye celestial
23 rows × 2 columns

122 | Chapter 5: Working With Relations Using SQL

Notice that the code above runs a query on babynames.db, the
same database that contains the larger baby relation from the previ‐
ous sections. SQL databases can hold more than one relation, mak‐
ing them very useful when we need to work with many data tables
at once. CSV files, on the other hand, typically contain one data
table each—if we perform a data analysis that uses twenty data
tables, we might need to keep track of the names, locations, and
versions of twenty CSV files. Instead, it could be simpler to store all
the data tables in a SQLite database stored in a single file.
To see how popular the categories of names are, we join the nyt relation with the

baby relation to get the name counts from baby. We’ll start by displaying the first few

rows of the baby relation:

query = '''
SELECT *
FROM baby
LIMIT 10;
'''
pd.read_sql(query, db)
Name Sex Count Year
0 Liam M^196592020
1 Noah M^182522020
2 Oliver M^141472020
7 Lucas M^112812020
8 Henry M^107052020
9 Alexander M^101512020
10 rows × 4 columns

Imagine going down each row in baby and asking, is this name in the nyt table? If so,

then add the value in the category column to the row. That’s the basic idea behind a

join. Let’s look at a few simpler examples first.

Inner Joins
We’ve made smaller versions of the baby and nyt tables so it’s easier to see what hap‐

pens when we join tables together. The relations are called baby_small and

nyt_small.

query = '''
SELECT *
Joining | 123
FROM baby_small;
'''
pd.read_sql(query, db)
Name Sex Count Year
0 Noah M^182522020
1 Julius M^9602020
2 Karen M^62020
3 Karen F^3252020
4 Noah F^3052020
query = '''
SELECT *
FROM nyt_small;
'''
pd.read_sql(query, db)
nyt_name category
0 Karen boomer
1 Julius mythology
2 Freya mythology
To join tables in SQL, we use the JOIN clause to say which tables we want to join, and

the ON clause to specify a predicate for joining the tables. Here’s an example:

query = '''
SELECT *
FROM baby_small JOIN nyt_small
ON baby_small.Name = nyt_small.nyt_name
'''
pd.read_sql(query, db)
Name Sex Count Year nyt_name category
0 Julius M^9602020 Julius mythology
1 Karen M^62020 Karen boomer
2 Karen F^3252020 Karen boomer
Notice that the new table has the columns of both baby_small and nyt_small tables.

The rows with the name Noah are gone. And the remaining rows have their matching

category from nyt_small.

The query above can be read as follows:

124 | Chapter 5: Working With Relations Using SQL

SELECT *
FROM baby_small JOIN nyt_small -- the tables to join
ON baby_small.Name = nyt_small.nyt_name
-- only join rows together when the names are equal
When we join two tables together, we tell SQL the column(s) from each table that we

want to use to join using a predicate with the ON keyword. SQL matches rows together

when the values in the joining columns match, as shown in Figure 5-3.

Figure 5-3. To join, SQL matches rows using the values in the Name and nyt_name col‐

umns. For inner joins (the default), rows that don’t have matching values are dropped.

By default, SQL does an inner join. If either table has rows that don’t have matches in

the other table, SQL drops those rows from the result. In this case, the Noah rows in

baby_small don’t have matches in nyt_small, so they are dropped. Also, the Freya

row in nyt_small doesn’t have matches in baby_small, so it’s dropped as well. Only

the rows with a match in both tables stay in the final result.

Left, Right, and Outer Joins
We sometimes want to keep rows without a match instead of dropping them entirely.

There are other types of joins—left, right, and outer—that keep rows even when they

don’t have a match.

In a left join, rows in the left table without a match are kept in the final result, as

shown in Figure 5-4.

Joining | 125
Figure 5-4. In a left join, rows in the left table that don’t have matching values are kept.

To do a left join in pandas, use LEFT JOIN instead of JOIN:

query = '''
SELECT *
FROM baby_small LEFT JOIN nyt_small
ON baby_small.Name = nyt_small.nyt_name
'''
pd.read_sql(query, db)
Name Sex Count Year nyt_name category
0 Noah M^182522020 None None
1 Julius M^9602020 Julius mythology
2 Karen M^62020 Karen boomer
3 Karen F^3252020 Karen boomer
4 Noah F^3052020 None None
Notice that the Noah rows are kept in the final table. Since those rows didn’t have a

match in the nyt_small dataframe, SQL leaves NULL values in the nyt_name and cate

gory columns (which are then converted to None values when read into a pandas

dataframe). Also, notice that the Freya row in nyt_small is still dropped.

A right join works similarly to the left join, except that non-matching rows in the

right table are kept instead of the left table. SQLite doesn’t support right joins directly,

but we can perform the same join by reversing the order of relations when we use

LEFT JOIN:

query = '''
SELECT *
126 | Chapter 5: Working With Relations Using SQL

FROM nyt_small LEFT JOIN baby_small
ON baby_small.Name = nyt_small.nyt_name
'''
pd.read_sql(query, db)
nyt_name category Name Sex Count Year
0 Karen boomer Karen F 325.0 2020.0
1 Karen boomer Karen M 6.0 2020.0
2 Julius mythology Julius M 960.0 2020.0
3 Freya mythology None None NaN NaN
Finally, an outer join keeps rows from both tables even when they don’t have a match.

SQLite doesn’t have a built-in keyword for outer joins. In cases where an outer join is

needed, we have to either use a different SQL engine or perform an outer join via

pandas. However, in our (the author’s) experience, outer joins are rarely used in prac‐

tice compared to inner and left joins.

Example: Popularity of NYT Name Categories
Now, let’s return to the full baby and nyt relations.

query = '''
SELECT *
FROM baby
LIMIT 10
'''
pd.read_sql(query, db)
Name Sex Count Year
0 Liam M^196592020
1 Noah M^182522020
2 Oliver M^141472020
... ... ... ... ...
7 Lucas M^112812020
8 Henry M^107052020
9 Alexander M^101512020
10 rows × 4 columns

query = '''
SELECT *
FROM nyt
'''
Joining | 127
pd.read_sql(query, db)
nyt_name category
0 Lucifer forbidden
1 Lilith forbidden
2 Danger forbidden
... ... ...
20 Venus celestial
21 Celestia celestial
22 Skye celestial
23 rows × 2 columns

We want to know how the popularity of name categories in nyt have changed over

time. To answer this question:

Inner join baby with nyt, matching rows where the names are equal.
Group the table by category and Year
Aggregate the counts using a sum.
query = '''
SELECT
category,
Year,
SUM(Count) AS count -- [3]
FROM baby JOIN nyt -- [1]
ON baby.Name = nyt.nyt_name -- [1]
GROUP BY category, Year -- [2]
'''
cate_counts = pd.read_sql(query, db)
cate_counts
category Year count
0 boomer^1880292
1 boomer^1881298
2 boomer^1882326
... ... ... ...
647 mythology^20182944
648 mythology^20193320
649 mythology^20203489
650 rows × 3 columns

128 | Chapter 5: Working With Relations Using SQL

The bracketed numbers ([1], [2], [3]) in the query above show how each step in our

plan maps to the parts of the SQL query. Notice that the numbers appear out of order.

We often think of the SELECT statement as the last piece of the query to execute

although it appears first.

Now, we can plot the popularity of individual categories:

As the NYT article claims, “baby boomer” names have become less popular after

2000, while mythological names have become more popular.

We can also plot the popularities of all the categories at once. Take a look at the plots

below and see whether they support the claims made in the New York Times article.

Joining | 129
Takeaways
When joining relations together, we match rows using the JOIN keyword and a

boolean predicate. SQL also allows us to specify the type of join (INNER, LEFT) when

performing a join. In the next section, we’ll explain how to transform values in a rela‐

tion.

Transforming
Data scientists transform columns when they need to change each value in a feature

in the same way. For example, if a feature contains heights of people in feet, a data

scientist might want to transform the heights to centimeters. In this section, we’ll

show how to apply functions that transform columns of data using SQL.

# Set up connection to database
import sqlalchemy
db = sqlalchemy.create_engine('sqlite:///babynames.db')
query = '''
SELECT *
FROM baby
LIMIT 10;
'''
pd.read_sql(query, db)
Name Sex Count Year
0 Liam M^196592020
1 Noah M^182522020
2 Oliver M^141472020
... ... ... ... ...
7 Lucas M^112812020
8 Henry M^107052020
9 Alexander M^101512020
10 rows × 4 columns

In the baby names New York Times article (Williams 2021), Pamela mentions that

names starting with the letter “L” and “K” became popular after 2000. On the other

hand, names starting with the letter “J” peaked in popularity in the 1970s and 1980s

and have dropped off in popularity since. We can verify these claims using the baby

dataset.

We approach this problem using the following steps:

130 | Chapter 5: Working With Relations Using SQL

4 https://www.sqlite.org/lang_corefunc.html
Transform the Name column into a new column that contains the first letters of
each value in Name.
Group the dataframe by the first letter and year.
Aggregate the name counts by summing.
To complete the first step, we’ll apply a function to the Name column.

SQL Functions
SQLite provides scalar functions, or functions that transform single data values.

When called on a column of data, SQLite will apply these functions on each value in

the column. In contrast, aggregation functions like SUM and COUNT take a column of

values as input and compute a single value as output.

SQLite provides a comprehensive list of the built-in scalar functions on its website^4.

For instance, to find the lengths of each name, we use the LENGTH function.

query = '''
SELECT Name, LENGTH(Name)
FROM baby
LIMIT 10;
'''
pd.read_sql(query, db)
Name LENGTH(Name)
0 Liam^4
1 Noah^4
2 Oliver^6
... ... ...
7 Lucas^5
8 Henry^5
9 Alexander^9
10 rows × 2 columns

Notice that the LENGTH function is applied to each value within the Name column.

Transforming | 131
5 https://www.postgresql.org/docs/9.2/functions.html
Like aggregation functions, each implementation of SQL provides a
different set of scalar functions. SQLite has a relatively minimal set
of functions, while PostgreSQL has many more^5. Most SQL imple‐
mentations provide some equivalent to SQLite’s LENGTH, ROUND,
SUBSTR, and LIKE functions.
Calling a scalar function uses the same syntax as an aggregation function. This can

result in confusing output if the two are mixed together in a single query:

query = '''
SELECT Name, LENGTH(Name), COUNT(Name)
FROM baby
LIMIT 10;
'''
pd.read_sql(query, db)
Name LENGTH(Name) COUNT(Name)
0 Liam^42020722
For this reason, we must be careful when when scalar and aggregation functions

appear together within a SELECT statement.

To extract the first letter of each name, we can use the SUBSTR function (short for

“substring”). As described in the documentation, the SUBSTR function takes three

arguments. The first is the input string, the second is the position to begin the sub‐

string (1-indexed), and the third is the length of the substring.

query = '''
SELECT Name, SUBSTR(Name, 1, 1)
FROM baby
LIMIT 10;
'''
pd.read_sql(query, db)
Name SUBSTR(Name, 1, 1)
0 Liam L
1 Noah N
2 Oliver O
... ... ...
7 Lucas L
8 Henry H
132 | Chapter 5: Working With Relations Using SQL

Name SUBSTR(Name, 1, 1)
9 Alexander A
10 rows × 2 columns

Now, we can use the AS keyword to rename the column:

query = '''
SELECT *, SUBSTR(Name, 1, 1) AS Firsts
FROM baby
LIMIT 10;
'''
pd.read_sql(query, db)
Name Sex Count Year Firsts
0 Liam M^196592020 L
1 Noah M^182522020 N
2 Oliver M^141472020 O
7 Lucas M^112812020 L
8 Henry M^107052020 H
9 Alexander M^101512020 A
10 rows × 5 columns

This completes step 1 of our analysis plan. SQL provides several options to break

queries into smaller steps, which is helpful in a more complex analysis like this one.

One option is to create an entirely new relation using the CREATE TABLE statement.

Another option is to use the WITH keyword to create a temporary relation just for the

query at hand. We’ll demonstrate the WITH keyword for this example.

Multistep Queries Using a WITH Clause
The WITH clause lets us assign a name to any SELECT query. Then, we can treat that

query as though it exists as a relation in the database. For instance, we can take the

query above that calculates the first letter of each name and call it letters:

query = '''
WITH letters AS (
SELECT *, SUBSTR(Name, 1, 1) AS Firsts
FROM baby
)
SELECT *
FROM letters
LIMIT 10;
Transforming | 133
'''
pd.read_sql(query, db)
Name Sex Count Year Firsts
0 Liam M^196592020 L
1 Noah M^182522020 N
2 Oliver M^141472020 O
7 Lucas M^112812020 L
8 Henry M^107052020 H
9 Alexander M^101512020 A
10 rows × 5 columns

We can read this query as follows:

-- Create a temporary relation called letters by calculating the first
-- letters for each name in baby
WITH letters AS (
SELECT *,
SUBSTR(Name, 1, 1) AS Firsts
FROM baby
)
-- Then, select the first ten rows from letters
SELECT *
FROM letters
LIMIT 10;
WITH statements are highly useful since they can be chained together. We can create

multiple temporary relations in a WITH statement that each perform a bit of work on

the previous result, which lets us gradually build complicated queries a step at a time.

Example: Popularity of “L” Names
Now, we can perform the next steps in our analysis. We’ll group the relation by the

first letter and year, then aggregate the Count column using a sum.

query = '''
WITH letters AS (
SELECT *, SUBSTR(Name, 1, 1) AS Firsts
FROM baby
)
SELECT Firsts, Year, SUM(Count) AS Count
FROM letters
GROUP BY Firsts, Year;
'''
134 | Chapter 5: Working With Relations Using SQL

letter_counts = pd.read_sql(query, db)
letter_counts
Firsts Year Count
0 A^188016740
1 A^188116257
2 A^188218790
... ... ... ...
3638 Z^201855996
3639 Z^201955293
3640 Z^202054011
3641 rows × 3 columns

Finally, we use plotly to plot the popularity of “L” names over time:

fig = px.line(letter_counts.loc[letter_counts['Firsts'] == 'L'],
x='Year', y='Count', title='Popularity of "L" names',
width=350, height=250)
margin(fig, t=30)
The plot shows that “L” names were popular in the 1960s, dipped in the decades after,

but have indeed resurged in popularity after 2000.

What about “J” names?

fig = px.line(letter_counts.loc[letter_counts['Firsts'] == 'J'],
x='Year', y='Count', title='Popularity of "J" names',
width=350, height=250)
margin(fig, t=30)
Transforming | 135
The NYT article says that “J” names were popular in the 1970s and 80s. The plot

agrees, and also shows that they have become less popular after 2000.

Takeaways
To transform values in a relation, we commonly use SQL functions like LENGTH() or

SUBSTR(). We also explained how to build up complex queries using the WITH clause.

In the next section, we’ll compare relations with other ways to represent and manipu‐

late data tables.

How are Relations Different from Other Data Representations?
Relations are just one way to represent data stored in a table. In practice, data scien‐

tists encounter many other types of data tables, like spreadsheets, matrices, and data‐

frames. In this section, we’ll compare and contrast the relations with other

representations to explain why relations have become so widely used for data analysis.

We’ll also point out scenarios where other representations might be more appropri‐

ate.

Relations and Spreadsheets
Spreadsheets are computer applications where users can enter data in a grid and use

formulas to perform calcuations. One famous example today is Microsoft Excel,

although spreadsheets date back to at least 1979 with VisiCalc (Grad 2007). Spread‐

sheets make it easy to see and directly manipulate data. These properties make

spreadsheets highly popular—by a 2005 estimate, there are over 55 million spread‐

136 | Chapter 5: Working With Relations Using SQL

sheet users compared to 3 million professional programmers in industry (Scaffidi et

al. 2005).

Relations have several key advantages over spreadsheets. Writing SQL code in a com‐

putational notebook like Jupyter naturally produces a data lineage. Someone who

opens the notebook can see the input files for the notebook and how the data were

changed. Spreadsheets do not make a data lineage visible; if a person manually edits

data values in a cell, it is difficult for future users to see which values were manually

edited or how they were edited. Relations can also handle larger datasets than spread‐

sheets; users can use SQL systems to work with huge datasets that would be very hard

to load into a spreadsheet.

Relations and Matrices
A matrix is a two-dimensional array of data used primarily for linear algebra opera‐

tions. In the example below, 𝐀 is a matrix with three rows and two columns.

𝐀=

1 0
0 4
0 0
Matrices are mathematical objects defined by the operators that they allow. For

instance, matrices can be added or multiplied together. Matrices also have a trans‐

pose. These operators have very useful properties which data scientists rely on for

statistical modeling.

One important difference between a matrix and a relation: when treated as a mathe‐

matical object, matrices can only contain numbers. Relations, on the other hand, can

also have other types of data like text. This makes relations more useful for loading

and processing real-world data which may contain all kinds of data types.

Data scientists refer to matrices not only as mathematical objects,
but also as program objects as well. For instance, the R program‐
ming language has a matrix object, while in Python we could repre‐
sent a matrix using a two-dimensional numpy array. Matrices as
implemented in Python and R can contain other data types besides
numbers, but lose mathematical properties when doing so. This is
yet another example of how domains can refer to different things
with the same term.
Relations and Dataframes
Dataframes are one of the most common ways to represent data tables in general pur‐

pose programming languages like Python and R. (We cover dataframes in the Chap‐

ter 4 chapter of this book.) Dataframes share many similarities with relations; both

How are Relations Different from Other Data Representations? | 137
6 https://www.postgresql.org/about/
use rows to represent records and columns to represent features. Both have column

names, and data within a column have the same type.

One key advantage of dataframes is that they don’t require rows to represent records

and columns to represent features. Many times, raw data don’t come in a convenient

format that can directly be put into a relation. In these scenarios, data scientists use

the dataframe to load and process data since dataframes are more flexible in this

regard. Often, data scientists will load raw data into a dataframe, then process the

data into a format that can easily stored into a relation.

One key advantage that relations have over dataframes is that relations are used by

relational database systems like PostgreSQL^6 that have highly useful features for data

storage and management. Consider a data scientist at a company that runs a large

social media website. The database might hold data that is far too large to read into a

pandas dataframe all at once; instead, data scientists use SQL queries to subset and

aggregate data since database systems are more capable of handling large datasets.

Also, website users constantly make updates to their data by making posts, uploading

pictures, and editing their profile. Here, database systems let data scientists reuse

their existing SQL queries to update their analyses with the latest data rather than

having to repeatedly download large CSV files.

For a more rigourous description of the difference between dataframes and relations,

see (Petersohn et al. 2020).

Conclusion
In this chapter, we explain what relations are, why they’re useful, and how to work

with them using SQL code. Subsetting, aggregating, joining, and transforming are

useful in nearly every data analysis. We’ll rely on these operations often in the rest of

the book, especially in the Chapter 6, ???, and ??? chapters.

Exercises
Write a SQL query on the baby relation, read the result into a pandas dataframe,
and create a plot of how popular your name was over time. If you used that plot
to make a guess at your age, what would you guess? Is that close to your actual
age? Think of a potential reason.
When plotting male and female baby names over time, you might notice that
after 1950 there are generally more male babies. Is this trend reflected in the U.S.
138 | Chapter 5: Working With Relations Using SQL

census data? Go to the Census website (https://data.census.gov/cedsci/) and
check.
Let’s say you want to find the most popular male and female baby name each
year. You might write this query:
SELECT Year, Sex, MAX(Count), MAX(Name)
FROM baby
GROUP BY Year, Sex
But this code doesn’t produce the right result. Why?

Now, write code to produce the most popular male and female name each year,
along with its count. Hint: The answer is a bit simpler than you might expect
because of a special SQLite rule (search for “Bare columns in an aggregate quer‐
ies”).
Come up with a realistic data example where a data scientist would prefer an
inner join to a left join, and an example where a data scientist would prefer a left
join to an inner join.
In the section on Joins, the nyt table doesn’t have any duplicate names. But a
name could feasibly belong to multiple categories—for instance, Elizabeth is a
name from the Bible and a name for royalty. Let’s say we have a relation called
multi_cat that can list a name multiple times—once for each category it belongs
to:
query = '''
SELECT *
FROM multi_cat
'''
pd.read_sql(query, db)
nyt_name category
0 Elizabeth bible
1 Elizabeth royal
2 Arjun hindu
3 Arjun mythological
What happens when we join baby with this table? In general, what happens when

there are multiple rows that match in both left and right tables?

In a self-join, we take a table and join it with itself. For example, the friends rela‐
tion contains pairs of people who are friends with each other.
Exercises | 139
query = '''
SELECT *
FROM friends
'''
pd.read_sql(query, db)
self other
0 Jim Scott
1 Scott Philip
2 Philip Tricia
3 Philip Ailie
Why might a data scientist find the following self-join useful?

query = '''
SELECT *
FROM friends AS f1
INNER JOIN friends AS f2
ON f1.other = f2.self
'''
pd.read_sql(query, db)
self other self other
0 Jim Scott Scott Philip
1 Scott Philip Philip Ailie
2 Scott Philip Philip Tricia
The self-join query in the previous exercise uses the AS keyword to rename each
relation. Why do we need to rename the relations in that query? In general,
under what circumstances do we need to rename relations in the FROM clause?
Have names become longer on average over time? Write a SQL query, read the
result into a pandas dataframe, then produce a plot to answer this question.
What does the following SQL query compute? What does the stat column con‐
tain?
query = '''
WITH yearly_avgs AS (
SELECT Name, AVG(Count) AS avg
FROM baby
GROUP BY Name
),
140 | Chapter 5: Working With Relations Using SQL

sq_diffs AS (
SELECT b.Name, POWER(Count - avg, 2) AS sq_diff
FROM baby AS b JOIN yearly_avgs AS y
ON b.Name = y.Name
)
SELECT Name, POWER(AVG(sq_diff), 0.5) AS stat
FROM sq_diffs
GROUP BY Name
ORDER BY stat DESC
LIMIT 10
'''
pd.read_sql(query, db)
Name stat
0 Michael 28296.71
1 Robert 26442.69
2 James 26434.03
... ... ...
7 Richard 15849.65
8 Patricia 13428.25
9 Matthew 13165.15
10 rows × 2 columns

In this chapter we found that you could make reasonable guesses at a person’s age
just by knowing their name. For instance, the name “Luna” has sharply risen in
popularity after 2000, so you could guess that a person named “Luna” was born
around after 2000. Can you make reasonable guesses at a person’s age just from
the first letter of their name?
Answer this question by writing a SQL query, reading the result into a pandas
dataframe, then producing plots to see whether this is possible. Then, see which
first letters provide the most information about a person’s age.
Exercises | 141
CHAPTER 6

Wrangling Files

A Note for Early Release Readers

With Early Release ebooks, you get books in their earliest form—the author’s raw and
unedited content as they write—so you can take advantage of these technologies long
before the official release of these titles.
This will be the 8th chapter of the final book. Please note that the GitHub repo will be
made active later on.
If you have comments about how we might improve the content and/or examples in
this book, or if you notice missing material within this chapter, please reach out to the
author at mpotter@oreilly.com.
Before you can work with data in Python, it helps to understand the files that store

the source of the data. You want answers to a couple of basic questions like:

How much data do you have?
How is the source file formatted?
Answers to these questions can be very helpful. If your file is too large, you may need

special approaches to read it in. If your file isn’t formatted the way you expect, you

may run into bad values after loading it into a dataframe.

Although there are many types of structures that can represent data, in this book we

primarily work with data tables, such as Pandas DataFrames and SQL relations. (But

do note that ??? examines less-structured text data, and ??? works with hierarchical

data structures). We focus on data tables for several reasons. First, research on how to

store and manipulate data tables has resulted in stable and efficient tools for working

143
1 https://www.datafiles.samhsa.gov/study-series/drug-abuse-warning-network-dawn-nid13516
with tables. Second, data in a tabular format are close cousins of matrices, the mathe‐

matical objects of the immensely rich field of linear algebra. Finally, data tables are

very common.

In this chapter, we introduce typical file formats and encodings for plain text and

describe measures of file size, and we use Python tools to examine source files. Later

in the chapter, we introduce an alternative approach for working with files, the shell

interpreter. Shell commands give us a programmatic way to get information about a

file outside of the Python environment, and the shell can be very useful with big data.

Finally, we’ll check the data table’s shape (the number of rows and columns) and

granularity (what a row represents). These simple checks are the starting point for

cleaning and analyzing your data.

The first section provides brief descriptions of two datasets that we use as examples

throughout this chapter.

Data Source Examples
In this chapter, we use two datasets as examples: a government survey about drug

abuse and administrative data from the City of San Francisco about restaurant

inspections. In later sections, we demonstrate how taking stock of the format, encod‐

ing, and size of the files that contain the “raw” data can prevent problems with load‐

ing the source file into a data frame. Before we get started, we want to give an

overview of these datasets and their scope (Chapter 1).

Drug Abuse Warning Network (DAWN) Survey
DAWN is a national healthcare survey that monitors trends in drug abuse and the

emergence of new substances of abuse. The survey also aims to estimate the impact of

drug abuse on the country’s health care system and to improve how emergency

departments monitor substance-abuse crises. DAWN is administered by the U.S. Sub‐

stance Abuse and Medical Health Services Administration (SAMHSA). DAWN was

administered annually from 1998 through 2011. Due in part to the opioid epidemic,

the DAWN survey was restarted in 2018. We examine the 2011 data that have been

made available through the SAMHSA Data Archive^1.

The target population of the survey is all drug-related, emergency-room visits in the

U.S. These visits are accessed through a frame of emergency rooms in hospitals (and

their records). Hospitals are selected for the survey through probability sampling

(covered in Chapter 2), and all drug-related visits to the sampled hospital’s emergency

144 | Chapter 6: Wrangling Files

2 https://data.sfgov.org/Health-and-Social-Services/Restaurant-Scores-LIVES-Standard/pyih-qa8i/data
3 In 2020, the city began giving restaurants color-coded placards indicating whether the restaurant passed
(green), conditionally passed (yellow), or failed (red) the inspection. These new placards no longer display a
numeric inspection score. However, a restaurant’s scores and violations are still available at DataSF.
room are included in the survey. All types of drug-related visits are included, such as

drug misuse, abuse, accidental ingestion, suicide attempts, malicious poisonings, and

adverse reactions. For each visit, as many as 16 different drugs can be recorded,

including illegal drugs, prescription drugs, and over-the-counter medications.

The source file for this dataset is an example of fixed-width formatting that rquires a

codebook to decipher. Also, it is a reasonably large file and so motivates the topic of

how to find a file’s size. And the granularity is unusual because an ER visit, not a per‐

son, is the subject of investigation and because of the complex survey design.

The San Francisco restaurant files have other characteristics that make them a good

example for this chapter.

San Francisco Restaurant Food Safety
The San Francisco Department of Public Health routinely makes unannounced visits

to restaurants and inspects them for food safety. The inspector calculates a score

based on the violations observed and provides descriptions of the violations that were

found. The target population here is all restaurants in the City of San Francisco.

These restaurants are accessed through a frame of restaurant inspections that were

conducted between 2013 and 2016. Some restaurants have multiple inspections in a

year, and not all of the 7000+ restaurants are inspected annually.

Food safety scores are available through the city’s Open Data initiative, called DataSF.

DataSF is one example of city governments around the world making their data pub‐

licly available; the DataSF mission is to “empower the use of data in decision making

and service delivery” with the goal of improving the quality of life and work for resi‐

dents, employers, employees and visitors^2.

The City of San Francisco requires restaurants to publicly display their scores (see

Figure 6-1 below for an example placard)^3. These data offer an example of multiple

files with different structures, fields, and granularity. One dataset contains summary

results of inspections, another provides details about violations found during an

inspection, and a third contains information about the restaurants. The violations

include both serious problems related to the transmission of food borne illnesses and

minor issues such as not properly displaying the inspection placard.

Data Source Examples | 145
Figure 6-1. A food safety scorecard displayed in restaurant. Scores range between 0 and

146 | Chapter 6: Wrangling Files

Both the DAWN survey data and the San Francisco restaurant inspection data are

available online as plain text files. However, their formats are different, and in the

next section, we demonstrate how to figure out a file format so that we can read the

data into a data frame.

File Formats
A file format describes how data are stored on the computer. Understanding the file

format helps us figure out how to read the data into Python in order to work with it

as a table. In this section, we introduce several popular formats used to store data

tables.

The file format and the structure of the data are two different
things. The data’s structure is a mental representation of the data
and tells us what kinds of operations we can do. For example, a
table structure corresponds to data values arranged in rows and
columns. But, the same table can be stored in many different types
of file formats.
Delimited format
Delimited formats use a specific character to separate data values. Usually, these sepa‐

rators are either: a comma (Comma-Separated-Values or CSV for short), a tab (Tab-

Separated Values or TSV), white-space, or a colon. These formats are natural for

storing data that have a table structure. Each line in the file represents a record, which

are delimited by newlines (\n or \r\n) characters. And, within a line, the record’s

information is delimited by the comma character (,) for CSV or the tab character

(\t) for TSV, and so on. The first line of these files often contains the names of the

table’s columns/features.

The San Francisco restaurant scores are stored in CSV-formatted files. Let’s display

the first few lines of the inspections.csv file. In Python, the built-in pathlib library

has a useful Path object to specify paths to files and folders that work across plat‐

forms. The data are stored in the file data/inspections.csv, so we’ll use Path() to

create the full path name. The Path object below has many useful methods, such as

open() as shown below.

from pathlib import Path
# Create a Path pointing to our data file
insp_path = Path() / 'data' / 'inspections.csv'
with insp_path.open() as f:
# Display first five lines of file
File Formats | 147
for _ in range(5):
print(f.readline(), end='')
"business_id","score","date","type"
19,"94","20160513","routine"
19,"94","20171211","routine"
24,"98","20171101","routine"
24,"98","20161005","routine"
Paths are tricky when working across different operating systems
(OS). For instance, a typical path in Windows might look like C:
\files\data.csv, while a path in Unix or MacOS might look like
~/files/data.csv. Because of this, code that works on one OS can
fail to run on other operating systems.
The pathlib Python library was created to avoid OS-specific path
issues. By using it, the code shown here is more portable—it works
across Windows, MacOS, and Unix.
Displaying the first few lines of a file is something we’ll do often, so we create a func‐

tion as a shortcut:

def head(filepath, n=5):
'''Prints the first n lines of filepath'''
with filepath.open() as f:
for _ in range(n):
print(f.readline(), end='')
head(insp_path)
"business_id","score","date","type"
19,"94","20160513","routine"
19,"94","20171211","routine"
24,"98","20171101","routine"
24,"98","20161005","routine"
Notice that the field names appear in the first line of the file; they are comma-

separated and in quotations. We see four fields: the business identifier, the restaurant’s

score, the date of the inspection, and the type of inspection. Each line in the file cor‐

responds to one inspection, and the ID, score, date and type values are separated by

commas. In addition to identifying the file format, we also want to identify the format

of the features. We see two things of note: the scores and dates both appear as strings.

We will want to convert the scores to numbers so we can calculate summary statistics

and create a histogram of scores. And, we will convert the date into a date-time for‐

mat so that we can make time-series plots. We show how to carry out these transfor‐

mations in ???.

All three of the restaurant source files are CSV formatted. On the otherhand, the

DAWN source has a fixed-width format. We describe this format next.

148 | Chapter 6: Wrangling Files

Fixed-width Format
The fixed-width format (FWF) does not use delimiters to separate data values.

Instead, the values for a specific field appear in the exact same position in each line.

The DAWN source file has this format. Each line in the file is very long. For display

purposes, we’ll only show the first few characters from the first 5 lines in the file.

dawn_path = Path() / 'data' / 'DAWN-Data.txt'
width = 65
with dawn_path.open() as f:
for _ in range(5):
print(f.readline()[:width])
1 2251082 .9426354082 3 4 1 2201141 2 865 105 1102005 1
2 2291292 5.9920106887 911 1 3201134 12077 81 82 283-8
3 7 7 251 4.7231718669 611 2 2201143 12313 1 12 -7-8
410 8 292 4.0801470012 6 2 1 3201122 1 234 358 99 215 2
5 122 942 5.1777093467 10 6 1 3201134 3 865 105 1102005 1
Notice how the values appear to align from one row to the next. Notice also that they

seem to be squished together with no separators. We need to know the exact position

of each piece of information in a line in order to make sense of it. SAMHSA provides

a 2,000-page codebook with all of the information needed to read the file. In the

codebook we find that the age field appears in positions 34-35 and is coded in inter‐

vals from 1 to 11. For instance, the first two records shown above have age categories

of 4 and 11, and the codebook tells us that a code of 4 stands for the age bracket “6 to

11”, and 11 is for “65+”.

A widely adopted convention is to use the filename extension, such
as .csv, .tsv, and .txt, to indicate the format of the contents of
the file. File names that end with .csv are expected to contain
comma-separated values, .tsv tab-separated values, and .txt gen‐
erally is plain text without a particular format. However, these
extension names are only suggestions. Even if a file has a .csv
extension, the actual contents might not be formatted properly! It’s
good practice to inspect the contents of the file before loading it
into a data frame. If the file is not too large, you can open and
examine it with a plain text editor. Otherwise, you view a couple of
lines using .readline() or shell commands.
Other plain text formats that are popular include hierarchical formats and loosely

structured formats (in contrast to formats that directly support table structures).

These are covered in greater detail in other chapters, but for completeness we briefly

describe them here.

File Formats | 149
Hierarchical Formats
Hierarchical data formats store data with a nested structure. For instance, the Java‐

Script Object Format (JSON) is a common format used for communication by web

servers. JSON files have a hierarchical structure with keys and values similar to a

Python dictionary. Each record in a JSON file can have different fields and records

can contain other records. The eXtensible Markup Language (XML) and HyperText

Markup Language (HTML) are other common formats for storing documents on the

Internet. Like JSON, these files also contain data in a hierarchical, key-value format.

We cover both formats (JSON and XML) in more detail in ???.

Next we briefly describe other plain text files that don’t fall into any of the previous

categories but still have some structure to them that enables us to read and extract

information.

Loosely Structured Formats
Web logs, instrument readings, and program logs typically provide data in plain text.

For example, below is one line of a Web log (we’ve split it across multiple lines for

readability). It contains information such as the date and time and type of request

made to the Web site.

169.237.46.168 - -
[26/Jan/2004:10:47:58 -0800]"GET /stat141/Winter04 HTTP/1.1" 301 328
"http://anson.ucdavis.edu/courses"
"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.0; .NET CLR 1.1.4322)"
There is structure present, but not in a simple delimited file format, which is what we

mean by “loosely structured”. We see that the date and time appear between square

brackets and the type of request (GET in this case) follows the date-time information

and appears in quotes. Later in ???, we will use these observations about the log’s

structure and string manipulation tools to extract the values of interest into a data

table.

As another example, below is a single recording of measurements taken with a wire‐

less device. The device reports the timestamp, identifier, location of the device, and

the signal strengths that it picks up from other devices. This information uses a com‐

bination of formats: key=value pairs, semicolon delimited, and comma delimited val‐

ues.

t=1139644637174;id=00:02:2D:21:0F:33;pos=2.0,0.0,0.0;degree=45.5;
00:14:bf:b1:97:8a=-33,2437000000,3;00:14:bf:b1:97:8a=-38,2437000000,3;
Like with the Web logs, we can use string manipulation and the patterns in the

recordings to extract features into a table.

Although we focus on data tables in this chapter, all of this is to show: there are many

types of file formats that store data!

150 | Chapter 6: Wrangling Files

So far, we have used the term ‘plain text’ to broadly cover formats that can be viewed

with a text editor. However, a plain text file may have many different encodings, and

if we don’t specify the encoding correctly, the values in the data frame might contain

gibbersih. We give an overview of file encoding next.

File Encoding
Modern computers store data as long sequences of bits: 0 s and 1 s. Character encod‐

ings, like ASCII, tell the computer how to translate between bits and actual text. For

example, in ASCII, the bits 100 001 stand for the letter A, and 100 010 for B. The

most basic kind of plain text supports only standard ASCII characters, which

includes the upper and lowercase English letters, numbers, punctuation symbols, and

spaces.

ASCII encoding is not sufficient to represent a lot of special characters and characters

from other languages. Other, more modern, character encodings have many more

characters that can be represented. Common encodings for documents and Web

pages are Latin-1 (ISO-8859-1) and UTF-8. UTF-8 has over one million characters,

and is backwards compatible with ASCII, meaning that it uses the same representa‐

tion for English letters, numbers, and punctuation as ASCII.

When we have a text file, we usually need to figure out its encoding. If we choose the

wrong encoding to read in a file, Python either reads incorrect values or errors. The

best way to find the encoding is by checking the data’s documentation which often

explicitly says what the encoding is.

When we don’t know what the encoding is, we have to make a guess. The chardet

package has a function called detect() that infers a file’s encoding. Since these

guesses are imperfect, the function also returns a confidence between 0 and 1. We use

this function to look at the files in the data folder for this chapter.

import chardet
line = '{:<25} {:<10} {}'.format
# for each file, print its name, encoding & confidence in the encoding
print(line('File', 'Encoding', 'Confidence'))
for filepath in Path('data').glob('*'):
result = chardet.detect(filepath.read_bytes())
print(line(str(filepath), result['encoding'], result['confidence']))
File Encoding Confidence
data/inspections.csv ascii 1.0
data/co2_mm_mlo.txt ascii 1.0
data/violations.csv ascii 1.0
data/DAWN-Data.txt ascii 1.0
File Formats | 151
data/legend.csv ascii 1.0
data/businesses.csv ISO-8859-1 0.73
The detection function is quite certain that all but one of the files are ASCII encoded.

The exception is businesses.csv, which appears to have an ISO-8859-1 encoding.

We run into trouble, if we ignore this encoding and try to read the business file into

Pandas without specifying the special encoding.

# naively reads file without considering encoding
>>> pd.read_csv('data/businesses.csv')
[...stack trace omitted...]
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xd1 in
position 8: invalid continuation byte
To successfully read the data, we must specify the ISO-8859-1 encoding.

bus = pd.read_csv('data/businesses.csv', encoding='ISO-8859-1')
business_id name address postal_code
0^19 NRGIZE LIFESTYLE CAFE 1200 VAN NESS AVE, 3RD FLOOR^94109
1^24 OMNI S.F. HOTEL - 2ND FLOOR PANTRY 500 CALIFORNIA ST, 2ND FLOOR^94104
2^31 NORMAN’S ICE CREAM AND FREEZES 2801 LEAVENWORTH ST^94133
3^45 CHARLIE’S DELI CAFE 3202 FOLSOM ST^94110
In this section, we have introduced formats for plain text data that are widely used for

storing and exchanging tables. The comma-separated-value format is the most com‐

mon, but others, such as tab-separated and fixed-width, are also prevelant. Even

though a file name has a ‘.csv’ extension, it’s still a good idea to confirm that it is

indeed a CSV file. Likewise, file encoding can be a bit mysterious to figure out, and

unless there is metadata that explicitly gives us the encoding, guesswork comes into

play. When an encoding is not 100% confirmed then its a good idea to seek additional

documentation.

People often confuse CSV and TSV files with spreadsheets. This is
in part because most spreadsheet software (like Microsoft Excel)
will automatically display a CSV file as a table in a workbook.
Behind the scenes, Excel looks at the file format and encoding just
like we’ve done in this section. However, Excel files have a different
format than CSV and TSV files, and we need to use different pan
das functions to read these formats into Python.
Another potentially important aspect of a source file is its size. If a file is huge then we

might not be able to read it into a data frame. In the next section, we discuss how to

figure out a source file’s size.

152 | Chapter 6: Wrangling Files

File Size
Computers have finite resources. You have likely encountered these limits firsthand if

your computer has slowed down from having too many applications open at once.

We want to make sure that we do not exceed the computer’s limits while working

with data, and we might examine a file differently, depending on its size. If we know

that our dataset is relatively small, then a text editor or a spreadsheet can be conve‐

nient to look at the data. On the other hand, for large datasets, a more programmatic

exploration or even distributed computing tools may be needed.

In many situations, we analyze datasets downloaded from the Internet. These files

reside on the computer’s disk storage. In order to use Python to explore and manipu‐

late the data, we need to read the data into the computer’s memory, also known as

random access memory (RAM). All Python code requires the use of RAM, no matter

how short the code is.

A computer’s RAM is typically much smaller than a computer’s disk storage. For

example, one computer model released in 2018 had 32 times more disk storage than

RAM. Unfortunately, this means that data files can often be much bigger than what is

feasible to read into memory. Both disk storage and RAM capacity are measured in

terms of bytes. Roughly speaking, each character in a text file adds one byte to a file’s

size. To succinctly describe the sizes of larger files, we use the prefixes as described in

the following Table 6-1.

Table 6-1. Table 8.1 Prefixes for common filesizes.

Multiple Notation Number of Bytes
Kibibyte KiB 1024
Mebibyte MiB 1024²
Gibibyte GiB 1024³
Tebibyte TiB 1024⁴
Pebibyte PiB 1024⁵
For example, a file containing 52428800 characters takes up

52428800/1024^2 = 50 mebibytes, or 50 MiB on disk.

File Size | 153
Why use multiples of 1024 instead of simple multiples of 1000 for
these prefixes? This is a historical result of the fact that most com‐
puters use a binary number scheme where powers of 2 are simpler
to represent (1024 = 2^10 ). You will also see the typical SI prefixes
used to describe size—kilobytes, megabytes, and gigabytes, for
example. Unfortunately, these prefixes are used inconsistently.
Sometimes a kilobyte refers to 1000 bytes; other times, a kilobyte
refers to 1024 bytes. To avoid confusion, we will stick to kibi-,
mebi-, and gibibytes which clearly represent multiples of 1024.
Many computers have much more disk storage than available memory. It is not

uncommon to have a data file happily stored on a computer that will overflow the

computer’s memory if we attempt to manipulate it with a program, including Python

programs. We often begin our data work by making sure the files we are of managea‐

ble size. To accomplish this, we can use the built-in os library.

from pathlib import Path
import os
kib = 1024
line = '{:<25} {}'.format
print(line('File', 'Size (KiB)'))
for filepath in Path('data').glob('*'):
size = os.path.getsize(filepath)
print(line(str(filepath), np.round(size / kib)))
File Size (KiB)
data/inspections.csv 455.0
data/co2_mm_mlo.txt 50.0
data/violations.csv 3639.0
data/DAWN-Data.txt 273531.0
data/legend.csv 0.0
data/businesses.csv 645.0
We see that the businesses.csv file takes up 645 KiB on disk, making it well within

the memory capacities of most systems. Although the violations.csv file takes up

3.6 MiB of disk storage, most machines can easily read it into a Pandas DataFrame

too. But DAWN-Data.txt, which contains the DAWN survey data, is much larger.

The DAWN file takes up nearly 270 MiB of disk storage, and while some computers

can work with this file in memory, it can slow down other systems. To make this data

more manageable in Python, we can load in a subset of the columns rather than all of

them.

Sometimes we are interested in the total size of a folder instead of the size of individ‐

ual files. For example, if we have one file of inspections for each month in a year, we

might like to see whether we can combine all the data into a single data frame.

154 | Chapter 6: Wrangling Files

mib = 1024**2
total = 0
for filepath in Path('data').glob('*'):
total += os.path.getsize(filepath) / mib
print(f'The data/ folder contains {total:.2f} MiB')
The data/ folder contains 271.80 MiB
As a rule of thumb, reading in a file using pandas usually requires
at least five times the available memory as the file size. For exam‐
ple, reading in a 1 GiB file will typically require at least 5 GiB of
available memory. Memory is shared by all programs running on a
computer, including the operating system, web browsers, and
Jupyter notebook itself. A computer with 4 GiB total RAM might
have only 1 GiB available RAM with many applications running.
With 1 GiB available RAM, it is unlikely that pandas will be able to
read in a 1 GiB file.
Next, we discuss strategies for working with data that are far larger than what is feasi‐

ble to load into memory.

Working With Large Datasets
The popular term “big data” generally refers to the scenario where the data are large

enough that even top-of-the-line computers can’t read the data directly into memory.

This is a common scenario in scientific domains like astronomy, where telescopes

capture many large images of space. It’s also common for companies that have lots of

users.

Figuring out how to draw insights from large datasets is an important research prob‐

lem that motivates the fields of database engineering and distributed computing.

While we won’t cover these fields in depth in this book, we can provide a brief over‐

view of basic approaches.

Subset The Data.

One simple approach is to subset the data. Rather than loading in the entire source

file, we can either select a specific part of it (e.g. one day’s worth of data), or we can

randomly sample the dataset. Because of its simplicity, we use this approach quite

often in this book. The natural downside is that this approach loses many of the bene‐

fits of analyzing a large dataset, like being able to study rare events.

Use a Database System.

As we discussed in Chapter 5, relational database management systems (RDBMS)

were specifically designed to store large datasets. These systems can manipulate data

File Size | 155
that are too large to fit into memory using SQL queries. Because of their advantages,

RDBMS’s are common in research and industry settings for data storage. One down‐

side is that they often require a separate server for the data that needs its own config‐

uration. Another downside is that SQL is less flexible in what it can compute than

Python, which becomes especially relevant for modeling and prediction. One useful

hybrid approach is to use SQL to subset, aggregate, or sample the data into batches

that are small enough to read into Python. Then, we can use Python for more sophis‐

ticated analyses.

Use a Distributed Computing System.

Another approach to handle complex computations on large datasets is to use a dis‐

tributed computing system like MapReduce, Spark, or Ray. These systems work best

on tasks that can be split into many smaller parts since they split up large datasets

into smaller ones, then run programs on all of the smaller datasets at once. Because of

this, these systems have great flexibility and can be used in a variety of scenarios like

modeling and prediction. Their main downside is that they can require a lot of work

to install and configure properly, since these systems are typically installed across

many computers that need to coordinate with each other.

In sum, this section introduced common file size notation and showed how to check

file sizes in Python. It is convenient to use Python to determine a file format, encod‐

ing, and size. Another powerful tool for working with files is the shell, which is widely

used and has a more succinct syntax than Python. In the next section, we introduce a

few command-line tools available in the shell for carrying out the same tasks of find‐

ing out information about a file before reading it into a data frame.

The Shell and Command Line Tools
Nearly all computers provide access to a shell interpreter, such as sh or bash. Shell

interpreters typically perform operations on the files on a computer, and they have

their own language, syntax, and built-in commands.

We use the term command-line interface (CLI) tools to refer to the commands avail‐

able in a shell interpreter. Although we only cover a few CLI tools in this section,

there are many useful CLI tools that enable all sorts of operations on files. For

instance, running the following command in the bash shell produces a list of all the

files in the figures/ folder along with their file sizes:

ls -l -h figures/
The basic syntax for a shell command is:

command -options arg1 arg2
156 | Chapter 6: Wrangling Files

CLI tools often take one or more arguments, similar to how Python functions take

arguments. In the shell, we wrap arguments with spaces, not with parentheses and

commas. The arguments appear at the end of the command line, and they are usually

the name of a file or some text. In the ls example above, the argument to ls is fig

ures/. Additionally, CLI tools support flags that provide additional options. These

flags are specified immediately following the command name using a dash as a delim‐

iter. In the ls example above, we provided the flags -l (to provide extra information

about each file) and -h (to provide filesizes in a more human-readable format). Many

commands have default arguments and options, and the man command prints a list of

acceptable options, examples, and defaults for any command. For example, man ls

describes the 30 or so flags available for ls.

All CLI tools we cover in this book are specific to the sh shell inter‐
preter, the default interpreter for Jupyter installations on MacOS
and Linux systems at the time of writing. Windows systems have a
different interpreter and the commands shown in the book may
not run on Windows, although Windows gives access to a sh inter‐
preter through its Linux Subsystem.
The commands in this section can be run in a terminal application,
or through a terminal opened by Jupyter.
We begin with an exploration of the file system for this chapter, using the ls tool.

ls
data wrangling_granularity.ipynb
figures wrangling_intro.ipynb
wrangling_command_line.ipynb wrangling_structure.ipynb
wrangling_datasets.ipynb wrangling_summary.ipynb
wrangling_formats.ipynb
To dive deeper and list the files in the data/ directory, we provide the directory name

as an argument to ls.

ls -l -L -h data/
total 556664
-rw-r--r-- 1 nolan staff 267M Dec 10 14:03 DAWN-Data.txt
-rw-r--r-- 1 nolan staff 645K Dec 10 14:01 businesses.csv
-rw-r--r-- 1 nolan staff 50K Jan 22 13:09 co2_mm_mlo.txt
-rw-r--r-- 1 nolan staff 455K Dec 10 14:01 inspections.csv
-rw-r--r-- 1 nolan staff 120B Dec 10 14:01 legend.csv
-rw-r--r-- 1 nolan staff 3.6M Dec 10 14:01 violations.csv
We also added the -l flag to the command, which specifies the format of the output

to have information about each file on a separate line along with additional metadata.

The Shell and Command Line Tools | 157
In particular, the fifth column of the listing shows the file size. To make the file sizes

more readable, we used the -h flag. When we have multiple simple option flags like -

l, -h, and -L, we can combine them together as a shorthand:

ls -lLh data/
When working with datasets in this book, our code will often use
an additional -L flag for ls and other CLI tools, such as du. We do
this because we set up the datasets in our book using shortcuts
(called symlinks). Usually, your code won’t need the -L flag unless
you’re working with symlinks too.
Other CLI tools for checking the size of files, are wc and du. The command wc (short

for wordcount) provides helpful information about a file’s size in terms of the number

of lines, words, and characters in the file.

wc data/DAWN-Data.txt
229211 22695570 280095842 data/DAWN-Data.txt
We can see from the output that DAWN-Data.txt has 229211 lines and 280095842

characters. (The middle value is the file’s word count, which is useful for text but not

very useful for files containing data.)

The ls tool does not calculate the cumulative size of the contents of a folder. To prop‐

erly calculate the total size of a folder, including the files in the folder, we use du

(short for disk usage). By default, the du tool shows the size in units called blocks.

du -L data/
556664 data/
We commonly also add the -s flag to du to show the file sizes for both files and fold‐

ers and the -h flag to display quantities in the standard KiB, MiB, GiB format. The

asterisk in data/* below tells du to show the size of every item in the data folder.

du -Lsh data/*
267M data/DAWN-Data.txt
648K data/businesses.csv
52K data/co2_mm_mlo.txt
456K data/inspections.csv
4.0K data/legend.csv
3.6M data/violations.csv
To check the formatting of a file, we can examine the first few lines with the head

command, or the last few lines with tail. These CLIs are very useful for peeking at a

file’s contents to determine whether it’s formatted as a CSV, TSV, etc. As an example,

let’s look at the inspections.csv file.

158 | Chapter 6: Wrangling Files

head data/inspections.csv
"business_id","score","date","type"
19,"94","20160513","routine"
19,"94","20171211","routine"
24,"98","20171101","routine"
24,"98","20161005","routine"
24,"96","20160311","routine"
31,"98","20151204","routine"
45,"78","20160104","routine"
45,"88","20170307","routine"
45,"85","20170914","routine"
By default, head displays the first 10 lines of a file. If we want to show, say, 6 lines,

then we add the option -n 6 to our command (or just -6 for short).

We can print the entire file’s contents using the cat command. However, you should

take care when using this command, as printing a large file can cause the browser to

crash. The legend.csv file is small, and we can use cat to concatenate and print its

contents.

cat data/legend.csv
"Minimum_Score","Maximum_Score","Description"
0,70,"Poor"
71,85,"Needs Improvement"
86,90,"Adequate"
91,100,"Good"
In many cases, using head or tail alone gives us a good enough sense of the file

structure to proceed with loading it into a data frame.

Finally, the file command can help use determine a file’s encoding.

file -I data/*
data/DAWN-Data.txt: text/plain; charset=us-ascii
data/businesses.csv: application/csv; charset=iso-8859-1
data/co2_mm_mlo.txt: text/plain; charset=us-ascii
data/inspections.csv: application/csv; charset=us-ascii
data/legend.csv: application/csv; charset=us-ascii
data/violations.csv: application/csv; charset=us-ascii
We see (again) that all of the files are ASCII, except for businesses.csv which has an

ISO-8859-1 encoding.

Commonly, we open a terminal program to start a shell interpreter.
However, Jupyter Notebooks provide a convenience: if a line of
code in a Python code cell is prefixed with the! character, the line
will go directly to the system’s shell interpreter. For example, run‐
ning !ls in a Python cell lists the files in the current directory.
The Shell and Command Line Tools | 159
In this section, we have introduced a few command-line tools: ls, du, wc, head, tail,

cat and file. These tools help us understand the format and structure of data files.

We can also use shell tools to ensure that the data file is small enough to read into

pandas and to get the correct encoding. Once a file is read into pandas, we have a

DataFrame and can proceed with analysis.

Shell commands give us a programmatic way to work with files, rather than a point-

and-click “manual” approach. They are useful for:

Documentation: if you need to record what you did
Error reduction: if you want to reduce typographical errors and other simple but
potentially harmful mistakes
Reproducibility: if you need to repeat the same process in the future or you plan
to share your process with others you have a record of your actions
Volume: if you have many repetitive operations to perform, the size of the file you
are working with is large, or you need to perform things quickly, CLI tools can
help.
After the data have been loaded into a data frame, our next task is to figure out the

table’s shape and granularity. We start by finding the number of rows and columns in

the table (its shape). Then, we need to understand what a row represents before we

begin to check the quality of the data. We’ll cover these topics in the next section.

Table Shape and Granularity
In this section, we introduce granularity: what each row in the table represents.

As described earlier, we refer to a dataset’s structure as a mental representation of the

data, and in particular, we represent data that has a table structure by arranging data

values in rows and columns. Now that we have investigated the restaurant inspection

files, we load them into dataframes and examine their shapes.

bus = pd.read_csv('data/businesses.csv', encoding='ISO-8859-1')
insp = pd.read_csv("data/inspections.csv")
viol = pd.read_csv("data/violations.csv")
print(" Businesses shape:", bus.shape)
print("Inspections shape:", insp.shape)
print(" Violations shape:", viol.shape)
Businesses shape: (6406, 9)
Inspections shape: (14222, 4)
Violations shape: (39042, 3)
160 | Chapter 6: Wrangling Files

We find that the businesses table has 6406 rows and 9 columns. Now, let’s figure out

the granularity of this table by understanding what each row represents. To start, we’ll

look at the first two rows.

business_id name address city ... postal_code latitude longitude phone_number
0^19 NRGIZE
LIFESTYLE
CAFE
1200 VAN
NESS AVE, 3RD
FLOOR
San
Francisco
... 94109 37.79 -122.42 +14157763262
1^24 OMNI S.F.
HOTEL - 2ND
FLOOR
PANTRY
500
CALIFORNIA
ST, 2ND FLOOR
San
Francisco
... 94104 37.79 -122.40 +14156779494
2 rows × 9 columns

Simply looking at these two rows, we get the impression that each record represents a

single restaurant. But, we can’t tell from just two records whether or not this is the

case. The field name business_id implies that it is the unique identifier for the busi‐

ness (restaurant). We can confirm this by checking whether the number of records in

bus matches the number of unique values in business_id.

print("Number of records:", len(bus))
print("Number of unique business ids:", len(bus['business_id'].unique()))
Number of records: 6406
Number of unique business ids: 6406
The number of unique business_ids matches the number of rows in the table. In

other words, business_id uniquely identifies each record in the DataFrame. We call

business_id the primary key for the table. A primary key is the feature that uniquely

identifies each row in a table. We use primary keys to join tables like in Chapter 4.

Sometimes a primary key consists of two (or more) features. This is the case for the

other two restaurant files. Let’s continue the examination of the inspections and viola‐

tions data frames and find their granularity.

Granularity of Restaurant Inspections and Violations
We saw earlier in this chapter that there are many more rows in the inspections table

compared to the business table. Let’s take a closer look at the first few rows of inspec‐

tions.

business_id score date type
0^199420160513 routine
1^199420171211 routine
2^249820171101 routine
Table Shape and Granularity | 161
business_id score date type
3^249820161005 routine
Like the businesses table, this table also contains a field called business_id, but we

see duplicate values of the ID. The two records for business #19 have different date

values, which implies that there is one record for each inspection of a restaurant. In

other words, the granularity of this table is a restaurant inspection. If this is indeed

the case, that would mean that the unique identifier of a row is the combination of

business_id and date. That is, the primary key consists of two fields.

To confirm that the two fields form a primary key, we can group insp by the combi‐

nation of business_id and date, and then find the size of each group. If business_id

and date uniquely define each row of the dataframe, then each group should have

size 1.

(insp
.groupby(['business_id', 'date'])
.size()
.sort_values(ascending=False)
.head(5)
)
business_id date
64859 20150924 2
87440 20160801 2
77427 20170706 2
19 20160513 1
71416 20171213 1
dtype: int64
The combination of ID and date, uniquely identifies each record in the inspections

table, with the exception of three restaurants, which have two records for their ID-

date combination. For example, restaurant 64859 has two records with an inspection

date of 20150924. Let’s examine these rows.

insp.query('business_id == 64859 and date == 20150924')
business_id score date type
7742^648599620150924 routine
7744^648599120150924 routine
Restaurant 64859 got two different inspection scores on 20150924 (Sept. 24, 2015).

How could this happen? It may be that the restaurant had two inspections in one day,

or it might be an error. We address these sorts of questions when we consider the data

quality in ???. In this case, we’ll assume that a restaurant can have two inspections on

the same date. So, the primary key for the inspections table is the combination of res‐

taurant ID and inspection date.

162 | Chapter 6: Wrangling Files

Note that the business_id field in the inspections table acts as a reference to the pri‐

mary key in the business table. Because of this, business_id in insp is called a foreign

key: it links each record in the inspections table to a record in the business table. This

means that we can readily join these two tables together.

Next, we examine the granularity of the violations table.

business_id date description
0^1920171211 Inadequate food safety knowledge or lack of ce...
1^1920171211 Unapproved or unmaintained equipment or utensils
2^1920160513 Unapproved or unmaintained equipment or utensi...
39039^9423120171214 High risk vermin infestation [ date violation...
39040^9423120171214 Moderate risk food holding temperature [ dat...
39041^9423120171214 Wiping cloths not clean or properly stored or ...
39042 rows × 3 columns

Just looking at the first few records in viol we see that each inspection has multiple

entries. The granularity is at the level of a violation found in an inspection. Reading

the descriptions, we see that if corrected, a date is listed in the description within

square brackets.

viol.loc[39039, 'description']
'High risk vermin infestation [ date violation corrected: 12/15/2017 ]'
In summary, we have found that these three tables have different granularities. If we

are interested in studying inspections, we can join the violations and inspections

together using the business ID and inspection date. This would let us find the number

of violations that happened in each inspection.

We can also reduce the inspection table by selecting the most recent inspection for

each restaurant. This reduced data table essentially has a granularity of restaurant,

which may be useful for a restaurant-based analysis. These kinds of actions reshape

the data table, transform columns, and create new columns. We’ll cover these opera‐

tions in ???.

We conclude this section with a look at the shape and granularity of the DAWN sur‐

vey data.

DAWN Survey Shape and Granularity
As noted earlier in this chapter, the DAWN file has fixed-width formatting, and we

need to rely on a codebook to find out where the fields are. For example, a snippet of

the codebook in Figure 6-2 tells us that age appears in positions 34 and 35 in the row,

Table Shape and Granularity | 163
and it is categorized into 11 age groups, 1 stands for 5 and under, 2 for 6 to 11, ...,

and 11 for 65 and older. Also, -8 represents a missing value.

Figure 6-2. Screenshot of a portion of the DAWN coding for age.

Earlier, we determined the file contains 200 thousand lines and over 280 million char‐

acters so, on average, there are about 1200 characters per line. This might be why they

used a fixed-width file rather than a CSV file; the file would be much larger if there

was a comma between every field!

Given the tremendous amount of information on each line, we read just a few vari‐

ables into a data frame. We can use the pandas.read_fwf method to do this. We spec‐

ify the exact positions of the fields to extract, provide names for these fields, and

other information about the header and index.

colspecs = [(0,6), (14,29), (33,35), (35, 37), (37, 39), (1213, 1214)]
varNames = ["id", "wt", "age", "sex", "race","type"]
dawn = pd.read_fwf('data/DAWN-Data.txt', colspecs=colspecs,
header=None, index_col=0, names=varNames)
wt age sex race type
id
1 0.94^4128
2 5.99^11134
3 4.72^11224
4 4.08^2134
5 5.18^6138
We can compare the rows in the table to the number of lines in the file.

dawn.shape
(229211, 5)
The number of rows in the data frame matches the number of lines in the file. That’s

good. The granularity of the data frame is a bit complicated due to the survey design.

Recall that these data are part of a large scientific study, with a complex sampling

scheme. A row represents an emergency room visit, so the granularity is at the emer‐

gency room visit level. However, in order to reflect the sampling scheme and be rep‐

164 | Chapter 6: Wrangling Files

resentative of the population, weights are provided. We must apply a weight to each

record when we compute summary statistics, build histograms, and fit models.

The wt field contains a value that takes into account the probability of an ER visit like

this one appearing in the sample. By “like this one” we mean a visit with similar fea‐

tures, like the visitor age, race, visit location, and time of day. We examine the differ‐

ent values in wt.

dawn['wt'].value_counts()
0.94 1719
84.26 1617
1.72 1435
...
3.33 1
6.20 1
3.31 1
Name: wt, Length: 3500, dtype: int64
What do these weights mean? As a simplified example, suppose
you ran a survey and 75% of your respondents reported their sex as
female. Since you know from the Census that roughly 50% of the
U.S. population is female, you can adjust your survey responses by
using a small weight (less than 1) for female responses and a larger
weight (greater than 1) for male responses. The DAWN survey uses
the same idea, except that they split the groups much more finely.
It is critical to include the survey weights in your analysis to get data that represents

the population at large. For example, we can compare the calculation of the propor‐

tion of females among the ER visits both with and without the weights.

unweighted = np.average(dawn["sex"] == 2)
weighted = np.average(dawn["sex"] == 2, weights=dawn["wt"])
print(f'Unweighted % female: {unweighted:.1%}')
print(f' Weighted % female: {weighted:.1%}')
Unweighted % female: 48.0%
Weighted % female: 52.3%
These figures differ by more than 4 percentage points. The weighted version is a more

accurate estimate of the proportion of females among the entire population of drug-

related ER visits.

In this section, we discovered the granularity for the restaurant dataset tables and the

DAWN data. Sometimes the granularity can be tricky to figure out, like we saw with

the inspections data. And, we needed to take the sampling weights into account when

looking at the granularity for the DAWN data. These examples show it’s important to

take your time and review the data descriptions before proceeding with analysis.

Table Shape and Granularity | 165
Summary
Data wrangling is an essential part of data analysis. Without it, we risk overlooking

problems in data that can have major consequences for future analysis. This chapter

covered an important first step in data wrangling: reading data from a plain text

source file into a Python DataFrame. We introduced different types of file formats

and encodings, and we wrote code that can read data from these formats. We checked

the size of source files and considered alternative tools for working with large data‐

sets.

We also introduced command-line tools as an alternative to Python for checking the

format, encoding, and size of a file. These CLI tools are especially handy for

filesystem-oriented tasks because of their simple syntax. In this chapter, we’ve only

touched the surface of what CLI tools can do. In reality, the shell is capable of sophis‐

ticated data processing and is well worth learning.

Understanding the shape and granularity of a table gives us insight into what a row in

a data table represents. This helps us determine whether the granularity is mixed,

aggregation is needed, or weights are required. After looking at the granularity of

your dataset, you should have answers to the following questions.

What does a record represent?
Do all records in a table capture granularity at the same level? Sometimes a table
contains additional summary rows that have a different granularity.
If the data were aggregated, how was the aggregation performed? Summing and
averaging are common types of aggregation.
What kinds of aggregations might we perform on the data?
Knowing your table’s granularity is a first step to cleaning your data, and it informs

you of how to analyze your data. For example, we saw the granularity of the DAWN

survey is an ER visit. That naturally leads us to think about comparisons of patient

demographics to the US as a whole.

The wrangling techniques in this chapter help us bring data from a source file into a

data frame and understand its structure. Once we have a data frame, further wran‐

gling is needed to assess quality and prepare the data for analysis. We’ll cover this in

the next chapter.

166 | Chapter 6: Wrangling Files

About the Authors
Sam Lau is a PhD candidate at UC San Diego. He designs novel interfaces for learn‐

ing and teaching data science, and his research has been published in top-tier confer‐

ences in human-computer interaction and end-user programming. Sam instructed

and helped design flagship data science courses at UC Berkeley. These courses have

grown to serve thousands of students every year and their curriculum is used by uni‐

versities across the world.

Deborah (Deb) Nolan is Professor of Statistics and Associate Dean for Undergradu‐

ate Studies in the Division of Computing, Data Science, and Society at the University

of California, Berkeley, where she holds the Zaffaroni Family Chair in Undergraduate

Education. Her research has involved the empirical process, high-dimensional mod‐

eling, and, more recently, technology in education and reproducible research. Her

pedagogical approach connects research, practice and education, and she is co-author

of 4 textbooks: Stat Labs, Teaching Statistics, Data Science in R, and Communicating

with Data.

Joseph (Joey) Gonzalez is an assistant professor in the EECS department at UC

Berkeley and a founding member of the new UC Berkeley RISE Lab. His research

interests are at the intersection of machine learning and data systems, including:

dynamic deep neural networks for transfer learning, accelerated deep learning for

high-resolution computer vision, and software platforms for autonomous vehicles.

Joey is also co-founder of Turi Inc. (formerly GraphLab), which was based on his

work on the GraphLab and PowerGraph Systems. Turi was recently acquired by

Apple Inc.

This is a offline tool, your data stays locally and is not send to any server!
Feedback & Bug Reports