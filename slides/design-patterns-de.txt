 PDF in Markdown Konverter
Debug-Ansicht
Ergebnis-Ansicht
Entwurfsmuster für maschinelles Lernen
Valliappa Lakshmanan,
Sara Robinson & Michael Munn
Maschinelles Lernen
Entwurfsmuster
Lösungen für häufige Herausforderungen bei Daten
Datenaufbereitung, Modellbildung und MLOps
Valliappa Lakshmanan, Sara Robinson,
und Michael Munn
Entwurfsmuster für maschinelles Lernen

Lösungen für gemeinsame Herausforderungen bei Daten

Vorbereitung, Modellbildung und MLOps

PekingPeking BostonBoston FarnhamFarnham SebastopolSebastopol TokioTokio
978-1-098-11578-

[LSI]

Entwurfsmuster für maschinelles Lernen
von Valliappa Lakshmanan, Sara Robinson und Michael Munn

Copyright © 2021 Valliappa Lakshmanan, Sara Robinson, und Michael Munn. Alle Rechte vorbehalten.

Gedruckt in den Vereinigten Staaten von Amerika.

Veröffentlicht von O'Reilly Media, Inc. 1005 Gravenstein Highway North, Sebastopol, CA 95472.

O'Reilly-Bücher können für Bildungszwecke, für Unternehmen oder zur Verkaufsförderung erworben werden. Online-Ausgaben sind
sind für die meisten Titel ebenfalls erhältlich (http://oreilly.com). Weitere Informationen erhalten Sie von unserer Vertriebsabteilung für Unternehmen/Institutionen.
tionalen Vertriebsabteilung: 800-998-9938 oder corporate@oreilly.com.

Akquisitionsredakteurin: Rebecca Novack Indexer: nSight, Inc.
Entwicklungsredaktion: Corbin Collins Innenarchitekt: David Futato
Produktionsredakteur: Beth Kelly Umschlaggestaltung: Karen Montgomery
Lektorat: Charles Roumeliotis Illustrator: Kate Dullea
Korrektorin: Holly Bauer Forsyth

Oktober 2020: Erste Ausgabe

Revisionsübersicht für die erste Ausgabe
2020-10-15: Erste Veröffentlichung

Siehe http://oreilly.com/catalog/errata.csp?isbn=9781098115784 für Einzelheiten zur Veröffentlichung.

Das O'Reilly-Logo ist eine eingetragene Marke von O'Reilly Media, Inc. Machine Learning Design Patterns, das
Titelbild und die zugehörige Aufmachung sind Marken von O'Reilly Media, Inc.

Die in diesem Werk geäußerten Ansichten sind die der Autoren und stellen nicht die Ansichten des Herausgebers dar.
Der Herausgeber und die Autoren haben sich nach bestem Wissen und Gewissen bemüht, die Richtigkeit der Informationen und
der Herausgeber und die Autoren nach bestem Wissen und Gewissen bemüht sind, die Richtigkeit der in diesem Werk enthaltenen Informationen und Anleitungen zu gewährleisten, lehnen der Herausgeber und die Autoren jede Verantwortung für Fehler oder Auslassungen ab.
für Fehler oder Auslassungen, einschließlich und ohne Einschränkung der Verantwortung für Schäden, die durch die Verwendung von
oder dem Vertrauen auf dieses Werk. Die Nutzung der in diesem Werk enthaltenen Informationen und Anleitungen erfolgt auf eigene
Risiko. Wenn Codebeispiele oder andere Technologien, die in diesem Werk enthalten oder beschrieben sind, Open-Source
Open-Source-Lizenzen oder den Rechten am geistigen Eigentum anderer unterliegen, sind Sie dafür verantwortlich, dass Ihre Nutzung
mit diesen Lizenzen und/oder Rechten übereinstimmt.

Inhaltsübersicht
Die Notwendigkeit von Entwurfsmustern für maschinelles Lernen. Vorwort. ix
Was sind Design Patterns?
Wie man dieses Buch benutzt
Terminologie des maschinellen Lernens
Modelle und Frameworks
Daten- und Merkmalstechnik
Der Prozess des maschinellen Lernens
Daten und Modellwerkzeuge
Rollen
Allgemeine Herausforderungen beim maschinellen Lernen
Qualität der Daten
Reproduzierbarkeit
Daten-Drift
Maßstab
Mehrere Zielsetzungen
Zusammenfassung
Entwurfsmuster für die Datendarstellung.
Einfache Datendarstellungen
Numerische Eingaben
Kategoriale Eingaben
Entwurfsmuster 1: Hash-Merkmal
Problem
Lösung
Warum es funktioniert
Kompromisse und Alternativen
Entwurfsmuster 2: Einbettungen
Problem
Lösung
Warum es funktioniert
Kompromisse und Alternativen
Entwurfsmuster 3: Feature Cross
Problem
Lösung
Warum es funktioniert
Kompromisse und Alternativen
Entwurfsmuster 4: Multimodale Eingabe
Problem
Lösung
Kompromisse und Alternativen
Zusammenfassung
Entwurfsmuster für die Problemdarstellung.
Entwurfsmuster 5: Reframing
Problem
Lösung
Warum es funktioniert
Kompromisse und Alternativen
Entwurfsmuster 6: Multilabel
Problem
Lösung
Kompromisse und Alternativen
Entwurfsmuster 7: Ensembles
Problem
Lösung
Warum es funktioniert
Kompromisse und Alternativen
Entwurfsmuster 8: Kaskade
Problem
Lösung
Kompromisse und Alternativen
Entwurfsmuster 9: Neutrale Klasse
Problem
Lösung
Warum es funktioniert
Kompromisse und Alternativen
Entwurfsmuster 10: Neugewichtung
Problem
Lösung
Kompromisse und Alternativen
Zusammenfassung
Modell-Trainingsmuster
Typische Trainingsschleife
Stochastischer Gradientenabstieg
Keras-Trainingsschleife
Training Entwurfsmuster
Entwurfsmuster 11: Nützliches Overfitting
Problem
Lösung
Warum es funktioniert
Kompromisse und Alternativen
Entwurfsmuster 12: Kontrollpunkte
Problem
Lösung
Warum es funktioniert
Kompromisse und Alternativen
Entwurfsmuster 13: Lernen übertragen
Problem
Lösung
Warum es funktioniert
Kompromisse und Alternativen
Entwurfsmuster 14: Vertriebsstrategie
Problem
Lösung
Warum es funktioniert
Kompromisse und Alternativen
Entwurfsmuster 15: Abstimmung der Hyperparameter
Problem
Lösung
Warum es funktioniert
Kompromisse und Alternativen
Zusammenfassung
Entwurfsmuster für robustes Serving.
Entwurfsmuster 16: Zustandslose Serving-Funktion
Problem
Lösung
Warum es funktioniert
Kompromisse und Alternativen
Entwurfsmuster 17: Batch-Servicing
Problem
Lösung
Warum es funktioniert
Kompromisse und Alternativen
Entwurfsmuster 18: Fortgesetzte Modellauswertung
Problem
Lösung
Warum es funktioniert
Kompromisse und Alternativen
Entwurfsmuster 19: Zwei-Phasen-Vorhersagen
Problem
Lösung
Kompromisse und Alternativen
Entwurfsmuster 20: Verschlüsselte Vorhersagen
Problem
Lösung
Kompromisse und Alternativen
Zusammenfassung
Reproduzierbarkeit von Entwurfsmustern.
Entwurfsmuster 21: Transformieren
Problem
Lösung
Kompromisse und Alternativen
Entwurfsmuster 22: Wiederholbare Aufteilung
Problem
Lösung
Kompromisse und Alternativen
Entwurfsmuster 23: Überbrücktes Schema
Problem
Lösung
Kompromisse und Alternativen
Entwurfsmuster 24: Gefensterte Inferenz
Problem
Lösung
Kompromisse und Alternativen
Entwurfsmuster 25: Workflow-Pipeline
Problem
Lösung
Warum es funktioniert
Kompromisse und Alternativen
Entwurfsmuster 26: Funktionsspeicher
Problem
Lösung
Warum es funktioniert
Kompromisse und Alternativen
Entwurfsmuster 27: Modellversionierung
Problem
Lösung
Kompromisse und Alternativen
Zusammenfassung
Verantwortliche KI.
Entwurfsmuster 28: Heuristischer Benchmark
Problem
Lösung
Kompromisse und Alternativen
Entwurfsmuster 29: Erklärbare Vorhersagen
Problem
Lösung
Kompromisse und Alternativen
Entwurfsmuster 30: Fairness-Objektiv
Problem
Lösung
Kompromisse und Alternativen
Zusammenfassung
Verbundene Muster.
Patterns Referenz
Interaktionen zwischen Mustern
Muster innerhalb von ML-Projekten
ML Lebenszyklus
KI-Bereitschaft
Gemeinsame Muster nach Anwendungsfall und Datentyp
Verstehen natürlicher Sprache
Computer Vision
Prädiktive Analytik
Empfehlungssysteme
Erkennung von Betrug und Anomalien
Index.
Vorwort
Für wen ist dieses Buch gedacht?
Einführende Bücher zum maschinellen Lernen konzentrieren sich normalerweise auf das Was und Wie des maschinellen Lernens
Lernens (ML). Sie erklären dann die mathematischen Aspekte neuer Methoden aus der KI
Forschungslabors und lehren, wie man KI-Frameworks verwendet, um diese Methoden zu implementieren. Dieses
Dieses Buch hingegen bündelt hart erarbeitete Erfahrungen rund um das "Warum", das
das den Tipps und Tricks zugrunde liegt, die erfahrene ML-Praktiker anwenden, wenn sie
maschinelles Lernen auf reale Probleme anwenden.

Wir gehen davon aus, dass Sie Vorkenntnisse über maschinelles Lernen und Datenverarbeitung haben.
Dies ist kein grundlegendes Lehrbuch über maschinelles Lernen. Stattdessen ist dieses Buch für Sie gedacht
wenn Sie Datenwissenschaftler, Dateningenieur oder ML-Ingenieur sind und ein zweites Buch über
Buch über praktisches maschinelles Lernen suchen. Wenn Sie die Grundlagen bereits kennen, wird dieses Buch
dieses Buch einen Katalog von Ideen vor, von denen Sie (als ML-Praktiker) einige vielleicht wiedererkennen.
kennen, und gibt diesen Ideen einen Namen, damit Sie sie selbstbewusst aufgreifen können.

Wenn Sie als Informatikstudent eine Stelle in der Industrie anstreben, wird dieses Buch
Ihr Wissen abrunden und Sie auf die Berufswelt vorbereiten. Es wird Ihnen helfen
lernen Sie, wie man hochwertige ML-Systeme baut.

Was nicht in diesem Buch steht
Dieses Buch ist in erster Linie für ML-Ingenieure in Unternehmen gedacht, nicht für ML-Wissenschaftler
in akademischen oder industriellen Forschungslaboratorien.

Wir erörtern absichtlich keine Bereiche aktiver Forschung - Sie werden hier nur sehr wenig finden,
über die Architektur von Modellen des maschinellen Lernens (z. B. bidirektionale Encoder oder
Aufmerksamkeitsmechanismus oder Kurzschlussschichten), weil wir davon ausgehen, dass
Sie eine vorgefertigte Modellarchitektur (wie ResNet-50 oder GRUCell) verwenden werden und nicht
Ihr eigenes Bildklassifizierungs- oder rekurrentes neuronales Netzwerk zu schreiben.

Vorwort | ix
Hier sind einige konkrete Beispiele für Bereiche, die wir absichtlich aussparen
weil wir glauben, dass diese Themen eher für Hochschulkurse und ML-Forscher geeignet sind
Forscher geeignet sind:

ML-Algorithmen
Wir behandeln nicht die Unterschiede zwischen Random Forests und neuronalen Netzen,
zum Beispiel. Dies wird in einführenden Lehrbüchern zum maschinellen Lernen behandelt.

Bausteine
Wir behandeln nicht die verschiedenen Arten von Gradientenabstiegsoptimierern oder Aktivierungs
Funktionen. Wir empfehlen die Verwendung von Adam und ReLU - unserer Erfahrung nach ist das
Erfahrung ist das Potenzial für Leistungsverbesserungen durch unterschiedliche
dieser Art von Dingen eher gering ist.

ML-Modellarchitekturen
Wenn Sie eine Bildklassifikation durchführen, empfehlen wir Ihnen, ein Standardmodell
Modell wie ResNet oder ein anderes Modell, das zu dem Zeitpunkt, zu dem Sie dies lesen, der letzte Schrei ist
dies lesen. Überlassen Sie die Entwicklung neuer Modelle zur Bild- oder Textklassifizierung
Forschern, die sich auf dieses Problem spezialisiert haben.

Modellschichten
Sie werden in diesem Buch keine Faltungsnetze oder rekurrente neuronale Netze finden.
diesem Buch. Sie sind doppelt disqualifiziert - erstens, weil sie ein Baustein sind und zweitens
zweitens, weil sie etwas sind, das Sie von der Stange verwenden können.

Benutzerdefinierte Trainingsschleifen
Ein einfacher Aufruf von model.fit() in Keras reicht für die Bedürfnisse von Praktikern aus.

In diesem Buch haben wir versucht, nur gängige Muster aufzuführen, wie sie
Ingenieure für maschinelles Lernen in Unternehmen bei ihrer täglichen Arbeit verwenden.

Nehmen wir als Analogie Datenstrukturen. Während ein Hochschulkurs über Datenstrukturen
sich mit den Implementierungen verschiedener Datenstrukturen befasst und ein Forscher, der sich mit Datenstrukturen
und ein Forscher, der sich mit Datenstrukturen beschäftigt, muss lernen, wie man ihre mathematischen Eigenschaften formal darstellt,
kann der Praktiker pragmatischer vorgehen. Ein Entwickler von Unternehmenssoftware muss einfach
muss einfach wissen, wie man effektiv mit Arrays, verknüpften Listen, Maps, Mengen und Bäumen arbeitet.
Dieses Buch ist für einen pragmatischen Praktiker des maschinellen Lernens geschrieben.

Code-Beispiele
Wir stellen Code für maschinelles Lernen (manchmal in Keras/TensorFlow, und andere
in scikit-learn oder BigQuery ML) und Datenverarbeitung (in SQL) zur Verfügung, um zu zeigen
um zu zeigen, wie die Techniken, die wir besprechen, in der Praxis umgesetzt werden. Der gesamte Code, auf den
Code, auf den im Buch verwiesen wird, ist Teil unseres GitHub-Repositorys, wo Sie vollständig
funktionierende ML-Modelle finden. Wir empfehlen Ihnen dringend, diese Codebeispiele auszuprobieren.

x | Vorwort

Der Code ist zweitrangig gegenüber den Konzepten und Techniken, die behandelt werden.
Unser Ziel war es, dass das Thema und die Prinzipien relevant bleiben, unabhängig von
Änderungen an TensorFlow oder Keras relevant bleiben, und wir können uns leicht vorstellen, das GitHub
Repository zu aktualisieren, um z.B. andere ML-Frameworks einzubeziehen, während der Text des Buches
unverändert bleibt. Daher sollte das Buch genauso informativ sein, wenn Ihr primäres ML
Framework PyTorch ist oder sogar ein Nicht-Python-Framework wie H20.ai oder R. In der Tat begrüßen wir
begrüßen Ihre Beiträge zum GitHub-Repository mit Implementierungen von einem oder
von einem oder mehreren dieser Muster in Ihrem bevorzugten ML-Framework.

Wenn Sie eine technische Frage oder ein Problem mit den Codebeispielen haben, senden Sie bitte eine
E-Mail an bookquestions@oreilly.com.

Dieses Buch soll Ihnen helfen, Ihre Arbeit zu erledigen. Wenn in diesem Buch Beispielcode angeboten wird
angeboten wird, können Sie diesen in Ihren Programmen und Ihrer Dokumentation verwenden. Sie müssen nicht
Sie brauchen uns nicht um Erlaubnis zu fragen, es sei denn, Sie reproduzieren einen großen Teil des
des Codes. Wenn Sie zum Beispiel ein Programm schreiben, das mehrere Teile des Codes aus diesem Buch verwendet
Buch verwendet, ist keine Erlaubnis erforderlich. Der Verkauf oder die Weitergabe von Beispielen aus O'Reilly
Büchern ist eine Genehmigung erforderlich. Die Beantwortung einer Frage durch Zitieren dieses Buches und von
Codebeispielen ist keine Genehmigung erforderlich. Das Einbinden einer signifikanten Menge von
Beispielcodes aus diesem Buch in die Dokumentation Ihres Produkts erfordert eine
Erlaubnis.

Wir freuen uns über eine Namensnennung, verlangen sie aber im Allgemeinen nicht. Eine Namensnennung umfasst normalerweise
den Titel, den Autor, den Verlag und die ISBN. Zum Beispiel: "Maschinelles Lernen
Design Patterns von Valliappa Lakshmanan, Sara Robinson und Michael Munn
(O'Reilly). Urheberrecht 2021 Valliappa Lakshmanan, Sara Robinson und Michael
Munn, 978-1-098-11578-4." Wenn Sie der Meinung sind, dass Ihre Verwendung von Code-Beispielen nicht in den Rahmen
Verwendung oder der oben genannten Erlaubnis fällt, können Sie uns gerne unter permissions@oreilly.com kontaktieren.

In diesem Buch verwendete Konventionen
Die folgenden typografischen Konventionen werden in diesem Buch verwendet:

Kursiv
Kennzeichnet neue Begriffe, URLs, E-Mail-Adressen, Dateinamen und Dateierweiterungen.

Konstante Breite
Wird für Programmlistings sowie innerhalb von Absätzen verwendet, um auf Programmelemente zu verweisen.
Programmteile wie Variablen- oder Funktionsnamen, Datenbanken, Datentypen, Umgebungsvariablen
Variablen, Anweisungen und Schlüsselwörter.

Konstante Breite fett
Zeigt Befehle oder anderen Text an, der vom Benutzer wörtlich eingegeben werden sollte.

Vorwort | xi
Konstante Breite kursiv
Zeigt Text an, der durch vom Benutzer eingegebene oder durch den Kontext bestimmte Werte ersetzt werden soll.
durch den Kontext bestimmt werden.

Dieses Element steht für einen Tipp oder eine Anregung.
Dieses Element kennzeichnet einen allgemeinen Hinweis.
Dieses Element kennzeichnet eine Warnung oder Vorsicht.
O'Reilly Online-Lernen
Seit mehr als 40 Jahren bietet O'Reilly Media technische und
Technologie- und Geschäftsschulungen, Wissen und Einblicke, die den
Unternehmen zum Erfolg zu verhelfen.
Unser einzigartiges Netzwerk von Experten und Innovatoren teilt sein Wissen und seine Erfahrung
durch Bücher, Artikel und unsere Online-Lernplattform. Die Online-Lernplattform von O'Reilly
Plattform von O'Reilly bietet Ihnen On-Demand-Zugang zu Live-Trainingskursen, vertieften Lernpfaden
Lernpfaden, interaktiven Programmierumgebungen und einer umfangreichen Sammlung von Texten und Videos von
O'Reilly und 200+ anderen Verlagen. Weitere Informationen finden Sie unter http://oreilly.com.

xii | Vorwort

Wie Sie uns kontaktieren können
Bitte richten Sie Kommentare und Fragen zu diesem Buch an den Herausgeber:

O'Reilly Media, Inc.
1005 Gravenstein Highway Nord
Sebastopol, CA 95472
800-998-9938 (in den Vereinigten Staaten oder Kanada)
707-829-0515 (international oder lokal)
707-829-0104 (Fax)
Wir haben eine Webseite für dieses Buch, auf der wir Errata, Beispiele und zusätzliche Informationen auflisten.
Informationen. Sie können diese Seite unter https://oreil.ly/MLDP aufrufen.

Senden Sie eine E-Mail an bookquestions@oreilly.com, um Kommentare oder technische Fragen zu diesem Buch zu stellen.
Buch zu stellen.

Neuigkeiten und Informationen über unsere Bücher und Kurse finden Sie unter http://oreilly.com.

Finden Sie uns auf Facebook: http://facebook.com/oreilly

Folgen Sie uns auf Twitter: http://twitter.com/oreillymedia

Sehen Sie uns auf YouTube: http://youtube.com/oreillymedia

Danksagungen
Ein Buch wie dieses wäre ohne die Großzügigkeit zahlreicher Googler nicht möglich,
insbesondere unsere Kollegen in den Teams für Cloud AI, Solution Engineering, Professional Servi-
Services und Developer Relations Teams. Wir sind ihnen dankbar, dass wir ihre Lösungen beobachten,
und ihre Lösungen für die herausfordernden Probleme, auf die sie beim Training, der
beim Training, der Verbesserung und der Operationalisierung von ML-Modellen. Dank an unsere Manager,
Karl Weinmeister, Steve Cellini, Hamidou Dia, Abdul Razack, Chris Hallenbeck, Pat-
rick Cole, Louise Byrne und Rochana Golani für die Förderung des Geistes der Offenheit
bei Google zu fördern und uns die Freiheit zu geben, diese Muster zu katalogisieren und dieses Buch zu veröffentlichen.

Salem Haykal, Benoit Dherin und Khalid Salama haben jedes Muster und jedes
Kapitel. Sal wies uns auf Nuancen hin, die wir übersehen hatten, Benoit grenzte unsere Forderungen ein,
und Khalid wies uns auf relevante Forschungsergebnisse hin. Dieses Buch wäre nicht annähernd so gut
gut ohne Ihre Beiträge. Wir danken Ihnen! Amy Unruh, Rajesh Thallam, Robbie Haertel,
Zhitao Li, Anusha Ramesh, Ming Fang, Parker Barnes, Andrew Zaldivar, James Wex-
ler, Andrew Sellergren und David Kanter überprüften Teile dieses Buches, die mit
und machten zahlreiche Vorschläge dazu, wie sich der kurzfristige Fahrplan auf unsere Empfehlungen
Fahrplan unsere Empfehlungen beeinflussen würde. Nitin Aggarwal und Matthew Yeager
haben das Manuskript mit dem Auge des Lesers geprüft und seine Klarheit verbessert. Besonderer Dank geht an

Vorwort | xiii
Rajesh Thallam für die Erstellung des Prototyps für die allerletzte Abbildung in Kapitel 8. Alle
Fehler, die bleiben, sind natürlich unsere.

O'Reilly ist der Verlag der Wahl für technische Bücher, und die Professionalität unseres
Team macht deutlich, warum. Rebecca Novak hat uns bei der Erstellung einer überzeugenden Gliederung geholfen.
überzeugende Gliederung, Kristen Brown leitete die gesamte Inhaltsentwicklung mit
Corbin Collins gab uns in jeder Phase hilfreiche Ratschläge, Elizabeth Kelly war eine
Elizabeth Kelly war eine wunderbare Kollegin während der Produktion, und Charles Roumeliotis hatte ein scharfes
Auge für das Lektorat. Vielen Dank für all Ihre Hilfe!

Michael: Ich danke meinen Eltern, dass sie immer an mich geglaubt und meine Interessen gefördert haben.
Interessen gefördert haben, sowohl akademisch als auch anderweitig. Sie werden die heimliche Tarnung genauso zu schätzen wissen wie ich.
heimliche Tarnung. Phil, ich danke dir dafür, dass du geduldig mit meinem nicht ganz so
erträglichen Zeitplan während der Arbeit an diesem Buch. Und jetzt werde ich schlafen gehen.

Sara: Jon, du bist ein wichtiger Grund dafür, dass es dieses Buch gibt. Danke, dass du mich dazu ermutigt hast
ermutigt hast, es zu schreiben, dass du mich immer zum Lachen bringst und meine Verrücktheit zu schätzen weißt,
und dass du an mich geglaubt hast, vor allem, als ich es nicht tat. Meinen Eltern, danke dafür, dass sie
meine größten Fans seit dem ersten Tag sind und meine Liebe zur Technik und zum Schreiben gefördert haben
so lange ich mich erinnern kann. An Ally, Katie, Randi und Sophie - danke, dass ihr eine
eine ständige Quelle des Lichts und des Lachens in diesen unsicheren Zeiten.

Lak: Ich habe dieses Buch in Angriff genommen, weil ich dachte, ich könnte daran arbeiten, während ich auf Flughäfen warte.
Dank COVID-19 konnte ich einen Großteil der Arbeit zu Hause erledigen. Danke Abirami,
Sidharth und Sarada für all eure Nachsicht, als ich mich wieder einmal zum Schreiben hinkauerte.
Mehr Wanderungen an den Wochenenden jetzt!

Wir drei spenden 100 % der Tantiemen aus diesem Buch an Girls Who Code,
eine Organisation, die es sich zur Aufgabe gemacht hat, eine große Pipeline von zukünftigen Ingenieurinnen aufzubauen.
Vielfalt, Gleichberechtigung und Inklusion sind beim maschinellen Lernen besonders wichtig, um
um sicherzustellen, dass KI-Modelle die bestehenden Vorurteile in der menschlichen Gesellschaft nicht aufrechterhalten.

xiv | Vorwort

KAPITEL 1

Der Bedarf an maschinellem Lernen
Entwurfsmuster
In den technischen Disziplinen erfassen Entwurfsmuster bewährte Verfahren und Lösungen für
häufig auftretende Probleme. Sie kodifizieren das Wissen und die Erfahrung von Experten
in Form von Ratschlägen, die alle Praktiker befolgen können. Dieses Buch ist ein Katalog von Entwurfsmustern für maschinelles Lernen.
Design Patterns für maschinelles Lernen, die wir im Laufe unserer Arbeit mit Hunderten von
Teams für maschinelles Lernen beobachtet haben.

Was sind Design Patterns?
Die Idee der Patterns und eines Katalogs bewährter Patterns wurde im Bereich der Architektur
Architektur von Christopher Alexander und fünf Co-Autoren in einem sehr einflussreichen
Buch mit dem Titel A Pattern Language (Oxford University Press, 1977) eingeführt. In ihrem Buch haben sie
253 Muster auf, die sie folgendermaßen einführen:

Jedes Muster beschreibt ein Problem, das in unserem Umfeld immer wieder auftritt.
und beschreibt dann den Kern der Lösung für dieses Problem, und zwar so, dass
dass man diese Lösung millionenfach anwenden kann, ohne sie jemals auf die gleiche Weise
zweimal.
...
Jede Lösung ist so formuliert, dass sie das wesentliche Beziehungsgeflecht enthält, das zur
um das Problem zu lösen, aber in einer sehr allgemeinen und abstrakten Weise, so dass Sie
das Problem für sich selbst lösen kann, auf seine eigene Art und Weise, indem er es an seine Vorlieben anpasst,
und an die örtlichen Gegebenheiten des Ortes, an dem Sie es lösen.
Einige der Muster, die menschliche Details beim Bau eines Hauses einbeziehen, sind zum Beispiel
Haus einbeziehen, sind zum Beispiel "Licht auf zwei Seiten jedes Zimmers" und "Sechs-Fuß-Balkon". Denken Sie an Ihr
Lieblingszimmer in Ihrem Haus und Ihr unbeliebtestes Zimmer. Ist Ihr Lieblingszimmer

1
Fenster an zwei Wänden haben? Wie sieht es mit Ihrem unbeliebtesten Zimmer aus? Laut
Alexander:

Räume, die von zwei Seiten mit natürlichem Licht beleuchtet werden, erzeugen weniger Blendung um Personen und Gegenstände herum;
Dies ermöglicht es uns, die Dinge genauer zu sehen, und - was am wichtigsten ist - wir können die
und vor allem können wir die winzigen Ausdrücke, die in den Gesichtern der Menschen aufblitzen, genau lesen....
Eine Bezeichnung für dieses Muster zu haben, erspart den Architekten die ständige Wiederentdeckung
neu entdecken zu müssen. Doch wo und wie man zwei Lichtquellen in einer bestimmten örtlichen Konstellation
der örtlichen Gegebenheiten zu erreichen, bleibt dem Geschick des Architekten überlassen. Ähnlich verhält es sich bei der Gestaltung eines Balkons: Wie groß
sollte er sein? Alexander empfiehlt eine Größe von 6 Fuß mal 6 Fuß, die für 2 (falsch
Stühle und einen Beistelltisch, und 12 Fuß mal 12 Fuß, wenn Sie sowohl einen überdachten
überdachten Sitzplatz und einen Sitzplatz in der Sonne wünscht.

Erich Gamma, Richard Helm, Ralph Johnson und John Vlissides brachten die Idee in die
Software, indem sie 23 objektorientierte Entwurfsmuster in einem 1994 erschienenen Buch mit dem Titel
Design Patterns: Elements of Reusable Object-Oriented Software (Addison-Wesley,
1995). Ihr Katalog enthält Muster wie Proxy, Singleton und Decorator und
haben das Feld der objektorientierten Programmierung nachhaltig beeinflusst. Im Jahr 2005 hat die Asso-
ciation of Computing Machinery (ACM) ihren jährlichen Programming Lan-
Programmiersprachen Achievement Award an die Autoren und würdigte damit den Einfluss ihrer Arbeit "auf die
Programmierpraxis und das Design von Programmiersprachen".

Die Erstellung von Produktionsmodellen für maschinelles Lernen wird zunehmend zu einer
Disziplin, die die Vorteile von ML-Methoden nutzt, die sich in der Forschung bewährt haben
erprobt wurden, auf Unternehmensprobleme anzuwenden. Da maschinelles Lernen immer mehr
Mainstream wird, ist es wichtig, dass Praktiker bewährte Methoden nutzen
Methoden nutzen, um wiederkehrende Probleme zu lösen.

Ein Vorteil unserer Arbeit im kundenorientierten Teil von Google Cloud ist, dass wir
dass wir mit einer Vielzahl von Teams für maschinelles Lernen und Data Science sowie
und einzelnen Entwicklern aus der ganzen Welt. Gleichzeitig arbeiten wir alle eng mit internen
mit internen Google-Teams zusammen, die innovative Probleme des maschinellen Lernens lösen. Zu guter Letzt,
haben wir das Glück, mit den Teams von TensorFlow, Keras, BigQuery ML, TPU und
Cloud AI Platform-Teams zusammenzuarbeiten, die die Demokratisierung der Forschung und Infrastruktur für maschinelles Lernen
Forschung und Infrastruktur vorantreiben. All dies gibt uns eine ziemlich einzigartige Perspektive, aus der wir
die besten Praktiken zu katalogisieren, die wir bei diesen Teams beobachtet haben.

Dieses Buch ist ein Katalog von Entwurfsmustern oder wiederholbaren Lösungen für häufig auftretende
Probleme in der ML-Entwicklung. Das Transform-Muster (Kapitel 6) zum Beispiel
die Trennung von Eingaben, Merkmalen und Transformationen durch und macht die Transfor-
Transformationen persistent, um die Übertragung eines ML-Modells in die Produktion zu vereinfachen. Ähnlich
Ähnlich ist Keyed Predictions (Kapitel 5) ein Muster, das die groß angelegte
Verteilung von Batch-Vorhersagen, z. B. für Empfehlungsmodelle.

2 | Kapitel 1: Die Notwendigkeit von Entwurfsmustern für maschinelles Lernen

Für jedes Muster beschreiben wir das häufig auftretende Problem, das angesprochen wird
und gehen dann eine Reihe von möglichen Lösungen für das Problem durch, die
Kompromisse zwischen diesen Lösungen und Empfehlungen für die Wahl zwischen diesen Lösungen.
tionen. Der Implementierungscode für diese Lösungen wird in SQL bereitgestellt (nützlich, wenn Sie
Vorverarbeitung und andere ETL in Spark SQL, BigQuery usw. durchführen),
scikit-learn, und/oder Keras mit einem TensorFlow-Backend.

Wie man dieses Buch benutzt
Dies ist ein Katalog von Mustern, die wir in der Praxis bei mehreren Teams beobachtet haben.
In einigen Fällen sind die zugrundeliegenden Konzepte schon seit vielen Jahren bekannt. Wir behaupten nicht
behaupten nicht, diese Muster erfunden oder entdeckt zu haben. Stattdessen hoffen wir, einen
einen gemeinsamen Bezugsrahmen und eine Reihe von Werkzeugen für ML-Praktiker bereitzustellen. Das ist uns gelungen.
wenn dieses Buch Ihnen und Ihrem Team ein Vokabular an die Hand gibt, mit dem Sie über Kon- zepte
Muster, die Sie bereits intuitiv in Ihre ML-Projekte einfließen lassen.

Wir erwarten nicht, dass Sie dieses Buch der Reihe nach lesen (obwohl Sie das können!). Stattdessen erwarten wir
erwarten, dass Sie das Buch überfliegen, einige Abschnitte intensiver als andere lesen
lesen, sich in Gesprächen mit Kollegen auf die Ideen beziehen und auf das Buch zurückgreifen
Buch zurückgreifen werden, wenn Sie sich an Probleme erinnern, über die Sie gelesen haben. Wenn Sie vorhaben, das Buch zu überspringen
empfehlen wir Ihnen, mit Kapitel 1 und Kapitel 8 zu beginnen, bevor Sie in einzelne Muster eintauchen.
in einzelne Muster eintauchen.

Jedes Muster enthält eine kurze Problemstellung, eine kanonische Lösung, eine Erklärung, warum
warum die Lösung funktioniert, und eine mehrteilige Diskussion über Kompromisse und Alternativen.
Wir empfehlen, dass Sie den Diskussionsteil mit der kanonischen Lösung
im Hinterkopf zu haben, um sie zu vergleichen und gegenüberzustellen. Die Beschreibung des Musters enthält
Codeschnipsel aus der Implementierung der kanonischen Lösung. Der vollständige Code
kann in unserem GitHub-Repository gefunden werden. Wir empfehlen Ihnen dringend, sich den Code anzusehen
Code zu lesen, während Sie die Musterbeschreibung lesen.

Terminologie des maschinellen Lernens
Da Fachleute für maschinelles Lernen heutzutage unterschiedliche
Software-Engineering, Datenanalyse, DevOps oder Statistik haben, kann es
kann es subtile Unterschiede in der Verwendung bestimmter Begriffe durch verschiedene Fachleute geben. In diesem Abschnitt
definieren wir die Begriffe, die wir im gesamten Buch verwenden.

Wie man dieses Buch benutzt | 3
Modelle und Frameworks
Im Kern geht es beim maschinellen Lernen um den Aufbau von Modellen, die aus Daten lernen.
Dies steht im Gegensatz zur traditionellen Programmierung, bei der wir explizite Regeln schreiben, die den
wie sich Programme verhalten sollen. Modelle für maschinelles Lernen sind Algorithmen, die Muster
aus Daten. Um dies zu veranschaulichen, stellen Sie sich vor, Sie sind ein Umzugsunternehmen und müssen
die Umzugskosten für potenzielle Kunden schätzen. In der traditionellen Programmierung könnten wir
dieses Problem mit einer if-Anweisung lösen:

wenn num_bedrooms == 2 und num_bathrooms == 2:
Schätzung = 1500
elif num_bedrooms == 3 und sq_ft > 2000:
Schätzung = 2500
Sie können sich vorstellen, dass dies schnell kompliziert wird, wenn wir weitere Variablen hinzufügen
(Anzahl der großen Möbelstücke, Menge der Kleidung, zerbrechliche Gegenstände usw.) und
versuchen, Randfälle zu behandeln. Außerdem kann es dazu führen, dass die Kunden, wenn sie all diese Informationen im Voraus
von den Kunden zu verlangen, kann dazu führen, dass sie den Schätzungsprozess abbrechen. Stattdessen können wir
können wir ein maschinelles Lernmodell trainieren, um die Umzugskosten auf der Grundlage von Daten aus
früheren Haushalten, die unser Unternehmen umgezogen hat.

In diesem Buch verwenden wir in erster Linie Feed-Forward-Modelle für neuronale Netze in unseren
Beispiele, aber wir werden auch lineare Regressionsmodelle, Entscheidungsbäume, Clustermodelle
Modelle und andere. Feed-forward neuronale Netze, die wir im Allgemeinen als neuronale Netze abkürzen werden
als neuronale Netze bezeichnen, sind eine Art von Algorithmus für maschinelles Lernen, bei dem mehrere Schichten
jede mit vielen Neuronen, Informationen analysieren und verarbeiten und diese Informationen dann an die nächste Schicht weiterleiten.
Informationen an die nächste Schicht weiterleiten, was zu einer letzten Schicht führt, die eine Vorhersage als Ergebnis liefert.
ausgibt. Obwohl sie in keiner Weise identisch sind, werden neuronale Netze oft mit den
Neuronen in unserem Gehirn verglichen, und zwar wegen der Konnektivität zwischen den Knoten und der Art und Weise, wie sie
und der Art und Weise, wie sie in der Lage sind, zu verallgemeinern und neue Vorhersagen aus den verarbeiteten Daten zu treffen. Neuronale Netze
Neuronale Netze mit mehr als einer versteckten Schicht (andere Schichten als die Eingabe- und Ausgabeschicht)
werden als Deep Learning klassifiziert (siehe Abbildung 1-1).

Modelle des maschinellen Lernens sind, unabhängig von ihrer visuellen Darstellung, mathematische Funktionen.
mathematische Funktionen und können daher von Grund auf mit einem numerischen
Softwarepaket implementiert werden. ML-Ingenieure in der Industrie neigen jedoch dazu, eines von mehreren
Open-Source-Frameworks, die intuitive APIs für die Erstellung von Modellen bieten. Die
meisten unserer Beispiele werden TensorFlow verwenden, ein Open-Source-Framework für maschinelles Lernen
Framework, das von Google mit Schwerpunkt auf Deep-Learning-Modellen entwickelt wurde. Innerhalb der
TensorFlow-Bibliothek werden wir in unseren Beispielen die Keras-API verwenden, die
über tensorflow.keras importiert werden kann. Keras ist eine übergeordnete API zum Aufbau
von neuronalen Netzen. Während Keras viele Backends unterstützt, werden wir sein
TensorFlow-Backend. In anderen Beispielen verwenden wir scikit-learn, XGBoost und
PyTorch, andere beliebte Open-Source-Frameworks, die Dienstprogramme für die
Aufbereitung Ihrer Daten sowie APIs für die Erstellung linearer und tiefer Modelle bieten. Maschine

4 | Kapitel 1: Die Notwendigkeit von Entwurfsmustern für maschinelles Lernen

Das Lernen wird immer zugänglicher, und eine interessante Entwicklung ist die Verfügbarkeit von
Verfügbarkeit von Modellen für maschinelles Lernen, die in SQL ausgedrückt werden können. Wir werden Big-
Query ML als Beispiel dafür verwenden, insbesondere in Situationen, in denen wir die
Datenvorverarbeitung und Modellerstellung kombinieren wollen.

Abbildung 1-1. Eine Aufschlüsselung der verschiedenen Arten des maschinellen Lernens mit einigen Beispielen für
jeder Art. Beachten Sie, dass neuronale Netze wie Autocoder auch für unüberwachtes Lernen verwendet werden können, obwohl sie in diesem Diagramm nicht enthalten sind.
Autocoder auch für unüberwachtes Lernen verwendet werden können.

Umgekehrt sind neuronale Netze mit nur einer Eingabe- und Ausgabeschicht eine weitere Untergruppe
des maschinellen Lernens, die als lineare Modelle bekannt sind. Lineare Modelle stellen die Muster
die sie aus Daten gelernt haben, mithilfe einer linearen Funktion dar. Entscheidungsbäume sind Modelle des maschinellen Lernens
Modelle, die Ihre Daten verwenden, um eine Untergruppe von Pfaden mit verschiedenen Verzweigungen zu erstellen. Diese
Verzweigungen nähern sich den Ergebnissen verschiedener Ergebnisse aus Ihren Daten an. Schließlich suchen Clus-
Schließlich suchen Clustering-Modelle nach Ähnlichkeiten zwischen verschiedenen Teilmengen Ihrer Daten und verwenden
diese identifizierten Muster, um Daten in Clustern zu gruppieren.

Probleme des maschinellen Lernens (siehe Abbildung 1-1) lassen sich in zwei Arten unterteilen: überwachtes
und unüberwachtes Lernen. Überwachtes Lernen bezeichnet Probleme, bei denen Sie die
Grundwahrheit für Ihre Daten im Voraus kennen. Dies könnte zum Beispiel die Kennzeichnung
ein Bild als "Katze" oder ein Baby, das bei der Geburt 2,3 kg wiegt. Sie füttern Ihr Modell mit diesen beschrifteten Daten
an Ihr Modell in der Hoffnung, dass es genug lernt, um neue Beispiele zu kennzeichnen. Mit

Terminologie des maschinellen Lernens | 5
Beim unüberwachten Lernen kennen Sie die Beschriftungen für Ihre Daten nicht im Voraus, und das
Ziel ist es, ein Modell zu erstellen, das natürliche Gruppierungen der Daten findet (Cluster genannt)
(sog. Clustering), den Informationsgehalt komprimieren (Dimensionalitätsreduktion) oder Assoziationsregeln finden
tionsregeln finden kann. Der Großteil dieses Buches konzentriert sich auf das überwachte Lernen, da die
die überwiegende Mehrheit der in der Produktion verwendeten maschinellen Lernmodelle überwacht werden.

Beim überwachten Lernen lassen sich die Probleme in der Regel entweder als Klassifizierung oder
Regression. Klassifizierungsmodelle weisen Ihren Eingabedaten ein Etikett (oder mehrere Etiketten) aus einer bestimmten
einer bestimmten, vordefinierten Menge von Kategorien zu. Beispiele für Klassifizierungsprobleme sind die Bestimmung
Rasse eines Haustiers in einem Bild, die Kennzeichnung eines Dokuments oder die Vorhersage, ob eine
ob eine Transaktion betrügerisch ist oder nicht. Regressionsmodelle ordnen den Eingaben kontinuierliche, numerische
Werte Ihren Eingaben zu. Beispiele für Regressionsmodelle sind die Vorhersage der Dauer
einer Fahrradtour, der zukünftige Umsatz eines Unternehmens oder der Preis eines Produkts.

Daten- und Merkmalstechnik
Daten sind das Herzstück eines jeden Problems des maschinellen Lernens. Wenn wir über Datensätze sprechen,
beziehen wir uns auf die Daten, die zum Trainieren, Validieren und Testen eines maschinellen Lernmodells
Modells verwendet werden. Der Großteil Ihrer Daten werden Trainingsdaten sein: die Daten, die Ihrem Modell während des
während des Trainingsprozesses. Validierungsdaten sind Daten, die aus dem Trainingsdatensatz herausgehalten
Daten, die aus dem Trainingsdatensatz herausgehalten und verwendet werden, um die Leistung des Modells nach jeder Trainingsepoche
(oder Durchlauf durch die Trainingsdaten). Die Leistung des Modells auf den Validierungsdaten
Die Leistung des Modells auf den Validierungsdaten wird verwendet, um zu entscheiden, wann der Trainingslauf gestoppt werden soll, und um Hyperparameter zu wählen,
wie zum Beispiel die Anzahl der Bäume in einem Random-Forest-Modell. Testdaten sind Daten, die nicht
Testdaten sind Daten, die im Trainingsprozess nicht verwendet werden und die dazu dienen, die Leistung des trainierten Modells zu bewerten.
bildet. Die Leistungsberichte des maschinellen Lernmodells müssen auf den
unabhängigen Testdaten und nicht auf den Trainings- oder Validierungstests berechnet werden. Außerdem ist es wichtig
dass die Daten so aufgeteilt werden, dass alle drei Datensätze (Training, Test, Validierung)
ähnliche statistische Eigenschaften haben.

Die Daten, die Sie zum Trainieren Ihres Modells verwenden, können je nach Modelltyp verschiedene Formen annehmen
Typ. Wir definieren strukturierte Daten als numerische und kategoriale Daten. Numerische Daten
Numerische Daten umfassen Ganzzahl- und Fließkommazahlen, kategorische Daten umfassen Daten, die in
in eine endliche Menge von Gruppen unterteilt werden können, wie z. B. Autotyp oder Bildungsgrad. Sie können auch
Sie können sich strukturierte Daten auch als Daten vorstellen, die Sie üblicherweise in einer Tabellenkalkulation finden. Im gesamten
In diesem Buch wird der Begriff "tabellarische Daten" synonym mit "strukturierte Daten" verwendet.
Unstrukturierte Daten hingegen umfassen Daten, die sich nicht so sauber darstellen lassen.
ordentlich dargestellt werden können. Dazu gehören in der Regel Freiformtexte, Bilder, Video- und Audiodaten.

Numerische Daten können oft direkt in ein maschinelles Lernmodell eingespeist werden, während andere Daten
verschiedene Datenvorverarbeitungen erfordern, bevor sie an ein Modell gesendet werden können. Diese Vor
schritt umfasst in der Regel die Skalierung numerischer Werte oder die Konvertierung nichtnumerischer
Daten in ein numerisches Format, das von Ihrem Modell verstanden werden kann. Eine weitere

6 | Kapitel 1: Die Notwendigkeit von Entwurfsmustern für maschinelles Lernen

Begriff für die Vorverarbeitung ist Feature Engineering. Wir werden diese beiden Begriffe in diesem Buch
im Laufe des Buches austauschbar.

Es gibt verschiedene Begriffe, die verwendet werden, um Daten zu beschreiben, während sie den Prozess des Feature Engineerings durchlaufen.
Prozess durchlaufen. Input beschreibt eine einzelne Spalte in Ihrem Datensatz, bevor er verarbeitet wurde.
und Feature beschreibt eine einzelne Spalte, nachdem sie verarbeitet worden ist. Für
Beispiel: Ein Zeitstempel könnte Ihre Eingabe sein, und das Merkmal wäre der Wochentag.
Um die Daten vom Zeitstempel in den Wochentag umzuwandeln, müssen Sie einige Daten
vorverarbeiten. Dieser Vorverarbeitungsschritt kann auch als Datentransformation bezeichnet werden.

Eine Instanz ist ein Element, das Sie zur Vorhersage an Ihr Modell senden möchten. Eine Instanz
kann eine Zeile in Ihrem Testdatensatz (ohne die Beschriftungsspalte) sein, ein Bild, das Sie
klassifizieren möchten, oder ein Textdokument, das an ein Stimmungsanalysemodell gesendet werden soll. Angesichts einer Reihe von Merkmalen
turen über die Instanz berechnet das Modell einen vorhergesagten Wert. Zu diesem Zweck
Dazu wird das Modell anhand von Trainingsbeispielen trainiert, die eine Instanz mit einem
Etikett zuordnen. Ein Trainingsbeispiel bezieht sich auf eine einzelne Instanz (Zeile) von Daten aus Ihrem Datensatz
die in Ihr Modell eingespeist wird. Ausgehend von dem Anwendungsfall Zeitstempel könnte ein vollständiges Trainingsbeispiel
Beispiel enthalten: "Wochentag", "Stadt" und "Autotyp". Ein Label ist die Ausgabespalte
Spalte in Ihrem Datensatz - das Element, das Ihr Modell vorhersagt. Label kann sich sowohl auf
die Zielspalte in Ihrem Datensatz (auch Ground-Truth-Label genannt) als auch auf die Ausgabe
die Ihr Modell liefert (auch Vorhersage genannt). Ein Beispiel-Label für das oben beschriebene Trainingsbeispiel
Beispiel könnte "Reisedauer" sein - in diesem Fall ein Float-Wert, der
Minuten.

Sobald Sie Ihren Datensatz zusammengestellt und die Merkmale für Ihr Modell festgelegt haben, müssen Sie die Daten validieren,
ist die Datenvalidierung der Prozess der Berechnung von Statistiken über Ihre Daten, das Verständnis
Schema zu verstehen und den Datensatz auszuwerten, um Probleme wie Drift und Trainingsverzerrung zu
Schieflage zu identifizieren. Die Auswertung verschiedener Statistiken über Ihre Daten kann Ihnen dabei helfen, sicherzustellen, dass der Daten-
Datensatz eine ausgewogene Repräsentation der einzelnen Merkmale enthält. In Fällen, in denen es nicht möglich ist
nicht möglich ist, mehr Daten zu sammeln, hilft Ihnen das Verständnis der Datenbalance dabei, Ihr Modell so zu
dies zu berücksichtigen. Zum Verständnis Ihres Schemas gehört die Definition des Datentyps für jedes
Merkmal und die Identifizierung von Trainingsbeispielen, bei denen bestimmte Werte falsch sein können oder
fehlen. Schließlich kann die Datenvalidierung Inkonsistenzen aufdecken, die die Qualität
Qualität Ihrer Trainings- und Testdaten beeinträchtigen. Zum Beispiel könnte der Großteil Ihres Trainings
Trainingsdatensatzes Beispiele für Wochentage, während der Testdatensatz hauptsächlich Wochenendbeispiele enthält.
Beispiele enthält.

Der Prozess des maschinellen Lernens
Der erste Schritt in einem typischen Arbeitsablauf des maschinellen Lernens ist das Training, d.h. die Übermittlung von
der Weitergabe von Trainingsdaten an ein Modell, so dass es lernen kann, Muster zu erkennen. Nach dem Training,
Nach dem Training besteht der nächste Schritt darin, zu testen, wie sich das Modell auf Daten außerhalb des
Trainingsdaten. Dies wird als Modellevaluation bezeichnet. Möglicherweise führen Sie Training und
Evaluierung mehrmals durchführen, wobei Sie zusätzliche Merkmale entwickeln und die

Terminologie des maschinellen Lernens | 7
Ihre Modellarchitektur. Wenn Sie mit der Leistung Ihres Modells während der Evaluation zufrieden sind
zufrieden sind, werden Sie Ihr Modell wahrscheinlich zur Verfügung stellen wollen, damit andere darauf zugreifen und
Vorhersagen. Wir verwenden den Begriff "Serving", um eingehende Anfragen zu akzeptieren und
Vorhersagen zurücksenden, indem das Modell als Microservice bereitgestellt wird. Die Serving
Infrastruktur kann sich in der Cloud, vor Ort oder auf dem Gerät befinden.

Der Prozess, neue Daten an Ihr Modell zu senden und dessen Ergebnisse zu nutzen, wird als
Vorhersage. Dies kann sich sowohl auf die Erstellung von Vorhersagen aus lokalen Modellen beziehen, die
die noch nicht eingesetzt wurden, als auch auf den Erhalt von Vorhersagen von eingesetzten Modellen. Für
eingesetzte Modelle beziehen wir uns sowohl auf Online- als auch auf Batch-Vorhersagen. Die Online-Vorhersage wird
wird verwendet, wenn Sie Vorhersagen für einige wenige Beispiele in nahezu Echtzeit erhalten möchten. Bei
Online-Vorhersage liegt der Schwerpunkt auf einer geringen Latenzzeit. Die Batch-Vorhersage hingegen
andererseits bezieht sich die Batch-Vorhersage auf die Offline-Erstellung von Vorhersagen für einen großen Datensatz. Batch-Vorhersage
dauert länger als die Online-Vorhersage und ist nützlich für die Vorberechnung von Vorhersagen
(z. B. in Empfehlungssystemen) und für die Analyse der Vorhersagen Ihres Modells
über eine große Stichprobe neuer Daten.

Das Wort Vorhersage ist treffend, wenn es um die Vorhersage zukünftiger Werte geht, wie z. B. bei der
Vorhersage der Dauer einer Fahrradtour oder der Vorhersage, ob ein Einkaufswagen
verlässt. Bei Modellen zur Klassifizierung von Bildern und Texten ist es weniger intuitiv. Wenn ein
ML-Modell eine Textbewertung betrachtet und ausgibt, dass die Stimmung positiv ist, ist das nicht
nicht wirklich eine "Vorhersage" (es gibt kein zukünftiges Ergebnis). Daher wird für Vorhersagen auch das Wort infer-
ence verwendet, um sich auf Vorhersagen zu beziehen. Der statistische Begriff "Inferenz" wird hier verwendet.
Hier wird der statistische Begriff Inferenz verwendet, aber es geht nicht wirklich um Schlussfolgerungen.

Oftmals werden die Prozesse der Sammlung von Trainingsdaten, des Feature Engineering, des Trainings und der
Evaluierung Ihres Modells getrennt von der Produktionspipeline abgewickelt werden. Wenn
dies der Fall ist, werden Sie Ihre Lösung immer dann neu bewerten, wenn Sie entscheiden, dass Sie über genügend
zusätzliche Daten zum Trainieren einer neuen Version Ihres Modells. In anderen Situationen können Sie
kontinuierlich neue Daten ein und müssen diese Daten sofort verarbeiten
verarbeiten, bevor Sie sie zum Training oder zur Vorhersage an Ihr Modell senden. Dies wird als Stream-
ing. Für die Verarbeitung von Streaming-Daten benötigen Sie eine mehrstufige Lösung zur Durchführung von
Engineering, Training, Auswertung und Vorhersagen. Solche mehrstufigen Lösungen werden als
ML-Pipelines.

Werkzeuge für Daten und Modelle
Es gibt verschiedene Google Cloud-Produkte, auf die wir uns beziehen werden, die Werkzeuge für die
Daten und maschinelles Lernen zur Verfügung stellen. Diese Produkte sind lediglich eine Option
für die Implementierung der in diesem Buch erwähnten Design Patterns und sind nicht als
erschöpfende Liste sein. Alle hier aufgeführten Produkte sind serverlos, so dass wir uns
Design Patterns für maschinelles Lernen und nicht auf die dahinterstehende Infrastruktur
Struktur dahinter.

8 | Kapitel 1: Die Notwendigkeit von Entwurfsmustern für maschinelles Lernen

BigQuery ist ein Data Warehouse für Unternehmen, das für die schnelle Analyse großer Datensätze
schnell mit SQL zu analysieren. Wir werden BigQuery in unseren Beispielen für die Datenerfassung und das Feature
Entwicklung. Die Daten in BigQuery sind in Datasets organisiert, und ein Dataset kann mehrere Tabellen enthalten.
mehrere Tabellen haben. Viele unserer Beispiele werden Daten aus Google Cloud Public Datasets verwenden, einem
einem Satz kostenloser, öffentlich verfügbarer Daten, die in BigQuery gehostet werden. Google Cloud Public Datasets
besteht aus Hunderten von verschiedenen Datensätzen, darunter NOAA-Wetterdaten seit 1929,
Stack Overflow-Fragen und -Antworten, Open-Source-Code von GitHub, Nativitätsdaten,
und vieles mehr. Um einige der Modelle in unseren Beispielen zu erstellen, werden wir BigQuery Machine
Learning (oder BigQuery ML). BigQuery ML ist ein Tool zum Erstellen von Modellen aus Daten
die in BigQuery gespeichert sind. Mit BigQuery ML können wir unsere Modelle mithilfe von SQL trainieren, bewerten und Vorhersagen generieren.
Vorhersagen über unsere Modelle mit SQL erstellen. Es unterstützt Klassifizierungs- und Regressionsmodelle,
zusammen mit unüberwachten Clustering-Modellen. Es ist auch möglich, zuvor trainierte
TensorFlow-Modelle für Vorhersagen in BigQuery ML zu importieren.

Cloud AI Platform umfasst eine Vielzahl von Produkten für das Training und die Bereitstellung von benutzerdefinierten
Machine-Learning-Modelle in der Google Cloud. In unseren Beispielen verwenden wir AI Plat-
form Training und AI Platform Prediction. AI Platform Training bietet eine Infrastruktur
Infrastruktur für das Training von maschinellen Lernmodellen in der Google Cloud. Mit AI Platform
Vorhersage können Sie Ihre trainierten Modelle bereitstellen und Vorhersagen für sie
unter Verwendung einer API. Beide Dienste unterstützen TensorFlow-, Scikit-Learn- und XGBoost-Modelle,
sowie benutzerdefinierte Container für Modelle, die mit anderen Frameworks erstellt wurden. Wir werden auch auf
Explainable AI, ein Tool zur Interpretation der Ergebnisse der Vorhersagen Ihres Modells,
das für Modelle verfügbar ist, die auf AI Platform bereitgestellt werden.

Rollen
Innerhalb eines Unternehmens gibt es viele verschiedene Aufgabenbereiche, die mit Daten und
maschinelles Lernen. Nachfolgend definieren wir einige gängige Rollen, auf die häufig
die in diesem Buch häufig erwähnt werden. Dieses Buch richtet sich in erster Linie an Data Scientists, Data Engineers
neure und ML-Ingenieure, also fangen wir mit diesen an.

Ein Datenwissenschaftler ist jemand, der sich auf das Sammeln, Interpretieren und Verarbeiten von Daten
sätze. Sie führen statistische und explorative Analysen von Daten durch. In Bezug auf das maschinelle
kann ein Datenwissenschaftler an der Datenerfassung, der Entwicklung von Merkmalen, der Modellbildung
Modellbildung und mehr. Datenwissenschaftler arbeiten oft in Python oder R in einer Notebook
Notebook-Umgebung und sind in der Regel die Ersten, die die Modelle für maschinelles Lernen eines Unternehmens
Modelle.

Ein Data Engineer befasst sich mit der Infrastruktur und den Arbeitsabläufen, die die Daten einer Organisation
der Daten eines Unternehmens. Er kann dabei helfen zu verwalten, wie ein Unternehmen Daten aufnimmt, Datenpipelines,
und wie die Daten gespeichert und übertragen werden. Data Engineers implementieren Infrastruktur und
Pipelines rund um Daten.

Ingenieure für maschinelles Lernen übernehmen ähnliche Aufgaben wie Dateningenieure, allerdings für ML-Modelle.
Sie verwenden Modelle, die von Datenwissenschaftlern entwickelt wurden, und verwalten die Infrastruktur und

Terminologie des maschinellen Lernens | 9
Operationen rund um das Training und den Einsatz dieser Modelle. ML-Ingenieure helfen beim Aufbau
Produktionssysteme für die Aktualisierung von Modellen, die Modellversionierung und die Bereitstellung von Vorhersagen
Vorhersagen für die Endnutzer.

Je kleiner das Data-Science-Team in einem Unternehmen und je agiler das Team ist, desto
desto wahrscheinlicher ist es, dass ein und dieselbe Person mehrere Rollen spielt. Wenn Sie sich in einer solchen Situation befinden
Wenn Sie sich in einer solchen Situation befinden, ist es sehr wahrscheinlich, dass Sie die obigen drei Beschreibungen lesen und sich teilweise in allen drei Kategorien wiederfinden.
in allen drei Kategorien wieder. In der Regel beginnen Sie ein Projekt zum maschinellen Lernen
als Dateningenieur und bauen Datenpipelines auf, um die Dateneingabe zu operationalisieren. Dann,
wechseln Sie in die Rolle des Datenwissenschaftlers und erstellen das/die ML-Modell(e). Schließlich setzen Sie
den Hut des ML-Ingenieurs auf und überführen das Modell in die Produktion. In größeren Unternehmen,
können Projekte des maschinellen Lernens dieselben Phasen durchlaufen, aber
können jedoch an jeder Phase beteiligt sein.

Forschungswissenschaftler, Datenanalysten und Entwickler können ebenfalls KI-Modelle erstellen und verwenden,
aber diese Berufsgruppen sind nicht die Zielgruppe dieses Buches.

Die Forscher konzentrieren sich in erster Linie darauf, neue Algorithmen zu finden und zu entwickeln, um
die Disziplin des ML voranzubringen. Dies könnte eine Vielzahl von Teilbereichen innerhalb des
maschinellen Lernens, wie Modellarchitekturen, Verarbeitung natürlicher Sprache, Computer
Vision, Abstimmung von Hyperparametern, Modellinterpretierbarkeit und vieles mehr. Im Gegensatz zu den anderen
anderen hier besprochenen Rollen verbringen Forscher die meiste Zeit damit, Prototypen zu entwickeln und
neue ML-Ansätze zu evaluieren, anstatt ML-Systeme für die Produktion zu entwickeln.

Datenanalysten werten Daten aus, sammeln Erkenntnisse und fassen diese für andere Teams
für andere Teams innerhalb ihrer Organisation zusammen. Sie arbeiten in der Regel mit SQL und Tabellen
Tabellenkalkulationen und verwenden Business-Intelligence-Tools zur Erstellung von Datenvisualisierungen, um ihre
Erkenntnisse. Datenanalysten arbeiten eng mit Produktteams zusammen, um zu verstehen, wie ihre
Erkenntnisse zur Lösung von Geschäftsproblemen und zur Schaffung von Mehrwert beitragen können. Während Datenanalysten
Datenanalysten sich darauf konzentrieren, Trends in vorhandenen Daten zu erkennen und daraus Erkenntnisse abzuleiten, geht es Datenwissenschaftlern
Datenwissenschaftler mit der Nutzung dieser Daten zur Erstellung von Zukunftsprognosen und der
die Generierung von Erkenntnissen zu automatisieren oder zu skalieren. Mit der zunehmenden Demokratisierung
des maschinellen Lernens können sich Datenanalysten zu Datenwissenschaftlern weiterbilden.

Die Entwickler sind für den Aufbau von Produktionssystemen zuständig, die den Endnutzern den
ML-Modelle zugreifen können. Sie sind oft an der Gestaltung der APIs beteiligt, die Modelle abfragen
und Vorhersagen in einem benutzerfreundlichen Format über eine Web- oder Mobilanwendung zurückgeben. Diese
kann es sich um Modelle handeln, die in der Cloud gehostet werden, oder um Modelle, die auf dem Gerät bereitgestellt werden. Entwickler
nutzen die von den ML-Ingenieuren implementierte Infrastruktur zur Modellbereitstellung, um Anwendungen und
Anwendungen und Benutzeroberflächen zu erstellen, um den Modellnutzern Vorhersagen zu präsentieren.

Abbildung 1-2 veranschaulicht, wie diese verschiedenen Rollen im Entwicklungsprozess von Modellen für maschinelles Lernen in einer Organisation zusammenarbeiten.
tion bei der Entwicklung von Modellen für maschinelles Lernen zusammenarbeiten.

10 | Kapitel 1: Die Notwendigkeit von Entwurfsmustern für maschinelles Lernen

Abbildung 1-2. Es gibt viele verschiedene Rollen im Zusammenhang mit Daten und maschinellem Lernen,
und diese Rollen arbeiten am ML-Workflow zusammen, von der Dateneingabe bis zur Modellbereitstellung
und der Endbenutzerschnittstelle. Der Dateningenieur arbeitet beispielsweise an der Dateneingabe und
Datenvalidierung und arbeitet eng mit Datenwissenschaftlern zusammen.

Gemeinsame Herausforderungen beim maschinellen Lernen
Warum brauchen wir ein Buch über Entwurfsmuster für maschinelles Lernen? Der Prozess der
ML-Systemen gibt es eine Vielzahl von einzigartigen Herausforderungen, die das ML
Entwurf beeinflussen. Diese Herausforderungen zu verstehen, wird Ihnen als ML-Praktiker helfen, einen
Referenzrahmen für die in diesem Buch vorgestellten Lösungen zu entwickeln.

Datenqualität
Modelle für maschinelles Lernen sind nur so zuverlässig wie die Daten, mit denen sie trainiert wurden. Wenn Sie
ein maschinelles Lernmodell auf einem unvollständigen Datensatz, auf Daten mit schlecht
ausgewählten Merkmalen oder auf Daten, die die Population, die das Modell verwendet, nicht genau
repräsentieren, spiegeln die Vorhersagen Ihres Modells direkt diese Daten wider. Daraus folgt,
werden Modelle für maschinelles Lernen oft als "Garbage in, Garbage out" bezeichnet. Hier werden wir
vier wichtige Komponenten der Datenqualität hervor: Genauigkeit, Vollständigkeit, Konsistenz
Konsistenz und Aktualität.

Die Datengenauigkeit bezieht sich sowohl auf die Merkmale Ihrer Trainingsdaten als auch auf die den Merkmalen
die diesen Merkmalen entsprechen. Wenn Sie verstehen, woher Ihre Daten stammen und
potenzielle Fehler bei der Datenerfassung können helfen, die Genauigkeit der Merkmale zu gewährleisten.
Nachdem Ihre Daten gesammelt wurden, ist es wichtig, eine gründliche Analyse durchzuführen, um
nach Tippfehlern, doppelten Einträgen, Messungsinkonsistenzen in Tabellendaten, fehlenden Merkmalen
und alle anderen Fehler, die die Datenqualität beeinträchtigen können. Duplikate in Ihrem Trainings
Trainingsdatensatz können beispielsweise dazu führen, dass Ihr Modell diesen Datenpunkten fälschlicherweise mehr Gewicht
Datenpunkte.

Gemeinsame Herausforderungen beim maschinellen Lernen | 11
Genaue Datenbeschriftungen sind ebenso wichtig wie die Genauigkeit der Merkmale. Ihr Modell verlässt sich
Ihr Modell stützt sich ausschließlich auf die wahren Bezeichnungen in Ihren Trainingsdaten, um seine Gewichte zu aktualisieren und den Verlust zu minimieren.
Verlust zu minimieren. Folglich können falsch beschriftete Trainingsbeispiele zu einer irreführenden
Modellgenauigkeit führen. Nehmen wir zum Beispiel an, Sie erstellen ein Modell zur Stimmungsanalyse
und 25 % Ihrer "positiven" Trainingsbeispiele wurden fälschlicherweise als "negativ" gekennzeichnet.
tiv" bezeichnet. Ihr Modell hat dann ein ungenaues Bild davon, was als negative Stimmung zu betrachten ist.
tive Stimmung angesehen werden sollte, was sich direkt in seinen Vorhersagen niederschlägt.

Zum Verständnis der Datenvollständigkeit: Nehmen wir an, Sie trainieren ein Modell zur Identifizierung von Katzen
Rassen. Sie trainieren das Modell auf einem umfangreichen Datensatz von Katzenbildern, und das resultierende
Modell ist in der Lage, Bilder in 1 von 10 möglichen Kategorien ("Bengalisch", "Siam" usw.)
usw.) mit einer Genauigkeit von 99 %. Wenn Sie Ihr Modell in der Produktion einsetzen, stellen Sie jedoch
stellen Sie jedoch fest, dass viele Ihrer Nutzer nicht nur Katzenfotos zur Klassifizierung hochladen, sondern auch
Fotos von Hunden hochladen und von den Ergebnissen des Modells enttäuscht sind.
Ergebnisse enttäuscht. Da das Modell nur für die Erkennung von 10 verschiedenen Katzenrassen trainiert wurde, ist das
alles, was es kann. Diese 10 Rassenkategorien sind im Grunde das gesamte "Weltbild" des Modells.
"Weltbild". Egal, was Sie dem Modell schicken, Sie können davon ausgehen, dass es es in
eine dieser 10 Kategorien einordnen wird. Es kann dies sogar mit großer Sicherheit für ein Bild tun, das
nicht wie eine Katze aussieht. Außerdem wird Ihr Modell auf keinen Fall in der Lage sein
"keine Katze" zurückgeben, wenn diese Daten und dieses Label nicht im Trainingsdatensatz enthalten waren.

Ein weiterer Aspekt der Datenvollständigkeit ist die Sicherstellung, dass die Trainingsdaten eine vielfältige
Repräsentation jedes Labels enthalten. Im Beispiel der Katzenrassenerkennung, wenn alle Bilder
Nahaufnahmen eines Katzengesichts sind, wird Ihr Modell nicht in der Lage sein, ein Bild einer
einer Katze von der Seite oder ein Ganzkörperbild einer Katze zu identifizieren. Um ein Beispiel für tabellarische Daten zu betrachten, wenn
wenn Sie ein Modell zur Vorhersage des Immobilienpreises in einer bestimmten Stadt erstellen, aber nur
aber nur Trainingsbeispiele von Häusern mit mehr als 2.000 Quadratmetern enthält, wird Ihr
Modell bei kleineren Häusern schlecht abschneiden.

Der dritte Aspekt der Datenqualität ist die Datenkonsistenz. Bei großen Datenbeständen ist es üblich, die
die Arbeit der Datenerfassung und -beschriftung auf eine Gruppe von Personen aufzuteilen. Die Entwicklung von
Entwicklung einer Reihe von Standards für diesen Prozess kann dazu beitragen, die Konsistenz des gesamten Datensatzes zu gewährleisten,
da jede beteiligte Person unweigerlich ihre eigenen Vorlieben in den Prozess einbringen wird.
Prozess einbringen. Wie bei der Datenvollständigkeit können auch bei den Daten Unstimmigkeiten sowohl bei den Datenmerkmalen
und Beschriftungen. Ein Beispiel für inkonsistente Merkmale ist die Erfassung von
atmosphärische Daten von Temperatursensoren. Wenn jeder Sensor nach unterschiedlichen Standards kalibriert wurde
Standards kalibriert, führt dies zu ungenauen und unzuverlässigen Modellvorhersagen.
Unstimmigkeiten können sich auch auf das Datenformat beziehen. Wenn Sie Standortdaten erfassen, schreiben manche
eine vollständige Straßenadresse als "Main Street" aus, während andere sie als "Main St." abkürzen.
als "Main St." abkürzen. Auch die Maßeinheiten, wie Meilen und Kilometer, können sich weltweit
der Welt unterscheiden.

12 | Kapitel 1: Die Notwendigkeit von Entwurfsmustern für maschinelles Lernen

Was die Ungereimtheiten bei der Kennzeichnung betrifft, so kehren wir zum Beispiel für das Textempfinden zurück. In
diesem Fall ist es wahrscheinlich, dass die Leute nicht immer übereinstimmen, was als positiv und
Trainingsdaten als positiv und negativ gelten. Um dieses Problem zu lösen, können Sie mehrere Personen beauftragen
jedes Beispiel in Ihrem Datensatz von mehreren Personen beschriften lassen und dann die am häufigsten verwendete
jedes Element. Wenn man sich der potenziellen Voreingenommenheit der Bezeichner bewusst ist und Systeme implementiert, die
zu berücksichtigen, wird die Konsistenz der Beschriftungen in Ihrem gesamten Datensatz sichergestellt. Wir werden das
Das Konzept der Voreingenommenheit wird im "Entwurfsmuster 30: Fairness-Objektiv" auf Seite 343 in Kapitel 7 untersucht.

Die Aktualität der Daten bezieht sich auf die Latenzzeit zwischen dem Auftreten eines Ereignisses und dem Zeitpunkt, zu dem es
zu Ihrer Datenbank hinzugefügt wurde. Wenn Sie z.B. Daten über Anwendungsprotokolle sammeln,
kann es einige Stunden dauern, bis ein Fehlerprotokoll in Ihrer Protokolldatenbank auftaucht. Bei einem Datensatz
der Kreditkartentransaktionen aufzeichnet, kann es ab dem Zeitpunkt der Transaktion einen Tag dauern
bis die Transaktion in Ihrem System angezeigt wird. Um die Aktualität zu berücksichtigen, ist es sinnvoll
so viele Informationen wie möglich über einen bestimmten Datenpunkt aufzuzeichnen und dafür zu sorgen
Daten in Merkmale für ein maschinelles Lernmodell umzuwandeln.
Modell für maschinelles Lernen. Genauer gesagt, können Sie den Zeitstempel aufbewahren
wann ein Ereignis aufgetreten ist und wann es zu Ihrem Datensatz hinzugefügt wurde. Dann können Sie bei der Durchführung
können Sie diese Unterschiede beim Feature-Engineering entsprechend berücksichtigen.

Reproduzierbarkeit
Bei der traditionellen Programmierung ist die Ausgabe eines Programms reproduzierbar und garantiert.
Wenn Sie zum Beispiel ein Python-Programm schreiben, das eine Zeichenkette umkehrt, wissen Sie, dass eine
Eingabe des Wortes "Banane" immer eine Ausgabe von "ananab" ergibt. Ähnlich verhält es sich, wenn
einen Fehler in Ihrem Programm gibt, der dazu führt, dass es Zeichenketten, die Zahlen enthalten, fälschlicherweise umkehrt
Zahlen enthält, könnten Sie das Programm an einen Kollegen schicken und erwarten, dass dieser
den Fehler mit denselben Eingaben reproduzieren kann, die Sie verwendet haben (es sei denn, der Fehler hat etwas damit zu tun
(es sei denn, der Fehler hat etwas damit zu tun, dass das Programm einen falschen internen Zustand beibehält, Unterschiede in der Archi-
Unterschiede in der Architektur, wie z.B. die Fließkommagenauigkeit, oder Unterschiede in der Ausführung, wie z.B.
Threading).

Modelle des maschinellen Lernens hingegen haben ein inhärentes Zufallselement.
ness. Beim Training werden die ML-Modellgewichte mit Zufallswerten initialisiert. Diese
Gewichte konvergieren dann während des Trainings, wenn das Modell iteriert und aus den Daten lernt.
Aus diesem Grund wird derselbe Modellcode mit denselben Trainingsdaten
Trainingsläufen leicht unterschiedliche Ergebnisse liefern. Dies stellt eine Herausforderung für die Reproduzierbarkeit dar.
Reproduzierbarkeit. Wenn Sie ein Modell mit einer Genauigkeit von 98,1 % trainieren, ist nicht gewährleistet, dass ein wiederholter Trainingslauf dasselbe Ergebnis erzielt.
nicht garantiert, dass er dasselbe Ergebnis erzielt. Dies kann es schwierig machen, Vergleiche zwischen
Experimenten.

Um dieses Problem der Wiederholbarkeit zu lösen, ist es üblich, den Zufallswert
Seed-Wert, der von Ihrem Modell verwendet wird, so einzustellen, dass bei jedem
jedes Mal angewendet wird, wenn Sie das Training durchführen. In TensorFlow können Sie dies tun, indem Sie
tf.random.set_seed(value) am Anfang Ihres Programms ausführen.

Gemeinsame Herausforderungen beim maschinellen Lernen | 13
In scikit-learn erlauben viele Dienstprogramme zum Mischen von Daten auch die
Sie einen zufälligen Seed-Wert festlegen:

from sklearn.utils import shuffle
Daten = shuffle(Daten, random_state=Wert)
Denken Sie daran, dass Sie dieselben Daten und denselben Zufallswert verwenden müssen, wenn Sie
Modell zu trainieren, um wiederholbare, reproduzierbare Ergebnisse in verschiedenen
Experimente zu gewährleisten.

Das Training eines ML-Modells beinhaltet mehrere Artefakte, die festgelegt werden müssen, um
Reproduzierbarkeit zu gewährleisten: die verwendeten Daten, der Splitting-Mechanismus zur Erzeugung von
Datensätze für Training und Validierung, Datenaufbereitung und Modellhyperparameter sowie
Variablen wie die Stapelgröße und der Lernratenplan.

Die Reproduzierbarkeit gilt auch für die Abhängigkeiten von maschinellen Lernsystemen. Zusätzlich
Neben dem manuellen Setzen eines Zufallsseeds implementieren Frameworks auch Elemente der Ran-
domness, die ausgeführt werden, wenn Sie eine Funktion zum Trainieren Ihres Modells aufrufen. Wenn
diese zugrundeliegende Implementierung zwischen verschiedenen Framework-Versionen ändert,
ist die Wiederholbarkeit nicht gewährleistet. Ein konkretes Beispiel: Wenn eine Version der train()-Methode eines Frame
Wenn eine Version der train()-Methode eines Frameworks 13 Aufrufe an rand() vornimmt und eine neuere Version desselben
Rahmens 14 Aufrufe macht, wird die Verwendung verschiedener Versionen zwischen den Experimenten zu
zu leicht abweichenden Ergebnissen, selbst bei gleichen Daten und gleichem Modellcode. Ausführen von ML
Workloads in Containern auszuführen und Bibliotheksversionen zu standardisieren, kann helfen, Wiederholbarkeit zu
Wiederholbarkeit zu gewährleisten. In Kapitel 6 wird eine Reihe von Mustern vorgestellt, mit denen ML-Prozesse
reproduzierbar zu machen.

Schließlich kann sich die Reproduzierbarkeit auf die Trainingsumgebung eines Modells beziehen. Aufgrund von
großen Datenmengen und der Komplexität benötigen viele Modelle viel Zeit, um
trainieren. Dies kann durch den Einsatz von Verteilungsstrategien wie Daten- oder Modellparallelität
Parallelität (siehe Kapitel 5). Diese Beschleunigung bringt jedoch eine zusätzliche Herausforderung mit sich.
len Problem der Wiederholbarkeit, wenn Sie Code, der verteiltes Training nutzt, erneut ausführen.

Daten-Drift
Während Modelle für maschinelles Lernen normalerweise eine statische Beziehung zwischen
Inputs und Outputs darstellen, können sich die Daten im Laufe der Zeit erheblich verändern. Die Datendrift bezieht sich auf die
Herausforderung, sicherzustellen, dass Ihre maschinellen Lernmodelle relevant bleiben und dass
Vorhersagen ein genaues Abbild der Umgebung sind, in der sie verwendet werden.

Nehmen wir zum Beispiel an, Sie trainieren ein Modell, das die Schlagzeilen von Nachrichtenartikeln in
Kategorien wie "Politik", "Wirtschaft" und "Technologie" einzuordnen. Wenn Sie Ihr Modell auf historischen
Modell auf historischen Nachrichtenartikeln aus dem 20. Jahrhundert trainieren und auswerten, wird es
auf aktuelle Daten. Heute wissen wir, dass ein Artikel mit dem Wort "Smartphone" in der
der Überschrift wahrscheinlich um Technologie geht. Ein auf historischen Daten trainiertes Modell
Daten trainiert wurde, würde dieses Wort nicht kennen. Um die Drift zu beheben, ist es wichtig, dass

14 | Kapitel 1: Die Notwendigkeit von Entwurfsmustern für maschinelles Lernen

Ihren Trainingsdatensatz ständig aktualisieren, Ihr Modell neu trainieren und die Gewichtung ändern
die Ihr Modell bestimmten Gruppen von Eingabedaten zuweist.

Ein weniger offensichtliches Beispiel für Drift ist der NOAA-Datensatz für schwere Stürme in
BigQuery. Wenn wir ein Modell trainieren würden, um die Wahrscheinlichkeit eines Sturms in einem bestimmten Gebiet vorherzusagen
müssen wir berücksichtigen, wie sich die Wetterberichterstattung im Laufe der Zeit verändert hat.
Zeit verändert hat. In Abbildung 1-3 ist zu sehen, dass die Gesamtzahl der aufgezeichneten schweren Stürme
seit 1950 stetig zugenommen hat.

Abbildung 1-3. Anzahl der in einem Jahr gemeldeten schweren Stürme, aufgezeichnet von der NOAA von
1950 bis 2011.

Anhand dieses Trends können wir erkennen, dass das Trainieren eines Modells mit Daten aus der Zeit vor 2000, um
Vorhersagen über heutige Stürme zu erstellen, zu ungenauen Vorhersagen führen würde. Neben der
Gesamtzahl der gemeldeten Stürme, ist es auch wichtig, andere Faktoren
toren zu berücksichtigen, die die Daten in Abbildung 1-3 beeinflusst haben könnten. Zum Beispiel hat sich die Technologie zur
Stürme im Laufe der Zeit verbessert, am dramatischsten mit der Einführung von
Wetterradargeräten in den 1990er Jahren. Im Zusammenhang mit den Merkmalen kann dies bedeuten, dass neuere Daten
Daten mehr Informationen über die einzelnen Stürme enthalten, und dass ein Merkmal, das in den heutigen
Daten verfügbar ist, 1950 vielleicht noch nicht beobachtet wurde. Eine explorative Datenanalyse kann helfen
Diese Art von Drift kann dabei helfen, diese Art von Drift zu identifizieren und das richtige Datenfenster für das Training zu bestimmen. Abschnitt
Abschnitt , "Entwurfsmuster 23: Überbrücktes Schema" auf Seite 266 bietet eine Möglichkeit, mit Daten
Datenmengen, bei denen sich die Verfügbarkeit von Merkmalen im Laufe der Zeit verbessert.

Gemeinsame Herausforderungen beim maschinellen Lernen | 15
Skalierung
Die Herausforderung der Skalierung ist in vielen Phasen eines typischen maschinellen
Arbeitsablaufs beim maschinellen Lernen. Sie werden wahrscheinlich bei der Datensammlung und -vorverarbeitung
Vorverarbeitung, Training und Bereitstellung. Beim Einlesen und Vorbereiten von Daten für ein
Modell für maschinelles Lernen diktiert die Größe des Datensatzes die für Ihre Lösung erforderlichen Werkzeuge.
Ihre Lösung. Es ist oft die Aufgabe von Dateningenieuren, Datenpipelines zu entwickeln, die
skalieren können, um Datensätze mit Millionen von Zeilen zu verarbeiten.

Bei der Modellschulung sind die ML-Ingenieure für die Festlegung der erforderlichen
Infrastruktur für einen bestimmten Trainingsauftrag. Je nach Art und Größe der Daten
Datenmenge kann die Modellschulung zeitaufwändig und rechenintensiv sein und erfordert
Infrastruktur (z. B. GPUs), die speziell für ML-Workloads entwickelt wurde. Bildmodelle, zum Beispiel
Bildmodelle beispielsweise benötigen in der Regel viel mehr Trainingsinfrastruktur als Modelle, die
ausschließlich auf Tabellendaten trainiert werden.

Im Zusammenhang mit dem Modellservice ist die Infrastruktur, die zur Unterstützung eines Teams von Datenwissenschaftlern
Datenwissenschaftler zu unterstützen, die Vorhersagen von einem Modellprototyp erhalten, ist völlig anders
Infrastruktur, die erforderlich ist, um ein Produktionsmodell zu unterstützen, das stündlich Millionen von
Anfragen pro Stunde erhält. Entwickler und ML-Ingenieure sind in der Regel für die Bewältigung
die Skalierungsherausforderungen zu bewältigen, die mit der Modellbereitstellung und der Bereitstellung
Anfragen.

Die meisten der ML-Muster in diesem Buch sind unabhängig von der organisatorischen
Reifegrad. Mehrere der Muster in den Kapiteln 6 und 7 befassen sich jedoch auf unterschiedliche Weise mit den Herausforderungen der Widerstandsfähigkeit und
Reproduzierbarkeit auf unterschiedliche Weise an, und die Wahl zwischen ihnen wird oft
hängt oft vom Anwendungsfall und der Fähigkeit Ihres Unternehmens ab, Komplexität zu absorbieren.

Mehrere Zielsetzungen
Obwohl oft ein einzelnes Team für die Erstellung eines Modells für maschinelles Lernen
Modells verantwortlich ist, werden viele Teams innerhalb eines Unternehmens das Modell in irgendeiner Weise nutzen.
Es ist unvermeidlich, dass diese Teams unterschiedliche Vorstellungen davon haben, was ein erfolgreiches Modell ausmacht.

Um zu verstehen, wie sich dies in der Praxis auswirkt, nehmen wir an, Sie bauen ein Modell, um
defekte Produkte anhand von Bildern zu identifizieren. Als Datenwissenschaftler könnte Ihr Ziel darin bestehen, den
den Cross-Entropie-Verlust Ihres Modells zu minimieren. Der Produktmanager hingegen möchte vielleicht
die Anzahl der defekten Produkte, die falsch klassifiziert und an Kunden geschickt werden
Kunden geschickt werden. Und schließlich könnte das Ziel der Geschäftsleitung darin bestehen, den Umsatz um 30 % zu steigern.
Jedes dieser Ziele unterscheidet sich in seinen Optimierungszielen, und es kann eine Herausforderung sein, diese unterschiedlichen
Bedürfnisse innerhalb einer Organisation auszugleichen, kann eine Herausforderung darstellen.

Als Datenwissenschaftler könnten Sie die Bedürfnisse des Produktteams in den Kontext Ihres Modells übersetzen
Sie könnten die Bedürfnisse des Produktteams in den Kontext Ihres Modells übersetzen, indem Sie sagen, dass falsch negative Ergebnisse fünfmal teurer sind als falsch positive Ergebnisse.
Daher sollten Sie die Rückrufquote gegenüber der Genauigkeit optimieren, um dies zu erfüllen, wenn

16 | Kapitel 1: Die Notwendigkeit von Entwurfsmustern für maschinelles Lernen

Ihr Modell zu entwerfen. So können Sie ein Gleichgewicht finden zwischen dem Ziel des Produktteams
zwischen dem Ziel des Produktteams, die Genauigkeit zu optimieren, und Ihrem Ziel, den Verlust des Modells zu minimieren.

Bei der Definition der Ziele für Ihr Modell ist es wichtig, die Bedürfnisse der verschiedenen
Teams innerhalb einer Organisation zu berücksichtigen und wie sich die Bedürfnisse der einzelnen Teams auf das
Modell. Wenn Sie vor der Entwicklung Ihrer Lösung analysieren, was die einzelnen Teams optimieren wollen, können Sie
Lösung zu entwickeln, können Sie Kompromissbereiche finden, um diese verschiedenen Ziele optimal
Ziele optimal auszugleichen.

Zusammenfassung
Entwurfsmuster sind eine Möglichkeit, das Wissen und die Erfahrung von Experten zu kodifizieren
Ratschläge, die alle Praktiker befolgen können. Die Entwurfsmuster in diesem Buch enthalten bewährte
Best Practices und Lösungen für häufig auftretende Probleme beim Entwurf, der Erstellung und
Einsatz von maschinellen Lernsystemen. Die häufigsten Herausforderungen beim maschinellen Lernen
Herausforderungen beim maschinellen Lernen sind Datenqualität, Reproduzierbarkeit, Datendrift, Skalierung und die
mehrere Ziele zu erfüllen.

Wir neigen dazu, verschiedene ML-Entwurfsmuster in verschiedenen Phasen des ML-Lebenszyklus zu verwenden.
Es gibt Muster, die bei der Formulierung von Problemen und der Bewertung der Machbarkeit nützlich sind. Die
Die meisten Muster beziehen sich entweder auf die Entwicklung oder den Einsatz, und einige Pat
einige Muster befassen sich mit dem Zusammenspiel zwischen diesen Phasen.

Zusammenfassung | 17
KAPITEL 2

Entwurfsmuster für die Datendarstellung
Das Herzstück eines jeden Modells für maschinelles Lernen ist eine mathematische Funktion, die so definiert ist
definiert ist, die nur auf bestimmte Datentypen angewendet werden kann. Gleichzeitig müssen reale maschinelle Lernmodelle
Gleichzeitig müssen reale Modelle für maschinelles Lernen mit Daten arbeiten, die nicht direkt in die mathematische Funktion eingesteckt werden können.
matische Funktion einfügen lassen. Der mathematische Kern eines Entscheidungsbaums arbeitet zum Beispiel mit
booleschen Variablen. Beachten Sie, dass es sich hier um den mathematischen Kern eines
Entscheidungsbaums sprechen - Software für maschinelles Lernen mit Entscheidungsbäumen enthält normalerweise auch
Funktionen zum Erlernen eines optimalen Baums aus Daten und Möglichkeiten zum Einlesen und Verarbeiten verschiedener
Arten von numerischen und kategorialen Daten. Die mathematische Funktion (siehe Abbildung 2-1)
Die mathematische Funktion (siehe Abbildung 2-1), die einem Entscheidungsbaum zugrunde liegt, arbeitet jedoch mit booleschen Variablen und verwendet Operationen
Operationen wie AND (&& in Abbildung 2-1) und OR (+ in Abbildung 2-1).

Abbildung 2-1. Das Herzstück eines maschinellen Entscheidungsbaum-Lernmodells zur Vorhersage, ob
ob ein Baby Intensivpflege benötigt oder nicht, ist ein mathematisches Modell, das mit booleschen
Variablen arbeitet.

19
1 Hier besteht die gelernte Datendarstellung aus dem Babygewicht als Eingangsvariable, dem Kleiner-als-Operator,
und dem Schwellenwert von 3 kg.
Angenommen, wir haben einen Entscheidungsbaum, der vorhersagen soll, ob ein Baby Intensivpflege benötigt
(IC) benötigt oder normal entlassen werden kann (ND), und nehmen wir an, dass der Entscheidungsbaum als
Eingaben zwei Variablen, x1 und x2. Das trainierte Modell könnte etwa so aussehen
Abbildung 2-1.

Es ist ziemlich klar, dass x1 und x2 boolesche Variablen sein müssen, damit f(x1, x2)
funktioniert. Nehmen wir an, dass zwei der Informationen, die das Modell berücksichtigen soll
ein Baby als intensivpflegebedürftig oder nicht intensivpflegebedürftig einstuft, sind das Krankenhaus, in dem das Baby
geboren wurde und das Gewicht des Babys. Können wir das Krankenhaus, in dem ein Baby geboren wurde, als
in den Entscheidungsbaum eingeben? Nein, denn das Krankenhaus nimmt weder den Wert Wahr noch
den Wert Falsch annimmt und nicht in den Operator && (AND) eingegeben werden kann. Das ist mathematisch
nicht kompatibel. Natürlich können wir den Wert des Krankenhauses boolesch "machen", indem wir
eine Operation wie die folgende durchführen:

x1 = (Krankenhaus IN Frankreich)
so dass x1 wahr ist, wenn das Krankenhaus in Frankreich liegt, und falsch, wenn nicht. In ähnlicher Weise kann das Gewicht eines Babys
Gewicht eines Babys nicht direkt in das Modell eingespeist werden, sondern durch eine Operation wie die folgende:

x1 = (Babygewicht < 3 kg)
können wir das Krankenhaus oder das Gewicht des Babys als Eingabe für das Modell verwenden. Dies ist ein Beispiel
Beispiel dafür, wie Eingabedaten (Krankenhaus, ein komplexes Objekt oder Babygewicht, eine Fließkommazahl)
Zahl) in der vom Modell erwarteten Form (boolesch) dargestellt werden können. Dies ist
was wir mit Datendarstellung meinen.

In diesem Buch verwenden wir den Begriff input für die realen Daten, die dem Modell
(z. B. das Gewicht des Babys) und den Begriff "Merkmal" für die umgewandelten
die umgewandelten Daten, mit denen das Modell tatsächlich arbeitet (z. B. ob das Gewicht des Babys
Gewicht weniger als 3 Kilogramm beträgt). Der Prozess der Erstellung von Merkmalen zur Darstellung der
der Eingabedaten wird als Feature Engineering bezeichnet, und wir können uns das Feature Engineering als
eine Art der Auswahl der Datendarstellung.

Anstatt Parameter wie den Schwellenwert von 3 Kilogramm fest zu kodieren
gramm, würde das maschinelle Lernmodell lieber lernen, wie jeder Knoten zu erstellen ist, indem es
Auswahl der Eingabevariablen und des Schwellenwerts. Entscheidungsbäume sind ein Beispiel für
Modelle des maschinellen Lernens, die in der Lage sind, die Datendarstellung zu lernen.^1 Viele
der Muster, die wir in diesem Kapitel betrachten, beinhalten ähnlich lernfähige
Datenrepräsentationen.

Das Entwurfsmuster "Einbettungen" ist das kanonische Beispiel für eine Datendarstellung, die
tiefe neuronale Netze in der Lage sind, selbständig zu lernen. Bei einer Einbettung ist die
gelernte Darstellung dicht und niedriger-dimensional als die Eingabe, was die

20 | Kapitel 2: Entwurfsmuster für die Datendarstellung

spärlich sein. Der Lernalgorithmus muss die auffälligsten Informationen aus der Eingabe extrahieren
aus den Eingaben extrahieren und sie in einer prägnanteren Form in den Merkmalen darstellen. Der Prozess des Lernens
Der Prozess des Lernens von Merkmalen zur Darstellung der Eingabedaten wird Merkmalsextraktion genannt, und wir können uns
lernbare Datenrepräsentationen (wie Einbettungen) als automatisch erzeugte
Merkmale.

Die Datendarstellung muss nicht einmal aus einer einzigen Eingangsvariablen bestehen - ein obli-
Entscheidungsbaum erstellt beispielsweise ein boolesches Merkmal durch Schwellenwertbildung einer linearen
Kombination von zwei oder mehr Eingabevariablen. Ein Entscheidungsbaum, bei dem jeder Knoten
nur eine Eingangsvariable darstellen kann, reduziert sich auf eine schrittweise lineare Funktion, während ein
schräger Entscheidungsbaum, bei dem jeder Knoten eine lineare Kombination von Eingangsvariablen darstellen kann
Variablen darstellen kann, auf eine stückweise lineare Funktion reduziert wird (siehe Abbildung 2-2). Wenn man bedenkt, wie
Wenn man bedenkt, wie viele Schritte gelernt werden müssen, um die Linie adäquat zu repräsentieren, ist das stückweise lineare
Ohr-Modell einfacher und schneller zu lernen. Eine Erweiterung dieser Idee ist das Feature Cross
das das Lernen von UND-Beziehungen zwischen mehrwertigen kategorialen Variablen vereinfacht.
kategorialen Variablen vereinfacht.

Abbildung 2-2. Ein Entscheidungsbaum-Klassifikator, bei dem jeder Knoten nur einen Eingabewert annehmen kann
(x1 oder x2) kann, führt zu einer schrittweisen linearen Grenzfunktion, während ein schräger
Baumklassifikator, bei dem ein Knoten einen Schwellenwert für eine lineare Kombination von Eingangsvariablen
eine stückweise lineare Grenzfunktion ergibt. Die stückweise lineare Funktion erfordert
weniger Knoten und kann eine höhere Genauigkeit erreichen.

Die Datendarstellung muss nicht erlernt oder festgelegt werden - auch eine Mischform ist möglich.
Das Hashed Feature Design Pattern ist deterministisch, erfordert aber nicht, dass ein Modell
alle möglichen Werte, die eine bestimmte Eingabe annehmen kann, zu kennen.

Die bisher betrachteten Datendarstellungen sind alle eins-zu-eins. Obwohl wir
Eingabedaten verschiedener Typen getrennt darstellen oder jedes Datenelement als ein Merkmal darstellen
als nur ein Merkmal darstellen, kann es vorteilhafter sein, multimodale Eingaben zu verwenden. Dies ist das
vierte Entwurfsmuster, das wir in diesem Kapitel untersuchen werden.

Entwurfsmuster für die Datendarstellung | 21
Einfache Datenrepräsentationen
Bevor wir uns mit lernfähigen Datendarstellungen, Merkmalskreuzen und mehr befassen, lassen Sie uns
einfachere Datenrepräsentationen betrachten. Wir können uns diese einfachen Datenrepräsenta- tionen als
Datenrepräsentationen als gängige Redewendungen beim maschinellen Lernen betrachten - nicht als Muster, aber
aber dennoch häufig verwendete Lösungen.

Numerische Eingaben
Die meisten modernen, groß angelegten Modelle für maschinelles Lernen (Random Forests, Support Vector
Maschinen, neuronale Netze) arbeiten mit numerischen Werten, und wenn unsere Eingabe numerisch ist
numerisch ist, können wir sie unverändert an das Modell weitergeben.

Warum Skalierung wünschenswert ist

Da das ML-Framework einen Optimierer verwendet, der so eingestellt ist, dass er gut mit
Zahlen im Bereich [-1, 1] abgestimmt ist, kann es von Vorteil sein, die numerischen Werte so zu skalieren, dass sie in diesem Bereich liegen.
vorteilhaft sein.

Warum sollten numerische Werte so skaliert werden, dass sie in [-1, 1] liegen?
Gradientenabstiegsoptimierer benötigen mehr Schritte zur Konvergenz, wenn die Krümmung der
Verlustfunktion zunimmt. Dies liegt daran, dass die Ableitungen von Merkmalen mit größeren relativen
Ableitungen von Merkmalen mit größeren relativen Größenordnungen tendenziell auch größer sind und somit zu anormalen Gewichtungsaktualisierungen führen.
Die abnormal großen Gewichtungsaktualisierungen erfordern mehr Schritte zur Konvergenz und erhöhen damit
und erhöhen damit die Berechnungslast.
Das "Zentrieren" der Daten im Bereich [-1, 1] macht die Fehlerfunktion sphärischer.
kal. Daher konvergieren Modelle, die mit transformierten Daten trainiert werden, tendenziell schneller und sind
daher schneller/kostengünstiger zu trainieren. Darüber hinaus bietet der [-1, 1]-Bereich die höchste Fließkomma-
punktgenauigkeit.
Ein kurzer Test mit einem der in scikit-learn eingebauten Datensätze kann dies belegen (dies ist
ein Auszug aus dem Code-Repository dieses Buches):

from sklearn import datasets, linear_model
diabetes_X, diabetes_y = datasets.load_diabetes(return_X_y=True)
raw = diabetes_X[:, None, 2]
max_raw = max(raw)
min_raw = min(raw)
scaled = (2*raw - max_raw - min_raw)/(max_raw - min_raw)
def train_raw():
linear_model.LinearRegression().fit(raw, diabetes_y)
def train_scaled():
22 | Kapitel 2: Entwurfsmuster für die Datendarstellung

linear_model.LinearRegression().fit(skaliert, diabetes_y)
raw_time = timeit.timeit(train_raw, number=1000)
scaled_time = timeit.timeit(train_scaled, number=1000)
Bei der Ausführung dieses Modells ergab sich eine Verbesserung von fast 9 % gegenüber dem Modell, das nur
ein Eingabe-Merkmal verwendet. Bedenkt man die Anzahl der Merkmale in einem typischen Modell für maschinelles Lernen
Modells, können sich die Einsparungen summieren.

Ein weiterer wichtiger Grund für die Skalierung ist, dass einige Algorithmen und Techniken des maschinellen Lernens
Techniken sehr empfindlich auf die relative Größe der verschiedenen Merkmale reagieren. Unter
ein k-means-Clustering-Algorithmus, der den euklidischen Abstand als Näherungsmaß
als Proximitätsmaß verwendet, wird sich am Ende stark auf Merkmale mit größeren Werten stützen. Fehlende
Skalierung beeinträchtigt auch die Wirksamkeit der L1- oder L2-Regularisierung, da die Größe der
Gewichte für ein Merkmal von der Größe der Werte dieses Merkmals abhängt und somit verschiedene
verschiedene Merkmale unterschiedlich von der Regularisierung betroffen sind. Durch Skalierung aller Merkmale auf
zwischen [-1, 1] liegen, stellen wir sicher, dass es keinen großen Unterschied in den relativen
Größenordnungen der verschiedenen Merkmale gibt.

Lineare Skalierung

Im Allgemeinen werden vier Formen der Skalierung verwendet:

Min-Max-Skalierung
Der numerische Wert wird linear skaliert, so dass der kleinste Wert, den die Eingabe
auf -1 und der maximal mögliche Wert auf 1 skaliert wird:
x1_skaliert = (2*x1 - max_x1 - min_x1)/(max_x1 - min_x1)
Das Problem bei der Min-Max-Skalierung ist, dass der maximale und minimale Wert
(max_x1 und min_x1) aus dem Trainingsdatensatz geschätzt werden müssen, und das sind
oft Ausreißerwerte sind. Die realen Daten werden oft auf einen sehr engen Bereich im
[-1, 1]-Band.

Clipping (in Verbindung mit Min-Max-Skalierung)
Hilft, das Problem der Ausreißer zu lösen, indem "vernünftige" Werte verwendet werden, anstatt
das Minimum und Maximum aus dem Trainingsdatensatz zu schätzen. Der numerische
Wert wird linear zwischen diesen beiden sinnvollen Grenzen skaliert und dann so beschnitten, dass er
im Bereich [-1, 1] liegt. Dies bewirkt, dass Ausreißer als -1 oder 1 behandelt werden.

Z-Score-Normalisierung
Behebt das Problem der Ausreißer, ohne dass man vorher wissen muss, was der
vernünftigen Bereich durch lineare Skalierung der Eingabe unter Verwendung des Mittelwerts und der Standardabweichung
Standardabweichung, die über den Trainingsdatensatz geschätzt werden:
x1_skaliert = (x1 - Mittelwert_x1)/Stddev_x1

Einfache Datendarstellungen | 23
Der Name der Methode spiegelt die Tatsache wider, dass der skalierte Wert einen Mittelwert von Null hat und
durch die Standardabweichung normalisiert wird, so dass er eine Einheitsvarianz über den
Trainingsdatensatz hat. Der skalierte Wert ist unbegrenzt, liegt aber in den meisten Fällen zwischen [-1, 1].
(67 %, wenn die zugrunde liegende Verteilung normal ist). Werte außerhalb
Werte, die außerhalb dieses Bereichs liegen, werden seltener, je größer ihr absoluter Wert wird, sind aber immer noch vorhanden.
Winsorisierung
Verwendet die empirische Verteilung im Trainingsdatensatz, um den Datensatz auf
Grenzen, die durch das 10. und 90. Perzentil der Datenwerte (oder das 5. und 95.
Perzentil und so weiter). Der winsorisierte Wert ist minimal-maximal skaliert.

Alle bisher besprochenen Methoden skalieren die Daten linear (im Falle von Clipping und
winsorizing, linear innerhalb des typischen Bereichs). Min-Max und Clipping funktionieren am besten
für gleichmäßig verteilte Daten, und Z-Score für normal verteilte Daten.
verteilten Daten. Die Auswirkung der verschiedenen Skalierungsfunktionen auf die Spalte mother_age im
dem Beispiel für die Vorhersage des Babygewichts ist in Abbildung 2-3 dargestellt (siehe den vollständigen Code).

Keine "Ausreißer" wegschmeißen
Beachten Sie, dass wir das Clipping so definiert haben, dass skalierte Werte kleiner als -1 als -1 behandelt werden.
als -1 und skalierte Werte größer als 1 als 1 zu behandeln. Wir entfernen solche "Ausreißer" nicht einfach.
solche "Ausreißer" nicht einfach aus, weil wir erwarten, dass das maschinelle Lernmodell
dass das maschinelle Lernmodell in der Produktion auf solche Ausreißer trifft. Nehmen wir zum Beispiel Babys, die von 50-jährigen
alten Müttern. Da wir nicht genügend ältere Mütter in unserem Datensatz haben, führt das Clipping
alle Mütter, die älter als 45 Jahre sind (zum Beispiel), als 45 Jahre behandeln. Diese gleiche Behandlung
wird in der Produktion angewandt, so dass unser Modell auch mit älteren Müttern umgehen kann.
Mütter. Das Modell würde nicht lernen, Ausreißer zu berücksichtigen, wenn wir einfach
alle Trainingsbeispiele von Babys, die von Müttern im Alter von 50+ geboren wurden!
Man kann dies auch so sehen, dass es zwar akzeptabel ist, ungültige Eingaben wegzuwerfen
Eingaben wegzuwerfen, ist es nicht akzeptabel, gültige Daten wegzuwerfen. Es wäre also gerechtfertigt, wenn wir
Zeilen wegzuwerfen, in denen mother_age negativ ist, weil es sich wahrscheinlich um einen Dateneingabe
Fehler ist. In der Produktion wird durch die Validierung des Eingabeformulars sichergestellt, dass der Sachbearbeiter
das Alter der Mutter erneut eingeben muss. Es ist jedoch nicht gerechtfertigt, Zeilen wegzuwerfen
zu verwerfen, in denen mother_age 50 ist, weil 50 eine absolut gültige Eingabe ist und wir erwarten
50-jährige Mütter zu finden, sobald das Modell in der Produktion eingesetzt wird.
In Abbildung 2-3 sehen Sie, dass minmax_scaled die x-Werte in den gewünschten Bereich von
[-1, 1] bringt, aber weiterhin Werte an den extremen Enden der Verteilung beibehält, wo
es nicht genügend Beispiele gibt. Das Clipping rollt viele der problematischen Werte auf, aber
erfordert jedoch, dass die Schwellenwerte für das Clipping genau richtig gewählt werden - hier ist der langsame Rückgang der
Anzahl der Babys mit einem Alter der Mutter von über 40 Jahren Probleme bei der Festlegung eines harten
Schwelle. Die Winsorisierung erfordert, ähnlich wie das Clipping, die genaue Festlegung der Perzentilschwellenwerte
genau zu treffen. Die Z-Score-Normalisierung verbessert den Bereich (schränkt ihn aber nicht ein).

24 | Kapitel 2: Entwurfsmuster für die Datendarstellung

Werte zwischen [-1, 1] liegen) und schiebt die problematischen Werte weiter nach außen. Von diesen
drei Methoden funktioniert die Nullnormierung am besten für mother_age, da die rohen Alterswerte
eher einer Glockenkurve entsprechen. Bei anderen Problemen sind Min-Max-Skalierung, Clipping oder
winsorizing besser geeignet sein.

Abbildung 2-3. Das Histogramm von mother_age im Beispiel der Vorhersage des Babygewichts ist
und verschiedene Skalierungsfunktionen (siehe die Beschriftung der x-Achse) sind in den übrigen Feldern
in den übrigen Feldern dargestellt.

Einfache Datendarstellungen | 25
Nichtlineare Transformationen

Was ist, wenn unsere Daten schief sind und weder gleichmäßig noch wie eine Glockenkurve verteilt sind?
Glockenkurve? In diesem Fall ist es besser, eine nichtlineare Transformation auf die Eingabe anzuwenden, bevor
skalieren. Ein gängiger Trick besteht darin, den Logarithmus des Eingabewerts zu nehmen, bevor man ihn skaliert.
skalieren. Weitere gängige Transformationen sind die Sigmoid- und Polynom-Erweiterungen
(Quadrat, Quadratwurzel, Würfel, Kubikwurzel und so weiter). Wir wissen, dass wir eine gute
Transformationsfunktion, wenn die Verteilung des transformierten Wertes uni-
Form oder normalverteilt ist.

Angenommen, wir erstellen ein Modell zur Vorhersage der Verkaufszahlen eines Sachbuchs. Eine der
der Eingaben in das Modell ist die Popularität der Wikipedia-Seite, die dem Thema
Thema. Die Anzahl der Aufrufe von Seiten in Wikipedia ist jedoch stark verzerrt und
stark verzerrt und weist einen großen dynamischen Bereich auf (siehe Abbildung 2-4, linkes Feld: die Verteilung ist
sehr schräg zu selten aufgerufenen Seiten, aber die häufigsten Seiten werden
mehrere zehn Millionen Mal aufgerufen). Durch Bilden des Logarithmus der Aufrufe, anschließendes Ziehen der vierten Wurzel
Wurzel aus diesem logarithmierten Wert zieht und das Ergebnis linear skaliert, erhält man etwas, das im
gewünschten Bereich liegt und etwas glockenförmig ist. Für Details des Codes zur Abfrage der Wikipe-
dia-Daten abzufragen, diese Transformationen anzuwenden und diese Darstellung zu erzeugen, finden Sie im GitHub
Repository für dieses Buch.

Abbildung 2-4. Linkes Feld: Die Verteilung der Anzahl der Aufrufe von Wikipedia-Seiten ist
stark verzerrt und weist einen großen dynamischen Bereich auf. Das zweite Feld zeigt, dass
Probleme gelöst werden können, indem die Anzahl der Aufrufe mit dem Logarithmus, einer
Potenzfunktion und einer linearen Skalierung angegangen werden kann. Das dritte Feld zeigt die Auswirkung der His-
Das dritte Feld zeigt die Wirkung der Histogramm-Entzerrung und das vierte Feld die Wirkung der Box-Cox-Transformation.

Es kann schwierig sein, eine Linearisierungsfunktion zu finden, die die Verteilung wie eine Glockenkurve aussehen lässt.
einer Glockenkurve ähnelt. Ein einfacherer Ansatz besteht darin, die Anzahl der Ansichten zu schaufeln, indem man die

26 | Kapitel 2: Entwurfsmuster für die Datendarstellung

Eimergrenzen, um die gewünschte Ausgangsverteilung zu erreichen. Ein prinzipieller Ansatz zur
Auswahl dieser Buckets ist die Histogramm-Entzerrung, bei der die Bins des Histo-
gramms auf der Grundlage der Quantile der Rohverteilung ausgewählt werden (siehe das dritte Feld in
Abbildung 2-4). Im Idealfall führt die Histogramm-Entzerrung zu einer gleichmäßigen Verteilung.
Verteilung (obwohl dies in diesem Fall nicht der Fall ist, da sich die Werte in den Quantilen wiederholen).

Die Histogrammausgleichung in BigQuery kann wie folgt durchgeführt werden:

ML.BUCKETIZE(num_views, bins) AS bin
woher die Bins bezogen werden:

APPROX_QUANTILES(num_views, 100) AS bins
Alle Einzelheiten finden Sie im Notebook im Code-Repository dieses Buches.

Eine weitere Methode zur Behandlung schiefer Verteilungen ist die Verwendung einer parametrischen Transforma- tion
tionstechnik wie die Box-Cox-Transformation. Box-Cox wählt seinen einzigen Parameter,
lambda, um die "Heteroskedastizität" zu kontrollieren, so dass die Varianz nicht mehr von der
dem Betrag abhängt. In diesem Fall ist die Varianz zwischen selten angesehenen Wikipedia-Seiten
sehr viel kleiner sein als die Varianz zwischen häufig aufgerufenen Seiten, und Box-Cox versucht, die
und Box-Cox versucht, die Varianz über alle Bereiche der Anzahl der Aufrufe auszugleichen. Dies kann geschehen mit
Pythons SciPy-Paket:

traindf['boxcox'], est_lambda = (
scipy.stats.boxcox(traindf['num_views']))
Der über den Trainingsdatensatz geschätzte Parameter (est_lambda) wird dann verwendet, um
andere Werte zu transformieren:

evaldf['boxcox'] = scipy.stats.boxcox(evaldf['num_views'], est_lambda)
Array von Zahlen

Manchmal sind die Eingabedaten eine Reihe von Zahlen. Wenn das Array eine feste Länge hat, kann die Daten
die Darstellung der Daten recht einfach sein: Das Array wird geglättet und jede Position als separates Merkmal behandelt.
arate Merkmal. Oft hat die Matrix jedoch eine variable Länge. Zum Beispiel, eine der
Modells zur Vorhersage der Verkäufe eines Sachbuchs könnten die Verkäufe aller früheren
früheren Büchern zu diesem Thema sein. Eine Beispieleingabe könnte sein:

[2100, 15200, 230000, 1200, 300, 532100]
Natürlich variiert die Länge dieses Feldes in jeder Zeile, da es eine unterschiedliche
Anzahl von Büchern, die zu verschiedenen Themen veröffentlicht wurden.

Einfache Datendarstellungen | 27
Gängige Redewendungen für den Umgang mit Zahlenreihen sind unter anderem die folgenden:

Darstellung der Eingabematrix in Bezug auf ihre Massenstatistiken. Zum Beispiel könnten wir
die Länge (d.h. die Anzahl der bisherigen Bücher zum Thema), den Durchschnitt, den Median, den Mini-
mum, Maximum, und so weiter.
Die Darstellung des Eingabefeldes anhand seiner empirischen Verteilung - d.h. durch die
10/20/... Perzentil und so weiter.
Wenn das Array auf eine bestimmte Art und Weise geordnet ist (z.B. nach der Zeit oder nach der Größe),
das Eingabefeld nach den letzten drei oder einer anderen festen Anzahl von
Elemente. Bei Arrays mit einer Länge von weniger als drei wird das Merkmal auf eine Länge von
drei mit fehlenden Werten aufgefüllt.
All dies führt dazu, dass das Datenfeld mit variabler Länge als ein Merkmal mit fester Länge dargestellt wird.
Merkmal. Wir hätten dieses Problem auch als Zeitreihenvorhersageproblem formulieren können.
Vorhersageproblem formulieren können, d. h. das Problem der Vorhersage der Verkäufe des nächsten Buches zu diesem Thema auf der Grundlage der
Zeitverlauf der Verkäufe früherer Bücher. Durch die Behandlung der Verkäufe früherer Bücher als
Array-Input behandeln, gehen wir davon aus, dass die wichtigsten Faktoren für die Vorhersage der Verkaufszahlen eines Buches
die wichtigsten Faktoren für die Vorhersage der Verkäufe eines Buches sind (Autor, Verlag, Rezensionen usw.) und
und nicht die zeitliche Kontinuität der Verkaufszahlen.

Kategoriale Eingaben
Da die meisten modernen, groß angelegten Modelle für maschinelles Lernen (Random Forests, Support
Vektormaschinen, neuronale Netze) mit numerischen Werten arbeiten, müssen kategorische Eingaben
als Zahlen dargestellt werden.

Eine einfache Aufzählung der möglichen Werte und deren Zuordnung zu einer Ordinalskala wird
nicht gut funktionieren. Angenommen, eine der Eingaben für das Modell zur Vorhersage der Verkaufszahlen eines
Sachbuches ist die Sprache, in der das Buch geschrieben ist. Wir können nicht einfach eine
Mapping-Tabelle wie diese erstellen:

Kategorische Eingabe Numerisches Merkmal
Englisch 1.0
Chinesisch 2.0
Deutsch 3.0
Dies liegt daran, dass das maschinelle Lernmodell dann versucht, zwischen der
zwischen der Popularität von deutschen und englischen Büchern zu interpolieren, um die Popularität des Buches auf Chinesisch zu erhalten!
nesisch zu erhalten! Da es keine ordinale Beziehung zwischen den Sprachen gibt, müssen wir eine
eine kategoriale bis numerische Abbildung verwenden, die es dem Modell ermöglicht, den Markt für Bücher
die in diesen Sprachen geschrieben sind, unabhängig zu lernen.

28 | Kapitel 2: Entwurfsmuster für die Datendarstellung

One-Hot-Codierung

Die einfachste Methode, um kategoriale Variablen abzubilden und gleichzeitig sicherzustellen, dass die Variablen
ables unabhängig sind, ist die One-Hot-Codierung. In unserem Beispiel würde die kategoriale Eingangsvariable
in einen Drei-Elemente-Merkmalsvektor umgewandelt, indem die folgende
Abbildung:

Kategoriale Eingabe Numerisches Merkmal
Englisch [1.0, 0.0, 0.0]
Chinesisch [0,0, 1,0, 0,0]
Deutsch [0.0, 0.0, 1.0]
Die One-Hot-Kodierung erfordert, dass wir das Vokabular der kategorialen Eingabe vorher kennen.
Hand. Hier besteht das Vokabular aus drei Token (Englisch, Chinesisch und Deutsch),
und die Länge des resultierenden Merkmals ist die Größe dieses Vokabulars.

Dummy-Codierung oder One-Hot-Codierung?
Technisch gesehen reicht ein 2-Elemente-Merkmalsvektor aus, um eine eindeutige Zuordnung für ein
Vokabular der Größe 3:
Kategoriale Eingabe Numerisches Merkmal
Englisch [0.0, 0.0]
Chinesisch [1.0, 0.0]
Deutsch [0.0, 1.0]
Dies wird als Dummy-Kodierung bezeichnet. Da die Dummy-Kodierung eine kompaktere Repräsenta- tion ist, wird sie
Da die Dummy-Kodierung eine kompaktere Darstellung ist, wird sie in statistischen Modellen bevorzugt, die besser funktionieren, wenn die Eingaben zeilenunabhängig sind.
arly unabhängig sind.
Moderne Algorithmen des maschinellen Lernens verlangen jedoch nicht, dass ihre Eingaben zeilenunabhängig sind.
und verwenden Methoden wie die L1-Regularisierung, um redundante Eingaben auszusortieren.
Eingaben. Der zusätzliche Freiheitsgrad ermöglicht es dem System, eine fehlende
eine fehlende Eingabe in der Produktion transparent als Nullen zu behandeln:
Kategoriale Eingabe Numerisches Merkmal
Englisch [1.0, 0.0, 0.0]
Chinesisch [0,0, 1,0, 0,0]
Deutsch [0,0, 0,0, 1,0]
(fehlt) [0.0, 0.0, 0.0]
Daher unterstützen viele Frameworks für maschinelles Lernen oft nur die Ein-Hot-Kodierung.
Einfache Datenrepräsentationen | 29
2 Bei Zwillingen ist die Mehrzahl 2, bei Drillingen ist die Mehrzahl 3.
Unter bestimmten Umständen kann es hilfreich sein, eine numerische Eingabe als kategorisch zu behandeln und
sie auf eine One-Hot-codierte Spalte abzubilden:

Wenn die numerische Eingabe ein Index ist
Wenn wir zum Beispiel versuchen, das Verkehrsaufkommen vorherzusagen, und eine unserer Eingaben ist der
Tag der Woche ist, könnten wir den Wochentag als numerisch behandeln (1, 2, 3, ..., 7), aber
aber es ist hilfreich zu erkennen, dass der Wochentag hier keine kontinuierliche Skala ist
sondern wirklich nur ein Index ist. Es ist besser, ihn als kategorisch zu behandeln (Sonntag, Montag, ...,
Samstag), da die Indizierung willkürlich ist. Soll die Woche am Sonntag beginnen (wie
in den USA), Montag (wie in Frankreich) oder Samstag (wie in Ägypten)?

Wenn die Beziehung zwischen Eingabe und Bezeichnung nicht kontinuierlich ist
Was dafür sprechen sollte, den Wochentag als kategoriales Merkmal zu behandeln
ist, dass das Verkehrsaufkommen am Freitag nicht durch das Verkehrsaufkommen am Donnerstag und
Samstag beeinflusst wird.

Wann ist es vorteilhaft, die numerische Variable zu schaufeln?
In den meisten Städten hängt das Verkehrsaufkommen davon ab, ob es Wochenende ist, und das kann
Dies kann je nach Ort variieren (Samstag und Sonntag in den meisten Ländern der Welt, Donnerstag und Freitag in einigen islamischen Ländern).
Tag in einigen islamischen Ländern). In diesem Fall wäre es hilfreich, den Wochentag
als ein boolesches Merkmal (Wochenende oder Wochentag) zu behandeln. Ein solches Mapping, bei dem die Anzahl
der eindeutigen Eingaben (hier sieben) größer ist als die Anzahl der eindeutigen Merkmalswerte
Werte (hier zwei) ist, wird Bucketing genannt. Üblicherweise erfolgt die Gruppierung in Form von
Bereichen durchgeführt - zum Beispiel könnte man mother_age in Bereiche einteilen, die bei 20,
25, 30 usw. unterteilt werden und jedes dieser Felder als kategorisch behandelt werden, aber es sollte klar sein
dass dadurch die ordinale Natur von mother_age verloren geht.

Wenn wir verschiedene Werte der numerischen Eingabe als unabhängig behandeln wollen
wenn es um ihre Auswirkungen auf das Etikett geht
Zum Beispiel hängt das Gewicht eines Babys von der Pluralität^2 der Geburt ab, da
Zwillinge und Drillinge tendenziell weniger wiegen als Einzelgeburten. So kann ein Baby mit geringerem Gewicht, wenn es
Teil eines Drillings ist, gesünder sein als ein Zwillingsbaby mit demselben Gewicht. In diesem
Fall könnten wir die Pluralität auf eine kategoriale Variable abbilden, da eine kategoriale Variable dem Modell
kategorische Variable es dem Modell ermöglicht, unabhängige, einstellbare Parameter für die verschiedenen
Werte der Pluralität. Natürlich können wir dies nur tun, wenn wir genügend Beispiele von
Zwillinge und Drillinge in unserem Datensatz haben.

30 | Kapitel 2: Entwurfsmuster für die Datendarstellung

Array mit kategorialen Variablen

Manchmal sind die Eingabedaten ein Array von Kategorien. Wenn das Array eine feste Länge hat, können wir
können wir jede Array-Position als ein separates Merkmal behandeln. Aber oft hat das Array eine variable
variabler Länge. Eine der Eingaben für das Geburtsmodell könnte zum Beispiel die Art der
früheren Geburten dieser Mutter sein:

(Eingeleitet, Eingeleitet, Natürlich, Kaiserschnitt)
Natürlich variiert die Länge dieses Feldes in jeder Zeile, da es für jedes Baby eine unterschiedliche
Anzahl von älteren Geschwistern für jedes Baby gibt.

Gängige Redewendungen zur Handhabung von Arrays kategorischer Variablen sind unter anderem die folgenden:

Zählen der Anzahl der Vorkommen der einzelnen Vokabeln. Die Repräsenta- tion für unser Beispiel wäre also
tion für unser Beispiel wäre [2, 1, 1] unter der Annahme, dass das Vokabular
Eingeleitet, Natürlich und Kaiserschnitt (in dieser Reihenfolge). Dies ist nun ein Zahlenfeld fester Länge
Array mit fester Länge, das geglättet und in Positionsreihenfolge verwendet werden kann. Wenn wir ein
Array, in dem ein Element nur einmal vorkommen kann (z. B. die Sprachen, die eine Person
spricht), oder wenn das Merkmal nur das Vorhandensein und nicht die Anzahl angibt (z. B. ob
die Mutter jemals einen Kaiserschnitt hatte), dann ist die Anzahl an jeder Position
0 oder 1, und dies wird als Multi-Hot-Codierung bezeichnet.
Um große Zahlen zu vermeiden, kann anstelle der Zählung die relative Häufigkeit verwendet werden.
Die Darstellung für unser Beispiel wäre dann [0,5, 0,25, 0,25] statt [2,
1, 1]. Leere Felder (erstgeborene Babys ohne vorherige Geschwister) werden dargestellt
als [0, 0, 0] dargestellt. In der natürlichen Sprachverarbeitung wird die relative Häufigkeit eines Wortes
Wortes durch die relative Häufigkeit der Dokumente, die das Wort enthalten, normalisiert
um TF-IDF (kurz für Term Frequency-Inverse Document Frequency) zu erhalten.
TF-IDF spiegelt wider, wie eindeutig ein Wort in einem Dokument ist.
Wenn das Array auf eine bestimmte Weise geordnet ist (z.B. in zeitlicher Reihenfolge), wird das
Eingabefeld durch die letzten drei Elemente. Arrays, die kürzer als drei sind, werden mit
fehlenden Werten aufgefüllt.
Darstellung des Arrays durch Massenstatistiken, z. B. die Länge des Arrays, den Modus
(häufigster Eintrag), der Median, das 10/20/... Perzentil, usw.
Davon ist die Zählung/relative Häufigkeit die häufigste Form. Beachten Sie, dass beide
eine Verallgemeinerung der One-Hot-Codierung sind - wenn das Baby keine älteren Geschwister hätte,
wäre die Darstellung [0, 0, 0], und wenn das Baby ein älteres Geschwisterchen hätte, das
bei einer natürlichen Geburt geboren wurde, wäre die Darstellung [0, 1, 0].

Nachdem wir einfache Datendarstellungen gesehen haben, wollen wir nun Entwurfsmuster diskutieren, die bei der
Datendarstellung helfen.

Einfache Datendarstellungen | 31
Entwurfsmuster 1: Hashed Feature
Das Entwurfsmuster "Hashed Feature" adressiert drei mögliche Probleme im Zusammenhang mit
kategorischen Merkmalen verbunden sind: unvollständiges Vokabular, Modellgröße aufgrund der Kardinalität und Kaltstart
Start. Dies geschieht durch die Gruppierung der kategorialen Merkmale und die Inkaufnahme von
Kollisionen in der Datendarstellung.

Problem
Die One-Hot-Codierung einer kategorialen Eingangsvariablen erfordert die Kenntnis des Vokabulars
im Voraus zu kennen. Dies ist kein Problem, wenn es sich bei der Eingabevariablen um etwas wie die Sprache
ein Buch geschrieben ist oder der Wochentag, an dem das Verkehrsaufkommen vorhergesagt werden soll.

Was ist, wenn es sich bei der fraglichen kategorialen Variable um etwas wie die Krankenhaus_id des
wo das Baby geboren wurde oder die Arzt_id der Person, die das Baby entbindet? Katego-
gorische Variablen wie diese werfen ein paar Probleme auf:

Um das Vokabular zu kennen, muss es aus den Trainingsdaten extrahiert werden. Aufgrund von
Stichproben ist es möglich, dass die Trainingsdaten nicht alle möglichen
möglichen Krankenhäuser oder Ärzte enthalten. Das Vokabular könnte unvollständig sein.
Die kategorialen Variablen haben eine hohe Kardinalität. Anstatt Merkmalsvektoren
mit drei Sprachen oder sieben Tagen, haben wir Merkmalsvektoren, deren Länge in die
Tausenden bis Millionen liegt. Solche Merkmalsvektoren werfen in der Praxis mehrere Probleme auf.
Sie beinhalten so viele Gewichte, dass die Trainingsdaten möglicherweise unzureichend sind. Selbst wenn
wir das Modell trainieren können, wird das trainierte Modell viel Speicherplatz benötigen
weil das gesamte Vokabular zum Zeitpunkt der Ausgabe benötigt wird. Daher kann es sein, dass wir nicht in der Lage sind
das Modell auf kleineren Geräten einzusetzen.
Nachdem das Modell in Betrieb genommen wurde, werden möglicherweise neue Krankenhäuser gebaut und neue
Ärzte eingestellt werden. Das Modell wird nicht in der Lage sein, dafür Vorhersagen zu treffen, und daher
ist eine separate Infrastruktur erforderlich, um solche Kaltstartprobleme zu bewältigen.
Probleme zu bewältigen.
Selbst bei einfachen Darstellungen wie der One-Hot-Codierung ist es sinnvoll
Es lohnt sich, das Cold-Start-Problem zu antizipieren und explizit alle
Nullen für Eingaben außerhalb des Vokabulars zu reservieren.
Nehmen wir als konkretes Beispiel das Problem der Vorhersage der Ankunftsverspätung eines
Fluges. Eine der Eingaben für das Modell ist der Abflughafen. Es gab zum Zeitpunkt
der Datensatz gesammelt wurde, 347 Flughäfen in den Vereinigten Staaten:

32 | Kapitel 2: Entwurfsmuster für die Datendarstellung

SELECT
DISTINCT (abflug_flughafen)
FROM `bigquery-samples.airline_ontime_data.flights`
Einige Flughäfen hatten nur ein bis drei Flüge über den gesamten Zeitraum, so dass wir
erwarten wir, dass das Vokabular der Trainingsdaten unvollständig sein wird. 347 ist groß genug, dass
dass das Merkmal recht spärlich sein wird, und es ist sicher der Fall, dass neue Flughäfen
gebaut. Alle drei Probleme (unvollständiges Vokabular, hohe Kardinalität, Kaltstart) werden
bestehen, wenn wir den Abflughafen einmalig kodieren.

Der Fluggesellschaftsdatensatz ist, wie der Natalitätsdatensatz und fast alle anderen Datensätze, die wir
in diesem Buch zur Veranschaulichung verwenden, ist ein öffentlicher Datensatz in BigQuery, sodass Sie die
Abfrage ausprobieren. Zum Zeitpunkt, an dem wir dies schreiben, ist die Abfrage für 1 TB/Monat kostenlos, und es gibt
Es gibt eine Sandbox, mit der Sie BigQuery bis zu dieser Grenze nutzen können, ohne
ohne eine Kreditkarte zu hinterlegen. Wir empfehlen Ihnen, unser GitHub-Repository als Lesezeichen zu speichern. Für
siehe zum Beispiel das Notebook in GitHub für den vollständigen Code.

Lösung
Das Entwurfsmuster Hashed Feature stellt eine kategorische Eingabevariable wie folgt dar
das Folgende:

Umwandlung der kategorialen Eingabe in eine eindeutige Zeichenkette. Für den Abflughafen,
können wir den dreibuchstabigen IATA-Code des Flughafens verwenden.
Aufrufen eines deterministischen (keine zufälligen Seeds oder Salt) und portablen (so dass der
(so dass derselbe Algorithmus sowohl für das Training als auch für den Dienst verwendet werden kann) auf
der Zeichenkette.
Ermittlung des Rests, wenn das Hash-Ergebnis durch die gewünschte Anzahl von
Eimern. Normalerweise gibt der Hash-Algorithmus eine ganze Zahl zurück, die negativ sein kann
und das Modulo einer negativen ganzen Zahl ist negativ. Daher wird der absolute Wert des
Ergebnis genommen.
In BigQuery SQL werden diese Schritte wie folgt erreicht:

ABS ( MOD (FARM_FINGERPRINT(airport), numbuckets))
Die Funktion FARM_FINGERPRINT verwendet FarmHash, eine Familie von Hashing-Algorithmen, die
Familie von Hash-Algorithmen, die deterministisch und gut verteilt ist und für die es Implementierungen in einer
einer Reihe von Programmiersprachen verfügbar sind.

In TensorFlow werden diese Schritte durch die Funktion feature_column implementiert:

tf.feature_column.categorical_column_with_hash_bucket(
airport, num_buckets, dtype=tf.dtypes.string)
Tabelle 2-1 zeigt zum Beispiel den FarmHash einiger IATA-Flughafencodes, wenn
Hash in 3, 10 und 1.000 Buckets.

Entwurfsmuster 1: Hashed Feature | 33
Tabelle 2-1. Der FarmHash einiger IATA-Flughafencodes, wenn er in verschiedene
Anzahl von Buckets

Zeile Abflug_Flughafen hash3 hash10 hash1000
1 DTW 1 3 543
2 LBB 2 9 709
3 SNA 2 7 587
4 MSO 2 7 737
5 ANC 0 8 508
6 PIT 1 7 267
7 PWM 1 9 309
8 BNA 1 4 744
9 SAF 1 2 892
10 IPL 2 1 591
Warum es funktioniert
Angenommen, wir haben uns dafür entschieden, den Flughafencode mit Hilfe von 10 Buckets zu hashen (hash10 in
Tabelle 2-1). Wie lassen sich damit die Probleme lösen, die wir identifiziert haben?

Eingabe außerhalb des Vokabulars

Selbst wenn ein Flughafen mit einer Handvoll Flügen nicht Teil des Trainingsdatensatzes ist, liegt sein
Hash-Merkmalswert im Bereich [0-9] liegen. Daher gibt es kein Resilienz-Problem
der unbekannte Flughafen erhält die Vorhersagen, die den anderen Flughäfen im
mit anderen Flughäfen im Hash-Bucket. Das Modell wird nicht ausfallen.

Wenn wir 347 Flughäfen haben, erhalten durchschnittlich 35 Flughäfen denselben Hash Bucket Code, wenn
wir sie in 10 Hash-Bereiche aufteilen. Ein Flughafen, der im Trainingsdatensatz fehlt, wird
seine Merkmale von den anderen ähnlichen ~35 Flughäfen im Hash-Bucket "leihen". Unter
Natürlich wird die Vorhersage für einen fehlenden Flughafen nicht genau sein (es ist unvernünftig, für
Es ist unvernünftig, genaue Vorhersagen für unbekannte Eingaben zu erwarten, aber sie wird im richtigen Bereich liegen.

Wählen Sie die Anzahl der Hash-Buckets, indem Sie einen Ausgleich zwischen der Notwendigkeit, Eingaben außerhalb des
Vokabulareingaben vernünftig zu handhaben und die Notwendigkeit, dass das Modell die
kategorialen Eingaben widerspiegelt. Bei 10 Hash-Buckets werden ~35 Flughäfen vermischt. Eine gute Faustformel
Faustregel ist, die Anzahl der Hash-Buckets so zu wählen, dass jeder Bucket etwa fünf
Einträge erhält. In diesem Fall würde das bedeuten, dass 70 Hash-Buckets ein guter Kompromiss sind.

Hohe Kardinalität

Es ist leicht zu erkennen, dass das Problem der hohen Kardinalität gelöst ist, solange wir eine
eine ausreichend kleine Anzahl von Hash-Eimern wählen. Selbst wenn wir Millionen von Flughäfen, Krankenhäusern oder Ärzten haben
tals oder Ärzten haben, können wir sie in ein paar hundert Hash-Bereiche einteilen und so die
Speicher- und Modellgrößenanforderungen des Systems praktisch.

34 | Kapitel 2: Entwurfsmuster für die Datendarstellung

Wir brauchen das Vokabular nicht zu speichern, da der Transformationscode unabhängig vom
des tatsächlichen Datenwerts ist und der Kern des Modells sich nur mit num_buckets
Eingaben, nicht mit dem gesamten Vokabular.

Es stimmt, dass Hashing verlustbehaftet ist - da wir 347 Flughäfen haben, erhalten durchschnittlich 35 Flughäfen
denselben Hash-Bucket-Code erhalten, wenn wir ihn in 10 Buckets hashen. Wenn die Alternative
Alternative ist, die Variable zu verwerfen, weil sie zu breit ist, ist eine verlustbehaftete Kodierung ein akzeptabler
akzeptabler Kompromiss.

Kaltstart

Die Kaltstart-Situation ähnelt der Situation, in der kein Vokabular mehr vorhanden ist. Wenn ein neuer Flughafen
dem System hinzugefügt wird, erhält er zunächst die Vorhersagen, die anderen
Flughäfen im Hash-Bucket. Je beliebter ein Flughafen wird, desto mehr Flüge gibt es von
diesem Flughafen. Solange wir das Modell in regelmäßigen Abständen neu trainieren, werden seine Vorhersagen beginnen
Ankunftsverspätungen des neuen Flughafens widerspiegeln. Dies wird im Detail im Abschnitt
"Entwurfsmuster 18: Fortgesetzte Modellauswertung" auf Seite 220 in Kapitel 5.

Indem wir die Anzahl der Hash-Bereiche so wählen, dass jeder Bereich etwa fünf Einträge erhält,
können wir sicherstellen, dass jeder Bucket vernünftige Anfangsergebnisse hat.

Zielkonflikte und Alternativen
Die meisten Entwurfsmuster beinhalten irgendeine Art von Kompromiss, und das Entwurfsmuster Hashed Feature
Muster ist da keine Ausnahme. Der wichtigste Kompromiss besteht darin, dass wir die Modellgenauigkeit verlieren.

Eimerkollision

Der Modulo-Teil der Hashed-Feature-Implementierung ist eine verlustbehaftete Operation. Durch
eine Hashkorbgröße von 100 wählen, entscheiden wir uns dafür, dass sich 3-4 Flughäfen einen
Bucket teilen. Damit gehen wir explizit Kompromisse bei der Fähigkeit ein, die Daten genau darzustellen
(mit einem festen Vokabular und einer One-Hot-Codierung), um Eingaben außerhalb des Vokabulars
Eingaben, Kardinalitäts-/Modellgrößenbeschränkungen und Kaltstartprobleme zu bewältigen. Das ist kein kostenloses
Mittagessen. Wählen Sie Hashed Feature nicht, wenn Sie das Vokabular im Voraus kennen, wenn die
Vokabular relativ klein ist (Tausende von Beispielen sind für einen Datensatz mit
Millionen von Beispielen) und wenn ein Kaltstart kein Problem darstellt.

Beachten Sie, dass wir die Anzahl der Eimer nicht einfach auf eine extrem hohe Zahl erhöhen können
Zahl erhöhen, in der Hoffnung, Kollisionen ganz zu vermeiden. Selbst wenn wir die Anzahl der Buckets
auf 100.000 mit nur 347 Flughäfen erhöhen, ist die Wahrscheinlichkeit, dass sich mindestens zwei Flughäfen denselben
mindestens zwei Flughäfen denselben Hash-Bucket teilen, ist 45 % - inakzeptabel hoch (siehe Tabelle 2-2). Daher sollten wir
Hashed Features nur verwenden, wenn wir bereit sind, mehrere kategoriale Eingaben zu tolerieren
die denselben Hash Bucket-Wert haben.

Entwurfsmuster 1: Hashed Feature | 35
Tabelle 2-2. Die erwartete Anzahl von Einträgen pro Bucket und die Wahrscheinlichkeit von mindestens einer
Kollision, wenn die IATA-Flughafencodes in eine unterschiedliche Anzahl von Buckets gehasht werden

num_hash_buckets entries_per_bucket collision_prob
3 115.666667 1.000000
10 34.700000 1.000000
100 3.470000 1.000000
1000 0.347000 1.000000
10000 0.034700 0.997697
100000 0.003470 0.451739
Schräglage

Der Genauigkeitsverlust ist besonders groß, wenn die Verteilung der kategorialen
Eingabe stark verzerrt ist. Nehmen wir den Fall des Hash-Buckets, der ORD (Chi-
cago, einer der verkehrsreichsten Flughäfen der Welt). Wir können dies wie folgt ermitteln:

CREATE TEMPORARY FUNCTION hashed(airport STRING, numbuckets INT64) AS (
ABS ( MOD (FARM_FINGERPRINT(airport), numbuckets))
);
WITH airports AS (
SELECT
abflug_flughafen, COUNT (1) AS num_flights
FROM `bigquery-samples.airline_ontime_data.flights`
GROUP BY abflug_flughafen
)
SELECT
abflug_flughafen, anzahl_flüge
FROM flughäfen
WHERE hashed(abflug_flughafen, 100) = hashed('ORD', 100)
Das Ergebnis zeigt, dass es zwar ~3,6 Millionen Flüge von ORD gibt, aber nur
~67.000 Flüge von BTV (Burlington, Vermont):

Abflug_Flughafen num_flights
ORD 3610491
BTV 66555
MCI 597761
Dies zeigt, dass das Modell für alle praktischen Zwecke die langen Rollzeiten
Rollzeiten und wetterbedingte Verspätungen, die in Chicago auftreten, auf den städtischen Flughafen in
Burlington, Vermont! Die Modellgenauigkeit für BTV und MCI (Flughafen Kansas City)
wird ziemlich schlecht sein, weil es so viele Flüge von Chicago aus gibt.

36 | Kapitel 2: Entwurfsmuster für die Datendarstellung

Aggregiertes Merkmal

In Fällen, in denen die Verteilung einer kategorialen Variable schief ist oder die Anzahl der
so klein ist, dass es häufig zu Kollisionen kommt, kann es hilfreich sein
ein aggregiertes Merkmal als Eingabe für unser Modell hinzuzufügen. Zum Beispiel könnten wir für jeden Flughafen,
für jeden Flughafen die Wahrscheinlichkeit pünktlicher Flüge im Trainingsdatensatz und fügen sie als
Merkmal zu unserem Modell hinzufügen. Auf diese Weise können wir vermeiden, dass die Informationen, die mit
einzelnen Flughäfen, wenn wir die Flughafencodes hashen. In einigen Fällen könnten wir
den Flughafennamen nicht als Merkmal zu verwenden, da die relative Häufigkeit der
pünktlicher Flüge ausreichend sein könnte.

Abstimmung der Hyperparameter

Aufgrund der Kompromisse bei der Häufigkeit von Eimerkollisionen kann die Wahl der Anzahl von
Eimer schwierig sein. Sie hängt sehr oft vom Problem selbst ab. Daher empfehlen wir
empfehlen wir, die Anzahl der Buckets als einen Hyperparameter zu betrachten, der abgestimmt wird:

parameterName : nbuckets
Typ: INTEGER
minWert: 10
maxWert: 20
scaleType: UNIT_LINEAR_SCALE
Stellen Sie sicher, dass die Anzahl der Bereiche innerhalb eines sinnvollen Bereichs der Kardinalität
der kategorialen Variable bleibt, die gehasht wird.

Kryptographischer Hash

Was das Hashed Feature verlustbehaftet macht, ist der Modulo-Teil der Implementierung.
Was wäre, wenn wir das Modulo ganz vermeiden würden? Schließlich hat der Farm-Fingerabdruck eine
eine feste Länge (ein INT64 hat 64 Bits), so dass er mit 64 Merkmalswerten dargestellt werden kann, die jeweils 0 oder
dargestellt werden, von denen jeder 0 oder 1 ist. Dies wird als Binärkodierung bezeichnet.

Die binäre Kodierung löst jedoch nicht das Problem von Eingaben ohne Vokabular oder
Kaltstart (nur das Problem der hohen Kardinalität). In der Tat ist die bitweise Kodierung ein
Hering. Wenn wir kein Modulo machen, können wir eine eindeutige Darstellung erhalten, indem wir einfach
die drei Zeichen, die den IATA-Code bilden, kodieren (also ein Merkmal der Länge
3*26=78). Das Problem mit dieser Darstellung ist sofort offensichtlich: Flughäfen
deren Namen mit dem Buchstaben O beginnen, haben in Bezug auf ihre Flugverspätungsmerkmale nichts gemeinsam
Flugverspätungseigenschaften - die Kodierung hat eine falsche Korrelation zwischen
Flughäfen, die mit demselben Buchstaben beginnen. Die gleiche Erkenntnis gilt auch für den binären Raum.
Aus diesem Grund raten wir von der binären Kodierung von Farm-Fingerprint-Werten ab.

Die binäre Kodierung eines MD5-Hashes leidet nicht unter diesem falschen Korrelationsproblem.
lem, da die Ausgabe eines MD5-Hashes gleichmäßig verteilt ist und somit auch die resultierenden
Bits gleichmäßig verteilt sein werden. Anders als beim Farm-Fingerprint-Algorithmus,

Entwurfsmuster 1: Hash-Merkmal | 37
der MD5-Hash ist nicht deterministisch und nicht eindeutig - er ist ein Einweg-Hash und wird
viele unerwartete Kollisionen auftreten.

Bei dem Entwurfsmuster "Hashed Feature" müssen wir einen Fingerabdruck-Hashing-Algorithmus verwenden
und nicht einen kryptographischen Hash-Algorithmus. Der Grund dafür ist, dass das Ziel einer Fingerabdruck
Funktion ist es, einen deterministischen und eindeutigen Wert zu erzeugen. Wenn Sie darüber nachdenken, ist dies eine
eine wichtige Anforderung an Vorverarbeitungsfunktionen beim maschinellen Lernen, da wir
die gleiche Funktion während der Modellierung anwenden und den gleichen Hash-Wert erhalten. Eine Fin-
gerprint-Funktion erzeugt keine gleichmäßig verteilte Ausgabe. Kryptografische
Algorithmen wie MD5 oder SHA1 erzeugen zwar eine gleichmäßig verteilte Ausgabe, aber sie
aber sie sind nicht deterministisch und absichtlich rechenintensiv.
Daher ist ein kryptografischer Hash nicht in einem Feature-Engineering-Kontext verwendbar, in dem
der Hash-Wert, der für eine bestimmte Eingabe während der Vorhersage berechnet wird, derselbe sein muss wie
derselbe sein muss wie der beim Training berechnete Hash-Wert, und die Hash-Funktion das
das maschinelle Lernmodell nicht verlangsamen darf.

Der Grund dafür, dass MD5 nicht deterministisch ist, liegt darin, dass der zu verschlüsselnden Zeichenkette in der Regel ein "Salt" hinzugefügt wird.
der zu hashenden Zeichenfolge hinzugefügt wird. Das Salt ist eine zufällige Zeichenfolge, die
zu jedem Kennwort hinzugefügt wird, um sicherzustellen, dass selbst wenn zwei Benutzer das
dasselbe Passwort verwenden, der gehashte Wert in der Datenbank unterschiedlich sein wird.
Dies ist erforderlich, um Angriffe auf der Grundlage von "Regenbogentabellen" zu vereiteln, die
Angriffe, die auf Wörterbüchern mit häufig gewählten Passwörtern beruhen
Passwörtern beruhen und die den Hash-Wert des bekannten Passworts mit
Hashes in der Datenbank vergleichen. Da die Rechenleistung gestiegen ist, ist es
ist es möglich, einen Brute-Force-Angriff auf jedes mögliche Salz auszuführen
durchzuführen, weshalb moderne kryptografische Implementierungen ihren Hash
in einer Schleife, um den Rechenaufwand zu erhöhen. Selbst wenn wir
Selbst wenn wir das Salz ausschalten und die Anzahl der Iterationen auf eine reduzieren würden,
ist der MD5-Hash nur eine Möglichkeit. Er wird nicht eindeutig sein.
Unterm Strich müssen wir also einen Fingerabdruck-Hash-Algorithmus verwenden und
den resultierenden Hash zu modulieren.

Reihenfolge der Maßnahmen

Beachten Sie, dass wir zuerst den Modulo und dann den Absolutwert berechnen:

CREATE TEMPORARY FUNCTION hashed(airport STRING, numbuckets INT64) AS (
ABS ( MOD (FARM_FINGERPRINT(airport), numbuckets))
);
Die Reihenfolge von ABS, MOD und FARM_FINGERPRINT im vorangehenden Ausschnitt ist wichtig
weil der Bereich von INT64 nicht symmetrisch ist. Genauer gesagt, liegt sein Bereich zwischen
-9.223.372.036.854.775.808 und 9.223.372.036.854.775.807 (jeweils einschließlich).
Wenn wir also tun würden:

38 | Kapitel 2: Entwurfsmuster für die Datendarstellung

3 Dieser Datensatz ist in BigQuery verfügbar: bigquery-public-data.samples.natality.
ABS(FARM_FINGERPRINT(Flughafen))
würden wir auf einen seltenen und wahrscheinlich nicht reproduzierbaren Überlauffehler stoßen, wenn die
FARM_FINGERPRINT Operation zufällig -9.223.372.036.854.775.808
zurückgeben würde, da sein absoluter Wert nicht mit einem INT64 dargestellt werden kann!

Leere Hash-Eimer

Auch wenn es unwahrscheinlich ist, besteht die geringe Möglichkeit, dass selbst bei der Auswahl von 10 Hash-Buckets
ets für 347 Flughäfen wählen, einer der Hash-Buckets leer sein wird. Daher kann bei der
Hash-Merkmalsspalten zu verwenden, kann es daher von Vorteil sein, auch die L2-Regularisierung zu verwenden, damit
damit die Gewichte, die mit einem leeren Bucket assoziiert sind, fast auf Null gesetzt werden. Diese
Fall, dass ein Flughafen außerhalb des Vokabulars in einen leeren Bereich fällt, führt dies nicht zu
das Modell numerisch instabil wird.

Entwurfsmuster 2: Einbettungen
Einbettungen sind eine lernfähige Datendarstellung, die Daten mit hoher Kardinalität in einem
Raum abbilden, und zwar so, dass die für das Lernproblem relevanten Informationen
Problem erhalten bleibt. Einbettungen sind das Herzstück des modernen maschinellen Lernens
und sind in diesem Bereich in verschiedenen Varianten anzutreffen.

Problem
Modelle des maschinellen Lernens suchen systematisch nach Mustern in Daten, die erfassen, wie
wie die Eigenschaften der Eingabemerkmale des Modells mit der Ausgabebezeichnung zusammenhängen. Infolgedessen hat die
Datendarstellung der Eingabemerkmale direkt die Qualität des endgültigen Modells beeinflussen.
Während der Umgang mit strukturierten, numerischen Eingaben recht einfach ist, können die Daten, die zum Trainieren eines
Daten, die zum Trainieren eines maschinellen Lernmodells benötigt werden, können in unzähligen Varianten
kategorische Merkmale, Text, Bilder, Audio, Zeitreihen und vieles mehr. Für diese Datenrepräsenta- tionen
Daten benötigen wir einen aussagekräftigen numerischen Wert für unser maschinelles Lernmodell, damit
damit diese Merkmale in das typische Trainingsparadigma passen. Einbettungen bieten eine Möglichkeit
mit einigen dieser unterschiedlichen Datentypen so umzugehen, dass die Ähnlichkeit zwischen
zwischen den Elementen und verbessert so die Fähigkeit unseres Modells, diese wesentlichen Muster zu lernen.

Die One-Hot-Codierung ist eine gängige Methode zur Darstellung kategorialer Eingabevariablen. Für
Beispiel: die Eingabe "Pluralität" im Datensatz über die Geburtenrate.^3 Dies ist eine kategorische
Eingabe, die sechs mögliche Werte hat: ['Single(1)', 'Multiple(2+)', 'Twins(2)',
'Drillinge(3)', 'Vierlinge(4)', 'Fünflinge(5)']. Wir können diese
kategorialen Eingaben mit einer One-Hot-Codierung umgehen, die jeden potenziellen Eingabe
Wert auf einen Einheitsvektor in R^6 abbildet, wie in Tabelle 2-3 gezeigt.

Entwurfsmuster 2: Einbettungen | 39
Tabelle 2-3. Ein Beispiel für die One-Hot-Codierung kategorialer Eingaben für den Natality-Datensatz

Pluralität One-hot-Kodierung
Einfach(1) [1,0,0,0,0,0]
Mehrfach(2+) [0,1,0,0,0,0]
Zwillinge(2) [0,0,1,0,0,0]
Drillinge(3) [0,0,0,1,0,0]
Vierlinge(4) [0,0,0,0,1,0]
Fünflinge(5) [0,0,0,0,0,0,1]
Bei dieser Kodierung werden sechs Dimensionen benötigt, um jede der verschiedenen
Kategorien zu repräsentieren. Sechs Dimensionen sind vielleicht nicht so schlecht, aber was wäre, wenn wir viel, viel mehr
Kategorien zu berücksichtigen wären?

Was wäre zum Beispiel, wenn unser Datensatz aus der Betrachtungshistorie unserer Videodatenbank bestünde
Videodatenbank besteht und unsere Aufgabe darin besteht, eine Liste neuer Videos vorzuschlagen, die den bisherigen
Video-Interaktionen vorzuschlagen? In diesem Szenario könnte das Feld customer_id Millionen von
eindeutige Einträge haben. In ähnlicher Weise könnte die video_id der zuvor angesehenen Videos
ebenfalls Tausende von Einträgen enthalten. One-Hot-Codierung kategorischer Merkmale mit hoher Kardinalität
wie video_ids oder customer_ids als Eingaben für ein maschinelles Lernmodell führt zu einer
spärlichen Matrix, die für eine Reihe von Algorithmen für maschinelles Lernen nicht gut geeignet ist.

Das zweite Problem bei der One-Hot-Codierung besteht darin, dass sie die kategorialen Variablen
als unabhängig behandelt. Die Datendarstellung für Zwillinge sollte jedoch nahe an der
der Datenrepräsentation für Drillinge und ziemlich weit von der Datenrepräsentation
für Fünflinge. Ein Vielfaches ist höchstwahrscheinlich ein Zwilling, könnte aber auch ein Drilling sein. Ein Beispiel,
Tabelle 2-4 zeigt eine alternative Darstellung der Pluralitätsspalte in einer niedrigeren
Dimension, die diese Ähnlichkeitsbeziehung wiedergibt.

Tabelle 2-4. Verwendung von Einbettungen niedrigerer Dimensionalität zur Darstellung der Pluralitätsspalte
im Datensatz zur Geburtenhäufigkeit.

Pluralität Kandidatenkodierung
Einfach(1) [1.0,0.0]
Mehrfach(2+) [0.0,0.6]
Zwillinge(2) [0.0,0.5]
Drillinge(3) [0.0,0.7]
Vierlinge(4) [0.0,0.8]
Fünflinge(5) [0.0,0.9]
Diese Zahlen sind natürlich willkürlich. Aber ist es möglich, die bestmögliche Repräsentation der
Darstellung der Pluralitätsspalte mit nur zwei Dimensionen für die Geburtswahrscheinlichkeit
lem? Das ist das Problem, das durch das Embeddings-Entwurfsmuster gelöst wird.

40 | Kapitel 2: Entwurfsmuster für die Datendarstellung

Das gleiche Problem der hohen Kardinalität und der abhängigen Daten tritt auch bei Bildern und
Text. Bilder bestehen aus Tausenden von Pixeln, die nicht unabhängig voneinander sind.
voneinander unabhängig sind. Texte in natürlicher Sprache stammen aus einem Wortschatz von mehreren zehntausend Wörtern.
Wörtern entnommen, und ein Wort wie gehen ist dem Wort laufen näher als dem Wort Buch.

Lösung
Das Entwurfsmuster "Embeddings" befasst sich mit dem Problem der Darstellung von Daten mit hoher
Daten mit hoher Kardinalität dicht in einer niedrigeren Dimension darzustellen, indem die Eingabedaten durch eine
Einbettungsschicht geleitet werden, die trainierbare Gewichte hat. Dadurch wird die hochdimensionale, kat-
egorische Eingangsvariable auf einen reellwertigen Vektor in einem niedrigdimensionalen Raum. Die
Gewichte zur Erstellung der dichten Repräsentation werden als Teil der Optimierung des Modells gelernt
des Modells gelernt (siehe Abbildung 2-5). In der Praxis führen diese Einbettungen zur Erfassung enger
Beziehungen in den Eingabedaten.

Abbildung 2-5. Die Gewichte einer Einbettungsschicht werden beim Training als Parameter gelernt
Training gelernt.

Da Einbettungen enge Beziehungen in den Eingabedaten erfassen
Daten in einer niedrigdimensionalen Darstellung erfassen, können wir eine Einbettungs
Einbettungsschicht als Ersatz für Clustering-Techniken (z. B. Kunden
(z.B. Kundensegmentierung) und Dimensionalitätsreduktionsverfahren wie
Hauptkomponentenanalyse (PCA). Die Gewichte der Einbettung werden
Gewichte werden in der Trainingsschleife des Hauptmodells bestimmt, was die
Clustern oder einer PCA im Vorfeld.
Die Gewichte in der Einbettungsschicht werden als Teil des Gradientenabstiegs
beim Training des nativen Modells gelernt.

Am Ende des Trainings könnten die Gewichte der Einbettungsschicht so sein, dass die
Kodierung für die kategorialen Variablen wie in Tabelle 2-5 dargestellt ist.

Entwurfsmuster 2: Einbettungen | 41
Tabelle 2-5. One-hot und gelernte Kodierungen für die Pluralitätsspalte im Natalitätsdatensatz

Pluralität One-hot Kodierung Gelernte Kodierung
Einfach(1) [1,0,0,0,0,0] [0.4, 0.6]
Mehrfach(2+) [0,1,0,0,0,0] [0.1, 0.5]
Zwillinge(2) [0,0,1,0,0,0] [-0.1, 0.3]
Drillinge(3) [0,0,0,1,0,0] [-0,2, 0,5]
Vierlinge(4) [0,0,0,0,1,0] [-0.4, 0.3]
Fünflinge(5) [0,0,0,0,0,1] [-0.6, 0.5]
Die Einbettung bildet einen spärlichen, mit einem Punkt kodierten Vektor auf einen dichten Vektor in R^2 ab.

In TensorFlow konstruieren wir zuerst eine kategorische Merkmalsspalte für das Merkmal und
verpacken wir diese in eine einbettende Merkmalsspalte. Zum Beispiel, für unser Pluralitätsmerkmal, würden wir
haben:

Mehrzahl = tf.feature_column.categorical_column_with_vocabulary_list(
'plurality', ['Single(1)', 'Multiple(2+)', 'Twins(2)',
'Drillinge(3)', 'Vierlinge(4)', 'Fünflinge(5)'])
plurality_embed = tf.feature_column.embedding_column(plurality, dimension=2)
Die resultierende Merkmalsspalte (plurality_embed) wird als Eingabe für die nachgelagerten
nachgelagerten Knoten des neuronalen Netzes anstelle der mit einem Punkt kodierten Merkmalsspalte
(Mehrzahl) verwendet.

Text-Einbettungen

Text bietet ein natürliches Umfeld, in dem es vorteilhaft ist, eine Einbettungsebene zu verwenden.
Angesichts der Kardinalität eines Vokabulars (oft in der Größenordnung von Zehntausenden von
Wörter) ist es nicht praktikabel, jedes Wort einzeln zu kodieren. Dies würde eine unglaublich
große (hochdimensionale) und spärliche Matrix für das Training. Außerdem möchten wir, dass ähnliche Wörter
nahe beieinander liegen und nicht verwandte Wörter im Einbettungsraum weit entfernt sein.
Daher verwenden wir eine dichte Worteinbettung, um die diskrete Texteingabe zu vektorisieren, bevor
unserem Modell übergeben wird.

Um eine Texteinbettung in Keras zu implementieren, erstellen wir zunächst eine Tokenisierung für jedes Wort
in unserem Vokabular, wie in Abbildung 2-6 dargestellt. Dann verwenden wir diese Tokenisierung zur Abbildung auf
eine Einbettungsschicht zu mappen, ähnlich wie es für die Pluralitätsspalte gemacht wurde.

42 | Kapitel 2: Entwurfsmuster für die Datendarstellung

4 Dieser Datensatz ist in BigQuery verfügbar: bigquery-public-data.hacker_news.stories.
Abbildung 2-6. Der Tokenizer erstellt eine Nachschlagetabelle, die jedes Wort einem Index zuordnet.

Die Tokenisierung ist eine Nachschlagetabelle, die jedes Wort in unserem Vokabular auf einen
Index zuordnet. Man kann sich dies als eine One-Hot-Kodierung jedes Wortes vorstellen, wobei der tokenisierte
Index die Position des Nicht-Null-Elements in der One-Hot-Codierung ist. Dies erfordert einen
Dies erfordert einen vollständigen Durchlauf des gesamten Datensatzes (nehmen wir an, es handelt sich um die Titel von Artikeln^4 ), um die Nachschlagetabelle zu erstellen.
um die Lookup-Tabelle zu erstellen und kann in Keras durchgeführt werden. Den vollständigen Code finden Sie im
Repository für dieses Buch:

from tensorflow.keras.preprocessing.text import Tokenizer
Tokenizer = Tokenizer()
tokenizer.fit_on_texts(titles_df.title)
Hier können wir die Tokenizer-Klasse aus der keras.preprocessing.text Bibliothek verwenden. Der Aufruf von
fit_on_texts erstellt eine Nachschlagetabelle, die jedes der in unseren Titeln gefundenen Wörter
einem Index zuordnet. Durch den Aufruf von tokenizer.index_word können wir diese Nachschlagetabelle untersuchen
direkt untersuchen:

tokenizer.index_word
{1: 'der',
2: 'a',
3: 'to',
4: 'für',
5: 'in',
6: 'von',
7: 'und',
8: 's',
9: 'am',
10: 'mit',
11: 'zeigen',
...
Entwurfsmuster 2: Einbettungen | 43
Wir können diese Zuordnung dann mit der Methode texts_to_sequences unseres
Tokenizer aufrufen. Dadurch wird jede Wortfolge in der darzustellenden Texteingabe abgebildet
(hier nehmen wir an, dass es sich um Titel von Artikeln handelt) auf eine Sequenz von Token, die
die jedem Wort entsprechen, wie in Abbildung 2-7 dargestellt:

integerized_titles = tokenizer.texts_to_sequences(titles_df.title)
Abbildung 2-7. Mit Hilfe des Tokenizers wird jeder Titel auf eine Folge von ganzzahligen
Indexwerten abgebildet.

Der Tokenizer enthält weitere relevante Informationen, die wir später zur Erstellung einer
einer Einbettungsschicht verwenden werden. Insbesondere erfasst VOCAB_SIZE die Anzahl der Elemente der
der Indexnachschlagetabelle und MAX_LEN enthält die maximale Länge der Textstrings
im Datensatz:

VOCAB_SIZE = len(tokenizer.index_word)
MAX_LEN = max(len(sequence) for sequence in integerized_titles)
Bevor das Modell erstellt wird, müssen die Titel im Datensatz vorverarbeitet werden. Wir müssen
müssen wir die Elemente unseres Titels auffüllen, um sie in das Modell einzuspeisen. Keras verfügt über die Hilfsfunktion
Funktionen pad_sequence für diese Aufgabe am Anfang der Tokenizer-Methoden. Die Funktion
create_sequences nimmt sowohl die Titel als auch die maximale Satzlänge als Eingabe
und gibt eine Liste der Ganzzahlen zurück, die unseren an den Satz angehängten Token entsprechen
maximale Länge:

from tensorflow.keras.preprocessing.sequence import pad_sequences
def create_sequences(texts, max_len=MAX_LEN):
sequences = tokenizer.texts_to_sequences(texts)
padded_sequences = pad_sequences(sequences,
max_len,
padding='post')
return padded_sequences
Als Nächstes erstellen wir in Keras ein Deep Neural Network (DNN)-Modell, das eine
eine einfache Einbettungsschicht implementiert, um die Wort-Ganzzahlen in dichte Vektoren umzuwandeln. Die Keras
Einbettungsschicht kann man sich als eine Abbildung der Integer-Indizes bestimmter Wörter
in dichte Vektoren (ihre Einbettungen). Die Dimensionalität der Einbettung wird durch
durch output_dim bestimmt. Das Argument input_dim gibt die Größe des Vokabulars an,

44 | Kapitel 2: Entwurfsmuster für die Datendarstellung

und input_shape gibt die Länge der Eingabesequenzen an. Da wir hier die Titel vor der Übergabe an das Modell aufgefüllt
die Titel vor der Übergabe an das Modell aufgefüllt haben, setzen wir input_shape=[MAX_LEN]:

model = models.Sequential([layers.Embedding(input_dim=VOCAB_SIZE + 1,
output_dim=Einbettung_dim,
input_shape=[MAX_LEN]),
layers.Lambda( lambda x: tf.reduce_mean(x,axis=1)),
layers.Dense(N_CLASSES, activation='softmax')])
Beachten Sie, dass wir eine benutzerdefinierte Keras-Lambda-Schicht zwischen der Einbettungsschicht
Schicht und der dichten Softmax-Schicht eine benutzerdefinierte Keras-Lambda-Schicht einfügen müssen, um die von der Einbettungsschicht zurückgegebenen Wortvektoren zu mitteln.
Einbettungsschicht zurückgegeben werden. Dieser Mittelwert wird an die Dense-Softmax-Schicht weitergeleitet. Auf diese Weise
ein einfaches Modell, bei dem jedoch die Informationen über die Wortreihenfolge verloren gehen.
ein Modell, das Sätze als "Wortsack" betrachtet.

Bildeinbettungen

Während es sich bei Text um sehr spärliche Eingaben handelt, bestehen andere Datentypen, wie Bilder oder Audio,
bestehen aus dichten, hochdimensionalen Vektoren, in der Regel mit mehreren Kanälen, die
rohe Pixel- oder Frequenzinformationen enthalten. In dieser Situation erfasst eine Einbettung eine relevante
eine wichtige, niedrigdimensionale Darstellung der Eingabe.

Für die Einbettung von Bildern wird ein komplexes neuronales Faltungsnetzwerk wie Inception oder
ResNet - zunächst auf einem großen Bilddatensatz wie ImageNet trainiert, der Millionen
mit Millionen von Bildern und Tausenden von möglichen Klassifizierungsetiketten. Dann wird die letzte Softmax-Schicht
aus dem Modell entfernt. Ohne die letzte Softmax-Klassifizierungsschicht kann das Modell
verwendet werden, um einen Merkmalsvektor für eine bestimmte Eingabe zu extrahieren. Dieser Merkmalsvektor enthält alle
alle relevanten Informationen des Bildes, so dass es sich im Wesentlichen um eine niedrigdimensionale Einbettung
Einbettung des Eingangsbildes.

Ähnlich verhält es sich mit der Aufgabe der Bildbeschriftung, d. h. der Erstellung einer textlichen Beschriftung
eines gegebenen Bildes, wie in Abbildung 2-8 gezeigt.

Abbildung 2-8. Für die Aufgabe der Bildübersetzung erzeugt der Encoder eine niedrigdimensionale
Einbettungsdarstellung des Bildes.

Entwurfsmuster 2: Einbettungen | 45
Durch das Training dieser Modellarchitektur auf einem riesigen Datensatz von Bild/Bildunterschrift-Paaren lernt der
erlernt der Encoder eine effiziente Vektordarstellung für Bilder. Der Decoder lernt, wie
diesen Vektor in eine Textbeschriftung zu übersetzen. In diesem Sinne wird der Encoder zu einer
Image2Vec-Einbettungsmaschine.

Warum es funktioniert
Die Einbettungsschicht ist nur eine weitere versteckte Schicht des neuronalen Netzes. Die Gewichte
werden dann jeder der hochkardinalen Dimensionen zugeordnet, und die Ausgabe wird
durch den Rest des Netzes geleitet. Daher werden die Gewichte für die Einbettung
Gewichte für die Einbettung werden also wie alle anderen Gewichte im neuronalen Netz durch den Prozess des Gradientenabstiegs
des neuronalen Netzes. Dies bedeutet, dass die resultierenden Vektoreinbettungen die
die effizienteste niedrigdimensionale Repräsentation dieser Merkmalswerte in Bezug auf die
die Lernaufgabe.

Während diese verbesserte Einbettung letztendlich dem Modell hilft, haben die Einbettungen selbst
Die Einbettungen selbst haben einen inhärenten Wert und ermöglichen es uns, zusätzliche Erkenntnisse über unseren Datensatz zu gewinnen.

Betrachten wir noch einmal den Kundenvideodatensatz. Durch die Verwendung von One-Hot-Codierung haben zwei beliebige
Benutzer_i und Benutzer_j das gleiche Ähnlichkeitsmaß. In ähnlicher Weise wird das
Punktprodukt oder die Kosinusähnlichkeit für zwei verschiedene sechsdimensionale One-Hot
Kodierungen der Geburtsvielfalt eine Ähnlichkeit von Null. Dies ist sinnvoll, da die
Kodierung unserem Modell im Wesentlichen sagt, dass es zwei verschiedene Geburtspluralitäten
ralitäten als getrennt und unverbunden zu behandeln. Für unseren Datensatz von Kunden und Videozuschauern
verlieren wir jegliche Vorstellung von Ähnlichkeit zwischen Kunden oder Videos. Aber das fühlt sich nicht ganz
richtig an. Zwei verschiedene Kunden oder Videos weisen wahrscheinlich Ähnlichkeiten auf.
Das Gleiche gilt für die Geburtenvielfalt. Das Auftreten von Vierlingen und Fünflingen
beeinflusst das Geburtsgewicht wahrscheinlich in statistisch ähnlicher Weise wie bei Einzelkindern
Geburtsgewichten (siehe Abbildung 2-9).

Abbildung 2-9. Indem wir unsere kategoriale Variable in einen niedrigdimensionalen Einbettungsraum zwingen
Raum zwingt, können wir auch Beziehungen zwischen den verschiedenen Kategorien lernen.

46 | Kapitel 2: Entwurfsmuster für die Datendarstellung

Bei der Berechnung der Ähnlichkeit von Mehrfachkategorien als kodierte Vektoren mit einem Punkt, erhalten wir
erhalten wir die Identitätsmatrix, da jede Kategorie als ein eigenes Merkmal behandelt wird (siehe
Tabelle 2-6).

Tabelle 2-6. Wenn die Merkmale in einem Schritt kodiert werden, ist die Ähnlichkeitsmatrix nur die
Identitätsmatrix

Einzelkind(1) Mehrlinge(2+) Zwillinge(2) Drillinge(3) Vierlinge(4) Fünflinge(5)
Einling(1) 1 0 0 0 0
Mehrlinge(2+) - 1 0 0 0 0
Zwillinge(2) - - 1 0 0 0
Drillinge(3) - - - - 1 0 0
Vierlinge(4) - - - - - 1 0
Fünflinge(5) - - - - - - 1
Sobald die Vielzahl jedoch in zwei Dimensionen eingebettet ist, wird das Ähnlichkeitsmaß
wird das Ähnlichkeitsmaß jedoch nicht mehr trivial, und es ergeben sich wichtige Beziehungen zwischen den verschiedenen Kategorien
werden deutlich (siehe Tabelle 2-7).

Tabelle 2-7. Wenn die Merkmale in zwei Dimensionen eingebettet sind, gibt die Ähnlichkeitsmatrix
uns mehr Informationen

Einfach(1) Mehrfach(2+) Zwillinge(2) Drillinge(3) Vierlinge(4) Fünflinge(5)
Einling(1) 1 0,92 0,61 0,57 0,06 0,1
Mehrlinge(2+) - 1 0,86 0,83 0,43 0,48
Zwillinge(2) - 1 0,99 0,82 0,85
Drillinge(3) - 1 0,85 0,88
Vierlinge(4) - 1 0,99
Fünflinge(5) - - - - - - 1
Eine gelernte Einbettung erlaubt es uns also, inhärente Ähnlichkeiten zwischen zwei getrennten Kategorien zu extrahieren.
Kategorien zu extrahieren, und da es eine numerische Vektordarstellung gibt, können wir die
die Ähnlichkeit zwischen zwei kategorialen Merkmalen genau quantifizieren.

Dies lässt sich mit dem Natalitätsdatensatz leicht veranschaulichen, aber das gleiche Prinzip gilt auch für
wenn es um Kunden-IDs geht, die in einen 20-dimensionalen Raum eingebettet sind. Bei der Anwendung auf
unseren Kundendatensatz angewandt, ermöglichen uns die Einbettungen, ähnliche Kunden zu einer bestimmten
Kunden_id zu finden und auf der Grundlage der Ähnlichkeit Vorschläge zu machen, z. B. welche Videos sie
wahrscheinlich ansehen werden, wie in Abbildung 2-10 gezeigt. Außerdem können diese Benutzer- und Objekteinbettungen
Einbettungen mit anderen Merkmalen kombiniert werden, wenn ein separates Modell für maschinelles Lernen
Modells. Die Verwendung von vortrainierten Einbettungen in Modellen für maschinelles Lernen wird als
Transfer-Lernen bezeichnet.

Entwurfsmuster 2: Einbettungen | 47
Abbildung 2-10. Durch das Lernen eines niedrigdimensionalen, dichten Einbettungsvektors für jeden Kunden
und Video kann ein einbettungsbasiertes Modell gut verallgemeinert werden, ohne dass eine manuelle
manuellem Aufwand für die Merkmalstechnik.

Kompromisse und Alternativen
Der Hauptnachteil bei der Verwendung einer Einbettung ist die beeinträchtigte Darstellung von
der Daten. Der Übergang von einer Darstellung mit hoher Kardinalität zu einer Darstellung mit niedrigeren Dimensionen bedeutet einen Informationsverlust.
Repräsentation zu einer niedrigdimensionalen Repräsentation. Im Gegenzug erhalten wir Informationen
über die Nähe und den Kontext der Elemente.

Wahl der Einbettungsdimension

Die genaue Dimensionalität des Einbettungsraums ist etwas, das wir als
Praktiker. Sollten wir also eine große oder kleine Einbettungsdimension wählen? Ja, natürlich,
wie bei den meisten Dingen beim maschinellen Lernen gibt es einen Kompromiss. Die Verlorenheit der Darstellung
Darstellung wird durch die Größe der Einbettungsschicht gesteuert. Durch die Wahl einer sehr kleinen
einer Einbettungsschicht wählt, werden zu viele Informationen in einen kleinen
kleinen Vektorraum gepresst und der Kontext kann verloren gehen. Ist die Einbettungsdimension hingegen
Dimension zu groß ist, verliert die Einbettung die erlernte kontextuelle Bedeutung der
Merkmale. Im Extremfall sind wir wieder bei dem Problem der One-Hot-Codierung angelangt.
kodierung. Die optimale Einbettungsdimension wird oft durch Experimentieren gefunden,
ähnlich wie bei der Auswahl der Anzahl der Neuronen in einer tiefen neuronalen Netzwerkschicht.

Wenn wir es eilig haben, ist eine Faustregel, die vierte Wurzel aus der Gesamtzahl der
eindeutiger kategorialer Elemente zu verwenden, während eine andere besagt, dass die Einbettungsdimension
etwa das 1,6-fache der Quadratwurzel aus der Anzahl der eindeutigen Elemente in der
Kategorie und nicht weniger als 600 betragen sollte. Nehmen wir zum Beispiel an, wir wollten eine Einbettungsebene verwenden
Ebene ein Merkmal kodieren, das 625 eindeutige Werte hat. Unter Anwendung der ersten Faustregel würden wir
würden wir eine Einbettungsdimension für die Mehrzahl von 5 wählen, und mit der zweiten Regel

48 | Kapitel 2: Entwurfsmuster für die Datendarstellung

Als Faustregel würden wir 40 wählen. Wenn wir die Hyperparameter abstimmen, könnte es sich lohnen
innerhalb dieses Bereichs zu suchen.

Autokodierer

Das Training von Einbettungen unter Aufsicht kann schwierig sein, da es eine große Menge an
markierter Daten erfordert. Damit ein Bildklassifizierungsmodell wie Inception in der Lage ist, nützliche
nützliche Bildeinbettungen zu erzeugen, wird es auf ImageNet trainiert, das über 14 Millionen beschriftete
Bilder enthält. Autoencoder bieten eine Möglichkeit, diesen Bedarf an einer großen Menge markierter Daten zu umgehen.
Datensatzes zu umgehen.

Die typische Architektur eines Autoencoders, die in Abbildung 2-11 dargestellt ist, besteht aus einer Bottleneck
Schicht, die im Wesentlichen eine Einbettungsschicht ist. Der Teil des Netzwerks vor dem
Engpass (der "Encoder") bildet die hochdimensionale Eingabe in eine niedriger
dimensionalen Einbettungsschicht, während das letzte Netz (der "Decoder") diese
Repräsentation wieder auf eine höhere Dimension abbildet, in der Regel auf die gleiche Dimension wie die ursprüngliche.
nal. Das Modell wird in der Regel anhand einer Variante eines Rekonstruktionsfehlers trainiert, der
der die Ausgabe des Modells zwingt, der Eingabe so ähnlich wie möglich zu sein.

Abbildung 2-11. Beim Training eines Autoencoders sind das Merkmal und das Label gleich und
der Verlust ist der Rekonstruktionsfehler. Dadurch kann der Autoencoder eine nichtlineare
Dimensionsreduktion.

Da die Eingabe dieselbe ist wie die Ausgabe, werden keine zusätzlichen Etiketten benötigt. Der
Kodierer lernt eine optimale nichtlineare Dimensionsreduktion der Eingabe. Ähnlich wie
wie die PCA eine lineare Dimensionsreduktion erreicht, kann die Engpass-Schicht eines Autocodierers
Codierers durch die Einbettung eine nichtlineare Dimensionsreduktion erreichen.

So können wir ein schwieriges Problem des maschinellen Lernens in zwei Teile aufteilen. Erstens verwenden wir
alle unbeschrifteten Daten, die wir haben, um von einer hohen Kardinalität zu einer niedrigeren Kardinalität zu gelangen, indem wir
mit Hilfe von Autoencodern als zusätzliche Lernaufgabe. Dann lösen wir das eigentliche Bild
Klassifizierungsproblem, für das wir in der Regel viel weniger beschriftete Daten haben, indem wir die
Einbettung, die durch die Hilfsaufgabe des Autoencoders erzeugt wird. Dies wird wahrscheinlich die Modell
Leistung des Modells, da das Modell jetzt nur noch die Gewichte für die Einstellung mit niedrigerer
Dimension lernen muss (d.h. es muss weniger Gewichte lernen).

Entwurfsmuster 2: Einbettungen | 49
Zusätzlich zu den Bild-Autoencodern haben sich neuere Arbeiten auf die Anwendung von Deep-Learning
Techniken für strukturierte Daten. TabNet ist ein tiefes neuronales Netz, das speziell
TabNet ist ein tiefes neuronales Netz, das speziell für das Lernen aus tabellarischen Daten entwickelt wurde und auf unbeaufsichtigte Weise trainiert werden kann.
Durch die Modifizierung des Modells, so dass es eine Encoder-Decoder-Struktur hat, funktioniert TabNet als
Autoencoder auf tabellarischen Daten, was es dem Modell ermöglicht, Einbettungen aus
strukturierten Daten über einen Merkmalstransformator.

Kontextbezogene Sprachmodelle

Gibt es eine zusätzliche Lernaufgabe, die für Text funktioniert? Kontextuelle Sprachmodelle wie
Word2Vec und maskierte Sprachmodelle wie Bidirectional Encoding Representations
von Transformatoren (BERT) machen die Lernaufgabe zu einem Problem, bei dem es keinen
Knappheit an Bezeichnungen gibt.

Word2Vec ist eine bekannte Methode zur Konstruktion einer Einbettung unter Verwendung flacher
neuronaler Netze und der Kombination zweier Techniken - Continuous Bag of Words
(CBOW) und ein Skip-Gram-Modell - angewandt auf einen großen Textkorpus, wie Wikipe-
dia. Während das Ziel beider Modelle darin besteht, den Kontext eines Wortes zu erlernen, indem die eingegebenen
Ziel beider Modelle ist es, den Kontext eines Wortes zu erlernen, indem die Eingabewörter auf die Zielwörter mit einer Zwischenschicht für die Einbettung abgebildet werden.
das niedrigdimensionale Einbettungen lernt, die den Kontext der Wörter am besten erfassen.
Wörter erfassen. Die durch Word2Vec erlernten Worteinbettungen erfassen die
semantischen Beziehungen zwischen den Wörtern, so dass die Vektordarstellungen im Einbettungsraum
Repräsentationen im Einbettungsraum sinnvolle Abstände und Richtungen beibehalten (Abbildung 2-12).

Abbildung 2-12. Worteinbettungen erfassen semantische Beziehungen.

BERT wird mit einem maskierten Sprachmodell und der Vorhersage des nächsten Satzes trainiert. Für ein
maskierten Sprachmodells werden Wörter nach dem Zufallsprinzip aus dem Text maskiert und das Modell
errät das fehlende Wort bzw. die fehlenden Wörter. Die Vorhersage des nächsten Satzes ist eine Klassifizierungsaufgabe
bei der das Modell vorhersagt, ob zwei Sätze im Originaltext aufeinander folgen oder nicht.
Originaltext folgen. Daher eignet sich jeder beliebige Textkorpus als beschrifteter Datensatz. BERT wurde zunächst
mit dem gesamten englischen Wikipedia- und BooksCorpus trainiert. Trotz des Lernens auf diesen
Einbettungen aus BERT oder Word2Vec haben sich als sehr leistungsfähig erwiesen
als sehr leistungsfähig erwiesen, wenn sie für andere nachgelagerte Trainingsaufgaben verwendet wurden. Die Worteinbettungen

50 | Kapitel 2: Entwurfsmuster für die Datendarstellung

die von Word2Vec gelernt werden, sind dieselben, unabhängig davon, in welchem Satz das Wort
erscheint. Die BERT-Worteinbettungen sind jedoch kontextabhängig, d. h. der Einbettungsvektor
Das bedeutet, dass der Einbettungsvektor je nach dem Kontext, in dem das Wort verwendet wird, unterschiedlich ist.

Eine vortrainierte Texteinbettung, wie Word2Vec, NNLM, GLoVE oder BERT, kann
einem maschinellen Lernmodell hinzugefügt werden, um Textmerkmale in Verbindung mit strukturierten
strukturierten Eingaben und anderen gelernten Einbettungen aus unserem Kunden- und Videodatensatz
(Abbildung 2-13).

Letztendlich lernen Einbettungen, Informationen zu erhalten, die für die vorgegebene
Trainingsaufgabe. Im Fall von Bildunterschriften besteht die Aufgabe darin, zu lernen, wie der Kontext der
die Elemente eines Bildes mit dem Text in Beziehung stehen. In der Autoencoder-Architektur ist das Label
das gleiche wie das Merkmal, so dass die Dimensionsreduktion des Engpasses versucht, alles
alles zu lernen, ohne dass ein spezifischer Kontext für das, was wichtig ist, besteht.

Abbildung 2-13. Eine vortrainierte Texteinbettung kann zu einem Modell hinzugefügt werden, um Text
Merkmale.

Einbettung in ein Data Warehouse

Maschinelles Lernen auf strukturierten Daten lässt sich am besten direkt in SQL auf einem Data Warehouse
Lagerhaus. Dadurch wird vermieden, dass Daten aus dem Lagerhaus exportiert werden müssen, und es werden
Probleme mit dem Datenschutz und der Datensicherheit.

Viele Probleme erfordern jedoch eine Mischung aus strukturierten Daten und natürlichsprachigem Text
oder Bilddaten. In Data Warehouses wird natürlichsprachlicher Text (z. B. Bewertungen) direkt als Spalten gespeichert.
direkt als Spalten gespeichert, und Bilder werden normalerweise als URLs zu Dateien in einem Cloud-Speicher gespeichert.

Entwurfsmuster 2: Einbettungen | 51
Eimer. In diesen Fällen vereinfacht es das spätere maschinelle Lernen, zusätzlich die
Einbettungen der Textspalten oder der Bilder als arrayartige Spalten zu speichern. Auf diese Weise
ermöglicht die einfache Einbindung solcher unstrukturierter Daten in maschinelles Lernen
Modelle.

Um Texteinbettungen zu erstellen, können wir ein vortrainiertes Modell wie Swivel von
TensorFlow Hub in BigQuery laden. Der vollständige Code ist auf GitHub zu finden:

CREATE OR REPLACE MODEL advdata.swivel_text_embed
OPTIONS(model_type='tensorflow', model_path='gs://BUCKET/swivel/*')
Verwenden Sie dann das Modell, um die natürlichsprachliche Textspalte in ein Embed-
ding-Array umzuwandeln und die Einbettungsnachschlage in einer neuen Tabelle zu speichern:

CREATE OR REPLACE TABLE advdata.comments_embedding AS
SELECT
output_0 as kommentare_einbettung,
kommentare
FROM ML.PREDICT(MODEL advdata.swivel_text_embed,(
SELECT kommentare, LOWER(kommentare) AS sätze
FROM `bigquery-public-data.noaa_preliminary_severe_storms.wind_reports`
))
Es ist nun möglich, eine Verknüpfung mit dieser Tabelle herzustellen, um die Texteinbettung für jeden Kommentar zu erhalten.
Kom- ment zu erhalten. Für Bildeinbettungen können wir auf ähnliche Weise Bild-URLs in Einbettungen umwandeln
dings umwandeln und in das Data Warehouse laden.

Die Vorberechnung von Merkmalen auf diese Weise ist ein Beispiel für das "Entwurfsmuster 26: Merkmalsspeicher" auf Seite 295 (siehe Kapitel 6).
ture Store" auf Seite 295 (siehe Kapitel 6).

Entwurfsmuster 3: Feature Cross
Das Feature-Cross-Entwurfsmuster hilft Modellen, Beziehungen zwischen Eingaben schneller zu lernen
schneller zu lernen, indem jede Kombination von Eingabewerten explizit zu einem separaten Merkmal gemacht wird.

Problem
Betrachten Sie den Datensatz in Abbildung 2-14 und die Aufgabe, einen binären Klassifikator zu erstellen, der
die + und - Markierungen trennt.

Wenn man nur die Koordinaten x_1 und x_2 verwendet, ist es nicht möglich, eine lineare Grenzlinie zu finden
zu finden, die die Klassen + und - voneinander trennt.

Das bedeutet, dass wir das Modell komplexer gestalten müssen, um dieses Problem zu lösen,
vielleicht durch Hinzufügen weiterer Schichten zum Modell. Es gibt jedoch eine einfachere Lösung.

52 | Kapitel 2: Entwurfsmuster für die Datendarstellung

Abbildung 2-14. Dieser Datensatz ist nicht linear trennbar, wenn nur x_1 und x_2 als Eingaben verwendet werden.

Lösung
Beim maschinellen Lernen ist das Feature-Engineering der Prozess der Nutzung von Fachwissen
um neue Merkmale zu erstellen, die den maschinellen Lernprozess unterstützen und die Vorhersagekraft
Vorhersagekraft unseres Modells erhöhen. Eine häufig verwendete Technik des Feature Engineering ist die Erstellung
Erstellung eines Merkmalskreuzes.

Ein Merkmalskreuz ist ein synthetisches Merkmal, das durch die Verkettung von zwei oder mehr kategorialen
kalischen Merkmalen gebildet wird, um die Interaktion zwischen ihnen zu erfassen. Durch die Verbindung von zwei Merkmalen
ist es möglich, Nichtlinearität in das Modell zu kodieren, was zu einer Vorhersagefähigkeit führen kann
Vorhersagefähigkeiten zu ermöglichen, die über das hinausgehen, was jedes der Merkmale
einzeln. Merkmalskreuzungen ermöglichen es dem ML-Modell, Beziehungen zwischen den Merkmalen
zwischen den Merkmalen schneller lernen. Während komplexere Modelle wie neuronale Netze und
Bäume Feature-Crosses selbständig erlernen können, kann die explizite Verwendung von Feature-Crosses es
können wir mit dem Training eines linearen Modells auskommen. Folglich können Merkmalskreuze
Modelltraining beschleunigen (weniger kostspielig) und die Modellkomplexität reduzieren (weniger Trainingsdaten
Daten werden benötigt).

Um eine Merkmalsspalte für den obigen Datensatz zu erstellen, können wir x_1 und x_2 jeweils
je nach ihrem Vorzeichen in zwei Bereiche unterteilen. Dies wandelt x_1 und x_2 in kategorische
Merkmale. A bezeichne den Bereich, in dem x_1 >= 0 ist, und B den Bereich, in dem x_1 < 0 ist.
C bezeichne den Bereich, in dem x_2 >= 0 ist, und D den Bereich, in dem x_2 < 0 ist (Abbildung 2-15).

Entwurfsmuster 3: Feature Cross | 53
5 Das Notizbuch feature_cross.ipynb im Repository dieses Buches wird Ihnen helfen, die Diskussion
besser folgen.
Abbildung 2-15. Das Feature Cross führt vier neue boolesche Features ein.

Ein Merkmalskreuz aus diesen bucketisierten Merkmalen führt vier neue boolesche Merkmale für
unser Modell:

AC mit x_1 >= 0 und x_2 >= 0
BC mit x_1 < 0 und x_2 >= 0
AD mit x_1 >= 0 und x_2 < 0
BD, wenn x_1 < 0 und x_2 < 0
Jedes dieser vier booleschen Merkmale (AC, BC, AD und BD) würde beim Training des Modells eine eigene Gewichtung erhalten
wenn das Modell trainiert wird. Das bedeutet, dass wir jeden Quadranten als eigenes Merkmal behandeln können.
Da der ursprüngliche Datensatz durch die von uns erstellten Buckets perfekt aufgeteilt wurde, kann ein Merkmalskreuz
von A und B in der Lage, den Datensatz linear zu trennen.

Aber das ist nur eine Illustration. Was ist mit Daten aus der realen Welt? Betrachten wir einen öffentlichen Datensatz
von gelben Taxifahrten in New York City (siehe Tabelle 2-8).^5

Tabelle 2-8. Eine Vorschau auf den öffentlichen New York City Taxi-Datensatz in BigQuery

pickup_datetime pickuplon pickuplat dropofflon dropofflat passengers fare_amount
2014-05-17 15:15:00 UTC -73.99955 40.7606 -73.99965 40.72522 1 31
2013-12-09 15:03:00 UTC -73.99095 40.749772 -73.870807 40.77407 1 34.33
2013-04-18 08:48:00 UTC -73.973102 40.785075 -74.011462 40.708307 1 29
2009-11-05 06:47:00 UTC -73.980313 40.744282 -74.015285 40.711458 1 14.9
2009-05-21 09:47:06 UTC -73.901887 40.764021 -73.901795 40.763612 1 12.8
54 | Kapitel 2: Entwurfsmuster für die Datendarstellung

Dieser Datensatz enthält Informationen über Taxifahrten in New York City mit Merkmalen wie
wie dem Zeitstempel der Abholung, dem Breiten- und Längengrad der Abholung und der Rückgabe sowie der
Anzahl der Fahrgäste. Das Label hier ist fare_amount, die Kosten der Taxifahrt. Welche
Merkmalskreuze könnten für diesen Datensatz relevant sein?

Es könnten viele sein. Betrachten wir die pickup_datetime. Aus diesem Merkmal können wir
Informationen über die Uhrzeit und den Wochentag der Fahrt verwenden. Jedes dieser Merkmale ist eine kategori
kategorische Variable, und beide enthalten sicherlich eine Vorhersagekraft für die Bestimmung des Preises einer
Taxifahrt. Für diesen Datensatz ist es sinnvoll, ein Merkmalskreuz aus Wochentag
und Stunde_des_Tages zu betrachten, da es vernünftig ist, anzunehmen, dass Taxifahrten um 17 Uhr am Montag
anders behandelt werden sollten als Taxifahrten um 17 Uhr am Freitag (siehe Tabelle 2-9).

Tabelle 2-9. Eine Vorschau auf die Daten, die wir zur Erstellung eines Feature-Kreuzes verwenden: die Spalten Wochentag und
Stunde des Tages Spalten

Tag_der_Woche Stunde_des_Tages
Sonntag 00
Sonntag 01
... ...
Samstag 23
Ein Merkmalskreuz aus diesen beiden Merkmalen wäre ein 168-dimensionaler, einhändig kodierter
Vektor (24 Stunden × 7 Tage = 168), wobei das Beispiel "Montag um 17 Uhr" einen
Index (Wochentag ist Montag, verkettet mit Stunde des Tages
ist 17).

Während die beiden Merkmale für sich genommen wichtig sind, erleichtert die Berücksichtigung eines Merkmalskreuzes aus
Stunde_des_Tages und Tag_der_Woche macht es für ein Modell zur Vorhersage des Taxitarifs einfacher, zu erkennen
zu erkennen, dass die Hauptverkehrszeit am Ende der Woche die Dauer der Taxifahrt und damit den
Taxitarif auf seine eigene Weise beeinflusst.

Merkmal Cross in BigQuery ML

Um das Merkmal cross in BigQuery zu erstellen, können wir die Funktion ML.FEATURE_CROSS verwenden
verwenden und ein STRUCT mit den Merkmalen day_of_week und hour_of_day übergeben:

ML.FEATURE_CROSS(STRUCT(Tag_der_Woche,Stunde_der_Woche)) AS Tag_X_Stunde
Die STRUCT-Klausel erzeugt ein geordnetes Paar der beiden Merkmale. Wenn unser Software-Rahmen
keine Feature-Cross-Funktion unterstützt, können wir den gleichen Effekt mit String
Verkettung erzielen:

CONCAT(CAST(tag_der_woche AS STRING),
CAST(Stunde_der_Woche AS STRING)) AS Tag_X_Stunde
Entwurfsmuster 3: Feature Cross | 55
Nachfolgend ist ein vollständiges Trainingsbeispiel für das Nativitätsproblem dargestellt, mit einem Feature
Kreuz aus den Spalten is_male und plurality als Merkmal verwendet wird; den vollständigen Code finden Sie in diesem
Repository dieses Buches:

CREATE OR REPLACE MODELL babyweight.natality_model_feat_eng
TRANSFORM (gewicht_pfund,
is_male,
Mehrzahl,
gestation_weeks,
mutter_alter,
CAST (mutter_rasse AS string) AS mutter_rasse,
ML.FEATURE_CROSS(
STRUCT(
is_male,
Mehrzahl)
) AS gender_X_pluralität)
OPTIONEN
(MODEL_TYPE='linear_reg',
INPUT_LABEL_COLS=['weight_pounds'],
DATA_SPLIT_METHOD="NO_SPLIT") AS
SELECT
*
FROM
babyweight.babyweight_data_train
TrDas Transform-Muster (siehe Kapitel 6) wird hier verwendet, wenn
der Entwicklung von Merkmalen des Geburtsmodells. Dies ermöglicht es auch dem
Modell, sich während der Vorhersage daran zu "erinnern", die Merkmale der
Datenfelder während der Vorhersage durchzuführen.
Wenn wir genügend Daten haben, ermöglicht das Feature-Cross-Muster, dass Modelle einfacher werden.
pler werden. Für den Natality-Datensatz liegt der RMSE für den Evaluierungssatz für ein lineares Modell mit
dem Feature-Cross-Muster bei 1,056. Alternativ dazu kann ein tiefes neuronales Netzwerk in
BigQuery ML auf demselben Datensatz ohne Feature-Crosses einen RMSE von 1,074.
Es gibt eine leichte Verbesserung unserer Leistung trotz der Verwendung eines viel einfacheren Lin-
Ohrmodells, und die Trainingszeit ist ebenfalls drastisch reduziert.

Funktionskreuze in TensorFlow

Um ein Feature Cross mit den Features is_male und plurality in Tensor-
Flow zu implementieren, verwenden wir tf.feature_column.crossed_column. Die Methode crossed_column
nimmt zwei Argumente entgegen: eine Liste der zu kreuzenden Feature-Schlüssel und die Größe des Hash-Buckets.
Die gekreuzten Merkmale werden entsprechend der Größe des Hash-Buckets gehasht, sie sollte also groß genug sein
groß genug sein, um die Wahrscheinlichkeit von Kollisionen zu verringern. Da die Eingabe is_male
3 Werte annehmen kann (True, False, Unknown) und die Eingabe plurality 6 Werte annehmen kann
(Single(1), Twins(2), Triplets(3), Quadruplets(4), Quintuplets(5), Multiple(2+)),

56 | Kapitel 2: Entwurfsmuster für die Datendarstellung

gibt es 18 mögliche (is_male, plurality) Paare. Wenn wir hash_bucket_size auf
1.000 setzen, können wir zu 85 % sicher sein, dass es keine Kollisionen gibt.

Um schließlich eine gekreuzte Spalte in einem DNN-Modell zu verwenden, müssen wir sie entweder in eine
indicator_column oder einer embedding_column verpackt werden, je nachdem, ob wir sie ein-
hot kodieren oder in einer niedrigeren Dimension darstellen wollen (siehe "Design Pattern 2: Embed-
dings" auf Seite 39 in diesem Kapitel):

gender_x_plurality = fc.crossed_column(["is_male", "plurality"],
hash_bucket_size=1000)
crossed_feature = fc.embedding_column(gender_x_plurality, dimension=2)
oder

gender_x_plurality = fc.crossed_column(["is_male", "plurality"],
hash_bucket_size=1000)
gekreuzte_Merkmale = fc.indicator_column(gender_x_plurality)
Warum es funktioniert
Kreuzungen von Merkmalen sind ein wertvolles Mittel der Merkmalstechnik. Sie bieten mehr
Komplexität, mehr Ausdruckskraft und mehr Kapazität für einfache Modelle. Denken Sie noch einmal
an das gekreuzte Merkmal von is_male und Pluralität im Datensatz zur Geburtenrate. Dieses Fea-
ture Cross-Muster ermöglicht es dem Modell, männliche Zwillinge getrennt von weiblichen Zwillingen zu behandeln
Zwillingsmännern getrennt von weiblichen Zwillingen und getrennt von männlichen Drillingen und getrennt von weiblichen Einzelkindern usw. zu behandeln. Wenn
wir eine indicator_column verwenden, kann das Modell jede der resultierenden Kreuzungen
als unabhängige Variable zu behandeln, wodurch im Wesentlichen 18 zusätzliche binäre kategoriale Merkmale
kategoriale Merkmale zum Modell hinzu (siehe Abbildung 2-16 ).

Merkmalskreuze skalieren gut für große Datenmengen. Während das Hinzufügen zusätzlicher Schichten zu einem tiefen neuronalen
neuronalen Netzes möglicherweise genügend Nichtlinearität bieten, um zu lernen, wie sich Paare (is_male,
Pluralität) zu lernen, erhöht dies jedoch die Trainingszeit drastisch. Bei den Nativitätsdaten
beobachteten wir, dass ein lineares Modell mit einem in BigQuery ML trainierten Feature-Cross
vergleichbar mit einem DNN ist, das ohne ein Merkmalskreuz trainiert wurde. Allerdings trainiert das lineare
Modell wesentlich schneller trainiert.

Entwurfsmuster 3: Feature Cross | 57
Abbildung 2-16. Eine Merkmalskreuzung zwischen is_male und plurality erzeugt zusätzliche 18
binäre Merkmale in unserem ML-Modell.

Tabelle 2-10 vergleicht die Trainingszeit in BigQuery ML und den Auswertungsverlust sowohl für ein
lineares Modell mit einem Merkmalskreuz von (is_male, plurality) und ein tiefes neuronales Netz
Arbeit ohne Merkmalskreuz.

Tabelle 2-10. Ein Vergleich der BigQuery ML-Trainingsmetriken für Modelle mit und ohne
Feature-Kreuzen

Modelltyp Inkl. Merkmalskreuz Trainingszeit (Minuten) Eval.
(RMSE)
Linear Ja 0,42 1,05
DNN Nein 48 1,07
Eine einfache lineare Regression erzielt einen vergleichbaren Fehler auf dem Evaluierungsset, trainiert aber
hundertmal schneller. Die Kombination von Merkmalskreuzen mit massiven Daten ist eine alterna
tive Strategie zum Lernen komplexer Beziehungen in Trainingsdaten.

Kompromisse und Alternativen
Wir haben die Merkmalskreuze als eine Möglichkeit für den Umgang mit kategorialen Variablen besprochen, aber sie
können mit etwas Vorverarbeitung auch auf numerische Merkmale angewendet werden. Merkmalskreuze
verursachen Sparsamkeit in Modellen und werden oft zusammen mit Techniken verwendet, die dieser Sparsamkeit entgegenwirken.
dieser Spärlichkeit entgegenwirken.

58 | Kapitel 2: Entwurfsmuster für die Datendarstellung

Umgang mit numerischen Merkmalen

Wir würden niemals ein Feature-Kreuz mit einer kontinuierlichen Eingabe erstellen wollen. Denken Sie daran, wenn
eine Eingabe m mögliche Werte annimmt und eine andere Eingabe n mögliche Werte annimmt, dann würde das
Feature-Kreuz der beiden würde m*n Elemente ergeben. Eine numerische Eingabe ist dicht und nimmt
ein Kontinuum von Werten an. Es wäre unmöglich, alle möglichen Werte in einem
einem Merkmalskreuz von kontinuierlichen Eingabedaten aufzuzählen.

Wenn unsere Daten kontinuierlich sind, können wir sie stattdessen kategorisieren.
kategorisieren, bevor wir ein Merkmalskreuz anwenden. Breiten- und Längengrad sind zum Beispiel kontinuierliche Eingaben.
kontinuierliche Eingaben, und es macht intuitiv Sinn, ein Merkmalskreuz mit diesen Eingaben zu erstellen
da der Standort durch ein geordnetes Paar aus Breiten- und Längengrad bestimmt wird. Jedoch
Anstatt jedoch ein Merkmalskreuz unter Verwendung der Rohdaten für Breiten- und Längengrad zu erstellen, würden wir
diese kontinuierlichen Werte binden und die gebinnte Breite und die gebinnte Länge kreuzen:

import tensorflow.feature_column as fc
# Erstellen Sie eine Bucket-Feature-Spalte für den Breitengrad.
latitude_as_numeric = fc.numeric_column("latitude")
lat_bucketized = fc.bucketized_column(latitude_as_numeric,
lat_boundaries)
# Erstellen Sie eine Bucket-Feature-Spalte für den Längengrad.
longitude_as_numeric = fc.numeric_column("longitude")
lon_bucketized = fc.bucketized_column(longitude_as_numeric,
lon_boundaries)
# Erstellen eines Merkmalskreuzes aus Breitengrad und Längengrad
lat_x_lon = fc.crossed_column([lat_bucketized, lon_bucketized],
hash_bucket_size=nbuckets**4)
crossed_feature = fc.indicator_column(lat_x_lon)
Umgang mit hoher Kardinalität

Da die Kardinalität der aus einer Merkmalskreuzung resultierenden Kategorien um ein Vielfaches
mit der Kardinalität der Eingabemerkmale multipliziert, führen Merkmalskreuzungen zu
Sparsamkeit in unseren Modelleingaben. Selbst mit den Merkmalen Tag_der_Woche und Stunde_des_Tages
wäre ein Merkmalskreuz ein spärlicher Vektor der Dimension 168 (siehe Abbildung 2-17).

Es kann nützlich sein, ein Merkmalskreuz durch eine Einbettungsebene zu leiten (siehe "Entwurfsmuster
Muster 2: Einbettungen" auf Seite 39 in diesem Kapitel), um eine niedriger dimensionale
Darstellung zu erzeugen, wie in Abbildung 2-18 gezeigt.

Entwurfsmuster 3: Feature Cross | 59
Abbildung 2-17. Ein Merkmalskreuz aus Tag_der_Woche und Stunde_des_Tages ergibt einen spärlichen Vektor
der Dimension 168.

Abbildung 2-18. Eine Einbettungsebene ist eine nützliche Methode, um die Spärlichkeit eines Merkmals
Kreuz.

Weil das Embeddings Design Pattern uns erlaubt, enge Beziehungen zu erfassen,
die Weitergabe des Merkmalskreuzes durch eine Einbettungsschicht ermöglicht es dem Modell zu verallgemeinern
zu verallgemeinern, wie sich bestimmte Merkmalskreuze aus Stunden- und Tageskombinationen
die Ausgabe des Modells auswirken. In dem obigen Beispiel von Breiten- und Längengrad hätten wir
eine einbettende Merkmalsspalte anstelle der Indikatorspalte verwendet werden:

crossed_feature = fc.embedding_column(lat_x_lon, dimension=2)
60 | Kapitel 2: Entwurfsmuster für die Datendarstellung

6 Der vollständige Code befindet sich in 02_data_representation/feature_cross.ipynb im Code-Repository dieses Buches.
Notwendigkeit der Regularisierung

Wenn wir zwei kategoriale Merkmale mit großer Kardinalität kreuzen, erzeugen wir ein
Kreuzungsmerkmal mit multiplikativer Kardinalität. Natürlich kann bei mehr Kategorien für ein
kategorien für ein einzelnes Merkmal, kann die Anzahl der Kategorien in einem Kreuzungsmerkmal dramatisch
drastisch erhöhen. Wenn dies so weit geht, dass einzelne Bereiche zu wenige Elemente haben, wird die
die Fähigkeit des Modells zur Verallgemeinerung. Denken Sie an das Beispiel mit den Längen- und Breitengraden. Wenn
Wenn wir sehr feine Bereiche für Breiten- und Längengrade nehmen würden, dann wäre ein Merkmalskreuz
so genau sein, dass das Modell jeden Punkt auf der Karte speichern könnte.
Wenn diese Speicherung jedoch nur auf einer Handvoll von Beispielen beruht, würde die Speicherung
tatsächlich eine Überanpassung sein.

Zur Veranschaulichung ein Beispiel für die Vorhersage des Taxitarifs in New York anhand der
Abhol- und Zustiegsort und die Uhrzeit der Abholung:^6

CREATE OR REPLACE MODEL mlpatterns.taxi_l2reg
TRANSFORM (
fahrpreis_betrag
, ML.FEATURE_CROSS(STRUCT( CAST ( EXTRACT (DAYOFWEEK FROM pickup_datetime)
AS STRING) AS dayofweek,
CAST ( EXTRACT (HOUR FROM pickup_datetime)
AS STRING) AS hourofday), 2) AS day_hr
, CONCAT(
ML.BUCKETIZE(pickuplon, GENERATE_ARRAY(-78, -70, 0.01 )),
ML.BUCKETIZE(pickuplat, GENERATE_ARRAY(37, 45, 0.01)),
ML.BUCKETIZE(dropofflon, GENERATE_ARRAY(-78, -70, 0.01)),
ML.BUCKETIZE(dropofflat, GENERATE_ARRAY(37, 45, 0.01))
) AS pickup_and_dropoff
)
OPTIONS (input_label_cols=['fare_amount'],
model_type='linear_reg', l2_reg=0.1 )
AS
SELECT * FROM mlpatterns.taxi_data
Hier gibt es zwei Merkmalskreuze: eines in der Zeit (Wochentag und Tageszeit) und
das andere im Raum (der Abhol- und Absetzorte). Insbesondere der Ort hat eine
sehr hohe Kardinalität, und es ist wahrscheinlich, dass einige der Eimer nur sehr wenige
Beispiele.

Aus diesem Grund ist es ratsam, Merkmalskreuze mit einer L1-Regularisierung zu kombinieren, die
die eine geringe Anzahl von Merkmalen fördert, oder mit einer L2-Regularisierung, die eine Überanpassung begrenzt. Diese
ermöglicht es unserem Modell, das von den vielen synthetischen Merkmalen erzeugte Fremdrauschen zu ignorieren
und die Überanpassung zu bekämpfen. In der Tat verbessert die Regularisierung bei diesem Datensatz den
RMSE geringfügig, nämlich um 0,3 %.

Entwurfsmuster 3: Feature Cross | 61
7 Der Begriff "tabellarische Daten" bezieht sich auf numerische und kategoriale Eingaben, nicht aber auf Freiformtext. Sie können
Sie können sich unter tabellarischen Daten alles vorstellen, was Sie üblicherweise in einer Tabellenkalkulation finden. Zum Beispiel Werte wie Alter,
Autotyp, Preis oder Anzahl der Arbeitsstunden. Tabellarische Daten enthalten keinen Freitext wie Beschreibungen
oder Bewertungen.
Bei der Auswahl der zu kombinierenden Merkmale für einen Merkmalsvergleich sollten wir
nicht zwei Merkmale kreuzen, die stark korreliert sind. Wir können uns ein
Feature-Cross als die Kombination zweier Features, um ein geordnetes Paar zu bilden. In der Tat ist der Begriff
"cross" von "feature cross" bezieht sich auf das kartesische Produkt. Wenn zwei Merkmale stark
korreliert sind, dann bringt die "Spanne" ihres Merkmalskreuzes keine neuen Informationen
für das Modell. Ein extremes Beispiel: Angenommen, wir haben zwei Merkmale, x_1 und x_2,
wobei x_2 = 5*x_1. Wenn man die Werte für x_1 und x_2 nach ihrem Vorzeichen gruppiert und ein Merkmalskreuz erstellt, erhält man immer noch vier neue
kreuzt, entstehen immer noch vier neue boolesche Merkmale. Aufgrund der
Abhängigkeit von x_1 und x_2 sind jedoch zwei dieser vier Merkmale tatsächlich leer, und die
die anderen beiden sind genau die beiden für x_1 erstellten Bereiche.

Entwurfsmuster 4: Multimodale Eingabe
Das Entwurfsmuster "Multimodale Eingabe" befasst sich mit dem Problem der Darstellung verschiedener
Arten von Daten oder Daten, die auf komplexe Weise ausgedrückt werden können, durch Verkettung aller
verfügbaren Datendarstellungen.

Problem
Normalerweise kann eine Eingabe in ein Modell als Zahl, Kategorie, Bild oder Freitext dargestellt werden.
Bild oder Freiformtext dargestellt werden. Viele Standardmodelle sind nur für bestimmte Arten von
Eingaben definiert - ein Standardmodell zur Bildklassifizierung wie Resnet-50 zum Beispiel,
ist nicht in der Lage, andere Eingaben als Bilder zu verarbeiten.

Um den Bedarf an multimodalen Eingaben zu verstehen, nehmen wir an, wir haben eine Kamera, die
an einer Kreuzung aufnimmt, um Verkehrsverstöße zu identifizieren. Wir möchten, dass unser Modell
Bilddaten (Kameramaterial) und einige Metadaten darüber, wann das Bild aufgenommen wurde
aufgenommen wurde (Tageszeit, Wochentag, Wetter usw.), wie in Abbildung 2-19 zu sehen ist.

Dieses Problem tritt auch beim Training eines strukturierten Datenmodells auf, bei dem eine der
Eingaben ein Freiformtext ist. Im Gegensatz zu numerischen Daten können Bilder und Text nicht direkt
in ein Modell eingespeist werden. Daher müssen wir die Bild- und Texteingaben auf eine Weise darstellen, die unser
Modell verstehen kann (in der Regel unter Verwendung des Embeddings Design Pattern), dann kombinieren wir
diese Eingaben mit anderen tabellarischen^7 Merkmalen. Zum Beispiel könnten wir die Bewertung eines
die Bewertung eines Restaurantbesuchers auf der Grundlage seines Bewertungstextes und anderer Attribute
bezahlt hat und ob es sich um ein Mittag- oder Abendessen handelte (siehe Abbildung 2-20).

62 | Kapitel 2: Entwurfsmuster für die Datendarstellung

Abbildung 2-19. Modell, das Bild- und numerische Merkmale kombiniert, um vorherzusagen, ob das Bildmaterial
einer Kreuzung einen Verkehrsverstoß darstellt.

Abbildung 2-20. Modell, das Freitexteingaben mit Tabellendaten kombiniert, um die Bewertung eines Restaurants vorherzusagen.
einer Restaurantkritik.

Lösung
Nehmen wir zunächst das obige Beispiel mit dem Text aus einer Restaurantkritik in Kombination
mit tabellarischen Metadaten über das Essen, auf das sich die Bewertung bezieht. Wir kombinieren zunächst
die numerischen und kategorischen Merkmale. Es gibt drei mögliche Optionen für

Entwurfsmuster 4: Multimodale Eingabe | 63
8 Wenn wir ein kodiertes Array mit 30 Wörtern an unser Modell übergeben, wandelt die Keras-Schicht es in eine 64-
dimensionalen Einbettungsrepräsentation um, so dass wir eine [64×30]-Matrix erhalten, die die Überprüfung repräsentiert.
9 Der Ausgangspunkt ist ein Array, das aus 1.920 Zahlen besteht.
10 Siehe 02_data_representation/mixed_representation.ipynb im Code-Repository dieses Buches für den vollständigen Modell
Code.

meal_type, so dass wir dies in eine One-Hot-Codierung umwandeln können und das Abendessen als
[0, 0, 1]. Da dieses kategoriale Merkmal als Array dargestellt wird, können wir es nun mit meal_total kombinieren
mit meal_total kombinieren, indem wir den Preis der Mahlzeit als viertes Element des Arrays hinzufügen:
[0, 0, 1, 30.5].
Das Einbettungsmuster ist ein gängiger Ansatz zur Kodierung von Text für maschinelle
Lernmodelle. Wenn unser Modell nur Text enthielte, könnten wir es als Einbettungsschicht darstellen
Schicht darstellen, indem wir den folgenden tf.keras-Code verwenden:
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Embedding
model = Sequential()
model.add(Einbettung(batch_size, 64, input_length=30))
Hier müssen wir das Embedding^8 abflachen, um es mit meal_type zu verketten
und meal_total zu verbinden:
model.add(Flatten())
Wir könnten dann eine Reihe von Dense-Schichten verwenden, um das sehr große Array^9 in
kleinere umwandeln, so dass unsere Ausgabe aus einem Array von, sagen wir, drei Zahlen besteht:
model.add(Dense(3, activation="relu"))
Wir müssen nun diese drei Zahlen, die die Satzeinbettung der Rezension bilden, mit den
Einbettung der Rezension mit den früheren Eingaben bilden: [0, 0, 1, 30.5, 0.75, -0.82, 0.45].
Dazu verwenden wir die funktionale API von Keras und wenden die gleichen Schritte an. Schichten, die
mit der funktionalen API erstellt wurden, sind aufrufbar, so dass wir sie miteinander verketten können, beginnend mit
einer Eingabeschicht zu verketten.^10 Um dies zu nutzen, definieren wir zunächst sowohl unsere Einbettungs- als auch unsere
ularen Schichten:
embedding_input = Input(shape=(30,))
embedding_layer = Einbettung(batch_size, 64)(embedding_input)
embedding_layer = Flatten()(embedding_layer)
embedding_layer = Dense(3, activation='relu')(embedding_layer)
tabular_input = Eingabe(shape=(4,))
tabular_layer = Dense(32, activation='relu')(tabular_input)
Beachten Sie, dass wir die Eingabeteile dieser beiden Schichten als eigene Variablen definiert haben.
ables definiert haben. Das liegt daran, dass wir die Eingabeschichten übergeben müssen, wenn wir ein Modell mit dem
64 | Kapitel 2: Entwurfsmuster für die Datendarstellung
funktionalen API. Als Nächstes erstellen wir eine verkettete Schicht, füttern diese in unsere Output
Schicht ein und erstellen schließlich das Modell, indem wir die ursprünglichen Eingabeschichten übergeben, die wir
oben definiert haben:

merged_input = keras.layers.concatenate([embedding_layer, tabular_layer])
merged_dense = Dense(16)(merged_input)
output = Dense(1)(merged_dense)
model = Model(inputs=[embedding_input, tabular_input], outputs=output)
merged_dense = Dense(16, activation='relu')(merged_input)
output = Dense(1)(merged_dense)
model = Model(inputs=[embedding_input, tabular_input], outputs=output)
Jetzt haben wir ein einziges Modell, das die multimodale Eingabe akzeptiert.

Kompromisse und Alternativen
Wie wir gerade gesehen haben, untersucht das Entwurfsmuster für multimodale Eingaben, wie man verschiedene
Eingabeformate in demselben Modell darstellen kann. Zusätzlich zum Mischen verschiedener Datentypen können wir
die gleichen Daten auf unterschiedliche Weise darstellen, um es unserem Modell zu erleichtern
Modell, Muster zu erkennen. Wir können zum Beispiel ein Bewertungsfeld haben, das auf einer
Ordinalskala von 1 Stern bis 5 Sterne und behandeln dieses Bewertungsfeld sowohl als numerisch als auch als kate- gorisch.
gorisch. Hier beziehen wir uns auf multimodale Eingaben als beides:

Kombination verschiedener Datentypen, wie Bilder + Metadaten
Komplexe Daten auf verschiedene Arten darstellen
Wir beginnen damit, zu untersuchen, wie Tabellendaten auf unterschiedliche Weise dargestellt werden können.
dann werden wir uns mit Text- und Bilddaten beschäftigen.

Tabellarische Daten auf mehrere Arten

Um zu sehen, wie wir tabellarische Daten auf unterschiedliche Weise für dasselbe Modell darstellen können, lassen Sie uns
wieder auf das Beispiel der Restaurantbewertung zurück. Wir stellen uns stattdessen vor, dass die Bewertung eine Eingabe
Modell ist und wir versuchen, die Nützlichkeit der Bewertung vorherzusagen (wie viele Leute
die Bewertung gefallen hat). Als Eingabe kann die Bewertung sowohl als ganzzahliger Wert dargestellt werden
zwischen 1 und 5 als auch als kategoriales Merkmal dargestellt werden. Um die Bewertung kategorisch darzustellen, können wir
können wir sie einordnen. Die Art und Weise, wie wir die Daten einordnen, ist uns überlassen und hängt von unserem Datensatz
und Anwendungsfall. Um die Dinge einfach zu halten, wollen wir zwei Bereiche erstellen: "gut"
und "schlecht". Der Bereich "gut" umfasst Bewertungen von 4 und 5, der Bereich "schlecht" umfasst 3 und
darunter. Wir können dann einen booleschen Wert erstellen, um die Bewertungsbereiche zu kodieren und die
Integer- und Boolean-Wert in einem einzigen Array zusammenfassen (vollständiger Code auf GitHub).

Entwurfsmuster 4: Multimodale Eingabe | 65
So könnte dies für einen kleinen Datensatz mit drei Datenpunkten aussehen:

rating_data = [2, 3, 5]
def good_or_bad(rating):
if rating > 3:
return 1
else :
return 0
rating_processed = []
for i in rating_data:
rating_processed.append([i, good_or_bad(i)])
Das resultierende Merkmal ist ein Array mit zwei Elementen, bestehend aus der ganzzahligen Bewertung und ihrer
booleschen Darstellung:

[[2, 0], [3, 0], [5, 1]]
Hätten wir uns stattdessen entschieden, mehr als zwei Buckets zu erstellen, würden wir jede Eingabe mit einer One-Hot-Kodierung
kodieren und dieses One-Hot-Array an die Integer-Darstellung anhängen.

Der Grund, warum es sinnvoll ist, die Bewertung auf zwei Arten darzustellen, ist, dass der Wert der Bewertung, gemessen an
gemessen an 1 bis 5 Sternen nicht unbedingt linear ansteigt. Bewertungen von 4 und 5 sind
sehr ähnlich, und Bewertungen von 1 bis 3 zeigen höchstwahrscheinlich an, dass der Rezensent unzufrieden war.
zufrieden war. Ob Sie etwas, das Sie nicht mögen, mit 1, 2 oder 3 Sternen bewerten, hängt oft mit Ihren
Bewertungstendenzen und nicht mit der Bewertung selbst. Trotzdem ist es sinnvoll, die detaillierteren
Informationen in der Sternebewertung zu behalten, weshalb wir sie auf zwei Arten kodieren.
auf zwei Arten kodiert.

Berücksichtigen Sie auch Merkmale mit einer größeren Spanne als 1 bis 5, wie die Entfernung
zwischen dem Wohnort eines Rezensenten und einem Restaurant. Wenn jemand zwei Stunden fährt, um ein Restaurant zu besuchen
Restaurant zu besuchen, kann seine Bewertung kritischer ausfallen als bei jemandem, der von der anderen Straßenseite kommt.
Straße. In diesem Fall kann es zu Ausreißern kommen, so dass es sinnvoll wäre, sowohl
die numerische Entfernungsdarstellung bei etwa 50 km zu begrenzen und
eine separate kategorische Darstellung der Entfernung aufzunehmen. Das kategoriale Merkmal
könnte in "im Staat", "im Land" und "im Ausland" eingeteilt werden.

Multimodale Darstellung von Text

Sowohl Text als auch Bilder sind unstrukturiert und erfordern mehr Umwandlungen als tabellarische Daten.
larische Daten. Die Darstellung in verschiedenen Formaten kann unseren Modellen helfen, mehr Muster zu extrahieren.
tern. Wir werden die Diskussion über Textmodelle im vorangegangenen Abschnitt fortsetzen, indem wir
indem wir uns verschiedene Ansätze zur Darstellung von Textdaten ansehen. Dann führen wir
Bilder vorstellen und einige Optionen für die Darstellung von Bilddaten in ML-Modellen erläutern.

Textdaten auf mehrere Arten. Angesichts der komplexen Natur von Textdaten gibt es viele Möglichkeiten
um daraus eine Bedeutung zu extrahieren. Das Embeddings Design Pattern ermöglicht einem Modell die Gruppierung

66 | Kapitel 2: Entwurfsmuster für die Datendarstellung

11 Dieser Datensatz ist in BigQuery verfügbar: bigquery-public-data.stackoverflow.posts_questions.

ähnliche Wörter zusammen, erkennen Beziehungen zwischen Wörtern und verstehen syntaktische
syntaktische Elemente eines Textes. Während die Darstellung von Text durch Worteinbettungen am ehesten
spiegelt am ehesten wider, wie Menschen Sprache von Natur aus verstehen, aber es gibt zusätzliche
Textrepräsentationen, die die Fähigkeit unseres Modells zur Durchführung einer bestimmten Vorhersageaufgabe maximieren können.
In diesem Abschnitt werden wir uns den Bag of Words-Ansatz zur Darstellung von Text ansehen, zusammen mit
mit der Extraktion von tabellarischen Merkmalen aus Text.
Zur Veranschaulichung der Textdatendarstellung wird auf einen Datensatz Bezug genommen, der
den Text von Millionen von Fragen und Antworten von Stack Overflow,^11 zusammen mit
Metadaten über jeden Beitrag. Die folgende Abfrage gibt uns zum Beispiel eine Teilmenge von
Fragen, die entweder mit "keras", "matplotlib" oder "pandas" gekennzeichnet sind, zusammen mit der Anzahl
der Antworten, die jede Frage erhalten hat:
SELECT
titel,
Antwort_Anzahl,
REPLACE (tags, "|", ",") as tags
FROM
`bigquery- public - daten .stackoverflow.posts_questions`
WHERE
REGEXP_CONTAINS( tags, r"(?:keras|matplotlib|pandas)")
Die Abfrage ergibt die folgende Ausgabe:
Zeilentitel answer_count tags
1 Erstellen einer neuen Spalte in einem Pandas
Datenframe durch Abgleich von String-Werten in einer
Liste
6 python,python-2.7,pandas,replace,nested-loops
2 Extrahieren bestimmter ausgewählter Spalten in einen neuen
DataFrame als Kopie
6 python,pandas,chained-assignment
3 Wo rufe ich die Funktion BatchNormalization
Funktion in Keras auf?
7 python,keras,neural-network,data-
wissenschaft,batch-normalization
4 Verwendung eines Excel-ähnlichen Solvers in Python oder SQL 8 python,sql,numpy,pandas,solver
Bei der Darstellung von Text mit dem Bag of Words (BOW)-Ansatz stellen wir uns jeden
Texteingabe in unser Modell als eine Tasche mit Scrabble-Kacheln vor, wobei jede Kachel ein einzelnes Wort
Wort anstelle eines Buchstabens enthält. Bei BOW wird die Reihenfolge des Textes nicht beibehalten, aber es wird
aber es erkennt das Vorhandensein oder Fehlen bestimmter Wörter in jedem Textstück, das wir an unser
Modell senden. Dieser Ansatz ist eine Art Multi-Hot-Codierung, bei der jede Texteingabe in ein Array von 1s umgewandelt wird.
in ein Array von 1en und 0en umgewandelt wird. Jeder Index in diesem BOW-Array entspricht einem
Wort aus unserem Vokabular.
Entwurfsmuster 4: Multimodale Eingabe | 67
Wie Bag of Words funktioniert
Der erste Schritt bei der BOW-Kodierung ist die Auswahl der Größe unseres Vokabulars, das Folgendes umfasst
die N am häufigsten vorkommenden Wörter in unserem Textkorpus. Theoretisch könnte unser Vokabular
Theoretisch könnte die Größe unseres Vokabulars der Anzahl der eindeutigen Wörter in unserem gesamten Datensatz entsprechen. Aber
Dies würde jedoch zu sehr großen Eingabefeldern mit überwiegend Nullen führen, da viele Wörter
für eine einzelne Frage eindeutig sein könnten. Stattdessen sollten wir ein Vokabular wählen, das
Vokabulars wählen, das klein genug ist, um wichtige, wiederkehrende Wörter zu enthalten, die für unsere Vorhersageaufgabe von Bedeutung sind.
Bedeutung für unsere Vorhersageaufgabe vermitteln, aber groß genug, dass unser Vokabular nicht auf Wörter beschränkt ist, die in fast jeder
Frage vorkommen (wie "der", "ist", "und" usw.).
Jede Eingabe für unser Modell ist dann ein Array in der Größe unseres Vokabulars. Diese BOW
Darstellung lässt daher Wörter, die nicht in unserem Vokabular enthalten sind, völlig außer Acht.
Vokabular enthalten sind. Es gibt keine magische Zahl oder einen magischen Prozentsatz für die Wahl der Vokabulargröße -
Es ist hilfreich, einige davon auszuprobieren und zu sehen, welche für unser Modell am besten geeignet ist.
Um die BOW-Kodierung zu verstehen, schauen wir uns zunächst ein vereinfachtes Beispiel an. Für dieses Beispiel
Beispiel das Tag einer Stack Overflow-Frage aus einer Liste von drei möglichen Tags vorhersagen
möglichen Tags: "pandas", "keras" und "matplotlib". Um die Dinge einfach zu halten, nehmen wir an, dass unser
Vokabular nur aus den 10 unten aufgeführten Wörtern besteht:
dataframe
Ebene
Reihe
Diagramm
Spalte
Darstellung
Farbe
Achsen
lesen_csv
Aktivierung
Diese Liste ist unser Wortindex, und jede Eingabe, die wir in unser Modell eingeben, ist ein 10-
Element-Array sein, wobei jeder Index einem der oben aufgeführten Wörter entspricht. Für
eine 1 im ersten Index eines Eingabe-Arrays bedeutet beispielsweise, dass eine bestimmte Frage das Wort
das Wort Datenrahmen. Um die BOW-Kodierung aus der Perspektive unseres Modells zu verstehen
Modells zu verstehen, stellen Sie sich vor, Sie lernen eine neue Sprache und die 10 oben genannten Wörter sind die einzigen
Wörter, die wir kennen. Jede "Vorhersage", die wir treffen, basiert ausschließlich auf dem Vorhandensein oder
Vorhandensein oder Nichtvorhandensein dieser 10 Wörter und lässt alle Wörter außerhalb dieser Liste außer Acht.
Wenn die Frage lautet: "Wie stellt man ein Balkendiagramm für einen Datenrahmen dar", wie können wir dann
in eine BOW-Darstellung umwandeln? Notieren wir uns zunächst die Wörter, die in diesem Satz
tenz, die in unserem Vokabular vorkommen: plot, dataframe und graph. Die anderen Wörter in
diesem Satz werden beim Bag-of-Words-Ansatz ignoriert. Mit unserem Wortindex
wird dieser Satz zu:
[ 1 0 0 1 0 1 0 0 0 0 ]
68 | Kapitel 2: Entwurfsmuster für die Datendarstellung

Beachten Sie, dass die 1en in diesem Array mit den Indizes von dataframe, graph und
Plot entsprechen. Zusammenfassend zeigt Abbildung 2-21, wie wir unsere Eingabe
von Rohtext in ein BOW-kodiertes Array auf der Grundlage unseres Vokabulars umgewandelt haben.
Keras verfügt über einige Utility-Methoden für die Kodierung von Text als Bag of Words, so dass wir keinen
den Code für die Identifizierung der wichtigsten Wörter aus unserem Textkorpus und die Kodierung
Rohtext in Multi-Hot-Arrays von Grund auf neu schreiben.
Abbildung 2-21. Roher Eingabetext → Identifizierung von Wörtern aus unserem Vokabular, die in diesem Text vorkommen
lary → Umwandlung in eine Multi-Hot-BOW-Kodierung.
Da es zwei verschiedene Ansätze zur Darstellung von Text gibt (Embedding und
BOW), welchen Ansatz sollten wir für eine bestimmte Aufgabe wählen? Wie bei vielen Aspekten des
maschinellen Lernens hängt dies von unserem Datensatz, der Art unserer Vorhersageaufgabe und
der Art des Modells, das wir verwenden wollen.

Einbettungen fügen unserem Modell eine zusätzliche Ebene hinzu und liefern zusätzliche Informationen über
Wortbedeutung, die in der BOW-Kodierung nicht enthalten sind. Allerdings erfordern Einbettungen
müssen jedoch trainiert werden (es sei denn, wir können eine vortrainierte Einbettung für unser Problem verwenden). Während
ein Deep Learning-Modell eine höhere Genauigkeit erzielen kann, können wir auch versuchen, die BOW
Kodierung in einem linearen Regressions- oder Entscheidungsbaummodell mit Frameworks wie scikit-
learn oder XGBoost. Die Verwendung von BOW-Codierung mit einem einfacheren Modelltyp kann nützlich sein für
für ein schnelles Prototyping oder um zu überprüfen, ob die gewählte Vorhersageaufgabe in unserem
Datensatz funktioniert. Im Gegensatz zu Einbettungen berücksichtigt BOW nicht die Reihenfolge oder Bedeutung von
Wörter in einem Textdokument. Wenn beides für unsere Vorhersageaufgabe wichtig ist,
sind Einbettungen möglicherweise der beste Ansatz.

Es könnte auch von Vorteil sein, ein tiefes Modell zu erstellen, das sowohl Bag of Words
und Texteinbettungsrepräsentationen kombiniert, um mehr Muster aus unseren Daten zu extrahieren. Um dies zu tun
können wir den Ansatz der multimodalen Eingabe verwenden, mit dem Unterschied, dass wir anstelle der Verkettung von
Text- und Tabellenmerkmale zu verknüpfen, können wir die Embedding- und BOW-Darstellungen miteinander verknüpfen.
Darstellungen (siehe Code auf GitHub). Hier wäre die Form unserer Eingabeschicht die Vokabulargröße der BOW-Darstellung.
Größe der BOW-Darstellung entsprechen. Einige Vorteile der Darstellung von Text auf verschiedene
Darstellungsformen sind:

Entwurfsmuster 4: Multimodale Eingabe | 69
Die BOW-Kodierung liefert starke Signale für die wichtigsten Wörter in unserem
unserem Vokabular, während Einbettungen Beziehungen zwischen Wörtern in einem
viel größerem Vokabular.
Wenn wir einen Text haben, der zwischen verschiedenen Sprachen wechselt, können wir Einbettungen (oder
BOW-Kodierungen) für jede Sprache erstellen und sie miteinander verknüpfen.
Einbettungen können die Häufigkeit von Wörtern im Text kodieren, wobei der BOW
das Vorhandensein eines jeden Wortes als booleschen Wert behandelt. Beide Darstellungen sind wertvoll.
Die BOW-Kodierung kann Muster zwischen Rezensionen erkennen, die alle das Wort
"fantastisch" enthalten, während eine Einbettung lernen kann, die Phrase "nicht fantastisch" mit einer
mit einer unterdurchschnittlichen Bewertung korreliert. Auch hier sind beide Darstellungen wertvoll.
Extrahieren von tabellarischen Merkmalen aus Text. Neben der Kodierung von Rohtextdaten gibt es
gibt es oft andere Merkmale von Text, die als tabellarische Merkmale dargestellt werden können. Nehmen wir an
wir bauen ein Modell auf, um vorherzusagen, ob eine Stack Overflow-Frage eine Antwort
eine Antwort erhält. Verschiedene Faktoren des Textes, die nichts mit den genauen Wörtern selbst zu tun haben
können für das Training eines Modells für diese Aufgabe relevant sein. Zum Beispiel könnte die Länge einer Frage
Frage oder das Vorhandensein eines Fragezeichens die Wahrscheinlichkeit einer Antwort beeinflussen.
Wenn wir jedoch eine Einbettung erstellen, schneiden wir die Wörter normalerweise auf eine bestimmte
Länge. Die tatsächliche Länge einer Frage geht in dieser Datendarstellung verloren. Ähnlich,
werden Satzzeichen oft entfernt. Wir können das Entwurfsmuster für multimodale Eingaben verwenden, um
diese verlorenen Informationen in das Modell zurückzubringen.

In der folgenden Abfrage extrahieren wir einige tabellarische Merkmale aus dem Titelfeld des
Stack Overflow-Datensatzes, um vorherzusagen, ob eine Frage beantwortet wird oder nicht:

SELECT
LENGTH (Titel) AS title_len,
ARRAY_LENGTH(SPLIT(titel, " ")) AS word_count,
ENDS_WITH(title, "?") AS ends_with_q_mark,
IF
(answer_count > 0,
1,
0) AS is_answered,
FROM
`bigquery- public - data .stackoverflow.posts_questions`
Dies führt zu folgendem Ergebnis:

Zeile title_len word_count ends_with_q_mark is_answered
1 84 14 wahr 0
2 104 16 falsch 0
3 85 19 wahr 1
4 88 14 falsch 1
5 17 3 falsch 1
70 | Kapitel 2: Entwurfsmuster für die Datendarstellung

Zusätzlich zu diesen Merkmalen, die direkt aus dem Titel einer Frage extrahiert werden, können wir auch
Metadaten über die Frage als Merkmale darstellen. Zum Beispiel könnten wir Merkmale hinzufügen, die die Anzahl der
die Anzahl der Tags, die die Frage hatte, und den Wochentag, an dem sie gestellt wurde
veröffentlicht wurde. Wir könnten dann diese tabellarischen Merkmale mit unserem kodierten Text kombinieren und
und beide Darstellungen in unser Modell einspeisen, indem wir die Keras-Schicht Concatenate verwenden, um das
BOW-codierten Text-Array mit den tabellarischen Metadaten zu kombinieren, die unseren Text beschreiben.

Multimodale Darstellung von Bildern

Ähnlich wie bei unserer Analyse von Einbettungen und BOW-Kodierung für Text gibt es viele
gibt es viele Möglichkeiten, Bilddaten bei der Vorbereitung für ein ML-Modell darzustellen. Wie Rohtext,
Bilder nicht direkt in ein Modell eingespeist werden und müssen in ein numerisches Format umgewandelt
numerisches Format umgewandelt werden, das das Modell verstehen kann. Wir beginnen mit der Erörterung einiger gängiger
Ansätze zur Darstellung von Bilddaten: als Pixelwerte, als Sätze von Kacheln und als Sätze von
gefensterten Sequenzen. Das Entwurfsmuster "Multimodale Eingabe" bietet eine Möglichkeit zur Verwendung
mehr als eine Darstellung eines Bildes in unserem Modell zu verwenden.

Bilder als Pixelwerte. Im Kern sind Bilder Anordnungen von Pixelwerten. Ein Schwarz-Weiß
weißes Bild zum Beispiel enthält Pixelwerte zwischen 0 und 255. Wir könnten
ein 28×28-Pixel-Schwarz-Weiß-Bild in einem Modell als ein 28×28-Array
mit ganzzahligen Werten im Bereich von 0 bis 255 darstellen. In diesem Abschnitt beziehen wir uns auf den
MNIST-Datensatz, ein beliebter ML-Datensatz, der Bilder von handgeschriebenen Ziffern enthält.

Mit der Sequential API können wir unsere MNIST-Bilder mit Pixelwerten darstellen, indem wir
eine Flatten-Schicht, die das Bild in ein eindimensionales Array mit 784 (28 * 28) Elementen zerlegt
Array umwandelt:

layers.Flatten(input_shape=(28, 28))
Bei Farbbildern wird die Sache noch komplexer. Jedes Pixel in einem RGB-Farbbild hat drei
Werte - einen für Rot, Grün und Blau. Wären unsere Bilder im obigen Beispiel stattdessen
Farbe wären, würden wir dem input_shape des Modells eine dritte Dimension hinzufügen, so dass es eine wäre:

layers.Flatten(input_shape=(28, 28, 3))
Die Darstellung von Bildern als Arrays von Pixelwerten funktioniert zwar gut für einfache Bilder wie
Graustufenbilder des MNIST-Datensatzes gut funktioniert, bricht sie zusammen, wenn wir
Bilder mit mehr Kanten und Formen einführen. Wenn ein Netzwerk mit allen Pixeln eines Bildes gefüttert wird
Pixel eines Bildes gefüttert wird, fällt es ihm schwer, sich auf kleinere Bereiche benachbarter Pixel zu konzentrieren
die wichtige Informationen enthalten.

Bilder als gekachelte Strukturen. Wir brauchen eine Möglichkeit, komplexere Bilder aus der realen Welt darzustellen
die es unserem Modell ermöglicht, sinnvolle Details zu extrahieren und Muster zu verstehen.
tern. Wenn wir das Netzwerk jeweils nur mit kleinen Teilen eines Bildes füttern, wird es eher in der Lage sein
Es ist wahrscheinlicher, dass es Dinge wie räumliche Gradienten und Kanten in benachbarten Pixeln erkennt.

Entwurfsmuster 4: Multimodale Eingabe | 71
Eine gängige Modellarchitektur, um dies zu erreichen, ist ein Faltungsneuronales Netz
arbeit (CNN).

Schichten eines neuronalen Faltungsnetzwerks
Werfen Sie einen Blick auf Abbildung 2-22. In diesem Beispiel haben wir ein 4×4-Gitter, in dem jedes Quadrat
Pixelwerte in unserem Bild darstellt. Wir verwenden dann Max-Pooling, um den größten
Wert jedes Gitters zu nehmen und daraus eine kleinere Matrix zu erzeugen. Durch die Unterteilung unseres Bildes in
Kacheln unterteilt ist, kann unser Modell wichtige Erkenntnisse aus jeder Region eines Bildes
auf verschiedenen Ebenen der Granularität.
Abbildung 2-22. Max-Pooling auf einer einzelnen 4×4-Scheibe von Bilddaten.
In Abbildung 2-22 wird eine Kernelgröße von (2, 2) verwendet. Die Kernelgröße bezieht sich auf die Größe der einzelnen Teile des
unseres Bildes. Die Anzahl der Leerzeichen, die unser Filter vor der Erstellung des nächsten Teilstücks zurücklegt, auch
Da die Schrittweite gleich der Größe des Kerns ist, überlappen sich die erzeugten Chunks nicht.
nicht überlappen.
Bei dieser Kachelmethode bleiben zwar mehr Details erhalten als bei der Darstellung von Bildern als Arrays von
Pixelwerten darstellt, geht nach jedem Pooling-Schritt eine ganze Menge an Informationen verloren. In dem Diagramm
würde der nächste Pooling-Schritt einen skalaren Wert von 8 ergeben, wodurch unsere Matrix
in nur zwei Schritten von 4 ×4 auf einen einzigen Wert. Bei einem realen Bild können Sie sich vorstellen
Sie können sich vorstellen, wie dies ein Modell dazu verleiten könnte, sich auf Bereiche mit dominanten Pixelwerten zu konzentrieren, während
wichtige Details, die diese Bereiche umgeben können, verloren gehen.
Wie können wir diese Idee der Aufteilung von Bildern in kleinere Teile weiterverfolgen und gleichzeitig
wichtige Details in den Bildern zu erhalten? Wir werden dies tun, indem wir diese Teile überlappen lassen. Wenn
das Beispiel in Abbildung 2-22 stattdessen eine Schrittweite von 1 verwendet hätte, wäre die Ausgabe stattdessen
eine 3×3-Matrix (Abbildung 2-23).
72 | Kapitel 2: Entwurfsmuster für die Datendarstellung

Abbildung 2-23. Verwendung überlappender Fenster für Max-Pooling auf einem 4×4-Pixel-Raster.
Wir könnten dies dann in ein 2×2-Gitter umwandeln (Abbildung 2-24).
Abbildung 2-24. Umwandlung des 3×3-Gitters in ein 2×2-Gitter mit Schiebefenstern und Max-Pooling.
ing.
Am Ende erhalten wir einen Einzelwert von 127. Während der Endwert derselbe ist, können Sie sehen
wie die Zwischenschritte mehr Details der ursprünglichen Matrix erhalten haben.
Keras bietet Faltungsschichten zum Erstellen von Modellen, die Bilder in kleinere, gewinnbringende Teile aufteilen.
chunks aufteilen. Nehmen wir an, wir erstellen ein Modell zur Klassifizierung von 28×28 Farbbildern als
entweder "Hund" oder "Katze". Da diese Bilder farbig sind, wird jedes Bild als ein
28×28×3-dimensionales Array dargestellt, da jedes Pixel drei Farbkanäle hat. So würden wir
wir die Eingaben für dieses Modell mit einer Faltungsschicht und der Sequential
API:

Conv2D(filters=16, kernel_size=3, activation='relu', input_shape=(28,28,3))
In diesem Beispiel teilen wir unsere Eingabebilder in 3×3 Teile auf, bevor wir sie
sie durch eine Max-Pooling-Schicht geleitet werden. Aufbau einer Modellarchitektur, die Bilder aufteilt

Entwurfsmuster 4: Multimodale Eingabe | 73
in Teile von Schiebefenstern zu unterteilen, ermöglicht es unserem Modell, feinere Details
in einem Bild wie Kanten und Formen zu erkennen.

Kombinieren verschiedener Bilddarstellungen. Darüber hinaus kann es, wie bei der Bag of Words und
Text-Einbettung, kann es nützlich sein, dieselben Bilddaten auf verschiedene Arten darzustellen.
Auch dies können wir mit der funktionalen API von Keras erreichen.

So kombinieren wir unsere Pixelwerte mit der Darstellung des Schiebefensters
unter Verwendung der Keras Concatenate-Schicht:

# Bildeingabeebene definieren (gleiche Form für Pixel- und Kacheldarstellung
# Darstellung)
image_input = Input(shape=(28,28,3))
# Pixeldarstellung definieren
pixel_layer = Flatten()(image_input)
# Definieren der Kacheldarstellung
tiled_layer = Conv2D(filters=16, kernel_size=3,
activation='relu')(image_input)
tiled_layer = MaxPooling2D()(tiled_layer)
tiled_layer = tf.keras.layers.Flatten()(tiled_layer)
# Zusammenfügen zu einer einzigen Ebene
merged_image_layers = keras.layers.concatenate([pixel_layer, tiled_layer])
Um ein Modell zu definieren, das diese multimodale Eingabedarstellung akzeptiert, können wir dann
unsere konkatenierte Schicht in unsere Ausgabeschicht einspeisen:

merged_dense = Dense(16, activation='relu')(merged_image_layers)
merged_output = Dense(1)(merged_dense)
model = Model(inputs=image_input, outputs=merged_output)
Die Wahl der zu verwendenden Bildrepräsentation oder der Verwendung multimodaler Repräsenta- tionen
Repräsentationen zu verwenden, hängt weitgehend von der Art der Bilddaten ab, mit denen wir arbeiten. Im Allgemeinen gilt, je
je detaillierter unsere Bilder sind, desto wahrscheinlicher ist es, dass wir sie als
Kacheln oder gleitende Fenster von Kacheln. Für den MNIST-Datensatz kann die Darstellung von Bildern als Pixel
Werte allein ausreichen. Bei komplexen medizinischen Bildern hingegen können wir
eine höhere Genauigkeit durch die Kombination mehrerer Darstellungen. Warum mehrere
ple Bilddarstellungen? Durch die Darstellung von Bildern als Pixelwerte kann das Modell
übergeordnete Fokuspunkte in einem Bild wie dominante, kontrastreiche Objekte zu identifizieren.
Kacheldarstellungen hingegen helfen den Modellen, feinere, kontrastärmere Kanten und Formen zu erkennen.
kontrastreicheren Kanten und Formen.

74 | Kapitel 2: Entwurfsmuster für die Datendarstellung

Verwendung von Bildern mit Metadaten. Zuvor haben wir verschiedene Arten von Metadaten besprochen, die
die mit Text verbunden sein können, und wie man diese Metadaten als tabellarische Merkmale für unser Modell extrahiert und darstellt.
Merkmale für unser Modell zu extrahieren und darzustellen. Wir können dieses Konzept auch auf Bilder anwenden. Dazu kehren wir
Wir kehren zu dem in Abbildung 2-19 gezeigten Beispiel eines Modells zurück, das das Bildmaterial eines Inter
um vorherzusagen, ob es einen Verkehrsverstoß enthält oder nicht. Unser Modell kann
viele Muster aus den Verkehrsbildern extrahieren, aber es können auch andere Daten
zur Verfügung, die die Genauigkeit unseres Modells verbessern könnten. Zum Beispiel könnte ein bestimmtes
Verhalten (z. B. das Rechtsabbiegen bei Rot) während der Hauptverkehrszeit nicht erlaubt, aber zu
anderen Tageszeiten. Oder vielleicht verstoßen Fahrer bei schlechtem Wetter eher gegen die Verkehrsregeln
Wetter. Wenn wir Bilddaten von mehreren Kreuzungen sammeln, kann die Kenntnis des Standorts unseres Bildes
unseres Bildes auch für unser Modell nützlich sein.

Wir haben nun drei zusätzliche tabellarische Merkmale identifiziert, die unser Bild verbessern könnten
modellieren:

Zeit des Tages
Wetter
Standort
Lassen Sie uns nun über mögliche Darstellungen für jedes dieser Merkmale nachdenken. Wir könnten
die Zeit als ganze Zahl darstellen, die die Stunde des Tages angibt. Dies könnte uns helfen, Muster zu identifizieren
Muster zu identifizieren, die mit verkehrsreichen Zeiten wie der Rushhour verbunden sind. Im Zusammenhang mit diesem
Modells ist es vielleicht nützlicher zu wissen, ob es dunkel war, als das Bild aufgenommen wurde
aufgenommen wurde. In diesem Fall könnten wir die Zeit als boolesches Merkmal darstellen.

Auch das Wetter kann auf verschiedene Weise dargestellt werden, sowohl als numerische als auch als kategorische
Werte. Wir könnten die Temperatur als Merkmal einbeziehen, aber in diesem Fall ist die Sichtbarkeit vielleicht
nützlicher sein. Eine weitere Möglichkeit, das Wetter darzustellen, ist eine kategorische Variable, die das Vorhandensein von Regen oder Schnee anzeigt.
Variable, die das Vorhandensein von Regen oder Schnee anzeigt.

Wenn wir Daten von vielen Orten sammeln, würden wir dies wahrscheinlich auch als Merkmal kodieren wollen.
Merkmal kodieren. Dies wäre am sinnvollsten als kategorisches Merkmal und könnte sogar
mehrere Merkmale sein (Stadt, Land, Staat usw.), je nachdem, an wie vielen Orten wir
Filmmaterial sammeln.

Für dieses Beispiel würden wir gerne die folgenden tabellarischen Merkmale verwenden:

Zeit als Stunde des Tages (Integer)
Sichtbarkeit (Float)
Schlechtes Wetter (kategorisch: Regen, Schnee, kein Wetter)
Standort-ID (kategorisch mit fünf möglichen Standorten)
Hier sehen Sie, wie eine Teilmenge dieses Datensatzes für die drei Beispiele aussehen könnte:

Entwurfsmuster 4: Multimodale Eingabe | 75
Daten = {
'Zeit': [9,10,2],
'visibility': [0.2, 0.5, 0.1],
'inclement_weather': [[0,0,1], [0,0,1], [1,0,0]],
'location': [[0,1,0,0,0], [0,0,0,1,0], [1,0,0,0,0]]
}
Wir könnten dann diese tabellarischen Merkmale in einem einzigen Array für jedes Beispiel kombinieren, so dass
dass die Eingabeform unseres Modells 10 wäre. Das Eingabe-Array für das erste Beispiel
würde wie folgt aussehen:

[9, 0.2, 0, 0, 1, 0, 1, 0, 0, 0]
Wir könnten diese Eingabe in eine dichte, vollständig verbundene Schicht einspeisen, und die Ausgabe unseres
Modells wäre ein einzelner Wert zwischen 0 und 1, der angibt, ob die Instanz einen Verkehrsverstoß enthält oder nicht.
einen Verkehrsverstoß enthält. Um dies mit unseren Bilddaten zu kombinieren, werden wir einen ähnlichen
Ansatz, den wir für Textmodelle besprochen haben. Zuerst definieren wir eine Faltungsschicht
um unsere Bilddaten zu verarbeiten, dann eine Dense-Schicht, um unsere Tabellendaten zu verarbeiten, und schließlich
verketten wir beide zu einer einzigen Ausgabe.

Dieser Ansatz ist in Abbildung 2-25 skizziert.

Abbildung 2-25. Verkettung von Ebenen zur Bearbeitung von Bild- und tabellarischen Metadatenmerkmalen.

76 | Kapitel 2: Entwurfsmuster für die Datendarstellung

Multimodale Merkmalsdarstellungen und Modellinterpretierbarkeit

Deep-Learning-Modelle sind von Natur aus schwer zu erklären. Wenn wir ein Modell bauen, das eine
99 % Genauigkeit erreicht, wissen wir immer noch nicht genau, wie unser Modell die Vorhersagen trifft
und folglich auch nicht, ob die Art und Weise, in der es diese Vorhersagen trifft, korrekt ist. Ein Beispiel,
Nehmen wir an, wir trainieren ein Modell auf Bildern von Petrischalen, die in einem Labor aufgenommen wurden, und es erreicht eine hohe
Genauigkeit erzielt. Diese Bilder enthalten auch Anmerkungen des Wissenschaftlers, der sie aufgenommen hat.
der die Bilder aufgenommen hat. Was wir nicht wissen, ist, dass das Modell die Anmerkungen fälschlicherweise
Anmerkungen und nicht den Inhalt der Petrischalen für seine Vorhersagen verwendet.

Es gibt verschiedene Techniken zur Erklärung von Bildmodellen, mit denen die Bilder hervorgehoben werden können.
die die Vorhersage eines Modells signalisiert haben. Wenn wir mehrere Datenrepräsenta- tionen in einem
Wenn wir jedoch mehrere Datendarstellungen in einem einzigen Modell kombinieren, werden diese Merkmale voneinander abhängig.
Infolgedessen kann es schwierig sein, zu erklären, wie das Modell Vorhersagen trifft.
Die Erklärbarkeit wird in Kapitel 7 behandelt.

Zusammenfassung
In diesem Kapitel haben wir verschiedene Ansätze zur Darstellung von Daten für unser Modell kennen gelernt.
Zunächst haben wir uns angeschaut, wie man mit numerischen Eingaben umgeht und wie die Skalierung dieser
Eingaben die Trainingszeit des Modells beschleunigen und die Genauigkeit verbessern kann. Dann haben wir erforscht
kategorialen Eingaben, insbesondere mit der One-Hot-Kodierung und der
und die Verwendung von Arrays kategorialer Werte.

Im weiteren Verlauf des Kapitels haben wir vier Entwurfsmuster für die Darstellung von Daten diskutiert.
von Daten. Das erste war das Entwurfsmuster "Hashed Feature", bei dem kategorische
kategorialen Eingaben als eindeutige Zeichenketten. Wir untersuchten einige verschiedene Ansätze für Hash
anhand des Flughafendatensatzes in BigQuery. Das zweite Muster, das wir in diesem Kapitel
Kapitel untersuchten, war Embeddings, eine Technik zur Darstellung von Daten mit hoher Kardinalität wie
Eingaben mit vielen möglichen Kategorien oder Textdaten. Einbettungen repräsentieren Daten im mehr
tidimensionalen Raum dar, wobei die Dimension von unseren Daten und der Vorhersageaufgabe
Aufgabe abhängt. Als Nächstes haben wir uns Feature-Crosses angesehen, einen Ansatz, der zwei Features miteinander verbindet, um Beziehungen zu extrahieren
Beziehungen zu extrahieren, die durch die Kodierung der Merkmale allein nicht leicht zu
kodieren. Schließlich haben wir uns mit multimodalen Eingabedarstellungen befasst, indem wir das Problem angegangen sind
Problem, wie man Eingaben verschiedener Typen in einem Modell kombiniert und wie ein
ein einzelnes Merkmal auf mehrere Arten dargestellt werden kann.

In diesem Kapitel ging es um die Vorbereitung der Eingabedaten für unsere Modelle. Im nächsten Kapitel,
werden wir uns auf die Modellausgabe konzentrieren, indem wir verschiedene Ansätze zur Darstellung unserer
Vorhersageaufgabe.

Zusammenfassung | 77
KAPITEL 3

Entwurfsmuster für die Problemdarstellung
Kapitel 2 befasste sich mit Entwurfsmustern, die die unzähligen Möglichkeiten katalogisieren, mit denen Eingaben in
Modelle des maschinellen Lernens dargestellt werden können. Dieses Kapitel befasst sich mit verschiedenen Arten von
Probleme des maschinellen Lernens und analysiert, wie die Modellarchitekturen je nach Problem variieren.
je nach Problemstellung.

Die Eingabe- und Ausgabetypen sind zwei Schlüsselfaktoren, die sich auf die Modellarchitektur auswirken.
So kann die Ausgabe bei überwachten maschinellen Lernproblemen unterschiedlich sein, je nachdem, ob es sich
je nachdem, ob es sich bei dem zu lösenden Problem um ein Klassifizierungs- oder ein Regressionsproblem handelt.
Es gibt spezielle neuronale Netzwerkschichten für bestimmte Arten von Eingabedaten: Faltungsschichten für Bilder, Sprache
für Bilder, Sprache, Text und andere Daten mit räumlich-zeitlicher Korrelation, rekurrente
Netze für sequentielle Daten und so weiter. Eine umfangreiche Literatur hat sich mit speziellen
Techniken wie Max-Pooling, Aufmerksamkeit und so weiter für diese Arten von Schichten. Unter
Darüber hinaus wurden spezielle Klassen von Lösungen für häufig auftretende Probleme wie
Probleme wie Empfehlungen (z. B. Matrixfaktorisierung) oder Zeitreihenvorhersagen
(zum Beispiel ARIMA). Schließlich kann eine Gruppe von einfacheren Modellen zusammen mit gemeinsamen
Idiome verwendet werden, um komplexere Probleme zu lösen, z. B. bei der Textgenerierung
ein Klassifizierungsmodell, dessen Ergebnisse mit einem Balkensuchalgorithmus nachbearbeitet werden.
Suchalgorithmus nachbearbeitet werden.

Um unsere Diskussion zu begrenzen und Bereiche aktiver Forschung zu vermeiden, ignorieren wir
Muster und Redewendungen, die mit speziellen Bereichen des maschinellen Lernens verbunden sind. Stattdessen,
wir uns auf Regression und Klassifikation und untersuchen Muster der Problemdarstellung
Repräsentation in nur diesen beiden Arten von ML-Modellen.

Das Reframing-Entwurfsmuster nimmt eine Lösung, die intuitiv ein Regressionsproblem ist
ist, als Klassifikationsproblem (und umgekehrt). Das Multilabel-Entwurfsmuster
tern behandelt den Fall, dass Trainingsbeispiele zu mehr als einer Klasse gehören können. Das
Kaskaden-Entwurfsmuster behandelt Situationen, in denen ein Problem des maschinellen Lernens
gewinnbringend in eine Reihe (oder Kaskade) von ML-Problemen aufgeteilt werden kann. Das Ensemble-Entwurfsmuster

79
Muster löst ein Problem, indem es mehrere Modelle trainiert und deren Antworten zusammenfasst.
ses. Das Entwurfsmuster "Neutrale Klasse" befasst sich mit der Frage, wie man mit Situationen umgeht, in denen
nicht übereinstimmen. Das Entwurfsmuster Rebalancing empfiehlt Ansätze für den Umgang mit stark
stark verzerrten oder unausgewogenen Daten.

Entwurfsmuster 5: Reframing
Das Reframing-Entwurfsmuster bezieht sich auf die Änderung der Darstellung der Ausgabe eines
maschinellen Lernproblems. Wir könnten zum Beispiel ein Problem, das intuitiv ein Regressionsproblem ist
Regressionsproblem ist, stattdessen als Klassifizierungsproblem darstellen (und andersherum).

Problem
Der erste Schritt bei der Entwicklung einer Lösung für maschinelles Lernen besteht darin, das Problem zu formulieren. Ist
es sich um ein überwachtes Lernproblem? Oder ein unbeaufsichtigtes? Wie lauten die Merkmale? Wenn es sich um ein
Wenn es sich um ein überwachtes Problem handelt, wie lauten die Bezeichnungen? Welche Fehlerquote ist akzeptabel? Unter
Die Antworten auf diese Fragen müssen natürlich im Zusammenhang mit den Trainingsdaten
den Trainingsdaten, der jeweiligen Aufgabe und den Erfolgsmaßstäben betrachtet werden.

Nehmen wir zum Beispiel an, dass wir ein maschinelles Lernmodell erstellen wollen, um zukünftige
Niederschlagsmengen an einem bestimmten Ort vorherzusagen. Wäre dies, grob gesagt, eine Regressions- oder
Klassifizierungsaufgabe? Nun, da wir versuchen, die Niederschlagsmenge vorherzusagen (z. B.,
0,3 cm) vorhersagen wollen, ist es sinnvoll, dies als ein Problem der Zeitreihenvorhersage zu betrachten: Angesichts
aktuellen und historischen Klima- und Wettermustern, welche Niederschlagsmenge
in einem bestimmten Gebiet in den nächsten 15 Minuten zu erwarten? Alternativ dazu, weil die
(die Niederschlagsmenge) eine reale Zahl ist, könnten wir alternativ ein Regressionsmodell erstellen. Als
unser Modell zu entwickeln und zu trainieren, stellen wir (vielleicht nicht überraschend) fest, dass
Wettervorhersage schwieriger ist, als es klingt. Unsere vorhergesagten Niederschlagsmengen liegen alle daneben
weil es bei denselben Merkmalen manchmal 0,3 cm regnet und manchmal 0,5 cm.
regnet es 0,5 cm. Was sollten wir tun, um unsere Vorhersagen zu verbessern? Sollten wir mehr
Schichten zu unserem Netzwerk hinzufügen? Oder mehr Merkmale entwickeln? Vielleicht würden mehr Daten helfen?
Vielleicht brauchen wir eine andere Verlustfunktion?

Jede dieser Anpassungen könnte unser Modell verbessern. Aber halt. Ist Regression die einzige
Art und Weise, wie wir diese Aufgabe angehen können? Vielleicht können wir unser Ziel des maschinellen Lernens so umgestalten
so umformulieren, dass die Leistung unserer Aufgabe verbessert wird.

Lösung
Das Kernproblem besteht darin, dass Niederschläge probabilistisch sind. Für die gleiche Menge an Merkmalen kann es
regnet es manchmal 0,3 cm und ein anderes Mal 0,5 cm. Doch selbst wenn ein Regressionsmodell
Regressionsmodell in der Lage wäre, die beiden möglichen Mengen zu lernen, kann es nur eine
eine einzige Zahl.

80 | Kapitel 3: Entwurfsmuster für die Problemdarstellung

Anstatt zu versuchen, die Niederschlagsmenge als Regressionsaufgabe vorherzusagen, können wir
unser Ziel als Klassifizierungsproblem. Es gibt verschiedene Möglichkeiten, dies zu erreichen.
plished werden. Ein Ansatz ist die Modellierung einer diskreten Wahrscheinlichkeitsverteilung, wie sie in
Abbildung 3-1. Anstatt die Niederschlagsmenge als realen Wert vorherzusagen, modellieren wir die Ausgabe
Klassifizierung, die die Wahrscheinlichkeit angibt, dass die Niederschlagsmenge in den nächsten 15
Minuten innerhalb eines bestimmten Bereichs von Niederschlagsmengen liegt.

Abbildung 3-1. Anstatt den Niederschlag als Regressionsergebnis vorherzusagen, können wir stattdessen
eine diskrete Wahrscheinlichkeitsverteilung mit Hilfe einer Multiklassen-Klassifikation modellieren.

Sowohl der Regressionsansatz als auch dieser als Klassifizierung umgesetzte Ansatz liefern eine Vorhersage
Vorhersage der Niederschlagsmenge für die nächsten 15 Minuten. Der Klassifizierungsansatz jedoch
ermöglicht es dem Modell jedoch, die Wahrscheinlichkeitsverteilung der Niederschlagsmenge verschiedener Quan- ten zu erfassen
zu erfassen, anstatt den Mittelwert der Verteilung wählen zu müssen. Die Modellierung einer Verteilung
Verteilung zu modellieren, ist vorteilhaft, da der Niederschlag nicht die typische Glockenkurve einer
Glockenkurve einer Normalverteilung aufweist und stattdessen einer Tweedie-Verteilung folgt,
die ein Überwiegen der Punkte bei Null zulässt. Tatsächlich ist dies der Ansatz, der
in einem Google Research-Papier, das die Niederschlagsmenge an einem bestimmten Ort anhand einer
einer kategorialen 512-Wege-Verteilung. Weitere Gründe, warum die Modellierung einer Verteilung vorteilhaft sein kann
vorteilhaft sein kann, ist, wenn die Verteilung bimodal ist, oder sogar wenn sie normal ist, aber mit
einer großen Varianz. Eine aktuelle Arbeit, die alle Benchmarks bei der Vorhersage der Proteinfaltung übertrifft
Struktur von Proteinen übertrifft, sagt auch den Abstand zwischen Aminosäuren als ein 64-faches Klassifizierungsproblem
Klassifizierungsproblem, bei dem die Abstände in 64 Bins eingeteilt werden.

Ein weiterer Grund, ein Problem neu zu formulieren, besteht darin, dass das Ziel in der anderen Art von Modell besser zu erreichen ist.
des Modells besser ist. Nehmen wir zum Beispiel an, wir versuchen, ein Empfehlungssystem zu entwickeln für

Entwurfsmuster 5: Reframing | 81
Videos. Ein natürlicher Weg, dieses Problem als Klassifizierungsproblem zu betrachten, ist die Vorhersage
Vorhersage, ob ein Benutzer wahrscheinlich ein bestimmtes Video ansehen wird. Dieses Framing kann jedoch zu einem
zu einem Empfehlungssystem führen, das Klickköder bevorzugt. Es wäre vielleicht besser, das
Regressionsproblem der Vorhersage des Anteils des Videos, der angeschaut werden wird
angesehen wird.

Warum es funktioniert
Bei der Entwicklung einer Lösung für maschinelles Lernen kann es hilfreich sein, den Kontext zu ändern und die Aufgabe eines Problems neu zu formulieren.
Lösung für maschinelles Lernen. Anstatt eine einzelne reelle Zahl zu lernen, entspannen wir unsere Vorhersage
wir unser Vorhersageziel auf eine diskrete Wahrscheinlichkeitsverteilung. Wir verlieren ein wenig Präzision
ein wenig an Präzision, gewinnen aber die Aussagekraft einer vollständigen Wahrscheinlichkeitsdichtefunktion
Funktion (PDF). Die vom Klassifizierungsmodell gelieferten diskreten Vorhersagen sind
sind beim Lernen eines komplexen Ziels geschickter als das starrere Regressionsmodell.

Ein zusätzlicher Vorteil dieses Klassifizierungsrahmens ist, dass wir eine posteriore Pro- blemverteilung
Wahrscheinlichkeitsverteilung unserer vorhergesagten Werte erhalten, die nuanciertere Informa- tionen liefert.
tion liefert. Nehmen wir zum Beispiel an, dass die gelernte Verteilung bimodal ist. Durch die Modellierung einer
Klassifikation als diskrete Wahrscheinlichkeitsverteilung modelliert, ist das Modell in der Lage, die
bimodale Struktur der Vorhersagen erfassen, wie Abbildung 3-2 zeigt. Wird hingegen nur ein
einen einzelnen numerischen Wert vorgibt, würde diese Information verloren gehen. Je nach Anwendungsfall
Anwendungsfall könnte dies die Aufgabe leichter erlernbar und wesentlich vorteilhafter machen.

Abbildung 3-2. Die Umstrukturierung einer Klassifizierungsaufgabe zur Modellierung einer Wahrscheinlichkeitsverteilung ermöglicht
die Vorhersagen eine bimodale Ausgabe zu erfassen. Die Vorhersage ist nicht auf einen einzigen
Wert wie bei einer Regression.

Erfassen der Unsicherheit

Betrachten wir noch einmal den Geburtsdatensatz und die Aufgabe, das Gewicht des Babys vorherzusagen. Da
Babygewicht ein positiver realer Wert ist, handelt es sich intuitiv um ein Regressionsproblem. Allerdings
beachten Sie jedoch, dass für einen gegebenen Satz von Eingaben Gewicht_Pfund (das Label) viele verschiedene Werte annehmen kann.
Werte annehmen kann. Wir sehen, dass die Verteilung der Gewichte der Babys für einen bestimmten Satz von Eingabewerten
(männliche Babys, die von 25-jährigen Müttern in der 38. Woche geboren wurden) ungefähr einer
einer Normalverteilung folgt, die bei etwa 7,5 Pfund zentriert ist. Der Code zur Erstellung des Diagramms in
Abbildung 3-3 finden Sie im Repository für dieses Buch.

82 | Kapitel 3: Entwurfsmuster für die Problemdarstellung

Abbildung 3-3. Bei einem bestimmten Satz von Eingaben (z. B. männliche Babys, die von 25-jährigen Müttern
Müttern in der 38. Woche geboren wurden) nimmt die Variable weight_pounds eine Reihe von Werten an, die ungefähr
mäßig einer Normalverteilung folgt, deren Zentrum bei 7,5 lbs liegt.

Beachten Sie jedoch die Breite der Verteilung - auch wenn die Verteilung ihren Höhepunkt bei
7,5 Pfund erreicht, gibt es eine nicht unerhebliche Wahrscheinlichkeit (tatsächlich 33 %), dass ein bestimmtes Baby weniger
als 6,5 Pfund oder mehr als 8,5 Pfund wiegt! Die Breite dieser Verteilung zeigt
den irreduziblen Fehler, der mit dem Problem der Vorhersage des Babygewichts verbunden ist. In der Tat ist der
beste quadratische Fehlerwurzel, die wir für dieses Problem erhalten können, wenn wir es als Regressionsproblem betrachten.
Der beste quadratische Fehler, den wir bei diesem Problem erhalten können, wenn wir es als Regressionsproblem betrachten, ist die Standardabweichung der in Abbildung 3-3 dargestellten Verteilung.

Wenn wir dies als Regressionsproblem betrachten, müssten wir das Vorhersageergebnis wie folgt angeben
7,5 +/- 1,0 (oder was auch immer die Standardabweichung ist). Doch die Breite der Verteilung
für verschiedene Kombinationen von Eingaben variieren, so dass das Erlernen der Breite ein weiteres
Problem des maschinellen Lernens an und für sich. Zum Beispiel, in der 36. Woche, für Mütter
desselben Alters eine Standardabweichung von 1,16 Pfund. Die Quantilsregression, die
später in der Musterdiskussion behandelt wird, versucht genau dies zu tun, allerdings auf nichtparametrische Weise.

Wäre die Verteilung multimodal (mit mehreren Spitzen), wäre die
wäre es noch sinnvoller, das Problem als Klassifizierung zu betrachten
stärker. Es ist jedoch hilfreich zu erkennen, dass aufgrund des Gesetzes der
großen Zahlen, solange wir alle relevanten Eingaben erfassen,
viele der Verteilungen, auf die wir bei großen Datensätzen stoßen, glockenförmig
glockenförmig sein werden, obwohl auch andere Verteilungen möglich sind. Je breiter
die Glockenkurve ist, und je mehr diese Breite bei verschiedenen Werten der
Inputs variiert, desto wichtiger ist es, die Unsicherheit zu erfassen, und desto
desto mehr spricht dafür, das Regressionsproblem als Klassifizierungsproblem zu betrachten.
Klassifizierungsproblem.
Indem wir das Problem neu formulieren, trainieren wir das Modell als Multiklassen-Klassifikation, die
eine diskrete Wahrscheinlichkeitsverteilung für die gegebenen Trainingsbeispiele. Diese diskreten
Vorhersagen sind flexibler in Bezug auf die Erfassung von Unsicherheiten und besser geeignet, um

Entwurfsmuster 5: Reframing | 83
das komplexe Ziel näher als ein Regressionsmodell. Zum Zeitpunkt der Inferenz sagt das
Modell dann eine Sammlung von Wahrscheinlichkeiten vor, die diesen potenziellen Ergebnissen
Ergebnisse. Das heißt, wir erhalten eine diskrete PDF, die die relative Wahrscheinlichkeit eines bestimmten
Gewichtung. Natürlich ist hier Vorsicht geboten, denn Klassifizierungsmodelle können sehr stark
unkalibriert sein (z. B. wenn das Modell zu zuversichtlich und falsch ist).

Änderung des Ziels

In einigen Szenarien kann es von Vorteil sein, eine Klassifizierungsaufgabe als Regression zu betrachten.
Nehmen wir zum Beispiel an, wir hätten eine große Filmdatenbank mit Kundenbewertungen auf einer Skala
von 1 bis 5 für alle Filme, die der Benutzer gesehen und bewertet hat. Unsere Aufgabe ist die Erstellung eines
Modell des maschinellen Lernens zu erstellen, mit dem wir unseren Nutzern Empfehlungen geben können.

Als Klassifizierungsaufgabe betrachtet, könnten wir ein Modell erstellen, das als Eingabe
eine Benutzer-ID zusammen mit den bisherigen Videoaufrufen und -bewertungen des Benutzers verwendet und vorhersagt
welcher Film aus unserer Datenbank als nächstes empfohlen werden soll. Es ist jedoch möglich
dieses Problem als Regression umzudeuten. Anstatt dass das Modell ein kategorisches Out-
einem Film in unserer Datenbank entspricht, könnte unser Modell stattdessen ein
Multitasking-Lernen durchführen, wobei das Modell eine Reihe von Schlüsselmerkmalen lernt (wie
Einkommen, Kundensegment usw.) von Nutzern, die sich wahrscheinlich einen bestimmten Film ansehen werden.

Umgestaltet als Regressionsaufgabe sagt das Modell nun die Darstellung im Benutzerraum
für einen bestimmten Film. Um Empfehlungen auszusprechen, wählen wir die Filme aus, die
die den bekannten Eigenschaften eines Benutzers am nächsten kommen. Auf diese Weise liefert das Modell nicht mehr
die Wahrscheinlichkeit, dass ein Nutzer einen Film mag, wie bei einer Klassifizierung, erhalten wir
eine Gruppe von Filmen, die von Nutzern wie diesem gesehen wurden.

Indem wir das Klassifizierungsproblem bei der Empfehlung von Filmen als eine Regression
von Nutzereigenschaften, können wir unser Empfehlungsmodell leicht anpassen
Videos, Filmklassiker oder Dokumentarfilme zu empfehlen, ohne jedes Mal ein
ohne jedes Mal ein separates Klassifizierungsmodell trainieren zu müssen.

Diese Art von Modellansatz ist auch nützlich, wenn die numerische Darstellung eine
intuitiv zu interpretieren ist; so kann beispielsweise ein Paar aus Breiten- und Längengraden anstelle von
anstelle von Vorhersagen für Stadtgebiete. Angenommen, wir wollten vorhersagen, in welcher Stadt
die nächste Virenepidemie ausbricht oder in welchem New Yorker Stadtteil die Preise für Immobilien
sprunghaft ansteigt. Es könnte einfacher sein, den Breiten- und Längengrad vorherzusagen und die Stadt
Stadt oder das Viertel zu wählen, das diesem Ort am nächsten liegt, anstatt die Stadt oder das Viertel
Viertel selbst vorherzusagen.

Kompromisse und Alternativen
Es gibt selten nur einen Weg, ein Problem zu lösen, und es ist hilfreich, sich über alle
Kompromisse oder Alternativen für eine bestimmte Implementierung zu kennen. Zum Beispiel ist die Gruppierung der
Ausgangswerte einer Regression ist ein Ansatz, um das Problem als Klassifizierungsproblem zu betrachten.

84 | Kapitel 3: Entwurfsmuster für die Problemdarstellung

tionsaufgabe. Ein anderer Ansatz ist das Multitasking, das beide Aufgaben (Klassifizierung und Regression) in einem einzigen Modell kombiniert.
cation und Regression) in einem einzigen Modell unter Verwendung mehrerer Vorhersageköpfe kombiniert. Bei jeder
Reframing-Technik ist es wichtig, sich der Datenbeschränkungen oder des Risikos der Einführung
ist wichtig.

Bucketized Outputs

Der typische Ansatz für die Umgestaltung einer Regressionsaufgabe in eine Klassifizierung ist die Eingruppierung
der Ausgabewerte. Wenn unser Modell zum Beispiel dazu verwendet werden soll, um anzuzeigen, wann ein Baby
nach der Geburt kritisch behandelt werden muss, könnten die Kategorien in Tabelle 3-1 ausreichend sein.

Tabelle 3-1. Bucketized Outputs für Babygewicht

Kategorie Beschreibung
Hohes Geburtsgewicht Mehr als 8.8 lbs
Mittleres Geburtsgewicht Zwischen 5.5 lbs und 8.8 lbs
Geringes Geburtsgewicht Zwischen 3.31 lbs und 5.5 lbs
Sehr geringes Geburtsgewicht Weniger als 3,31 lbs
Unser Regressionsmodell wird nun zu einer Multiklassen-Klassifikation. Intuitiv ist es einfacher
einen von vier möglichen kategorialen Fällen vorherzusagen als einen einzelnen Wert aus dem Kontinuum
aus dem Kontinuum der reellen Zahlen vorherzusagen - genauso wie es einfacher wäre, eine binäre 0
versus 1 für is_underweight vorherzusagen, anstatt vier separate Kategorien high_weight
versus avg_weight versus low_weight versus very_low_weight. Durch die Verwendung kategorischer
kategorischen Ausgaben hat unser Modell weniger Anreize, willkürlich nahe an den tatsächlichen Wert heranzukommen.
Ausgabewert zu kommen, da wir die Ausgabebezeichnung im Wesentlichen in einen Wertebereich geändert haben
einer einzelnen reellen Zahl.

In dem Notebook zu diesem Abschnitt trainieren wir sowohl ein Regressions- als auch ein
Klassen-Klassifikationsmodell. Das Regressionsmodell erreicht einen RMSE von 1,3 auf dem Vali-
dationsset, während das Klassifikationsmodell eine Genauigkeit von 67 % aufweist. Der Vergleich dieser
beiden Modelle ist schwierig, da eine Bewertungskennzahl der RMSE und die andere die Genauigkeit ist.
Letztendlich hängt die Designentscheidung vom Anwendungsfall ab. Wenn medizinische Entscheidungen
basieren, dann sollte unser Modell eine Klassifikation sein, die diese Werte verwendet.
ets sein. Wenn jedoch eine genauere Vorhersage des Babygewichts benötigt wird, dann ist es sinnvoll, das
sinnvoll, das Regressionsmodell zu verwenden.

Andere Möglichkeiten zur Erfassung von Unsicherheit

Es gibt andere Möglichkeiten, die Unsicherheit in der Regression zu erfassen. Ein einfacher Ansatz ist die
die Durchführung einer Quantilsregression. Anstatt nur den Mittelwert vorherzusagen, können wir zum Beispiel
können wir das bedingte 10., 20., 30., ..., 90. Perzentil des zu prognostizierenden Wertes schätzen.
vorhersagen. Die Quantilsregression ist eine Erweiterung der linearen Regression. Reframing hingegen
Andererseits kann Reframing mit komplexeren maschinellen Lernmodellen arbeiten.

Entwurfsmuster 5: Reframing | 85
Ein anderer, anspruchsvollerer Ansatz ist die Verwendung eines Frameworks wie TensorFlow Proba-
bility zu verwenden, um eine Regression durchzuführen. Allerdings müssen wir die Verteilung der Ausgabe explizit modellieren.
der Ausgabe modellieren. Wenn zum Beispiel erwartet wird, dass die Ausgabe normalverteilt ist, um einen
Mittelwert, der von den Eingaben abhängt, wäre die Ausgabeschicht des Modells die folgende:

tfp.layers.DistributionLambda( lambda t: tfd.Normal(loc=t, scale=1))
Wenn wir andererseits wissen, dass die Varianz mit dem Mittelwert zunimmt, können wir
können wir sie mit der Lambda-Funktion modellieren. Beim Reframing hingegen ist es nicht
die posteriore Verteilung zu modellieren.

Beim Training eines maschinellen Lernmodells sind die Daten der Schlüssel. Mehr
komplexere Beziehungen erfordern in der Regel mehr Trainingsdaten
Beispielen, um die schwer fassbaren Muster zu finden. Vor diesem Hintergrund ist es wichtig
wichtig, wie die Datenanforderungen für Regressions- oder Klassifikationsmodelle
Klassifikationsmodelle. Eine gängige Faustregel für Klassifikations
Faustregel für Klassifizierungsaufgaben lautet, dass wir die 10-fache Anzahl von Modellmerkmalen
für jede Label-Kategorie. Für ein Regressionsmodell lautet die Faustregel
50-mal die Anzahl der Modellmerkmale. Natürlich sind diese Zahlen
nur grobe Heuristiken und sind nicht präzise. Die Intuition ist jedoch, dass
dass Regressionsaufgaben in der Regel mehr Trainingsbeispiele erfordern. Außerdem
Darüber hinaus steigt dieser Bedarf an umfangreichen Daten mit der Komplexität der Aufgabe.
plexität der Aufgabe. Es könnte also Datenbeschränkungen geben, die
die bei der Wahl des Modells oder, im Falle der Klassifizierung
bei der Klassifizierung die Anzahl der Label-Kategorien.
Genauigkeit der Vorhersagen

Wenn man ein Regressionsmodell als Mehrklassen-Klassifikation betrachtet, bestimmt die
Breite der Bins für die Ausgabebezeichnung die Genauigkeit des Klassifizierungsmodells
Modells. Im Fall unseres Beispiels mit dem Gewicht eines Babys würden wir, wenn wir genauere Informationen
der diskreten Wahrscheinlichkeitsdichtefunktion benötigen würden, müssten wir die
Anzahl der Bins unseres kategorialen Modells erhöhen. Abbildung 3-4 zeigt, wie die diskreten Wahrscheinlichkeits
Wahrscheinlichkeitsverteilungen entweder als 4- oder 10-fache Klassifikation aussehen würden.

86 | Kapitel 3: Entwurfsmuster für die Problemdarstellung

Abbildung 3-4. Die Genauigkeit der Multiklassenklassifizierung wird durch die Breite der
Bins für das Label gesteuert.

Die Schärfe der PDF zeigt die Präzision der Aufgabe als Regression an. A
schärfere PDF weist auf eine geringere Standardabweichung der Ausgangsverteilung hin, während eine
breitere PDF eine größere Standardabweichung und damit mehr Varianz anzeigt. Bei einer sehr
scharfen Dichtefunktion ist es besser, sich an ein Regressionsmodell zu halten (siehe Abbildung 3-5).

Entwurfsmuster 5: Reframing | 87
Abbildung 3-5. Die Genauigkeit der Regression wird durch die Schärfe der Wahrscheinlichkeits
Wahrscheinlichkeitsdichtefunktion für einen festen Satz von Eingabewerten.

Eingrenzung des Vorhersagebereichs

Ein weiterer Grund, das Problem neu zu formulieren, besteht darin, dass der Bereich der
der Vorhersage zu begrenzen. Nehmen wir zum Beispiel an, dass realistische Ausgabewerte für ein Regres- sionsproblem im
Regressionsproblem in einem Bereich von [3, 20] liegen. Wenn wir ein Regressionsmodell trainieren, bei dem die Ausgabe
Schicht eine lineare Aktivierungsfunktion ist, besteht immer die Möglichkeit, dass die Modellvorhersagen
Vorhersagen außerhalb dieses Bereichs liegen. Eine Möglichkeit, den Bereich der Ausgabe zu begrenzen, besteht darin
das Problem neu zu formulieren.

Machen Sie die Aktivierungsfunktion der vorletzten Schicht zu einer Sigmoidfunktion (die
typischerweise mit Klassifizierung assoziiert), so dass sie im Bereich [0,1] liegt, und lassen Sie die letzte
Schicht diese Werte auf den gewünschten Bereich skalieren:

MIN_Y = 3
MAX_Y = 20
eingabe_größe = 10
inputs = keras.layers.Input(shape=(input_size,))
h1 = keras.layers.Dense(20, 'relu')(inputs)
h2 = keras.layers.Dense(1, 'sigmoid')(h1) # Bereich 0-1
output = keras.layers.Lambda(
lambda y : (y*(MAX_Y-MIN_Y) + MIN_Y))(h2) # skaliert
model = keras.Model(inputs, output)
Wir können überprüfen (siehe das Notizbuch auf GitHub für den vollständigen Code), dass dieses Modell nun
Zahlen im Bereich [3, 20] ausgibt. Da die Ausgabe ein Sigmoid ist, wird das Modell
Modell niemals das Minimum und Maximum des Bereichs erreichen wird, sondern nur sehr
nahe daran. Als wir das obige Modell mit einigen Zufallsdaten trainierten, erhielten wir Werte im
dem Bereich [3.03, 19.99].

88 | Kapitel 3: Entwurfsmuster für die Problemdarstellung

Etikettenschwindel

Empfehlungssysteme wie die Matrixfaktorisierung können im Kontext von
Neuronale Netze, sowohl als Regression als auch als Klassifikation. Ein Vorteil dieses Kontextwechsels
ist, dass ein neuronales Netz, das als Regressions- oder Klassifikationsmodell
ein neuronales Netz, das als Regressions- oder Klassifikationsmodell konzipiert ist, viel mehr zusätzliche Merkmale als nur die Benutzer- und
Einbettungen, die bei der Matrixfaktorisierung gelernt wurden. Es kann also eine attraktive Alternative sein.

Es ist jedoch wichtig, die Art der Zielkennzeichnung zu berücksichtigen, wenn man das Problem neu formuliert.
Problem. Nehmen wir zum Beispiel an, dass wir unser Empfehlungsmodell in eine Klassifizierungsaufgabe umwandeln
Klassifizierungsaufgabe umgestaltet, die die Wahrscheinlichkeit vorhersagt, dass ein Nutzer auf ein bestimmtes Video klicken wird.
Nagel klicken wird. Dies scheint eine sinnvolle Umformung zu sein, da unser Ziel darin besteht, Inhalte zu liefern, die ein
Nutzer auswählen und ansehen wird. Aber Vorsicht. Diese Änderung der Bezeichnung entspricht nicht wirklich
mit unserer Vorhersageaufgabe. Durch die Optimierung für Benutzerklicks wird unser Modell ungewollt
versehentlich Klickköder fördern und dem Nutzer nicht wirklich nützliche Inhalte empfehlen.

Stattdessen wäre eine vorteilhaftere Bezeichnung die Videobeobachtungszeit, was unsere Empfehlung als Regression umrahmt.
Empfehlung stattdessen als Regression. Oder wir können das Klassifizierungsziel dahingehend ändern
um die Wahrscheinlichkeit vorherzusagen, dass ein Nutzer mindestens die Hälfte des Videoclips anschaut. Es gibt
oft mehr als einen geeigneten Ansatz, und es ist wichtig, das Problem ganzheitlich zu
ganzheitlich zu betrachten, wenn man eine Lösung entwirft.

Seien Sie vorsichtig, wenn Sie das Label und die Trainingsaufgabe Ihres
Modells für maschinelles Lernen, da dies versehentlich zu einer
in Ihre Lösung einbringen. Betrachten Sie noch einmal das Beispiel der Videoempfehlun-
Empfehlung, das wir in "Warum es funktioniert" auf Seite 82 besprochen haben.
Multitasking-Lernen

Eine Alternative zum Reframing ist das Multitasking-Lernen. Anstatt zu versuchen, zwischen
zwischen Regression und Klassifikation zu entscheiden, machen Sie beides! Allgemein gesprochen bezieht sich das Multitasking
jedes maschinelle Lernmodell, bei dem mehr als eine Verlustfunktion opti-
optimiert wird. Dies kann auf viele verschiedene Arten erreicht werden, aber die beiden häufigsten
Formen des Multitasking-Lernens in neuronalen Netzen sind das Hard-Parameter-Sharing
und Soft-Parameter-Sharing.

Die gemeinsame Nutzung von Parametern bezieht sich auf die gemeinsame Nutzung der Parameter des neuronalen Netzes
zwischen den verschiedenen Ausgabeaufgaben, wie Regression und Klassifizierung, geteilt werden. Hard param-
Parameter-Sharing liegt vor, wenn die versteckten Schichten des Modells von allen
Ausgabeaufgaben geteilt werden. Beim Soft-Parameter-Sharing hat jedes Label sein eigenes neuronales Netz mit eigenen
eigenen Parametern, und die Parameter der verschiedenen Modelle werden durch
durch eine Form der Regularisierung zu ähneln. Abbildung 3-6 zeigt die typische Archi- tektur für
Abbildung 3-6 zeigt die typische Architektur für die gemeinsame Nutzung von harten und weichen Parametern.

Entwurfsmuster 5: Reframing | 89
Abbildung 3-6. Zwei gängige Implementierungen des Multitasking-Lernens sind das Hard
Parameter-Sharing und Soft-Parameter-Sharing.

In diesem Zusammenhang könnte unser Modell zwei Köpfe haben: einen für die Vorhersage einer Regression
Ergebnis und einen weiteren für die Vorhersage des Klassifikationsergebnisses. In dieser Arbeit wird zum Beispiel ein
ein Computer-Vision-Modell, das eine Klassifizierungsausgabe von Softmax-Wahrscheinlichkeiten zusammen
zusammen mit einer Regressionsausgabe zur Vorhersage von Bounding Boxes. Sie zeigen, dass dieser Ansatz
eine bessere Leistung erzielt als verwandte Arbeiten, bei denen Netzwerke separat für die
Klassifizierungs- und Lokalisierungsaufgaben trainieren. Die Idee ist, dass durch die gemeinsame Nutzung von Parametern die
Die Idee ist, dass durch die gemeinsame Nutzung von Parametern die Aufgaben gleichzeitig erlernt werden und die Gradientenaktualisierungen aus den beiden Verlustfunktionen beide Ausgaben informieren und zu einem Ergebnis führen.
Verlustfunktionen in beide Ergebnisse einfließen und zu einem besser verallgemeinerbaren Modell führen.

Entwurfsmuster 6: Multilabel
Das Multilabel-Entwurfsmuster bezieht sich auf Probleme, bei denen wir mehr als ein
einem gegebenen Trainingsbeispiel mehr als ein Label zuweisen können. Bei neuronalen Netzen erfordert dieser Entwurf eine Änderung
Aktivierungsfunktion, die in der letzten Ausgabeschicht des Modells verwendet wird, und die Art und Weise
unsere Anwendung die Modellausgabe analysieren wird. Beachten Sie, dass sich dies von Mehrklassen
Klassifizierungsproblemen, bei denen einem einzelnen Beispiel genau ein Label aus einer
Gruppe von vielen (> 1) möglichen Klassen zugeordnet wird. Sie werden das Multilabel-Entwurfsmuster auch
auch als Multilabel- oder Multiklassenklassifikation bezeichnet, da hier mehr als ein
ein Label aus einer Gruppe von mehr als einer möglichen Klasse auszuwählen. Bei der Erörterung dieses Musters
werden wir uns hauptsächlich auf neuronale Netze konzentrieren.

Problem
Bei Modellvorhersagen geht es oft darum, eine einzige Klassifizierung auf ein gegebenes Trainingsbeispiel anzuwenden.
Trainingsbeispiel. Diese Vorhersage wird aus N möglichen Klassen bestimmt, wobei N größer als 1 ist.
In diesem Fall ist es üblich, Softmax als Aktivierungsfunktion für die äußere Schicht zu verwenden.
schicht zu verwenden. Bei der Verwendung von Softmax ist die Ausgabe unseres Modells ein Feld mit N Elementen, wobei die
Summe aller Werte den Wert 1 ergibt. Jeder Wert gibt die Wahrscheinlichkeit an, dass ein bestimmtes
Jeder Wert gibt die Wahrscheinlichkeit an, dass ein bestimmtes Trainingsbeispiel mit der Klasse bei diesem Index assoziiert ist.

90 | Kapitel 3: Entwurfsmuster für die Problemdarstellung

Wenn unser Modell zum Beispiel Bilder als Katzen, Hunde oder Kaninchen klassifiziert, könnte die Softmax
Ausgabe für ein bestimmtes Bild wie folgt aussehen: [.89, .02, .09]. Das bedeutet, dass unser Modell
prognostiziert eine 89%ige Chance, dass das Bild eine Katze ist, eine 2%ige Chance, dass es ein Hund ist, und eine 9%ige Chance
dass es ein Kaninchen ist. Da jedes Bild in diesem Szenario nur eine mögliche Bezeichnung haben kann, können wir
können wir den argmax (Index der höchsten Wahrscheinlichkeit) nehmen, um die von unserem Modell vorhergesagte Klasse zu bestimmen.
Klasse zu bestimmen. Das weniger häufige Szenario ist, dass jedem Trainingsbeispiel
mehr als ein Etikett zugewiesen werden kann, worauf dieses Muster abzielt.

Das Multilabel-Designmuster gibt es für Modelle, die auf allen Datenmodalitäten trainiert werden. Für
Bildklassifizierung könnten wir im früheren Beispiel Katze, Hund, Kaninchen stattdessen
Trainingsbilder verwenden, die jeweils mehrere Tiere abbilden und daher mehrere Labels haben können.
ple Etiketten. Bei Textmodellen können wir uns einige Szenarien vorstellen, in denen Text mit mehreren Etiketten versehen werden kann
mit mehreren Tags versehen werden kann. Anhand des Datensatzes von Stack Overflow-Fragen auf BigQuery als
Beispiel könnten wir ein Modell zur Vorhersage der Tags erstellen, die mit einer bestimmten Frage
Frage verbunden sind. So könnte beispielsweise die Frage "Wie kann ich einen Pandas DataFrame plotten?
als "Python", "Pandas" und "Visualisierung" getaggt werden. Ein weiteres Beispiel für eine mehrstufige Textklassifizierung
Beispiel ist ein Modell, das giftige Kommentare identifiziert. Für dieses Modell möchten wir vielleicht
Kommentare mit mehreren Toxizitätskennzeichnungen markieren. Ein Kommentar könnte daher als
sowohl als "hasserfüllt" als auch als "obszön" gekennzeichnet werden.

Dieses Entwurfsmuster kann auch auf Tabellendatensätze angewendet werden. Stellen Sie sich einen Datensatz aus dem Gesundheitswesen vor
mit verschiedenen physischen Merkmalen für jeden Patienten, wie Größe, Gewicht, Alter, Blutdruck
Blutdruck und mehr. Diese Daten könnten verwendet werden, um das Vorhandensein mehrerer Krankheiten vorherzusagen.
Zustände. So könnte ein Patient beispielsweise ein Risiko für Herzkrankheiten und Diabetes aufweisen.

Lösung
Die Lösung für den Aufbau von Modellen, die einem gegebenen Trainingsbeispiel mehr als ein Label zuordnen können, ist die Verwendung einer
Die Lösung für den Aufbau von Modellen, die einem bestimmten Trainingsbeispiel mehrere Bezeichnungen zuordnen können, ist die Verwendung der Sigmoid-Aktivierungsfunktion in unserer letzten Ausgabeschicht. Eher
ein Array zu erzeugen, bei dem alle Werte die Summe 1 ergeben (wie bei Softmax), ist jeder einzelne
jeder einzelne Wert in einem Sigmoid-Array eine Fließkommazahl zwischen 0 und 1. Das bedeutet, dass bei der Implementierung
des Multilabel-Entwurfsmusters muss unser Label mehrfach kodiert werden. Die Länge des
des Multi-Hot-Arrays entspricht der Anzahl der Klassen in unserem Modell, und jede
Ausgabe in diesem Label-Array wird ein sigmoidaler Wert sein.

Nehmen wir an, unser Trainingsdatensatz enthielte Bilder mit mehreren Tieren, wie im obigen Beispiel.
mit mehr als einem Tier. Die sigmoide Ausgabe für ein Bild, das eine Katze
und einen Hund, aber kein Kaninchen enthält, könnte folgendermaßen aussehen: [.92, .85, .11]. Diese Ausgabe
bedeutet, dass das Modell zu 92 % sicher ist, dass das Bild eine Katze enthält, zu 85 %, dass es einen Hund enthält
einen Hund und 11 % ein Kaninchen enthält.

Eine Version dieses Modells für 28×28-Pixel-Bilder mit sigmoider Ausgabe könnte wie folgt aussehen
so aussehen, wenn man die Keras Sequential API verwendet:

model = keras.Sequential([
keras.layers.Flatten(input_shape=(28, 28)),
Entwurfsmuster 6: Multilabel | 91
keras.layers.Dense(128, activation='relu'),
keras.layers.Dense(3, activation='sigmoid')
])
Der Hauptunterschied in der Ausgabe zwischen dem sigmoid-Modell hier und dem softmax
Beispiel im Abschnitt Problem ist, dass das softmax-Array garantiert drei Werte enthält
drei Werte enthält, die sich zu 1 summieren, während die Sigmoid-Ausgabe drei Werte enthält, die jeweils
zwischen 0 und 1.

Sigmoid- versus Softmax-Aktivierung
Sigmoid ist eine nichtlineare, kontinuierliche und differenzierbare Aktivierungsfunktion, die die
die Ausgänge jedes Neurons in der vorherigen Schicht des ML-Modells nimmt und den
Wert dieser Ausgaben zwischen 0 und 1 zerquetscht. Abbildung 3-7 zeigt, wie die Sigmoid-Funktion
aussieht.
Abbildung 3-7. Eine sigmoide Funktion.
Während sigmoid einen einzelnen Wert als Eingabe annimmt und einen einzelnen Wert als Ausgabe liefert,
Softmax nimmt eine Reihe von Werten als Eingabe und wandelt sie in eine Reihe von Wahrscheinlichkeiten um, die sich zu 1 summieren.
Die Eingabe für die Softmax-Funktion könnte die Ausgabe von N
Sigmoiden sein.
Bei einem Mehrklassen-Klassifizierungsproblem, bei dem jedes Beispiel nur ein Label haben kann,
Softmax als letzte Schicht verwenden, um eine Wahrscheinlichkeitsverteilung zu erhalten. Bei dem Multilabel
Muster ist es akzeptabel, dass die Summe des Ausgabefeldes nicht 1 ergibt, da wir die
Wahrscheinlichkeit jedes einzelnen Labels auswertet.
Nachfolgend finden Sie Beispiele für Sigmoid- und Softmax-Ausgabe-Arrays:
92 | Kapitel 3: Entwurfsmuster der Problemdarstellung

sigmoid = [.8, .9, .2, .5]
softmax = [.7, .1, .15, .05]
Kompromisse und Alternativen
Es gibt mehrere Sonderfälle, die zu berücksichtigen sind, wenn man dem Multilabel-Designpfad folgt
und die Verwendung einer sigmoiden Ausgabe. Als Nächstes werden wir untersuchen, wie man Modelle mit
mit zwei möglichen Label-Klassen strukturiert werden, wie sigmoidale Ergebnisse sinnvoll genutzt werden können und andere
Überlegungen für Multilabel-Modelle.

Sigmoidale Ausgabe für Modelle mit zwei Klassen

Es gibt zwei Arten von Modellen, bei denen die Ausgabe zu zwei möglichen Klassen gehören kann:

Jedes Trainingsbeispiel kann nur einer Klasse zugeordnet werden. Dies wird auch als binäre
Klassifikation genannt und ist eine spezielle Art von Klassifikationsproblem mit mehreren Klassen.
Einige Trainingsbeispiele können zu beiden Klassen gehören. Dies ist eine Art von Multilabel
Klassifikationsproblem.
Abbildung 3-8 zeigt den Unterschied zwischen diesen Klassifizierungen.

Abbildung 3-8. Verstehen der Unterscheidung zwischen Mehrklassen-, mehrstufigen und binären
Klassifizierungsproblemen.

Entwurfsmuster 6: Mehrstufige Klassifizierung | 93
Der erste Fall (binäre Klassifikation) ist insofern einzigartig, als es sich um die einzige Art von Single-Label
Klassifizierungsproblem ist, bei dem wir die Verwendung von Sigmoid als Aktivierungsfunktion
Funktion zu verwenden. Für fast jedes andere Mehrklassen-Klassifizierungsproblem (z. B. die Klassifizierung von Text in eine von fünf möglichen Kategorien)
Klassifizierung von Text in eine von fünf möglichen Kategorien), würden wir Softmax verwenden. Wenn jedoch
wir nur zwei Klassen haben, ist softmax überflüssig. Nehmen wir zum Beispiel ein Modell, das vor
vorhersagt, ob eine bestimmte Transaktion betrügerisch ist oder nicht. Hätten wir in diesem Beispiel eine Softmax-Aus-
verwendet, könnte die Vorhersage eines betrügerischen Modells folgendermaßen aussehen:

[.02, .98]
In diesem Beispiel entspricht der erste Index der Kategorie "nicht betrügerisch" und der zweite
Index entspricht "betrügerisch". Dies ist redundant, da wir dies auch mit einem
Dies ist redundant, denn wir könnten dies auch mit einem einzigen skalaren Wert darstellen und somit eine sigmoide Ausgabe verwenden. Die gleiche Vorhersage
Vorhersage könnte einfach als .98 dargestellt werden. Da jeder Eingabe nur eine Klasse zugeordnet werden kann
Klasse zugeordnet werden kann, können wir aus dieser Ausgabe von .98 ableiten, dass das Modell eine 98%ige
Wahrscheinlichkeit von Betrug und eine 2 %ige Wahrscheinlichkeit von Nichtbetrug vorhergesagt hat.

Daher ist es für binäre Klassifizierungsmodelle optimal, eine Ausgabeform von 1
mit einer sigmoidalen Aktivierungsfunktion. Modelle mit einem einzigen Ausgangsknoten sind auch
effizienter, da sie weniger trainierbare Parameter haben und wahrscheinlich schneller trainiert werden können.
So würde die Ausgabeschicht eines binären Klassifikationsmodells aussehen:

keras.layers.Dense(1, activation='sigmoid')
Für den zweiten Fall, in dem ein Trainingsbeispiel zu beiden möglichen Klassen gehören könnte
und in das Multilabel-Design-Muster passt, wollen wir ebenfalls sigmoid verwenden, dieses Mal
mit einer Zwei-Elemente-Ausgabe:

keras.layers.Dense(2, activation='sigmoid')
Welche Verlustfunktion sollten wir verwenden?

Jetzt, da wir wissen, wann wir Sigmoid als Aktivierungsfunktion in unserem Modell verwenden sollten, wie
welche Verlustfunktion wir dafür verwenden sollten? Für den Fall der binären Klassifizierung
in dem unser Modell eine Ein-Element-Ausgabe hat, verwenden wir den binären Kreuzentropieverlust. In Keras,
stellen wir eine Verlustfunktion bereit, wenn wir unser Modell kompilieren:

model.compile(loss='binary_crossentropy', optimizer='adam',
metrics=['accuracy'])
Interessanterweise verwenden wir den binären Cross-Entropie-Verlust auch für Multilabel-Modelle mit sig-
moider Ausgabe. Dies liegt daran, dass, wie in Abbildung 3-9 gezeigt, ein Multilabel-Problem mit drei
Klassen im Wesentlichen drei kleinere binäre Klassifikationsprobleme sind.

94 | Kapitel 3: Entwurfsmuster für die Problemdarstellung

Abbildung 3-9. Verstehen des Multilabel-Musters durch Aufteilung des Problems in
kleinere binäre Klassifikationsaufgaben.

Analyse der sigmoidalen Ergebnisse

Um das vorhergesagte Label für ein Modell mit Softmax-Ausgabe zu extrahieren, können wir einfach
den argmax-Wert (Index des höchsten Wertes) des Ausgabe-Arrays nehmen, um die vorhergesagte Klasse zu erhalten. Pars-
sigmoid-Ausgaben ist weniger geradlinig. Anstatt die Klasse mit der höchsten
der höchsten vorhergesagten Wahrscheinlichkeit zu nehmen, müssen wir die Wahrscheinlichkeit jeder Klasse in unserer
Ausgabeschicht bewerten und den Wahrscheinlichkeitsschwellenwert für unseren Anwendungsfall berücksichtigen. Diese beiden
Entscheidungen hängen weitgehend von der Endanwendung unseres Modells ab.

Mit Schwellenwert bezeichnen wir die Wahrscheinlichkeit, mit der wir
für die Bestätigung, dass eine Eingabe zu einer bestimmten Klasse gehört. Für
Wenn wir zum Beispiel ein Modell zur Klassifizierung verschiedener Tierarten in Bildern erstellen
Tiere in Bildern zu klassifizieren, können wir mit gutem Gewissen sagen, dass ein Bild eine Katze enthält
auch wenn das Modell nur zu 80 % sicher ist, dass das Bild eine Katze enthält.
Wenn wir jedoch ein Modell erstellen, das Vorhersagen für das Gesundheitswesen macht
Vorhersagen machen soll, wollen wir wahrscheinlich, dass das Modell zu 99 % zuversichtlich ist, bevor wir eine bestimmte
sein, bevor es bestätigt, dass ein bestimmter medizinischer Zustand vorhanden ist oder
nicht. Schwellenwerte sind zwar für jede Art von Klassifizierungsmodell zu
Klassifizierungsmodell in Betracht gezogen werden muss, ist sie besonders für das Multilabel
Entwurfsmuster, da wir für jede Klasse Schwellenwerte festlegen müssen
Klasse festlegen müssen und diese unterschiedlich sein können.
Um ein konkretes Beispiel zu betrachten, nehmen wir den Stack Overflow-Datensatz in BigQuery und
um ein Modell zu erstellen, das die mit einer Stack Overflow-Frage verbundenen Tags anhand des Titels vorhersagt.
Frage anhand ihres Titels vorhersagt. Wir beschränken unseren Datensatz auf Fragen, die nur fünf Tags enthalten, um
um die Dinge einfach zu halten:

SELECT
Titel,
REPLACE(tags, "|", ",") as tags
FROM
`bigquery-public-data.stackoverflow.posts_questions`
WHERE
Entwurfsmuster 6: Multilabel | 95
REGEXP_CONTAINS(tags,
r"(?:keras|tensorflow|matplotlib|pandas|scikit-learn)")
Die Ausgabeschicht unseres Modells würde wie folgt aussehen (der vollständige Code für diesen Abschnitt
ist im GitHub-Repository verfügbar):

keras.layers.Dense(5, activation='sigmoid')
Nehmen wir die Stack Overflow-Frage "Was ist die Definition eines nicht trainierbaren
Parameter?" als Eingabebeispiel. Angenommen, unsere Ausgabeindizes entsprechen der
Reihenfolge der Tags in unserer Abfrage entsprechen, könnte eine Ausgabe für diese Frage wie folgt aussehen:

[.95, .83, .02, .08, .65]
Unser Modell ist zu 95% sicher, dass diese Frage mit Keras getaggt werden sollte, und zu 83%
sie mit TensorFlow getaggt werden sollte. Wenn wir die Modellvorhersagen auswerten, müssen wir
über jedes Element im Ausgabe-Array iterieren und festlegen, wie wir die
diese Ergebnisse für unsere Endnutzer anzeigen wollen. Wenn 80 % unser Schwellenwert für alle Tags ist, würden wir Keras
und TensorFlow im Zusammenhang mit dieser Frage. Vielleicht möchten wir aber auch
Benutzer dazu ermutigen, so viele Tags wie möglich hinzuzufügen, und wir möchten Optionen für alle
Tag mit einer Vorhersagewahrscheinlichkeit von über 50 % anzeigen.

Für Beispiele wie dieses, bei dem es in erster Linie darum geht, mögliche Tags vorzuschlagen
vorzuschlagen, wobei es weniger darauf ankommt, das Tag genau zu treffen, ist eine typische Faustregel
n_specific_tag / n_total_examples als Schwellenwert für jede Klasse zu verwenden. Hier,
n_specific_tag ist die Anzahl der Beispiele mit einem Tag im Datensatz (z. B.,
"pandas"), und n_total_examples ist die Gesamtzahl der Beispiele im Trainingssatz
über alle Tags hinweg. Damit wird sichergestellt, dass das Modell besser abschneidet, als wenn es ein bestimmtes
Label auf der Grundlage seines Vorkommens im Trainingsdatensatz zu erraten.

Für einen präziseren Ansatz zur Schwellenwertbildung sollten Sie S-Cut
oder die Optimierung für das F-Maß Ihres Modells. Einzelheiten zu beidem finden Sie
finden Sie in diesem Dokument. Die Kalibrierung der Wahrscheinlichkeiten pro Etikett ist oft
hilfreich, insbesondere wenn Tausende von Labels vorhanden sind und
Sie die besten K von ihnen berücksichtigen wollen (dies ist häufig bei Such- und
und Ranking-Problemen).
Wie Sie gesehen haben, bieten Multilabel-Modelle mehr Flexibilität bei der Analyse von Vorhersagen.
Vorhersagen zu analysieren, und erfordern, dass man sich die Ausgabe für jede Klasse genau überlegt.

Erwägungen zum Datensatz

Bei der Bearbeitung von Einzellabel-Klassifizierungsaufgaben können wir sicherstellen, dass unser Datensatz ausgewogen ist.
eine relativ gleiche Anzahl von Trainingsbeispielen für jede Klasse anstreben.
Der Aufbau eines ausgewogenen Datensatzes ist beim Multilabel-Designmuster differenzierter.

96 | Kapitel 3: Entwurfsmuster für die Problemdarstellung

Am Beispiel des Stack Overflow-Datensatzes wird es wahrscheinlich viele Fragen geben, die sowohl mit
sowohl als TensorFlow als auch als Keras gekennzeichnet sind. Aber es wird auch Fragen über Keras geben, die
die nichts mit TensorFlow zu tun haben. Ähnlich könnten wir Fragen über das Plotten von Daten sehen
Daten, die sowohl mit matplotlib als auch mit pandas getaggt sind, und Fragen zur Daten
Vorverarbeitung von Daten, die sowohl mit pandas als auch mit scikit-learn getaggt sind. Damit unser Modell
lernen kann, was für jedes Tag einzigartig ist, müssen wir sicherstellen, dass der Trainingsdatensatz aus
aus verschiedenen Kombinationen der einzelnen Tags besteht. Wenn die Mehrheit der Matplotlib-Fragen in unserem
Dataset auch mit dem Tag pandas versehen sind, wird das Modell nicht lernen, matplotlib von sich aus
allein zu klassifizieren. Um dies zu berücksichtigen, denken Sie über die verschiedenen Beziehungen zwischen den Bezeichnungen nach, die
die in unserem Modell vorhanden sein könnten, und zählen Sie die Anzahl der Trainingsbeispiele, die
die zu jeder überlappenden Kombination von Bezeichnungen gehören.

Bei der Erforschung der Beziehungen zwischen den Bezeichnungen in unserem Datensatz können wir auch auf
hierarchische Bezeichnungen. ImageNet, der beliebte Datensatz für die Bildklassifizierung, enthält tausende
Tausende von beschrifteten Bildern und wird häufig als Ausgangspunkt für das Transferlernen von
Bildmodelle verwendet. Alle in ImageNet verwendeten Beschriftungen sind hierarchisch, d. h. alle Bilder
alle Bilder mindestens ein Label haben, und viele Bilder haben spezifischere Labels, die Teil einer
Hierarchie sind. Hier ist ein Beispiel für eine Beschriftungshierarchie in ImageNet:

Tier → Wirbellose → Gliederfüßer → Spinnentier → Spinne

Je nach Größe und Art des Datensatzes gibt es zwei gängige Ansätze
für den Umgang mit hierarchischen Etiketten:

Verwenden Sie einen flachen Ansatz und legen Sie jedes Label unabhängig von der Hierarchie in das gleiche
Hierarchie und stellen Sie sicher, dass Sie genügend Beispiele für jede "Blattknoten"-Bezeichnung haben.
Verwenden Sie das Kaskadenentwurfsmuster. Erstellen Sie ein Modell, um Labels auf höherer Ebene zu identifizieren.
Senden Sie das Beispiel auf der Grundlage der High-Level-Klassifikation an ein separates Modell für eine
spezifischere Klassifizierungsaufgabe. Zum Beispiel könnten wir ein erstes Modell haben
das Bilder als "Pflanze", "Tier" oder "Person" kennzeichnet. Je nachdem, welche Bezeichnungen
das erste Modell anwendet, wird das Bild an andere Modelle weitergeleitet, die detailliertere
Etiketten wie "Sukkulente" oder "Berberlöwe" anzuwenden.
Der flache Ansatz ist geradliniger als das Kaskadenentwurfsmuster
da er nur ein Modell benötigt. Dies kann jedoch dazu führen, dass das Modell Informationen
Informationen über detailliertere Bezeichnungsklassen verliert, da es natürlich mehr Trainings
Trainingsbeispiele mit den höherwertigen Bezeichnungen in unserem Datensatz gibt.

Eingaben mit überlappenden Etiketten

Das Multilabel-Entwurfsmuster ist auch in Fällen nützlich, in denen die Eingabedaten gelegentlich
überlappende Bezeichnungen haben. Nehmen wir ein Bildmodell, das Kleidungsstücke für einen Katalog klassifiziert
Katalog klassifiziert. Wenn wir mehrere Personen haben, die jedes Bild im Trainingsdatensatz beschriften
Trainingsdatensatz beschriften, kann ein Beschrifter ein Bild eines Rocks als "Maxirock" bezeichnen, während ein anderer

Entwurfsmuster 6: Multilabel | 97
identifiziert es als "Faltenrock". Beides ist richtig. Wenn wir jedoch ein Multiklassen-Klassifizie- rungsmodell auf diesen Daten aufbauen
Klassifizierungsmodell auf diesen Daten aufbauen und ihm mehrere Beispiele desselben Bildes mit
Bildes mit unterschiedlichen Bezeichnungen, werden wir wahrscheinlich auf Situationen stoßen, in denen das Modell ähnliche
Bilder bei der Erstellung von Vorhersagen unterschiedlich beschriftet. Idealerweise wollen wir ein Modell, das dieses
Bild sowohl als "Maxirock" als auch als "Faltenrock" beschriftet, wie in Abbildung 3-10 zu sehen, und nicht
manchmal nur eine der beiden Bezeichnungen vorhersagt.

Abbildung 3-10. Verwendung der Eingaben von mehreren Beschriftern zur Erstellung überlappender Beschriftungen in Fällen
wenn mehrere Beschreibungen eines Artikels korrekt sind.

Das Multilabel-Entwurfsmuster löst dieses Problem, indem es uns erlaubt, beide überlappenden
Etiketten mit einem Bild zu verbinden. In Fällen mit überlappenden Labels, in denen wir mehrere Labeler haben
die jedes Bild in unserem Trainingsdatensatz bewerten, können wir die maximale Anzahl
Beschriftungen wählen, die die Beschrifter einem bestimmten Bild zuordnen sollen, und dann die am häufigsten
Tags, die einem Bild während des Trainings zugeordnet werden. Der Schwellenwert für "am häufigsten
am häufigsten gewählten Tags" hängt von unserer Vorhersageaufgabe und der Anzahl der menschlichen
Beschrifter ab, die wir haben. Wenn wir zum Beispiel 5 Beschrifter haben, die jedes Bild bewerten und 20
möglichen Tags für jedes Bild haben, könnten wir die Beschrifter auffordern, jedem Bild 3 Tags zu geben.
Aus dieser Liste von 15 "Stimmen" pro Bild könnten wir dann die 2 bis 3 mit den meisten
mit den meisten Stimmen von den Beschreibern. Bei der Auswertung dieses Modells müssen wir auf die
Vorhersagekonfidenz, die das Modell für jede Bezeichnung liefert, notieren und diese zur iterativen
iterativ unseren Datensatz und die Qualität der Kennzeichnung zu verbessern.

Einer gegen den Rest

Eine weitere Technik zur Handhabung der Multilabel-Klassifizierung ist das Training mehrerer binärer
Klassifikatoren zu trainieren, anstatt nur ein Multilabel-Modell. Dieser Ansatz wird als "one versus rest" bezeichnet. Unter
Fall des Stack Overflow-Beispiels, bei dem wir Fragen als Tensor-, Python
Flow, Python und Pandas markieren wollen, würden wir für jedes dieser drei Tags einen eigenen Klassifikator trainieren
Tags: Python oder nicht, TensorFlow oder nicht, und so weiter. Dann wählen wir eine Vertrauensschwelle
Konfidenzschwelle und kennzeichnen die ursprüngliche Eingabefrage mit Tags von jedem binären Klassifikator
über einem bestimmten Schwellenwert.

98 | Kapitel 3: Entwurfsmuster für die Problemdarstellung

Der Vorteil der einen gegenüber der anderen ist, dass wir sie mit Modellarchitekturen verwenden können, die
die nur binäre Klassifikationen durchführen können, wie SVMs. Es kann auch bei seltenen Kategorien helfen, da
das Modell jeweils nur eine Klassifizierungsaufgabe für jede Eingabe durchführt, und
es möglich ist, das Rebalancing-Entwurfsmuster anzuwenden. Der Nachteil dieses
Nachteil dieses Ansatzes ist die zusätzliche Komplexität des Trainings vieler verschiedener Klassifikatoren, die uns
wir unsere Anwendung so aufbauen müssen, dass sie Vorhersagen aus jedem dieser Modi generiert
generiert, anstatt nur einen zu haben.

Zusammenfassend lässt sich sagen, dass Sie das Multilabel-Entwurfsmuster verwenden sollten, wenn Ihre Daten in eines der folgenden Szenarien fallen
folgenden Klassifizierungsszenarien fallen:

Ein einzelnes Trainingsbeispiel kann mit sich gegenseitig ausschließenden Labels verbunden sein.
Ein einzelnes Trainingsbeispiel kann viele hierarchische Labels haben.
Kennzeichner beschreiben dasselbe Objekt auf unterschiedliche Weise, und jede Interpretation ist
genau.
Stellen Sie bei der Implementierung eines Multilabel-Modells sicher, dass Kombinationen von sich überschneidenden Labels
in Ihrem Datensatz gut repräsentiert sind, und überlegen Sie, welche Schwellenwerte Sie
Schwellenwerte, die Sie für jedes mögliche Label in Ihrem Modell akzeptieren können. Die Verwendung einer sigmoidalen Ausgabeschicht ist der
gebräuchlichste Ansatz für die Erstellung von Modellen, die Multilabel-Klassifizie
tion. Darüber hinaus kann die sigmoidale Ausgabe auch auf binäre Klassifizierungsaufgaben angewendet werden
wo ein Trainingsbeispiel nur eine von zwei möglichen Bezeichnungen haben kann.

Entwurfsmuster 7: Ensembles
Das Ensembles-Entwurfsmuster bezieht sich auf Techniken des maschinellen Lernens, die
die mehrere Modelle des maschinellen Lernens kombinieren und ihre Ergebnisse zusammenfassen, um Vorhersagen zu treffen.
Ensembles können ein effektives Mittel sein, um die Leistung zu verbessern und Vorhersagen zu
Vorhersagen zu treffen, die besser sind als jedes einzelne Modell.

Problem
Angenommen, wir haben unser Modell zur Vorhersage des Babygewichts trainiert, indem wir spezielle Eigenschaften
tures und das Hinzufügen zusätzlicher Schichten zu unserem neuronalen Netz, so dass der Fehler in unserer
Trainingsmenge nahezu Null ist. Ausgezeichnet, sagen Sie! Aber wenn wir unser Modell
Modell in der Produktion im Krankenhaus einsetzen oder die Leistung in der Testreihe bewerten,
sind unsere Vorhersagen alle falsch. Was ist passiert? Und, was noch wichtiger ist, wie können wir
beheben?

Entwurfsmuster 7: Ensembles | 99
Kein Modell für maschinelles Lernen ist perfekt. Um besser zu verstehen, wo und wie unser
Modell falsch ist, kann der Fehler eines ML-Modells in drei Teile aufgeteilt werden: den
irreduziblen Fehler, den Fehler aufgrund von Verzerrungen und den Fehler aufgrund von Varianz. Der irreduzible
Der irreduzible Fehler ist der inhärente Fehler des Modells, der sich aus dem Rauschen im Datensatz, dem Rahmen des Problems
Rahmen des Problems oder schlechten Trainingsbeispielen, wie Messfehlern oder Störfaktoren, resultiert.
Faktoren. Wie der Name schon sagt, können wir gegen den irreduziblen Fehler nicht viel tun.

Die beiden anderen, die Verzerrung und die Varianz, werden als reduzierbarer Fehler bezeichnet, und
hier können wir die Leistung unseres Modells beeinflussen. Kurz gesagt, die Verzerrung ist die
die Unfähigkeit des Modells, genug über die Beziehung zwischen den Merkmalen und Bezeichnungen des Modells zu lernen
und Bezeichnungen zu lernen, während die Varianz die Unfähigkeit des Modells zur Generalisierung auf neue,
ungesehenen Beispielen. Ein Modell mit einer hohen Verzerrung vereinfacht die Beziehung zu stark und wird als
unterangepasst sein. Ein Modell mit hoher Varianz hat zu viel über die Trainingsdaten gelernt
Daten gelernt und wird als überangepasst bezeichnet. Natürlich ist es das Ziel eines jeden ML-Modells, eine geringe Verzerrung
und eine geringe Varianz zu haben, aber in der Praxis ist es schwierig, beides zu erreichen. Dies ist bekannt als der
Verzerrungs-Varianz-Kompromiss. Wir können nicht beides haben und gleichzeitig essen. Zum Beispiel verringert die Erhöhung der
Modellkomplexität verringert die Verzerrung, erhöht aber die Varianz, während eine geringere
Komplexität die Varianz verringert, aber zu mehr Verzerrungen führt.

Neuere Arbeiten legen nahe, dass bei der Verwendung moderner maschineller Lerntechniken wie
große neuronale Netze mit hoher Kapazität, dieses Verhalten nur bis zu einem gewissen Punkt gültig ist. In
beobachteten Experimenten gibt es eine "Interpolationsschwelle", jenseits derer Modelle mit sehr hoher
Modelle mit hoher Kapazität in der Lage sind, einen Trainingsfehler von Null und einen geringen Fehler bei ungesehenen
Daten. Natürlich brauchen wir viel größere Datensätze, um ein Overfitting bei Modellen mit hoher
kapazitiven Modellen zu vermeiden.

Gibt es eine Möglichkeit, diesen Kompromiss zwischen Verzerrung und Varianz bei kleinen und mittleren Problemen abzuschwächen?
Problemen?

Lösung
Ensemble-Methoden sind Meta-Algorithmen, die mehrere Methoden des maschinellen Lernens kombinieren.
kombinieren, um die Verzerrung und/oder Varianz zu verringern und die Modellleistung zu verbessern.
Leistung zu verbessern. Im Allgemeinen besteht die Idee darin, dass die Kombination mehrerer Modelle zur Verbesserung der
die Ergebnisse des maschinellen Lernens zu verbessern. Durch die Erstellung mehrerer Modelle mit unterschiedlichen
induktiven Verzerrungen und der Aggregation ihrer Ergebnisse hoffen wir, ein Modell mit besserer
Leistung. In diesem Abschnitt werden wir einige häufig verwendete Ensemble-Methoden besprechen,
einschließlich Bagging, Boosting und Stacking.

Absackung

Bagging (kurz für Bootstrap-Aggregation) ist eine Art von parallelem Ensembling-Verfahren und
wird verwendet, um hohe Varianz in Modellen des maschinellen Lernens zu berücksichtigen. Der Bootstrap-Teil des
Bagging bezieht sich auf die Datensätze, die für das Training der Ensemblemitglieder verwendet werden. Genauer gesagt, wenn
es k Teilmodelle gibt, dann gibt es k separate Datensätze, die für das Training jedes

100 | Kapitel 3: Entwurfsmuster für die Problemdarstellung

Teilmodell des Ensembles. Jeder Datensatz wird durch zufällige Stichproben (mit
Ersatz) aus dem ursprünglichen Trainingsdatensatz erstellt. Das bedeutet, dass eine hohe Wahrscheinlichkeit besteht
Wahrscheinlichkeit, dass in einem der k Datensätze einige Trainingsbeispiele fehlen, aber auch jeder
Datensatz wird wahrscheinlich wiederholte Trainingsbeispiele enthalten. Die Aggregation findet statt auf
die Ergebnisse der verschiedenen Mitglieder des Ensemblemodells - entweder ein Durchschnitt im Falle einer
einer Regressionsaufgabe oder ein Mehrheitsvotum im Falle einer Klassifizierung.

Ein gutes Beispiel für eine Bagging-Ensemble-Methode ist der Random Forest: Mehrere Entscheidungsbäume
sionsbäume werden auf zufällig ausgewählten Teilmengen der gesamten Trainingsdaten trainiert, dann
werden die Baumvorhersagen aggregiert, um eine Vorhersage zu erstellen, wie in Abbildung 3-11 dargestellt.

Abbildung 3-11. Bagging ist gut geeignet, um die Varianz in der Ausgabe von Modellen des maschinellen Lernens zu verringern.

Beliebte Bibliotheken für maschinelles Lernen verfügen über Implementierungen von Bagging-Methoden. Für
Beispiel: Um eine Random-Forest-Regression in Scikit-Learn zu implementieren, um das Gewicht eines Babys
Gewicht aus unserem Geburtsdatensatz vorherzusagen:

from sklearn.ensemble import RandomForestRegressor
# Erstelle das Modell mit 50 Bäumen
RF_model = RandomForestRegressor( n_estimators=50,
max_features='sqrt',
n_jobs=-1, verbose = 1)
# Anpassung an Trainingsdaten
RF_model.fit(X_train, Y_train)
Die Modellmittelung, wie sie beim Bagging eingesetzt wird, ist eine leistungsstarke und zuverlässige Methode zur Verringerung der
Modellvarianz. Wie wir sehen werden, kombinieren verschiedene Ensemble-Methoden mehrere Teilmodelle auf unterschiedliche Weise.
dels auf unterschiedliche Weise, manchmal mit unterschiedlichen Modellen, unterschiedlichen Algorithmen oder
sogar unterschiedliche Zielfunktionen. Beim Bagging sind das Modell und die Algorithmen
gleich. Bei Random Forest beispielsweise sind die Teilmodelle alle kurze Entscheidungsbäume.

Entwurfsmuster 7: Ensembles | 101
Verstärkung

Boosting ist eine weitere Ensemble-Technik. Im Gegensatz zum Bagging wird beim Boosting jedoch ein
ein Ensemble-Modell mit einer höheren Kapazität als die der einzelnen Mitglieder
Modelle. Aus diesem Grund bietet Boosting ein effektiveres Mittel zur Reduzierung von Verzerrungen
als Varianz. Die Idee hinter Boosting ist der iterative Aufbau eines Ensembles von Modellen
wobei sich jedes nachfolgende Modell auf das Lernen der Beispiele konzentriert, die das vorherige Modell
falsch lag. Kurz gesagt, Boosting verbessert iterativ eine Reihe von schwachen Lernmodellen
wobei ein gewichteter Durchschnitt gebildet wird, um schließlich einen starken Lerner zu erhalten.

Zu Beginn des Boosting-Verfahrens wird ein einfaches Basismodell f_0 gewählt. Für eine
Regressionsaufgabe könnte das Basismodell einfach der durchschnittliche Zielwert sein: f_0 =
np.mean(Y_train). Im ersten Iterationsschritt werden die Residuen delta_1 gemessen
gemessen und durch ein separates Modell approximiert. Dieses Residualmodell kann beliebig sein, aber
typischerweise ist es nicht sehr ausgeklügelt; wir verwenden oft einen schwachen Lerner wie einen Entscheidungsbaum.
Die vom Residualmodell gelieferte Annäherung wird dann zur aktuellen Vorhersage hinzugefügt.
diction hinzugefügt, und der Prozess wird fortgesetzt.

Nach vielen Iterationen tendieren die Residuen gegen Null und die Vorhersage wird immer besser
immer besser, den ursprünglichen Trainingsdatensatz zu modellieren. Beachten Sie, dass in Abbildung 3-12 die
die Residuen für jedes Element des Datensatzes mit jeder aufeinanderfolgenden Iteration abnehmen.

Abbildung 3-12. Boosting wandelt schwache Lerner in starke Lerner um, indem es die Modellvorhersage iterativ verbessert.
Verbesserung der Modellvorhersage.

Einige der bekannteren Boosting-Algorithmen sind AdaBoost, Gradient Boosting
Maschinen und XGBoost, und sie haben einfach zu verwendende Implementierungen in beliebten
Frameworks für maschinelles Lernen wie Scikit-Learn oder TensorFlow.

102 | Kapitel 3: Entwurfsmuster für die Problemdarstellung

Die Implementierung in scikit-learn ist ebenfalls unkompliziert:

from sklearn.ensemble import GradientBoostingRegressor
# Erstellen des Gradient Boosting Regressors
GB_model = GradientBoostingRegressor(n_estimators=1,
max_depth=1,
learning_rate=1,
criterion='mse')
# Anpassung an Trainingsdaten
GB_model.fit(X_train, Y_train)
Stapeln

Stacking ist eine Ensemble-Methode, die die Ergebnisse einer Sammlung von Modellen kombiniert
um eine Vorhersage zu treffen. Die anfänglichen Modelle, die in der Regel aus verschiedenen Modelltypen bestehen,
werden bis zur Vollendung auf dem gesamten Trainingsdatensatz trainiert. Dann wird ein sekundäres Metamodell
trainiert, das die anfänglichen Modellausgaben als Merkmale verwendet. Dieses zweite Metamodell lernt
wie die Ergebnisse der ersten Modelle am besten kombiniert werden können, um den Trainingsfehler zu verringern
und kann jede Art von maschinellem Lernmodell sein.

Um ein Stacking-Ensemble zu implementieren, trainieren wir zunächst alle Mitglieder des Ensembles auf
dem Trainingsdatensatz. Der folgende Code ruft eine Funktion, fit_model, auf, die als
Argumente ein Modell und die Eingaben X_train und Y_train des Trainingsdatensatzes. Diese
Weise ist members eine Liste, die alle trainierten Modelle in unserem Ensemble enthält. Der vollständige Code
für dieses Beispiel finden Sie im Code-Repository für dieses Buch:

members = [model_1, model_2, model_3]
# Modelle anpassen und speichern
n_members = len(members)
for i in range(n_members):
# Modell anpassen
model = fit_model(members[i])
# Modell speichern
filename = 'models/model_' + str(i + 1) + '.h5'
model.save(dateiname, save_format='tf')
print ('Gespeicherte {} \n '.format(filename))
Diese Teilmodelle werden in ein größeres Stacking-Ensemble-Modell als individuelle
individuelle Eingaben. Da diese Eingabemodelle zusammen mit dem sekundären Ensemblemodell trainiert werden
Modell trainiert werden, werden die Gewichte dieser Eingabemodelle festgelegt. Dies kann durch Setzen von
layer.trainable auf False für die Ensemble-Modelle gesetzt wird:

for i in range(n_members):
model = members[i]
for layer in model.layers:
# nicht trainierbar machen
layer.trainable = False
Entwurfsmuster 7: Ensembles | 103
# umbenennen, um das Problem des 'eindeutigen Layernamens' zu vermeiden
layer._name = 'ensemble_' + str(i+1) + '_' + layer.name
Wir erstellen das Ensemble-Modell, indem wir die Komponenten mit der Keras
funktionalen API:

member_inputs = [model.input for model in members]
# Merge-Ausgabe von jedem Modell verketten
member_outputs = [model.output for model in members]
merge = layers.concatenate(member_outputs)
hidden = layers.Dense(10, activation='relu')(merge)
ensemble_output = layers.Dense(1, activation='relu')(hidden)
ensemble_model = Model(inputs=Mitglieder_inputs, outputs=ensemble_output)
# Diagramm des Ensembles erstellen
tf.keras.utils.plot_model(ensemble_model, show_shapes=True,
to_file='ensemble_graph.png')
# kompilieren
ensemble_model.compile(loss='mse', optimizer='adam', metrics=['mse'])
In diesem Beispiel ist das sekundäre Modell ein dichtes neuronales Netz mit zwei versteckten Schichten.
schen. Durch Training lernt dieses Netz, wie es die Ergebnisse der Ensemblemitglieder am besten
Ensemblemitglieder bei der Erstellung von Vorhersagen kombiniert.

Warum es funktioniert
Modell-Mittelwertbildungsmethoden wie Bagging funktionieren, weil die einzelnen Modelle
die das Ensemblemodell bilden, nicht alle dieselben Fehler in der Testmenge machen. Unter
einer idealen Situation liegt jedes einzelne Modell um einen zufälligen Betrag daneben, so dass
Ergebnisse gemittelt werden, heben sich die zufälligen Fehler auf, und die Vorhersage kommt der
richtigen Antwort. Kurz gesagt, die Menge macht's.

Boosting funktioniert gut, weil das Modell in jedem Iterationsschritt mehr und mehr nach den
Residuen bei jedem Iterationsschritt. Mit jeder Iteration wird das Ensemblemodell ermutigt
ermutigt, die schwer vorhersagbaren Beispiele immer besser vorherzusagen. Stapeln
funktioniert, weil es das Beste aus Bagging und Boosting kombiniert. Das sekundäre
Modell kann als eine ausgefeiltere Version der Modell-Mittelwertbildung betrachtet werden.

Absackung

Genauer gesagt, nehmen wir an, wir haben k Regressionsmodelle für neuronale Netze trainiert und deren Ergebnisse gemittelt.
ihre Ergebnisse, um ein Ensemble-Modell zu erstellen. Wenn jedes Modell einen Fehler error_i für
jedes Beispiel, wobei error_i aus einer multivariaten Normalverteilung mit Nullmittelwert
Normalverteilung mit Varianz var und Kovarianz cov entnommen wird, dann hat der Ensemble-Prädiktor
einen Fehler aufweisen:

ensemble_error = 1./k * np.sum([error_1, error_2,...,error_k])
104 | Kapitel 3: Entwurfsmuster für die Problemdarstellung

1 Für die explizite Berechnung dieser Werte siehe Ian Goodfellow, Yoshua Bengio, und Aaron Courville, Deep
Learning (Cambridge, MA: MIT Press, 2016), Kap. 7.
Wenn die Fehler error_i perfekt korreliert sind, so dass cov = var, dann reduziert sich der mittlere quadratische
Fehler des Ensemblemodells auf var. In diesem Fall hilft die Modellmittelung nicht
überhaupt nicht hilfreich. Im anderen Extremfall, wenn die Fehler error_i vollkommen unkorreliert sind,
dann ist cov = 0 und der mittlere quadratische Fehler des Ensemblemodells ist var/k. Also nimmt der
erwartete quadratische Fehler nimmt also linear mit der Anzahl k der Modelle im Ensem-
ble.^1 Zusammenfassend lässt sich sagen, dass das Ensemble im Durchschnitt mindestens so gut abschneidet wie jedes der
der einzelnen Modelle im Ensemble. Außerdem, wenn die Modelle im Ensemble
unabhängige Fehler machen (z. B. cov = 0), dann wird das Ensemble sig-
signifikant besser. Letztendlich liegt der Schlüssel zum Erfolg beim Bagging in der Modellvielfalt.

Dies erklärt auch, warum Bagging in der Regel weniger effektiv ist für stabilere Lerner wie
k-nearest neighbors (kNN), naive Bayes, lineare Modelle oder Support-Vektor-Maschinen
(SVMs), da die Größe der Trainingsmenge durch Bootstrapping reduziert wird. Auch
können neuronale Netze selbst bei Verwendung derselben Trainingsdaten eine Vielzahl von Lösungen erreichen
durch zufällige Gewichtsinitialisierungen, zufällige Mini-Batch-Auswahl oder unterschiedliche
Hyperparametern zu unterschiedlichen Lösungen kommen, wodurch Modelle entstehen, deren Fehler teilweise unabhängig sind. Daher,
kann die Modellmittelung sogar neuronale Netze begünstigen, die mit demselben Datensatz trainiert wurden. Unter
Tatsächlich besteht eine empfohlene Lösung zur Behebung der hohen Varianz neuronaler Netze darin, mehrere Modelle zu
mehrere Modelle zu trainieren und deren Vorhersagen zu aggregieren.

Ankurbelung

Der Boosting-Algorithmus verbessert das Modell iterativ, um den Vorhersagefehler zu reduzieren.
diktionsfehler zu reduzieren. Jeder neue schwache Lerner korrigiert die Fehler der vorherigen Vorhersage, indem er die
tion durch Modellierung der Residuen delta_i jedes Schritts. Die endgültige Vorhersage ist die Summe
der Ausgaben des Basis-Learners und jedes der aufeinanderfolgenden schwachen Lerner, wie
in Abbildung 3-13 dargestellt.

Abbildung 3-13. Beim Boosting wird iterativ ein starker Lerner aus einer Folge von schwachen Lernern
Lernern, die den Restfehler der vorherigen Iteration modellieren.

So wird das resultierende Ensemble-Modell nach und nach immer komplexer,
und hat mehr Kapazität als jedes einzelne seiner Mitglieder. Dies erklärt auch, warum Boosting
besonders gut geeignet ist, um hohe Verzerrungen zu bekämpfen. Zur Erinnerung: Die Verzerrung steht im Zusammenhang mit dem Modell

Entwurfsmuster 7: Ensembles | 105
Tendenz zur Unterbewertung. Durch die iterative Konzentration auf die schwer vorhersagbaren Beispiele,
wird durch Boosting die Verzerrung des resultierenden Modells effektiv verringert.

Stapeln

Stacking kann als eine Erweiterung der einfachen Modell-Mittelwertbildung betrachtet werden, bei der wir
k Modelle bis zur Vervollständigung auf dem Trainingsdatensatz trainieren und dann die Ergebnisse mitteln, um
eine Vorhersage. Die einfache Modell-Mittelwertbildung ähnelt dem Bagging, aber die Modelle im
können jedoch von unterschiedlichem Typ sein, während beim Bagging die Modelle vom selben Typ sind.
Typs sind. Allgemeiner ausgedrückt könnte man die Mittelwertbildung so modifizieren, dass ein gewichteter Durchschnitt gebildet wird,
zum Beispiel, um einem Modell in unserem Ensemble mehr Gewicht zu geben als den anderen, wie
in Abbildung 3-14 dargestellt.

Abbildung 3-14. Die einfachste Form der Modell-Mittelwertbildung bildet den Durchschnitt der Ausgaben von zwei oder mehr
verschiedener Modelle für maschinelles Lernen. Alternativ kann der Durchschnitt durch einen
gewichteten Durchschnitt ersetzt werden, wobei die Gewichtung auf der relativen Genauigkeit der
Modelle basiert.

Sie können sich das Stacking als eine erweiterte Version der Modellmittelung vorstellen, bei der
anstatt einen Durchschnitt oder gewichteten Durchschnitt zu nehmen, trainieren wir ein zweites maschinelles Lernmodell
Modell auf die Ausgaben trainiert wird, um zu lernen, wie die Ergebnisse am besten mit den Modellen in unserem
Ensemble zu kombinieren, um eine Vorhersage zu erstellen, wie in Abbildung 3-15 dargestellt. Dies bietet alle Vorteile
Dies bietet alle Vorteile einer abnehmenden Varianz wie bei Bagging-Techniken, kontrolliert aber auch eine hohe Verzerrung.

Abbildung 3-15. Stacking ist eine Technik des Ensemble-Lernens, bei der die Ergebnisse von
verschiedener ML-Modelle als Eingabe für ein sekundäres ML-Modell kombiniert, das
Vorhersagen macht.

Kompromisse und Alternativen
Ensemble-Methoden sind im modernen maschinellen Lernen sehr beliebt geworden und haben
haben eine große Rolle beim Gewinn bekannter Wettbewerbe gespielt, vor allem beim

106 | Kapitel 3: Entwurfsmuster für die Problemdarstellung

Netflix-Preis. Es gibt auch viele theoretische Belege für den Erfolg, der bei diesen
die bei diesen realen Herausforderungen nachgewiesen wurden.

Mehr Zeit für Schulung und Entwurf

Ein Nachteil des Ensemble-Lernens ist der erhöhte Zeitaufwand für Training und Design. Zum Beispiel
Beispiel: Bei einem gestapelten Ensemble-Modell kann die Auswahl der Ensemble-Modelle
ein eigenes Maß an Fachwissen erfordern und wirft eigene Fragen auf: Ist es am besten, dieselben
gleichen Architekturen wiederverwenden oder die Vielfalt fördern? Wenn wir verschiedene Architekturen verwenden, welche
sollten wir verwenden? Und wie viele? Anstatt ein einziges ML-Modell zu entwickeln
zu entwickeln (was für sich genommen schon eine Menge Arbeit sein kann!), entwickeln wir jetzt k Modelle. Wir haben
zusätzlichen Aufwand bei der Modellentwicklung eingeführt, ganz zu schweigen von
ganz zu schweigen von der Wartung, der Komplexität der Inferenz und der Ressourcennutzung, wenn das Ensemble-Modell
Modell in Produktion gehen soll. Dies kann schnell unpraktisch werden, wenn die Anzahl der
der Modelle im Ensemble zunimmt.

Beliebte Bibliotheken für maschinelles Lernen, wie scikit-learn und TensorFlow, bieten einfach
benutzerfreundliche Implementierungen für viele gängige Bagging- und Boosting-Methoden, wie Ran- dom Forest
dom forest, AdaBoost, Gradient Boosting und XGBoost. Wir sollten jedoch
sorgfältig abwägen, ob sich der mit einer Ensemble-Methode verbundene Mehraufwand
Methode verbunden ist, es wert ist. Vergleichen Sie immer die Genauigkeit und den Ressourcenverbrauch mit einem linearen oder
DNN-Modell. Beachten Sie, dass das Destillieren (siehe "Entwurfsmuster 11: Nützliches Overfitting" auf Seite
141 ) eines Ensembles von neuronalen Netzen oft die Komplexität reduzieren und die
Leistung verbessern.

Ausstieg als Absackung

Techniken wie Dropout bieten eine leistungsfähige und wirksame Alternative. Dropout ist
bekannt als Regularisierungstechnik beim Deep Learning, kann aber auch als
eine Annäherung an Bagging. Dropout schaltet in einem neuronalen Netz zufällig (mit einer bestimmten
Wahrscheinlichkeit) Neuronen des Netzes für jeden Mini-Batch des Trainings "aus",
Im Wesentlichen wird ein Ensemble aus exponentiell vielen neuronalen Netzen ausgewertet.
Das Training eines neuronalen Netzes mit Dropout ist jedoch nicht genau dasselbe wie
Bagging. Es gibt zwei bemerkenswerte Unterschiede. Erstens sind die Modelle beim Bagging
unabhängig, während die Modelle beim Training mit Dropout gemeinsame Parameter haben.
Zweitens werden die Modelle beim Bagging bis zur Konvergenz auf ihrer jeweiligen Trainingsmenge trainiert.
Trainingsmenge trainiert. Beim Training mit Dropout hingegen werden die Modelle der Ensemblemitglieder
nur für einen einzigen Trainingsschritt trainiert werden, da in jeder Iteration der Trainingsschleife verschiedene
jeder Iteration der Trainingsschleife ausfallen.

Geringere Interpretierbarkeit des Modells

Ein weiterer Punkt, den es zu beachten gilt, ist die Interpretierbarkeit von Modellen. Schon beim Deep Learning,
kann es schwierig sein, effektiv zu erklären, warum unser Modell die Vorhersagen macht, die es macht.
Dieses Problem wird bei Ensemble-Modellen noch verschärft. Nehmen wir zum Beispiel die Entscheidung

Entwurfsmuster 7: Ensembles | 107
Bäume im Vergleich zum Random Forest. Ein Entscheidungsbaum lernt schließlich Grenzwerte für
jedes Merkmal, die eine einzelne Instanz zur endgültigen Vorhersage des Modells führen. Als solches ist es
einfach zu erklären, warum ein Entscheidungsbaum die Vorhersagen macht, die er gemacht hat. Der Random Forest,
der ein Ensemble aus vielen Entscheidungsbäumen ist, verliert diese Ebene der lokalen Interpretierbarkeit.

Auswahl des richtigen Werkzeugs für das Problem

Es ist auch wichtig, den Kompromiss zwischen Verzerrung und Varianz im Auge zu behalten. Einige Ensemble-Technologien
niken können Verzerrungen oder Varianz besser berücksichtigen als andere (Tabelle 3-2). Insbesondere
Boosting eignet sich insbesondere für die Korrektur einer hohen Verzerrung, während Bagging für die Korrektur einer
hohe Varianz. Wie wir im Abschnitt "Bagging" auf Seite 100 gesehen haben, ist die Kombination zweier Modelle mit stark korrelierten Fehlern
zwei Modelle mit stark korrelierten Fehlern zu kombinieren, nicht dazu beitragen, die
Varianz. Kurz gesagt, die Verwendung der falschen Ensemble-Methode für unser Problem wird nicht notwendigerweise die
Die Verwendung der falschen Ensemble-Methode für unser Problem wird nicht unbedingt die Leistung verbessern, sondern nur unnötigen Overhead verursachen.

Tabelle 3-2. Eine Zusammenfassung des Kompromisses zwischen Verzerrung und Varianz

Problem Ensemble-Lösung
Hohe Verzerrung (Unteranpassung) Boosting
Hohe Varianz (Überanpassung) Bagging
Andere Ensemble-Methoden

Wir haben einige der gebräuchlichsten Ensemble-Techniken beim maschinellen Lernen erörtert.
erörtert. Die zuvor besprochene Liste ist keineswegs vollständig, und es gibt verschiedene
Algorithmen, die in diese groben Kategorien passen. Es gibt auch andere Ensemble-Techniken
auch andere Ensemble-Techniken, darunter viele, die einen Bayes'schen Ansatz beinhalten oder die neuronale
Architektur-Suche und Reinforcement Learning kombinieren, wie Googles AdaNet oder AutoML
Techniken. Kurz gesagt, das Ensemble-Designmuster umfasst Techniken, die mehrere
die mehrere Modelle für maschinelles Lernen kombinieren, um die Gesamtleistung des Modells zu verbessern.
kann besonders nützlich sein, wenn es um häufige Trainingsprobleme wie hohe Verzerrungen oder
hohe Varianz.

Entwurfsmuster 8: Kaskade
Das Cascade-Entwurfsmuster befasst sich mit Situationen, in denen ein Problem des maschinellen Lernens
gewinnbringend in eine Reihe von ML-Problemen aufgeteilt werden kann. Eine solche Kaskade erfordert oft
eine sorgfältige Planung des ML-Experiments.

Problem
Was passiert, wenn wir einen Wert sowohl bei normaler als auch bei ungewöhnlicher Aktivität vorhersagen müssen?
Das Modell wird lernen, die ungewöhnliche Aktivität zu ignorieren, weil sie selten ist. Wenn die ungewöhnliche
Aktivität auch mit abnormalen Werten verbunden ist, leidet die Trainierbarkeit.

108 | Kapitel 3: Entwurfsmuster für die Problemdarstellung

Nehmen wir zum Beispiel an, dass wir versuchen, ein Modell zu trainieren, das die Wahrscheinlichkeit vorhersagt, dass ein
Kunde einen gekauften Artikel zurückgeben wird. Wenn wir ein einziges Modell trainieren, geht das
das Rückgabeverhalten der Wiederverkäufer verloren, da es Millionen von Einzelhandelskäufern (und
Einzelhandelstransaktionen) und nur ein paar tausend Wiederverkäufer gibt. Wir wissen nicht wirklich zum Zeitpunkt
zum Zeitpunkt des Kaufs nicht wirklich, ob es sich um einen Einzelhandelskäufer oder einen Wiederverkäufer handelt. Wie-
Durch die Beobachtung anderer Marktplätze haben wir jedoch festgestellt, wann bei uns gekaufte Artikel
weiterverkauft werden, so dass unser Trainingsdatensatz eine Kennzeichnung enthält, die
dass ein Kauf von einem Wiederverkäufer getätigt wurde.

Eine Möglichkeit, dieses Problem zu lösen, ist die Übergewichtung der Wiederverkäufer-Instanzen beim Training
des Modells zu übergewichten. Dies ist suboptimal, da wir den häufigeren Anwendungsfall des Einzelhandelskäufers
Anwendungsfall so korrekt wie möglich abbilden. Wir wollen nicht eine geringere Genauigkeit im
für eine höhere Genauigkeit beim Anwendungsfall Wiederverkäufer eintauschen. Aber Einzelhändler
verhalten sich Einzelhandelskäufer und Wiederverkäufer sehr unterschiedlich; während Einzelhandelskäufer zum Beispiel
Artikel innerhalb von etwa einer Woche zurückgeben, geben Wiederverkäufer Artikel nur zurück, wenn sie sie nicht verkaufen können,
Die Rückgabe kann also auch nach mehreren Monaten erfolgen. Die geschäftliche Entscheidung über
der Lagerhaltung ist bei wahrscheinlichen Rückgaben von Einzelhandelskäufern anders als bei Wiederverkäufern.
Daher ist es notwendig, beide Arten von Rückgaben so genau wie möglich zu erfassen. Einfach
Übergewichtung der Wiederverkäufer wird nicht funktionieren.

Ein intuitiver Weg, dieses Problem zu lösen, ist die Verwendung des Cascade-Designmusters. Wir
gliedern das Problem in vier Teile:

Vorhersage, ob eine bestimmte Transaktion von einem Wiederverkäufer getätigt wurde
Training eines Modells auf Verkäufe an Einzelhandelskäufer
Trainieren des zweiten Modells auf Verkäufe an Wiederverkäufer
In der Produktion werden die Ergebnisse der drei separaten Modelle kombiniert, um die
Rückgabewahrscheinlichkeit für jeden gekauften Artikel und die Wahrscheinlichkeit, dass die Transaktion
Transaktion von einem Wiederverkäufer stammt
Dies ermöglicht unterschiedliche Entscheidungen über Artikel, die wahrscheinlich zurückgegeben werden
und stellt sicher, dass die Modelle in den Schritten 2 und 3 so genau wie möglich
in ihrem Segment der Trainingsdaten so genau wie möglich sind. Jedes dieser Modelle ist rel-
relativ einfach zu trainieren. Das erste ist einfach ein Klassifikator, und wenn die ungewöhnliche Aktivität
Wenn die ungewöhnliche Aktivität extrem selten ist, können wir sie mit dem Rebalancing-Muster angehen. Die nächsten beiden Modi
sind im Wesentlichen Klassifizierungsmodelle, die auf verschiedenen Segmenten der
Daten trainiert. Die Kombination ist deterministisch, da wir wählen, welches Modell ausgeführt werden soll, je nachdem
ob die Aktivität zu einem Wiederverkäufer gehörte.

Das Problem tritt bei der Vorhersage auf. Zum Zeitpunkt der Vorhersage haben wir keine echten Bezeichnungen,
sondern nur die Ausgabe des ersten Klassifizierungsmodells. Auf der Grundlage der Ausgabe des ersten Modells
müssen wir bestimmen, welches der beiden Verkaufsmodelle wir heranziehen. Das Problem ist
dass wir auf Basis von Bezeichnungen trainieren, aber zum Zeitpunkt der Inferenz Entscheidungen treffen müssen

Entwurfsmuster 8: Kaskade | 109
basiert auf Vorhersagen. Und Vorhersagen sind fehlerhaft. Daher müssen das zweite und dritte Modell
müssen also Vorhersagen zu Daten treffen, die sie während des Trainings möglicherweise nie gesehen
Ausbildung gesehen haben.

Nehmen wir als extremes Beispiel an, dass die Adresse, die die Wiederverkäufer angeben, immer in einem
Industriegebiet der Stadt liegt, während Einzelhandelskäufer überall wohnen können. Wenn das erste (Klassifizierungs-) Modell
(Klassifizierungs-)Modell einen Fehler macht und ein Einzelhandelskäufer fälschlicherweise als Wiederverkäufer identifiziert wird,
wird das aufgerufene Modell für die Kündigungsvorhersage die Nachbarschaft
wo der Kunde wohnt, nicht in seinem Vokabular.

Wie trainiert man eine Kaskade von Modellen, bei der die Ausgabe eines Modells eine Eingabe für
das folgende Modell ist oder die Auswahl der nachfolgenden Modelle bestimmt?

Lösung
Jedes Problem des maschinellen Lernens, bei dem die Ausgabe des einen Modells eine Eingabe für das
folgenden Modell ist oder die Auswahl der nachfolgenden Modelle bestimmt, wird als Kaskade bezeichnet.
Beim Training einer Kaskade von ML-Modellen ist besondere Vorsicht geboten.

Zum Beispiel kann ein Problem des maschinellen Lernens, das manchmal ungewöhnliche Umstände
kann gelöst werden, indem man es als eine Kaskade von vier Problemen des maschinellen Lernens behandelt:

Ein Klassifizierungsmodell zur Identifizierung des Umstandes
Ein Modell, das auf ungewöhnliche Umstände trainiert wurde
Ein separates Modell, das auf typische Umstände trainiert wurde
Ein Modell, das die Ergebnisse der beiden getrennten Modelle kombiniert, denn das Ergebnis ist
eine probabilistische Kombination der beiden Ergebnisse
Dies scheint auf den ersten Blick ein spezieller Fall des Entwurfsmusters Ensemble zu sein, aber
wird jedoch wegen der besonderen Versuchsplanung, die bei einer Kaskade erforderlich ist, gesondert betrachtet.
einer Kaskade.

Ein Beispiel: Um die Kosten für die Bereitstellung von Fahrrädern an den Stationen zu
die Entfernung zwischen Verleih- und Rückgabestationen für Fahrräder in San Francisco vorhersagen
in San Francisco vorhersagen. Das Ziel des Modells ist es also, die Entfernung vorherzusagen, die wir benötigen, um das Fahrrad
die wir für den Rücktransport des Fahrrads zum Verleihort benötigen, wenn wir
die Uhrzeit des Mietbeginns, den Ort, an dem das Fahrrad ausgeliehen wird, ob der
Mieter ein Abonnent ist oder nicht, usw. Das Problem ist, dass bei Ausleihen, die länger als vier
Das Problem besteht darin, dass bei Ausleihen, die länger als vier Stunden dauern, ein ganz anderes Verhalten des Mieters vorliegt als bei kürzeren Ausleihen, und der Bestandsalgorithmus
Algorithmus benötigt beide Ausgaben (die Wahrscheinlichkeit, dass der Verleih länger als vier Stunden dauert
vier Stunden dauert, und die wahrscheinliche Entfernung, die das Fahrrad transportiert werden muss). Allerdings ist nur
nur ein sehr kleiner Teil der Verleihvorgänge umfasst solche abnormalen Fahrten.

110 | Kapitel 3: Entwurfsmuster für die Problemdarstellung

Eine Möglichkeit, dieses Problem zu lösen, besteht darin, ein Klassifizierungsmodell zu trainieren, das zunächst die Fahrten klassifiziert
zu klassifizieren, ob sie lang oder typisch sind (der vollständige Code ist im Code-Repository
dieses Buches):

CREATE OR REPLACE MODEL mlpatterns.classify_trips
TRANSFORM (
trip_type,
EXTRACT (HOUR FROM start_date) AS start_hour,
EXTRACT (DAYOFWEEK FROM start_date) AS day_of_week,
start_station_name,
teilnehmer_typ,
...
)
OPTIONS (model_type='logistic_reg',
auto_class_weights= True ,
input_label_cols=['trip_type']) AS
SELECT
start_date, start_station_name, subscriber_type, ...
IF(dauer_sec > 3600*4, 'Lang', 'Typisch' ) AS trip_type
FROM `bigquery- public - data .san_francisco_bikeshare.bikeshare_trips`
Es kann verlockend sein, den Trainingsdatensatz einfach in zwei Teile zu teilen, basierend auf der
Dauer der Anmietung aufzuteilen und die nächsten beiden Modelle zu trainieren, eines für lange Anmietungen und
das andere für typische Anmietungen. Das Problem dabei ist, dass das soeben beschriebene Klassifizierungsmodell
besprochene Modell Fehler aufweisen wird. Die Auswertung des Modells anhand eines Teils der Fahrraddaten aus San
Francisco zeigt, dass die Genauigkeit des Modells nur etwa 75 % beträgt (siehe
Abbildung 3-16). In Anbetracht dessen führt das Training eines Modells auf einer perfekten Aufteilung der Daten zu
Risse.

Abbildung 3-16. Die Genauigkeit eines Klassifikationsmodells zur Vorhersage atypischen Verhaltens ist
wird wahrscheinlich nicht 100 % betragen.

Stattdessen müssen wir nach dem Training dieses Klassifizierungsmodells die Vorhersagen dieses Modells verwenden
Modells verwenden, um den Trainingsdatensatz für den nächsten Satz von Modellen zu erstellen. Zum Beispiel könnten wir
den Trainingsdatensatz für das Modell zur Vorhersage der Entfernung typischer Vermietungen erstellen
verwenden:

Entwurfsmuster 8: Kaskade | 111
CREATE OR REPLACE TABLE mlpatterns. Typische_Reisen AS
SELECT
* EXCEPT (vorausgesagte_Reise_art_probs, vorausgesagte_Reise_art)
FROM
ML.PREDICT(MODEL mlpatterns.classify_trips,
( SELECT
start_date, start_station_name, subscriber_type, ...,
ST_Distance(start_station_geom, end_station_geom) AS distance
FROM `bigquery- public - data .san_francisco_bikeshare.bikeshare_trips`)
)
WHERE predicted_trip_type = 'Typical' AND distance IS NOT NULL
Dann sollten wir diesen Datensatz verwenden, um das Modell zur Vorhersage von Entfernungen zu trainieren:

CREATE OR REPLACE MODEL mlpatterns.predict_distance_Typical
TRANSFORM (
Entfernung,
EXTRACT (HOUR FROM start_date) AS start_hour,
EXTRACT (DAYOFWEEK FROM start_date) AS day_of_week,
start_station_name,
teilnehmer_typ,
...
)
OPTIONS (model_type='linear_reg', input_label_cols=['distance']) AS
SELECT
*
FROM
mlpatterns.Typical_trips
Schließlich sollte bei der Auswertung, Vorhersage usw. berücksichtigt werden, dass wir drei trainierte Modelle verwenden müssen
drei trainierte Modelle verwenden müssen, nicht nur eines. Dies bezeichnen wir als das Kaskaden-Designmuster.

In der Praxis kann es schwierig werden, einen Kaskaden-Workflow aufrechtzuerhalten. Anstatt
die Modelle einzeln zu trainieren, ist es besser, den gesamten Arbeitsablauf mit Hilfe des
Workflow-Pipelines-Muster (Kapitel 6) zu automatisieren, wie in Abbildung 3-17 dargestellt. Der Schlüssel dazu ist, sicherzustellen
dass die Trainingsdatensätze für die beiden nachgelagerten Modelle jedes Mal erstellt werden, wenn das
Experiment auf der Grundlage der Vorhersagen der vorgelagerten Modelle erstellt werden.

Obwohl wir das Kaskadenmuster als eine Möglichkeit zur Vorhersage eines Wertes während
und ungewöhnlichen Aktivitäten vorhersagen kann, ist die Lösung des Kaskadenmusters in der Lage
eine allgemeinere Situation. Das Pipeline-Framework ermöglicht es uns, jede Situation zu behandeln
Situation, in der ein Problem des maschinellen Lernens gewinnbringend in eine Reihe (oder
Kaskade) von ML-Problemen unterteilt werden kann. Wann immer die Ausgabe eines maschinellen Lernmodells
Modells in ein anderes Modell eingespeist werden soll, muss das zweite Modell mit den
Vorhersagen des ersten Modells trainiert werden. In all diesen Situationen ist ein formales Pipeline-Experimenta-
tionsrahmen hilfreich.

112 | Kapitel 3: Entwurfsmuster für die Problemdarstellung

Abbildung 3-17. Eine Pipeline zum Trainieren der Kaskade von Modellen in einem einzigen Auftrag.

Kubeflow Pipelines bietet einen solchen Rahmen. Da es mit Containern arbeitet, können die
können die zugrundeliegenden Machine-Learning-Modelle und der Glue-Code in fast jeder Pro- grammier- oder
grammier- oder Skriptsprache geschrieben werden. Hier werden wir die oben genannten BigQuery-SQL-Modelle
in Python-Funktionen unter Verwendung der BigQuery-Client-Bibliothek. Wir könnten TensorFlow oder
scikit-learn oder sogar R verwenden, um einzelne Komponenten zu implementieren.

Der Pipeline-Code, der Kubeflow Pipelines verwendet, kann ganz einfach wie folgt ausgedrückt werden
(den vollständigen Code finden Sie im Code-Repository dieses Buches):

@dsl.pipeline(
name='Kaskaden-Pipeline auf SF Bikeshare',
description='Kaskaden-Pipeline auf SF bikeshare'
)
def cascade_pipeline(
projekt_id = PROJEKT_ID
):
ddlop = comp.func_to_container_op(run_bigquery_ddl,
packages_to_install=['google-cloud-bigquery'])
c1 = train_classification_model(ddlop, PROJECT_ID)
c1_model_name = c1.outputs['created_table']
c2a_input = create_training_data(ddlop,
PROJEKT_ID, c1_Modell_name, 'Typisch')
c2b_input = create_training_data(ddlop,
PROJECT_ID, c1_model_name, 'Lang')
Entwurfsmuster 8: Kaskade | 113
c3a_model = train_distance_model(ddlop,
PROJECT_ID, c2a_input.outputs['created_table'], 'Typical')
c3b_model = train_distance_model(ddlop,
PROJECT_ID, c2b_input.outputs['created_table'], 'Long')
Die gesamte Pipeline kann zur Ausführung vorgelegt werden, und verschiedene Durchläufe des Experiments
mit Hilfe des Pipelines-Frameworks nachverfolgt werden.

Wenn wir TFX als unser Pipeline-Framework verwenden (wir können TFX auf
Kubeflow Pipelines), dann ist es nicht notwendig, die
Upstream-Modelle einzusetzen, um ihre Ausgabevorhersagen in Down-Stream-Modellen
Strom-Modelle zu verwenden. Stattdessen können wir die TensorFlow Transform
Methode tft.apply_saved_model als Teil unserer Vorverarbeitungs
Operationen verwenden. Das Transform-Entwurfsmuster wird in
Kapitel 6.
Die Verwendung eines Pipeline-Experiment-Frameworks wird dringend empfohlen, wenn wir
verkettete ML-Modelle haben. Ein solcher Rahmen stellt sicher, dass die nachgelagerten Modelle
Modelle neu trainiert werden, wenn die vorgelagerten Modelle überarbeitet werden, und dass wir eine Historie aller
vorherigen Trainingsläufe.

Gegenleistungen und Alternativen
Übertreiben Sie es nicht mit dem Cascade-Entwurfsmuster - anders als viele der in diesem Buch behandelten Entwurfsmuster
wie viele der in diesem Buch behandelten Entwurfsmuster, ist Cascade nicht unbedingt die beste Praxis. Es erhöht die
Komplexität in Ihre Arbeitsabläufe für maschinelles Lernen und kann sogar zu einer
schlechteren Leistung führen. Beachten Sie, dass ein Pipeline-Experiment-Framework definitiv die beste
aber versuchen Sie, eine Pipeline so weit wie möglich auf ein einziges maschinelles Lernproblem zu beschränken
Problem zu beschränken (Einlesen, Vorverarbeitung, Datenvalidierung, Umwandlung, Training, Evaluierung,
und Einsatz). Vermeiden Sie, wie beim Kaskadenmuster, mehrere Modelle für maschinelles Lernen
Modelle in ein und derselben Pipeline.

114 | Kapitel 3: Entwurfsmuster für die Problemdarstellung

Deterministische Eingaben

Die Aufteilung eines ML-Problems ist in der Regel eine schlechte Idee, da ein ML-Modell
Kombinationen von mehreren Faktoren lernen kann/sollte. Zum Beispiel:

Wenn eine Bedingung deterministisch aus der Eingabe bekannt ist (Urlaubseinkäufe
im Vergleich zu Einkäufen an Wochentagen), sollten wir die Bedingung einfach als eine weitere Eingabe
in das Modell aufnehmen.
Wenn die Bedingung ein Extremum in nur einer Eingabe beinhaltet (einige Kunden, die
Kunden, die in der Nähe oder weit weg wohnen, wobei die Bedeutung von nah/fern aus den Daten gelernt werden muss)
den Daten gelernt werden muss), können wir die gemischte Eingabedarstellung verwenden, um damit umzugehen.
Das Cascade-Entwurfsmuster behandelt ein ungewöhnliches Szenario, für das wir keine
eine kategorische Eingabe haben und für die Extremwerte aus mehreren Eingaben gelernt werden
Eingaben gelernt werden müssen.

Einzelnes Modell

Das Kaskadenentwurfsmuster sollte nicht für allgemeine Szenarien verwendet werden, in denen ein einzelnes
Modell ausreicht. Nehmen wir zum Beispiel an, wir versuchen, die Kaufbereitschaft eines Kunden zu
Kaufbereitschaft eines Kunden zu lernen. Wir könnten denken, dass wir verschiedene Modelle für Personen lernen müssen, die
Vergleichseinkäufe getätigt haben und solche, die das nicht tun. Wir wissen nicht wirklich, wer
Vergleichseinkäufe getätigt hat, aber wir können eine fundierte Vermutung anstellen, basierend auf der Anzahl
Anzahl der Besuche, wie lange der Artikel im Einkaufswagen lag und so weiter. Dieses Problem erfordert nicht
das Cascade Design Pattern, weil es häufig genug vorkommt (ein großer Teil der
Kunden werden Vergleichseinkäufe tätigen), dass das maschinelle Lernmodell in der Lage sein sollte
in der Lage sein sollte, es im Laufe des Trainings implizit zu lernen. Trainieren Sie für gängige Szenarien ein einziges
einfaches Modell.

Interne Konsistenz

Die Kaskade wird benötigt, wenn wir die interne Konsistenz zwischen den
Vorhersagen mehrerer Modelle. Beachten Sie, dass wir versuchen, mehr zu tun als nur
die ungewöhnliche Aktivität vorherzusagen. Wir versuchen, die Renditen vorherzusagen, wobei wir berücksichtigen, dass es auch
Wiederverkäuferaktivität gibt. Wenn die Aufgabe nur darin bestünde, vorherzusagen, ob ein Verkauf von einem Wiederverkäufer stammt oder nicht
Wiederverkäufer ist, würden wir das Rebalancing-Muster verwenden. Der Grund für die Verwendung von Cascade ist, dass die
unausgewogene Label-Ausgabe als Eingabe für nachfolgende Modelle benötigt wird und für sich
und selbst nützlich ist.

Nehmen wir an, dass der Grund für das Training des Modells zur Vorhersage der Kaufbereitschaft eines Kunden
Kaufbereitschaft eines Kunden zu trainieren, ist, dass wir ein Rabattangebot machen. Ob wir das vergünstigte Angebot machen oder nicht
und die Höhe des Preisnachlasses hängen häufig davon ab, ob der Kunde
Kunde vergleichend einkauft oder nicht. Angesichts dessen brauchen wir interne Konsistenz
zwischen den beiden Modellen (dem Modell für Vergleichskäufer und dem Modell für die
Kaufneigung). In diesem Fall könnte das Cascade-Entwurfsmuster erforderlich sein.

Entwurfsmuster 8: Kaskade | 115
Vorgefertigte Modelle

Die Kaskade wird auch benötigt, wenn wir die Ausgabe eines bereits trainierten Modells
als Eingabe für unser Modell verwenden wollen. Nehmen wir zum Beispiel an, wir erstellen ein Modell zur Erkennung
zu erkennen, wer ein Gebäude betreten darf, damit wir das Tor automatisch öffnen können. Eine der
der Eingaben für unser Modell könnte das Nummernschild des Fahrzeugs sein. Anstatt das
Sicherheitsfoto direkt in unserem Modell zu verwenden, könnten wir es einfacher finden, die Ausgabe eines
Modells zur optischen Zeichenerkennung (OCR) zu verwenden. Es ist wichtig, dass wir erkennen, dass OCR
Systeme mit Fehlern behaftet sind, so dass wir unser Modell nicht mit perfekten
Nummernschildinformationen trainieren. Stattdessen sollten wir das Modell mit der tatsächlichen Ausgabe des
OCR-Systems trainieren. Da sich verschiedene OCR-Modelle unterschiedlich verhalten und unterschiedliche Fehler aufweisen
unterschiedliche Fehler aufweisen, muss das Modell neu trainiert werden, wenn wir den Hersteller unseres
OCR-Systems wechseln.

Ein gängiges Szenario für die Verwendung eines vorab trainierten Modells als ersten Schritt
einer Pipeline ist die Verwendung eines Objekterkennungsmodells, gefolgt von einem fein
feinkörnigen Bildklassifizierungsmodell. Zum Beispiel könnte das Objekt
Erkennungsmodell alle Handtaschen im Bild finden, ein
Zwischenschritt könnte das Bild auf die Bounding Boxes der erkannten
der erkannten Objekte, und das nachfolgende Modell könnte die Art der
Typ der Handtasche. Wir empfehlen die Verwendung einer Kaskade, damit die gesamte
Pipeline neu trainiert werden kann, wenn das Objekterkennungsmodell
aktualisiert wird (z. B. durch eine neue Version der API).
Reframing anstelle von Kaskade

Beachten Sie, dass wir in unserem Beispielproblem versucht haben, die Wahrscheinlichkeit vorherzusagen, dass ein
Artikel zurückgegeben wird, es sich also um ein Klassifikationsproblem handelt. Angenommen, wir wollen stattdessen
stündliche Verkaufsbeträge vorhersagen wollen. Die meiste Zeit werden wir nur Einzelhandelskäufer bedienen.
Käufer, aber hin und wieder (vielleicht vier- oder fünfmal im Jahr) haben wir einen Großhändler
Käufer.

Es handelt sich um ein Regressionsproblem bei der Vorhersage der täglichen Verkaufsmengen, bei dem wir
einen Störfaktor in Form von Großhandelskäufern haben. Eine Umdeutung des Regressionsproblems
Regressionsproblem in ein Klassifizierungsproblem verschiedener Verkaufsmengen umzuwandeln, könnte ein besserer
Ansatz sein. Dies erfordert zwar das Training eines Klassifizierungsmodells für jede
ein Klassifizierungsmodell für jede Verkaufsmenge trainiert werden muss, entfällt die Notwendigkeit, die Klassifizierung zwischen
richtig zu machen.

Regression in seltenen Situationen

Das Kaskadenentwurfsmuster kann bei der Durchführung von Regressionen hilfreich sein, wenn einige
Werte viel häufiger vorkommen als andere. Zum Beispiel könnten wir die Regenmenge
die Niederschlagsmenge aus einem Satellitenbild vorhersagen. Es könnte der Fall sein, dass in 99% der Fälle

116 | Kapitel 3: Entwurfsmuster für die Problemdarstellung

2 Dies ist nur ein Beispiel, das der Veranschaulichung dient; bitte verstehen Sie dies nicht als medizinischen Rat!
Pixel, regnet es nicht. In einem solchen Fall kann es hilfreich sein, ein gestapeltes Klassifikationsmodell zu erstellen
Modell zu erstellen, gefolgt von einem Regressionsmodell:

Sagen Sie zunächst voraus, ob es regnen wird oder nicht.
Für Pixel, für die das Modell keine Regenwahrscheinlichkeit vorhersagt, sagen Sie eine Niederschlagsmenge von
Null.
Trainieren Sie ein Regressionsmodell zur Vorhersage der Regenmenge für Pixel, bei denen das
Modell eine Regenwahrscheinlichkeit vorhersagt.
Es ist wichtig zu wissen, dass das Klassifizierungsmodell nicht perfekt ist und dass das Regressionsmodell
Regressionsmodell auf den Pixeln trainiert werden muss, die das Klassifizierungsmodell als wahrscheinlich für
Regen vorhergesagt hat (und nicht nur auf Pixeln, die im beschrifteten Datensatz Regen entsprechen). Für
ergänzende Lösungen für dieses Problem finden Sie auch in den Diskussionen zu "Entwurfsmuster
10: Rebalancing " auf Seite 122 und "Entwurfsmuster 5: Reframing " auf Seite 80.

Entwurfsmuster 9: Neutrale Klasse
In vielen Klassifizierungssituationen kann es hilfreich sein, eine neutrale Klasse zu erstellen. Zum Beispiel,
Anstatt einen binären Klassifikator zu trainieren, der die Wahrscheinlichkeit eines Ereignisses ausgibt, trainieren Sie einen
Drei-Klassen-Klassifikator, der disjunkte Wahrscheinlichkeiten für Ja, Nein und Vielleicht ausgibt. Ent-
getrennt bedeutet hier, dass sich die Klassen nicht überschneiden. Ein Trainingsmuster kann nur zu einer
Ein Trainingsmuster kann nur einer Klasse angehören, so dass es z. B. keine Überschneidung zwischen Ja und Vielleicht gibt. Die
Vielleicht ist in diesem Fall die neutrale Klasse.

Problem
Stellen Sie sich vor, dass wir versuchen, ein Modell zu erstellen, das eine Orientierungshilfe für Schmerzmittel bietet.
Es gibt zwei Möglichkeiten, Ibuprofen und Paracetamol,^2 und es stellt sich in unserem historischen Datensatz heraus, dass Paracetamol
dass Paracetamol bevorzugt Patienten mit einem Risiko für Magenprobleme verschrieben wird
Magenproblemen verschrieben wird, und Ibuprofen bevorzugt an Patienten mit
Risiko einer Leberschädigung verschrieben wird. Darüber hinaus sind die Dinge eher zufällig; einige Ärzte
standardmäßig Paracetamol und andere Ibuprofen.

Das Training eines binären Klassifikators auf einem solchen Datensatz führt zu einer schlechten Genauigkeit, da das
Modell die im Wesentlichen willkürlichen Fälle richtig erkennen muss.

Entwurfsmuster 9: Neutrale Klasse | 117
Lösung
Stellen Sie sich ein anderes Szenario vor. Angenommen, die elektronische Akte, die die Verordnungen des Arztes erfasst
Verordnungen des Arztes erfasst, fragt ihn auch, ob die alternative Schmerzmedikation akzeptabel ist.
tig ist. Wenn der Arzt Paracetamol verschreibt, fragt die Anwendung den Arzt, ob
der Patient Ibuprofen verwenden kann, wenn er es bereits in seiner Hausapotheke hat.

Ausgehend von der Antwort auf die zweite Frage, haben wir eine neutrale Klasse. Die Verschreibung
könnte immer noch als "Paracetamol" geschrieben sein, aber in den Unterlagen ist festgehalten, dass der Arzt
neutral für diesen Patienten war. Beachten Sie, dass dies grundsätzlich eine entsprechende Gestaltung der Datenerhebung erfordert.
Wir können keine neutrale Klasse im Nachhinein herstellen. Wir
müssen das Problem des maschinellen Lernens richtig entwerfen. Korrektes Design fängt in diesem Fall damit an,
fängt in diesem Fall damit an, wie wir das Problem in erster Linie formulieren.

Wenn wir nur einen historischen Datensatz haben, müssten wir einen Etikettierungsdienst einschalten.
Wir könnten die menschlichen Etikettierer bitten, die ursprüngliche Entscheidung des Arztes zu bestätigen und die Frage zu beantworten
die Frage beantworten, ob ein alternatives Schmerzmittel akzeptabel wäre.

Warum es funktioniert
Wir können den Mechanismus erforschen, durch den dies funktioniert, indem wir den Mechanismus
mit einem synthetischen Datensatz simulieren. Dann werden wir zeigen, dass etwas Ähnliches auch
auch in der realen Welt mit Grenzfällen geschieht.

Synthetische Daten

Erstellen wir einen synthetischen Datensatz der Länge N, bei dem 10 % der Daten Patienten darstellen
mit einer Vorgeschichte von Gelbsucht darstellen. Da bei ihnen das Risiko einer Leberschädigung besteht, ist ihre korrekte Vor
Ibuprofen verschrieben (der vollständige Code ist auf GitHub zu finden):

Gelbsucht[0:N//10] = Wahr
Verschreibung[0:N//10] = 'Ibuprofen'
Bei weiteren 10 % der Daten handelt es sich um Patienten mit einer Vorgeschichte von Magengeschwüren;
Da bei ihnen das Risiko eines Magengeschwürs besteht, ist ihre korrekte Verschreibung Paracetamol:

Geschwüre[(9*N)//10:] = Wahr
Verschreibung[(9*N)//10:] = 'Paracetamol'
Die übrigen Patienten werden willkürlich einem der beiden Medikamente zugewiesen. Natürlich,
wird die Gesamtgenauigkeit eines Modells, das auf nur zwei Klassen trainiert wurde
Klassen trainiert wurde, niedrig ist. Tatsächlich können wir die obere Grenze der Genauigkeit berechnen. Denn
80 % der Trainingsbeispiele zufällige Bezeichnungen haben, kann das Modell höchstens die
die Hälfte von ihnen richtig zu erraten. Die Genauigkeit für diese Teilmenge der Trainingsbeispiele
40 % betragen. Die verbleibenden 20 % der Trainingsbeispiele haben systematische Bezeichnungen, und
ein ideales Modell lernt dies, so dass wir erwarten, dass die Gesamtgenauigkeit bestenfalls 60 % betragen kann.

Wenn wir ein Modell mit scikit-learn wie folgt trainieren, erhalten wir eine Genauigkeit von 0,56:

118 | Kapitel 3: Entwurfsmuster für die Problemdarstellung

ntrain = 8*len(df)//10 # 80% der Daten für das Training
lm = linear_model.LogistischeRegression()
lm = lm.fit(df.loc[:ntrain-1, ['Gelbsucht', 'Geschwüre']],
df[label][:ntrain])
acc = lm.score(df.loc[ntrain:, ['Gelbsucht', 'Geschwüre']],
df[label][ntrain:])
Wenn wir drei Klassen bilden und alle zufällig zugewiesenen Verschreibungen in diese
Klasse zuordnen, erhalten wir erwartungsgemäß eine perfekte (100 %) Genauigkeit. Der Zweck der synthetischen Daten
sollte veranschaulichen, dass das Entwurfsmuster "Neutrale Klasse" - vorausgesetzt, es handelt sich um eine zufällige Zuweisung
Entwurfsmuster dabei helfen kann, den Verlust von Modellgenauigkeit aufgrund von willkürlich beschrifteten
Daten.

In der realen Welt

In realen Situationen sind die Dinge vielleicht nicht ganz so zufällig wie im synthetischen Datensatz.
Menge, aber das Paradigma der willkürlichen Zuweisung gilt immer noch. Zum Beispiel wird eine Minute nach
ein Baby geboren wird, wird ihm ein "Apgar-Score" zugewiesen, eine Zahl zwischen 1 und 10,
wobei 10 für ein Baby steht, das den Geburtsvorgang perfekt überstanden hat.

Nehmen wir ein Modell, das trainiert ist, um vorherzusagen, ob ein Baby den
die Geburt gesund übersteht oder ob es sofortige Hilfe benötigt (der vollständige Code ist auf
GitHub):

CREATE OR REPLACE MODEL mlpatterns.neutral_2classes
OPTIONS(model_type='logistic_reg', input_label_cols=['health']) AS
SELECT
IF(apgar_1min >= 9, 'Healthy', 'NeedsAttention') AS health,
Mehrzahl,
mutter_alter,
Schwangerschaft_Wochen,
je_geboren
FROM `bigquery-public-data.samples.natality`
WHERE apgar_1min <= 10
Wir setzen den Schwellenwert für den Apgar-Score auf 9 und behandeln Babys mit einem Apgar-Score von 9
oder 10 ist, als gesund und Babys, deren Apgar-Score 8 oder niedriger ist, als behandlungsbedürftig.
Die Genauigkeit dieses binären Klassifizierungsmodells, das mit dem Geburtsdatensatz trainiert
trainiert und anhand der ausgewerteten Daten evaluiert wurde, beträgt 0,56.

Die Zuweisung eines Apgar-Scores beinhaltet jedoch eine Reihe von relativ subjektiven Bewertungen,
und ob ein Baby mit 8 oder 9 bewertet wird, ist oft nur eine Frage der ärztlichen
Vorliebe. Solche Babys sind weder vollkommen gesund, noch benötigen sie ernsthafte medizinische
Intervention. Was wäre, wenn wir eine neutrale Klasse für diese "marginalen" Werte schaffen würden? Dies
müssen drei Klassen gebildet werden, wobei ein Apgar-Wert von 10 als gesund, Werte von
8 bis 9 als neutral und niedrigere Werte als behandlungsbedürftig definiert werden:

CREATE OR REPLACE MODEL mlpatterns.neutral_3classes
OPTIONS(model_type='logistic_reg', input_label_cols=['health']) AS
Entwurfsmuster 9: Neutrale Klasse | 119
SELECT
IF(apgar_1min = 10, 'Gesunde',
IF(apgar_1min >= 8, 'Neutral', 'NeedsAttention')) AS Gesundheit,
Pluralität,
mutter_alter,
Schwangerschaft_Wochen,
je_geboren
FROM `bigquery-public-data.samples.natality`
WHERE apgar_1min <= 10
Dieses Modell erreicht eine Genauigkeit von 0,79 auf einem ausgeklammerten Evaluierungsdatensatz, viel
höher als die 0,56, die mit zwei Klassen erreicht wurde.

Kompromisse und Alternativen
Das Entwurfsmuster "Neutrale Klasse" ist eines, das man zu Beginn eines
maschinellen Lernproblems. Wenn wir die richtigen Daten sammeln, können wir eine Menge schwieriger
Probleme im weiteren Verlauf vermeiden. Hier sind einige Situationen, in denen eine neutrale Klasse hilfreich sein kann
hilfreich sein kann.

Wenn menschliche Experten sich nicht einig sind

Die neutrale Klasse ist hilfreich im Umgang mit Unstimmigkeiten zwischen menschlichen Experten.
Nehmen wir an, wir haben menschliche Etikettierer, denen wir die Patientengeschichte zeigen und sie fragen
welches Medikament sie verschreiben würden. In einigen Fällen könnte es ein klares Signal für Acetamino-
phen, ein klares Signal für Ibuprofen in anderen Fällen und eine große Anzahl von Fällen, in denen
Fällen, bei denen sich die menschlichen Kennzeichner nicht einig sind. Die neutrale Klasse bietet eine Möglichkeit, mit
mit solchen Fällen umzugehen.

Im Fall der menschlichen Beschriftung (anders als bei dem historischen Datensatz der tatsächlichen Arzt
Handlungen, bei denen ein Patient nur von einem Arzt gesehen wurde), wird jedes Muster von mehreren Experten beschriftet.
Experten beschriftet. Daher wissen wir von vornherein, in welchen Fällen die Menschen unterschiedlicher Meinung sind. Es
Es mag viel einfacher erscheinen, solche Fälle einfach zu verwerfen und einen binären Klassifikator zu
fier zu trainieren. Schließlich spielt es keine Rolle, was das Modell in den neutralen Fällen tut. Dies hat zwei
Probleme:

Falsches Vertrauen beeinträchtigt tendenziell die Akzeptanz des Modells durch menschliche Experten. A
Modell, das eine neutrale Bestimmung liefert, wird von den Experten oft eher akzeptiert
als ein Modell, das in Fällen, in denen der menschliche Experte die Alternative gewählt hätte
die Alternative gewählt hätte.
Wenn wir eine Kaskade von Modellen trainieren, dann werden die nachgelagerten Modelle
extrem empfindlich auf die neutralen Klassen reagieren. Wenn wir dieses Modell weiter verbessern,
könnten sich die nachgelagerten Modelle von Version zu Version drastisch verändern.
120 | Kapitel 3: Entwurfsmuster für die Problemdarstellung

Eine weitere Alternative besteht darin, die Übereinstimmung zwischen menschlichen Etikettierern als Gewichtung eines
Musters während des Trainings zu verwenden. Wenn sich also 5 Experten über eine Diagnose einig sind, erhält das Trainingsmuster
eine Gewichtung von 1, während bei einer Aufteilung der Experten von 3 zu 2 die Gewichtung des Musters
nur 0,6 sein. Auf diese Weise kann ein binärer Klassifikator trainiert werden, der jedoch eine Übergewichtung der
für die "sicheren" Fälle. Der Nachteil dieses Ansatzes ist, dass bei einer Wahrscheinlichkeit von
Wenn die vom Modell ausgegebene Wahrscheinlichkeit 0,5 beträgt, ist unklar, ob dies auf eine Situation zurückzuführen ist
unzureichende Trainingsdaten vorliegen, oder ob es sich um eine Situation handelt, in der menschliche
Experten nicht übereinstimmen. Die Verwendung einer neutralen Klasse zur Erfassung von Bereichen, in denen Uneinigkeit herrscht, ermöglicht es uns
die beiden Situationen voneinander zu unterscheiden.

Kundenzufriedenheit

Die Notwendigkeit einer neutralen Klasse ergibt sich auch bei Modellen, die versuchen, die Kundenzufriedenheit
Zufriedenheit vorhersagen. Wenn die Trainingsdaten aus Umfrageantworten bestehen, bei denen die Kunden
Kunden ihre Erfahrungen auf einer Skala von 1 bis 10 bewerten, könnte es hilfreich sein, die Bewertungen in
drei Kategorien einzuteilen: 1 bis 4 als schlecht, 8 bis 10 als gut und 5 bis 7 als neutral. Wenn wir stattdessen
einen binären Klassifikator mit einem Schwellenwert von 6 zu trainieren, wird das Modell zu viel Aufwand betreiben
zu viel Mühe aufwenden, um im Wesentlichen neutrale Antworten korrekt zu erhalten.

Zur Verbesserung der Einbettung

Angenommen, wir erstellen ein Preismodell für Flüge und möchten vorhersagen, ob
ob ein Kunde einen Flug zu einem bestimmten Preis kaufen wird. Zu diesem Zweck können wir uns die historischen
Transaktionen von Flugkäufen und abgebrochenen Warenkörben betrachten. Nehmen wir jedoch an
viele unserer Transaktionen beinhalten auch Käufe von Consolidators und Reisebüros -
Dies sind Personen, die Flugpreise vertraglich vereinbart haben, so dass die Flugpreise für sie nicht
tatsächlich dynamisch festgelegt. Mit anderen Worten: Sie zahlen nicht den aktuell angezeigten Preis.

Wir könnten alle nicht-dynamischen Käufe weglassen und das Modell nur auf Kunden trainieren, die
Kunden trainieren, die sich aufgrund des angezeigten Preises für oder gegen einen Kauf entschieden haben.
Bei einem solchen Modell würden jedoch alle Informationen über die Reiseziele fehlen, an denen der
Reiseveranstalter oder das Reisebüro zu verschiedenen Zeitpunkten interessiert waren - dies hat Auswirkungen auf
wie Flughäfen und Hotels eingebettet sind. Eine Möglichkeit, diese Informationen beizubehalten und
ohne die Preisentscheidung zu beeinflussen, ist die Verwendung einer neutralen Klasse für diese Transaktionen.

Reframing mit neutraler Klasse

Nehmen wir an, wir trainieren ein automatisches Handelssystem, das Geschäfte auf der Grundlage
ob es erwartet, dass ein Wertpapier im Preis steigt oder fällt. Aufgrund der Volatilität des Aktienmarktes
der Volatilität des Aktienmarktes und der Geschwindigkeit, mit der sich neue Informationen in den Aktienkursen niederschlagen, führt der Versuch, auf
auf kleine vorhergesagte Auf- und Abschwünge zu setzen, führt wahrscheinlich zu hohen Handelskosten und
geringe Gewinne führen.

Entwurfsmuster 9: Neutrale Klasse | 121
3 Siehe https://oreil.ly/kDndF für eine Einführung in Call- und Put-Optionen.
In solchen Fällen ist es hilfreich, sich zu überlegen, was das Endziel ist. Das Endziel des ML
Modells ist es nicht, vorherzusagen, ob eine Aktie steigen oder fallen wird. Wir werden nicht in der Lage sein zu kaufen
Wir können nicht jede Aktie kaufen, für die wir einen Anstieg vorhersagen, und wir können keine Aktien verkaufen, die wir nicht halten.

Die bessere Strategie könnte darin bestehen, Call-Optionen^3 für die 10 Aktien zu kaufen, die in den nächsten 6 Monaten höchstwahrscheinlich
die in den nächsten 6 Monaten um mehr als 5 % steigen werden, und Put-Optionen für Aktien zu kaufen, die
die in den nächsten 6 Monaten höchstwahrscheinlich um mehr als 5 % sinken werden.

Die Lösung besteht also darin, einen Trainingsdatensatz zu erstellen, der aus drei Klassen besteht:

Aktien, die um mehr als 5 % gestiegen sind - Call.
Aktien, die um mehr als 5 % gefallen sind - Put.
Die übrigen Aktien fallen in die Kategorie "neutral".
Anstatt ein Regressionsmodell darüber zu erstellen, wie stark die Aktien steigen werden, können wir nun
ein Klassifikationsmodell mit diesen drei Klassen trainieren und die zuverlässigsten Vorhersagen
Vorhersagen aus unserem Modell auswählen.

Entwurfsmuster 10: Rebalancing
Das Rebalancing-Entwurfsmuster bietet verschiedene Ansätze zur Behandlung von Datensätzen
die von Natur aus unausgewogen sind. Darunter verstehen wir Datensätze, bei denen ein Label den
den größten Teil des Datensatzes ausmacht, so dass weit weniger Beispiele für andere Bezeichnungen übrig bleiben.

Dieses Entwurfsmuster geht nicht auf Szenarien ein, in denen ein Datensatz keine Repräsentation
für eine bestimmte Population oder ein reales Umfeld fehlt. Solche Fälle lassen sich oft nur
durch zusätzliche Datenerfassung gelöst werden. Das Rebalancing-Entwurfsmuster befasst sich in erster Linie
wie Modelle mit Datensätzen erstellt werden können, für die es nur wenige Beispiele für eine bestimmte
Klasse oder Klassen gibt.

Problem
Modelle des maschinellen Lernens lernen am besten, wenn sie eine ähnliche Anzahl von
Beispielen für jede Klasse eines Datensatzes erhalten. Viele Probleme der realen Welt sind jedoch nicht so
nicht so gut ausbalanciert. Nehmen Sie zum Beispiel einen Anwendungsfall der Betrugserkennung, bei dem Sie ein Modell erstellen
ein Modell zur Erkennung betrügerischer Kreditkartentransaktionen. Betrügerische Transaktionen sind
sehr viel seltener als reguläre Transaktionen, und daher sind weniger Daten über Betrugsfälle
zur Verfügung, um ein Modell zu trainieren. Das Gleiche gilt für andere Probleme wie die Erkennung, ob
ob jemand mit einem Kredit in Verzug gerät, defekte Produkte identifiziert, das Vorhandensein
einer Krankheit anhand medizinischer Bilder, das Filtern von Spam-E-Mails, das Markieren von Fehlerprotokollen
in einer Software-Anwendung und vieles mehr.

122 | Kapitel 3: Entwurfsmuster für die Problemdarstellung

Unausgewogene Datensätze gelten für viele Arten von Modellen, einschließlich der binären Klassifizierung,
Multiklassenklassifikation, Multilabel-Klassifikation und Regression. In Regressionsfällen,
Regression beziehen sich unausgewogene Datensätze auf Daten mit Ausreißerwerten, die entweder viel höher oder
niedriger sind als der Median in Ihrem Datensatz.

Ein häufiger Fallstrick beim Training von Modellen mit unausgewogenen Beschriftungsklassen ist das Verlassen auf falsche
führenden Genauigkeitswerten für die Modellbewertung. Wenn wir ein Betrugserkennungsmodell trainieren und
nur 5 % unseres Datensatzes betrügerische Transaktionen enthält, ist es wahrscheinlich, dass unser Modell
auf 95 % Genauigkeit trainieren, ohne dass Änderungen am Datensatz oder der zugrunde liegenden Modellarchitektur
Architektur. Diese 95 % Genauigkeit sind zwar technisch korrekt, aber es besteht eine gute Chance, dass das Modell
die Wahrscheinlichkeit groß, dass das Modell für jedes Beispiel die Mehrheitsklasse (in diesem Fall Nicht-Betrug) errät.
Beispiel. Es lernt also nichts darüber, wie man die Minderheitsklasse von anderen Beispielen in unserem Datenbestand unterscheidet.
Klasse von anderen Beispielen in unserem Datensatz unterscheiden kann.

Um sich nicht zu sehr auf diesen irreführenden Genauigkeitswert zu verlassen, lohnt sich ein Blick auf die
Konfusionsmatrix des Modells anzusehen, um die Genauigkeit für jede Klasse zu ermitteln. Die Konfusionsmatrix für
ein schlechtes Modell, das auf einem unausgewogenen Datensatz trainiert wurde, sieht oft so aus
wie in Abbildung 3-18.

Abbildung 3-18. Konfusionsmatrix für ein Modell, das auf einem unausgewogenen Datensatz ohne
Datensatz- oder Modellanpassungen.

In diesem Beispiel errät das Modell die Mehrheitsklasse in 95 % der Fälle richtig, aber
die Minderheitsklasse nur in 12 % der Fälle richtig. Normalerweise hat die Konfusionsmatrix
Matrix für ein leistungsstarkes Modell einen Prozentsatz von fast 100 entlang der Diagonale.

Lösung
Da die Genauigkeit bei unausgewogenen Datensätzen irreführend sein kann, ist es zunächst wichtig, eine
bei der Erstellung unseres Modells eine geeignete Bewertungsmetrik zu wählen. Dann gibt es
verschiedene Techniken für den Umgang mit inhärent unausgewogenen Datensätzen auf
sowohl auf der Ebene des Datensatzes als auch des Modells. Downsampling verändert die Ausgewogenheit unseres zugrunde liegenden
zugrundeliegenden Datensatzes, während die Gewichtung ändert, wie unser Modell bestimmte Klassen behandelt. Upsam-
pling dupliziert Beispiele aus unserer Minderheitenklasse und beinhaltet oft die Anwendung von

Entwurfsmuster 10: Neugewichtung | 123
Augmentierungen, um zusätzliche Stichproben zu erzeugen. Wir werden uns auch mit Ansätzen zur
das Problem neu zu formulieren: Umwandlung in eine Regressionsaufgabe, Analyse der Fehlerwerte unseres
Fehlerwerte unseres Modells für jedes Beispiel, oder Clustering.

Auswahl einer Bewertungsmetrik

Bei unausgewogenen Datensätzen, wie in unserem Beispiel zur Betrugserkennung, ist es am besten
Metriken wie Precision, Recall oder F-Measure, um ein vollständiges Bild von der Leistung unseres
Modell leistet. Die Präzision misst den Prozentsatz der positiven Klassifizierungen, die
die von allen positiven Vorhersagen des Modells richtig waren. Umgekehrt misst die Rückrufquote
den Anteil der tatsächlich positiven Beispiele, die vom Modell korrekt identifiziert wurden.
das Modell korrekt erkannt wurden. Der größte Unterschied zwischen diesen beiden Metriken ist der Nenner, der
der zu ihrer Berechnung verwendet wird. Bei der Genauigkeit ist der Nenner die Gesamtzahl der positiven Klassen
Vorhersagen durch unser Modell. Bei der Wiedererkennung handelt es sich um die Anzahl der tatsächlich positiven Klassen
Beispiele, die in unserem Datensatz vorhanden sind.

Ein perfektes Modell hätte sowohl eine Präzision als auch einen Recall von 1,0, aber in der Praxis sind diese
sind diese beiden Maße jedoch oft nicht miteinander vereinbar. Das F-Maß ist eine Metrik, die zwischen
von 0 bis 1 reicht und sowohl die Genauigkeit als auch die Wiederauffindbarkeit berücksichtigt. Es wird wie folgt berechnet:

2 * (Genauigkeit * Rückruf / (Genauigkeit + Rückruf))
Kehren wir zum Anwendungsfall der Betrugserkennung zurück, um zu sehen, wie sich jede dieser Metriken in der Praxis
in der Praxis auswirkt. Für dieses Beispiel nehmen wir an, dass unser Testsatz insgesamt 1.000 Beispiele enthält,
50 davon sollen als betrügerische Transaktionen gekennzeichnet werden. Für diese Beispiele sagt unser
Modell 930/950 nicht betrügerische Beispiele korrekt und 15/50 betrügerische
Beispiele richtig. Diese Ergebnisse werden in Abbildung 3-19 veranschaulicht.

Abbildung 3-19. Beispielhafte Vorhersagen für ein Betrugserkennungsmodell.

In diesem Fall liegt die Präzision unseres Modells bei 15/35 (42 %), die Wiedererkennung bei 15/50 (30 %) und das F-
liegt bei 35 %. Diese Werte geben die Unfähigkeit unseres Modells, betrügerische Transaktionen
betrügerische Transaktionen korrekt zu identifizieren, verglichen mit der Genauigkeit, die bei 945/1000
(94.5%). Daher sind für Modelle, die auf unausgewogenen Datensätzen trainiert wurden, andere Metriken als
Genauigkeit bevorzugt werden. Tatsächlich kann die Genauigkeit sogar sinken, wenn die Optimierung für

124 | Kapitel 3: Entwurfsmuster für die Problemdarstellung

4 Der Datensatz wurde auf der Grundlage der in diesem Papier vorgeschlagenen PaySim-Forschung erstellt: EdgarLopez-Rojas , Ahmad
Elmir, und Stefan Axelsson, "PaySim: A financial mobile money simulator for fraud detection," 28th Euro-
pean Modeling and Simulation Symposium, EMSS, Larnaca, Cyprus (2016): 249-255.
Aber das ist in Ordnung, da Präzision, Recall und F-Score in diesem Fall ein besseres Indiz für die Modellleistung sind.
Modellleistung in diesem Fall besser geeignet sind.

Beachten Sie, dass wir bei der Bewertung von Modellen, die auf unausgewogenen Datensätzen
unausgewogene Daten für die Berechnung der Erfolgskennzahlen verwenden müssen. Das bedeutet, dass wir, egal wie wir
wie wir unseren Datensatz für das Training gemäß den unten beschriebenen Lösungen modifizieren, sollten wir den
unseren Testdatensatz so belassen, wie er ist, so dass er eine genaue Darstellung des Originaldatensatzes bietet.
Mit anderen Worten: Unser Testdatensatz sollte ungefähr die gleiche Klassenverteilung aufweisen wie der ursprüngliche
Datensatz haben. Für das obige Beispiel wären das 5% Betrug/95% Nicht-Betrug.

Wenn wir nach einer Metrik suchen, die die Leistung des Modells über alle
Schwellenwerte erfasst, ist die durchschnittliche Abrufpräzision eine informativere Metrik als die Fläche unter der
ROC-Kurve (AUC) für die Modellbewertung. Dies liegt daran, dass die durchschnittliche Precision-Recall
mehr Gewicht darauf legt, wie viele Vorhersagen das Modell von der Gesamtzahl der
Anzahl, die es der positiven Klasse zuordnete. Dies verleiht der positiven Klasse mehr Gewicht,
was für unausgewogene Datensätze wichtig ist. Der AUC hingegen behandelt beide Klassen
Klassen gleich und reagiert weniger empfindlich auf Modellverbesserungen, was in Situationen mit unausgewogenen Daten nicht optimal ist.
Situationen mit unausgewogenen Daten nicht optimal ist.

Downsampling

Downsampling ist eine Lösung für den Umgang mit unausgewogenen Datensätzen, indem der
zugrunde liegenden Datensatzes und nicht des Modells. Beim Downsampling verringern wir die
Anzahl der Beispiele aus der Mehrheitsklasse, die während des Modelltrainings verwendet werden. Um zu sehen, wie
um zu sehen, wie das funktioniert, schauen wir uns den synthetischen Betrugserkennungsdatensatz auf Kaggle an.^4 Jedes
Beispiel in diesem Datensatz enthält verschiedene Informationen über die Transaktion, darunter
die Art der Transaktion, den Betrag der Transaktion und den Kontostand vor und nach
vor und nach der Transaktion. Der Datensatz enthält 6,3 Millionen Beispiele
Beispielen, von denen nur 8.000 betrügerische Transaktionen sind. Das sind gerade einmal 0,1 % des gesamten
Datensatzes.

Ein großer Datensatz kann zwar oft die Fähigkeit eines Modells verbessern, Muster zu erkennen, ist aber weniger hilfreich
weniger hilfreich, wenn die Daten deutlich unausgewogen sind. Wenn wir ein Modell auf diesem gesamten
Datensatz (6,3 Mio. Zeilen) ohne jegliche Änderungen trainieren, ist die Wahrscheinlichkeit groß, dass wir eine
Genauigkeit von 99,9 %, da das Modell jedes Mal zufällig die nicht betrügerische Klasse errät
jedes Mal. Wir können dieses Problem lösen, indem wir einen großen Teil der Mehrheitsklasse aus dem
aus dem Datensatz entfernen.

Wir nehmen alle 8.000 betrügerischen Beispiele und legen sie beiseite, um sie beim Training des Modells zu verwenden.
das Modell zu trainieren. Dann nehmen wir eine kleine, zufällige Stichprobe der nicht betrügerischen Transaktionen.

Entwurfsmuster 10: Neugewichtung | 125
tionen. Dann kombinieren wir sie mit unseren 8.000 betrügerischen Beispielen, mischen die Daten neu und
und verwenden diesen neuen, kleineren Datensatz zum Trainieren eines Modells. So könnten wir dies implementieren
mit Pandas:

Daten = pd.read_csv('fraud_data.csv')
# Aufteilung in separate Datenrahmen für Betrug/Nicht-Betrug
fraud = data[data['isFraud'] == 1]
nicht_Betrug = data[data['isBetrug'] == 0]
# Nehmen Sie eine Zufallsstichprobe von Nicht-Betrugszeilen
not_fraud_sample = not_fraud.sample(random_state=2, frac=.005)
# Wieder zusammensetzen und mischen
df = pd.concat([not_fraud_sample,fraud])
df = shuffle(df, random_state=2)
Auf diese Weise würde unser Datensatz 25 % betrügerische Transaktionen enthalten, was viel ausgewogener ist
ausgeglichener als der ursprüngliche Datensatz mit nur 0,1 % in der Minderheitenklasse. Es lohnt sich
Es lohnt sich, mit dem genauen Gleichgewicht zu experimentieren, das beim Downsampling verwendet wird. Hier haben wir eine
Aufteilung von 25/75 verwendet, aber bei anderen Problemen ist möglicherweise eine 50/50-Aufteilung erforderlich, um eine
anständige Genauigkeit zu erreichen.

Downsampling wird in der Regel mit dem Ensemble-Muster kombiniert, wobei folgende Schritte zu beachten sind:

Reduzieren Sie die Stichprobe der Mehrheitsklasse und verwenden Sie alle Instanzen der Minderheitsklasse.
Trainieren Sie ein Modell und fügen Sie es dem Ensemble hinzu.
Wiederholen Sie.
Während der Inferenz wird der Medianwert der Ensemble-Modelle ermittelt.

Wir haben hier ein Klassifizierungsbeispiel besprochen, aber Downsampling kann auch auf
Regressionsmodelle angewendet werden, bei denen wir einen numerischen Wert vorhersagen. In diesem Fall ist die Entnahme einer
Stichprobe aus Stichproben der Mehrheitsklasse nuancierter, da die Mehrheitsklasse
"Klasse" in unseren Daten eine Reihe von Werten und nicht nur eine einzige Bezeichnung umfasst.

Gewichtete Klassen

Ein anderer Ansatz für den Umgang mit unausgewogenen Datensätzen besteht darin, die Gewichtung zu ändern, die unser Modell
für Beispiele aus jeder Klasse zu ändern. Beachten Sie, dass dies eine andere Verwendung des Begriffs
"Gewicht" als die Gewichte (oder Parameter), die unser Modell während des Trainings lernt,
die Sie nicht manuell einstellen können. Durch die Gewichtung der Klassen weisen wir unser Modell an, bestimmte
bestimmte Beschriftungsklassen während des Trainings mit größerer Bedeutung behandeln. Wir möchten, dass unser Modell
Beispielen aus der Minderheitenklasse mehr Gewicht zuweist. Wie genau die Bedeutung, die Ihr Modell
Bedeutung Ihr Modell bestimmten Beispielen beimessen soll, bleibt Ihnen überlassen und ist ein Parameter, mit dem Sie
Parameter, mit dem Sie experimentieren können.

126 | Kapitel 3: Entwurfsmuster für die Problemdarstellung

In Keras können wir einen class_weights-Parameter an unser Modell übergeben, wenn wir es mit
fit(). Der Parameter class_weights ist ein dict, das jeder Klasse die Gewichtung zuordnet, die
das Keras den Beispielen dieser Klasse zuweisen sollte. Aber wie sollten wir die
Gewichte für jede Klasse genau bestimmen? Die Werte für die Klassengewichtung sollten sich auf das Gleichgewicht der
jeder Klasse in unserem Datensatz beziehen. Wenn zum Beispiel die Klasse der Minderheiten nur 0,1 % des Datensatzes ausmacht
des Datensatzes ausmacht, ist eine vernünftige Schlussfolgerung, dass unser Modell die Beispiele aus
dieser Klasse mit 1000x mehr Gewicht behandeln sollte als die Mehrheitsklasse. In der Praxis ist es üblich, dass
diesen Gewichtungswert für jede Klasse durch 2 zu teilen, so dass die durchschnittliche Gewichtung eines Beispiels bei
1.0. Bei einem Datensatz mit 0,1 % Werten, die die Minderheitenklasse repräsentieren, können wir also
können wir die Klassengewichte mit dem folgenden Code berechnen:

num_minority_examples = 1
num_majority_examples = 999
total_examples = num_minority_examples + num_majority_examples
Minderheitsklassengewichtung = 1/(Anzahl_Minderheitsproben/Gesamtproben)/2
majority_class_weight = 1/(num_majority_examples/total_examples)/2
# Übergeben Sie die Gewichte in einem Diktat an Keras.
# Der Schlüssel ist der Index der jeweiligen Klasse.
keras_class_weights = {0: majority_class_weight, 1: minority_class_weight}
Diese Gewichte würden wir dann während des Trainings an unser Modell weitergeben:

model.fit(
train_data,
train_labels,
class_weight=keras_class_weights
)
In BigQuery ML können wir AUTO_CLASS_WEIGHTS = True im OPTIONS-Block einstellen, wenn wir
bei der Erstellung unseres Modells setzen, um verschiedene Klassen auf der Grundlage der Häufigkeit ihres
Vorkommens in den Trainingsdaten gewichtet werden.

Es kann zwar hilfreich sein, bei der Festlegung der Klassengewichte einer Heuristik der Klassenbalance zu folgen,
kann die geschäftliche Anwendung eines Modells auch die Klassengewichtung diktieren, die wir
zuweisen. Nehmen wir zum Beispiel an, wir haben ein Modell, das Bilder von defekten Produkten klassifiziert.
Produkte. Wenn die Kosten für den Versand eines defekten Produkts 10-mal so hoch sind wie die Kosten für die falsche Klassifizierung eines normalen Produkts, dann
eines normalen Produkts, würden wir 10 als Gewicht für unsere Minderheitenklasse wählen.

Entwurfsmuster 10: Rebalancing | 127
Vorspannung der Ausgabeschicht
In Verbindung mit der Zuweisung von Klassengewichten ist es auch hilfreich, die Ausgabeschicht des Modells mit einer Vorspannung zu initialisieren
Ausgabeschicht des Modells mit einem Bias zu initialisieren, um das Ungleichgewicht der Datensätze zu berücksichtigen. Warum sollten wir
den anfänglichen Bias für unsere Ausgabeschicht manuell festlegen? Wenn wir unausgewogene Datensätze haben,
hilft das Festlegen der Ausgangsverzerrung unserem Modell, schneller zu konvergieren. Dies liegt daran, dass der Bias der
der letzten (Vorhersage-)Schicht eines trainierten Modells im Durchschnitt den Logarithmus des
Verhältnis von Minderheits- zu Mehrheitsbeispielen im Datensatz ausgibt. Durch die Einstellung der Vorspannung wird das Modell
bereits den "richtigen" Wert, ohne dass es diesen durch Gradientenabstieg ermitteln muss.
Abstieg.
Standardmäßig verwendet Keras einen Bias von Null. Dies entspricht dem Bias, den wir verwenden würden
für einen perfekt ausgeglichenen Datensatz, bei dem log(1/1) = 0 ist. Um den korrekten Bias zu berechnen
zu berechnen und dabei die Ausgewogenheit unseres Datensatzes zu berücksichtigen, verwenden Sie:
bias = log(num_minority_examples / num_majority_examples)
Upsampling

Eine weitere gängige Technik zur Bearbeitung unausgewogener Datensätze ist das Upsampling. Mit
Upsampling überrepräsentieren wir unsere Minderheitenklasse, indem wir sowohl Beispiele der Minderheitenklasse replizieren
überrepräsentiert, indem sowohl Beispiele aus der Minderheitenklasse repliziert als auch zusätzliche, synthetische Beispiele erzeugt werden. Dies geschieht oft in Kombination
Kombination mit Downsampling der Mehrheitsklasse durchgeführt. Dieser Ansatz - eine Kombination aus Downsam-
pling und Upsampling - wurde im Jahr 2002 vorgeschlagen und als Synthetic Minority
Over-sampling Technique (SMOTE) bezeichnet. SMOTE stellt einen Algorithmus zur Verfügung, der
synthetische Beispiele konstruiert, indem er den Merkmalsraum der Beispiele der Minderheitenklasse im Datensatz analysiert
Minderheitenklasse im Datensatz analysiert und dann ähnliche Beispiele innerhalb dieses Merkmalsraums mit einem
nächstgelegenen Nachbarn. Je nachdem, wie viele ähnliche Datenpunkte wir
Datenpunkte auf einmal betrachtet werden sollen (auch als Anzahl der nächsten Nachbarn bezeichnet), erzeugt der SMOTE
Ansatz zufällig ein neues Beispiel für eine Minderheitenklasse zwischen diesen Punkten.

Schauen wir uns den Pima-Indianer-Diabetes-Datensatz an, um zu sehen, wie dies auf hohem Niveau funktioniert.
34 % dieses Datensatzes enthalten Beispiele von Patienten mit Diabetes, also betrachten wir
unsere Minderheitenklasse. Tabelle 3-3 zeigt eine Teilmenge von Spalten für zwei Beispiele der Minderheitenklasse
Beispiele.

Tabelle 3-3. Eine Teilmenge von Merkmalen für zwei Trainingsbeispiele aus der Minderheitenklasse (hat
Diabetes) im Pima Indian Diabetes-Datensatz

Glukose Blutdruck Hautdicke BMI
148 72 35 33.6
183 64 0 23.3
128 | Kapitel 3: Entwurfsmuster für die Problemdarstellung

Ein neues, synthetisches Beispiel auf der Grundlage dieser beiden tatsächlichen Beispiele aus dem Datensatz könnte
wie Tabelle 3-4 aussehen, wobei der Mittelwert zwischen den einzelnen Spaltenwerten berechnet wird.

Tabelle 3-4. Ein synthetisches Beispiel, das aus den beiden Minderheitentrainingsbeispielen mit Hilfe des
dem SMOTE-Ansatz

Glukose Blutdruck Hautdicke BMI
165.5 68 17.5 28.4
Die SMOTE-Technik bezieht sich in erster Linie auf Tabellendaten, aber eine ähnliche Logik kann auch
auf Bilddatensätze angewendet werden. Wenn wir zum Beispiel ein Modell zur Unterscheidung zwischen
zwischen bengalischen und siamesischen Katzen und nur 10 % unseres Datensatzes enthält Bilder von
Bengalen enthält, können wir zusätzliche Variationen der bengalischen Katzen in unserem Datensatz erzeugen
durch Bildvergrößerung mit der Keras ImageDataGenerator-Klasse. Mit ein paar
Parametern erzeugt diese Klasse mehrere Variationen desselben Bildes durch Drehen,
Zuschneiden, Anpassen der Helligkeit und mehr.

Kompromisse und Alternativen
Es gibt einige andere alternative Lösungen für die Erstellung von Modellen mit inhärent unausgewogenen
unausgewogenen Datensätzen zu erstellen, einschließlich der Neudefinition des Problems und des Umgangs mit Anomalien
Erkennung. Wir werden auch einige wichtige Überlegungen für unausgewogene Daten
Datensätze: die Gesamtgröße des Datensatzes, die optimalen Modellarchitekturen für verschiedene Problemtypen,
und die Erklärung der Vorhersage von Minderheitenklassen.

Reframing und Kaskade

Ein weiterer Ansatz für den Umgang mit unausgewogenen Datensätzen ist die Umstrukturierung des Problems. Erstens,
könnte man zunächst erwägen, das Problem von Klassifikation auf Regression oder umgekehrt umzustellen.
Regression oder umgekehrt umzuwandeln, indem man die im Abschnitt Reframing-Designmuster beschriebenen Techniken anwendet und
Training einer Kaskade von Modellen. Nehmen wir zum Beispiel an, wir haben ein Regressionsproblem
Regressionsproblem, bei dem die Mehrheit der Trainingsdaten in einen bestimmten Bereich fällt, mit einigen Ausreißern.
Ausreißer. Unter der Annahme, dass uns die Vorhersage von Ausreißerwerten wichtig ist, könnten wir dies in ein Klassifizierungsproblem umwandeln, indem wir die Mehrheit der Daten in Gruppen einteilen.
Klassifizierungsproblem umwandeln, indem wir die Mehrheit der Daten in einen Bereich und die
Ausreißer in einen anderen.

Stellen Sie sich vor, wir erstellen ein Modell zur Vorhersage des Babygewichts unter Verwendung des BigQuery natality
Datensatz. Mit Pandas können wir ein Histogramm einer Stichprobe der Babygewichtsdaten erstellen
um die Gewichtsverteilung zu sehen:

%%bigquerydf
SELECT
gewicht_pfund
FROM
`bigquery- public - data .samples.natality`
LIMIT 10000
Entwurfsmuster 10: Neugewichtung | 129
df.plot(kind='hist')
Abbildung 3-20 zeigt das resultierende Histogramm.

Abbildung 3-20. Ein Histogramm, das die Verteilung des Babygewichts für 10.000 Beispiele
im BigQuery-Natalitätsdatensatz.

Wenn wir die Anzahl der Babys mit einem Gewicht von 3 lbs im gesamten Datensatz zählen, sind es
etwa 96.000 (0,06 % der Daten). Babys, die 12 lbs wiegen, machen nur 0,05 %
des Datensatzes aus. Um eine gute Regressionsleistung über den gesamten Bereich zu erhalten, können wir
Downsampling mit den Entwurfsmustern Reframing und Cascade kombinieren. Zunächst werden wir
die Daten in drei Bereiche unterteilt: "untergewichtig", "durchschnittlich" und "übergewichtig". Wir können
können wir dies mit der folgenden Abfrage tun:

SELECT
CASE
WHEN gewicht_pfund < 5,5 THEN "untergewichtig"
WHEN gewicht_pfund > 9,5 THEN "übergewichtig"
ELSE
"durchschnittlich"
END
AS gewicht,
COUNT (*) AS num_examples,
round( count (*) / sum ( count (*)) over(), 4) as percent_of_dataset
FROM
`bigquery- public - data .samples.natality`
GRUPPE BY
1
Tabelle 3-5 zeigt die Ergebnisse.

130 | Kapitel 3: Entwurfsmuster für die Problemdarstellung

Tabelle 3-5. Der prozentuale Anteil der einzelnen Gewichtsklassen im Datensatz zur Geburtenhäufigkeit

Gewicht num_examples percent_of_dataset
Durchschnittlich 123781044 0.8981
Untergewichtig 9649724 0,07
Übergewichtig 4395995 0,0319
Zu Demonstrationszwecken nehmen wir 100.000 Beispiele aus jeder Klasse, um ein Modell zu trainieren auf
einem aktualisierten, ausgewogenen Datensatz zu trainieren:

SELECT
is_male,
trächtigkeitswochen,
mutter_alter,
gewicht_pfund,
Gewicht
FROM (
SELECT
*,
ROW_NUMBER() OVER (PARTITION BY weight ORDER BY RAND()) AS row_num
FROM (
SELECT
is_male,
trächtigkeit_wochen,
mutter_alter,
gewicht_pfund,
CASE
WHEN gewicht_pfund < 5,5 THEN "untergewichtig"
WHEN gewicht_pfund > 9,5 THEN "übergewichtig"
ELSE
"durchschnittlich"
END
AS gewicht,
FROM
`bigquery- public - data .samples.natality`
LIMIT
4000000 ) )
WHERE
row_num < 100000
Wir können die Ergebnisse dieser Abfrage in einer Tabelle speichern, und mit einem ausgewogeneren Datensatz können wir
können wir nun ein Klassifizierungsmodell trainieren, um Babys als "untergewichtig", "durchschnittlich" oder
"übergewichtig" zu bezeichnen:

MODELL ERSTELLEN ODER ERSETZEN
projekt.dataset.baby_gewicht_klassifikation` OPTIONS (model_type='logistic_reg',
input_label_cols=['Gewicht']) AS
SELECT
is_male,
gewicht_pfund,
mutter_alter,
Entwurfsmuster 10: Neugewichtung | 131
trächtigkeit_wochen,
Gewicht
FROM
projekt.datensatz.baby_gewicht`
Ein anderer Ansatz ist die Verwendung des Kaskadenmusters, bei dem drei separate Regressionsmodelle
Modelle für jede Klasse. Dann können wir unsere Multidesign-Musterlösung verwenden, indem wir
Klassifizierungsmodell ein Beispiel übergeben und das Ergebnis dieser Klassifizierung verwenden
um zu entscheiden, an welches Regressionsmodell das Beispiel für die numerische Vorhersage gesendet werden soll.

Erkennung von Anomalien

Es gibt zwei Ansätze zur Handhabung von Regressionsmodellen für unausgewogene Datensätze:

Verwenden Sie den Fehler des Modells bei einer Vorhersage als Signal.
Clustern Sie die eingehenden Daten und vergleichen Sie den Abstand jedes neuen Datenpunktes zu bestehenden Clustern.
bestehenden Clustern.
Um jede Lösung besser zu verstehen, trainieren wir ein Modell auf Daten, die von einem
von einem Sensor gesammelten Daten trainieren, um die Temperatur in der Zukunft vorherzusagen. In diesem Fall muss die Modell
Ausgabe ein numerischer Wert sein.

Bei der ersten Methode - der Verwendung von Fehlern als Signal - würden wir nach dem Training eines Modells
den vorhergesagten Wert des Modells mit dem tatsächlichen Wert zum aktuellen Zeitpunkt
Zeitpunkt. Wenn es einen signifikanten Unterschied zwischen dem vorhergesagten und dem tatsächlichen
Wert, könnten wir den eingehenden Datenpunkt als Anomalie kennzeichnen. Dies erfordert natürlich ein
Modell, das mit guter Genauigkeit auf genügend historischen Daten trainiert wurde, um sich auf seine Qualität für
Vorhersagen zu vertrauen. Der größte Nachteil dieses Ansatzes ist, dass er voraussetzt, dass
neue Daten zur Verfügung stehen müssen, damit wir die eingehenden Daten mit der
Vorhersage des Modells vergleichen können. Daher eignet er sich am besten für Probleme mit Streaming- oder
Zeitseriendaten.

Beim zweiten Ansatz - dem Clustering von Daten - beginnen wir mit der Erstellung eines Modells mit einem Clustering-Algorithmus.
tering-Algorithmus, einer Modellierungstechnik, die unsere Daten in Clustern organisiert. Cluster-
Clustering ist eine unüberwachte Lernmethode, d. h. sie sucht nach Mustern im Datensatz
ohne Kenntnis der wahren Bezeichnungen. Ein gängiger Clustering-Algorithmus ist k-
means, den wir mit BigQuery ML implementieren können. Im Folgenden wird gezeigt, wie man
ein k-means-Modell auf dem BigQuery-Nativitätsdatensatz mit drei Merkmalen trainiert:

MODELL ERSTELLEN ODER ERSETZEN
projektname.datensatzname.baby_gewicht` OPTIONS (model_type='kmeans',
num_clusters=4) AS
SELECT
gewicht_pfund,
mutter_alter,
trächtigkeit_wochen
FROM
132 | Kapitel 3: Entwurfsmuster für die Problemdarstellung

`bigquery- public - data .samples.natality`
LIMIT 10000
Das resultierende Modell teilt unsere Daten in vier Gruppen ein. Sobald das Modell erstellt wurde
können wir dann Vorhersagen für neue Daten erstellen und den Abstand der Vorhersage
Abstand zu den vorhandenen Clustern. Wenn der Abstand groß ist, können wir den Datenpunkt als eine
Anomalie kennzeichnen. Um eine Clustervorhersage für unser Modell zu erstellen, können wir die folgende
Abfrage ausführen und ihr ein erfundenes Durchschnittsbeispiel aus dem Datensatz übergeben:

SELECT
*
FROM
ML.PREDICT (MODELL `Projekt-Name.Datensatz-Name.Baby_Gewicht`,
(
SELECT
7.0 as gewicht_pfund,
28 als mutter_alter,
40 als trächtigkeitswochen
)
)
Die Abfrageergebnisse in Tabelle 3-6 zeigen uns den Abstand zwischen diesem Datenpunkt und den
Modell generierten Clustern, den so genannten Zentroiden.

Tabelle 3-6. Der Abstand zwischen unserem durchschnittlich gewichteten Beispieldatenpunkt und jedem der
Clustern, die von unserem k-means-Modell erzeugt wurden

CENTROID_ID NEAREST_CENTROIDS_DISTANCE.CENTROID_ID NEAREST_CENTROIDS_DISTANCE.DISTANCE
4 4 0.29998627812137374
1 1.2370167418282159
2 1.376651161584178
3 1.6853517159990536
Dieses Beispiel passt eindeutig in den Schwerpunkt 4, wie der geringe Abstand (.29) zeigt.

Wir können dies mit den Ergebnissen vergleichen, die wir erhalten, wenn wir ein ausreißerisches, untergewichtetes Beispiel
an das Modell senden, wie in Tabelle 3-7 gezeigt.

Tabelle 3-7. Der Abstand zwischen unserem untergewichtigen Beispieldatenpunkt und jedem der
Clustern, die von unserem k-means-Modell erzeugt wurden

CENTROID_ID NEAREST_CENTROIDS_DISTANCE.CENTROID_ID NEAREST_CENTROIDS_DISTANCE.DISTANCE
3 3 3.061985789261998
4 3.3124603501734966
2 4.330205096751425
1 4.658614918595627
Entwurfsmuster 10: Neugewichtung | 133
Hier ist der Abstand zwischen diesem Beispiel und den einzelnen Schwerpunkten recht groß. Wir könnten
können wir aus diesen hohen Abstandswerten schließen, dass es sich bei diesem Datenpunkt um eine
Anomalie sein könnte. Dieser unüberwachte Clustering-Ansatz ist besonders nützlich, wenn wir die
Beschriftungen für unsere Daten im Voraus nicht kennen. Sobald wir Cluster-Vorhersagen für genügend Beispiele generiert haben
Beispielen generiert haben, können wir dann ein überwachtes Lernmodell erstellen, das die vorhergesagten
vorhergesagten Clustern als Labels verwenden.

Anzahl der verfügbaren Beispiele für Minderheitenklassen

Während die Minderheitenklasse in unserem ersten Beispiel zur Betrugserkennung nur 0,1 % der Daten ausmachte
der Daten ausmachte, war der Datensatz so groß, dass wir immer noch 8.000 betrügerische Datenpunkte hatten
arbeiten. Bei Datensätzen mit noch weniger Beispielen der Minderheitenklasse kann das Downsam-
Bei Datensätzen mit noch weniger Beispielen der Minderheitenklasse kann das Downsampling dazu führen, dass der resultierende Datensatz zu klein wird, damit ein Modell daraus lernen kann. Es gibt keine
Es gibt keine feste Regel, um zu bestimmen, wie viele Beispiele zu wenig sind, um Downsam- pling zu verwenden.
pling zu verwenden, da dies weitgehend von unserem Problem und der Modellarchitektur abhängt. Eine allgemeine Regel
Faustregel lautet: Wenn Sie nur Hunderte von Beispielen der Minderheitenklasse haben, sollten Sie
eine andere Lösung als Downsampling für die Behandlung des Ungleichgewichts im Datensatz
Ungleichgewicht.

Es ist auch erwähnenswert, dass der natürliche Effekt des Entfernens einer Teilmenge unserer Mehrheitsklasse
der Verlust einiger in diesen Beispielen gespeicherten Informationen ist. Dies könnte die Fähigkeit unseres
die Fähigkeit unseres Modells, die Mehrheitsklasse zu identifizieren, aber oft überwiegen die Vorteile des Downsampling
immer noch überwiegen.

Kombination verschiedener Techniken

Die oben beschriebenen Techniken des Downsamplings und der Klassengewichtung können kombiniert werden, um
optimale Ergebnisse. Zu diesem Zweck beginnen wir mit dem Downsampling unserer Daten, bis wir ein Gleichgewicht
Gleichgewicht gefunden haben, das für unseren Anwendungsfall geeignet ist. Dann verwenden wir auf der Grundlage der Label-Verhältnisse für den rebalancierten
die im Abschnitt über gewichtete Klassen beschriebene Methode, um unserem Modell neue Gewichte zuzuweisen.
an unser Modell. Die Kombination dieser Ansätze kann besonders nützlich sein, wenn wir ein
Anomalie-Erkennungsproblem haben und die Vorhersagen für unsere Minderheitenklasse am wichtigsten sind.
Wenn wir zum Beispiel ein Modell zur Erkennung von Betrug erstellen, sind wir wahrscheinlich viel mehr an den Transaktionen interessiert, die unser Modell vorhersagt.
die Transaktionen, die unser Modell als "Betrug" kennzeichnet, als die, die es als "Nicht-Betrug" kennzeichnet.
als "Nicht-Betrug". Darüber hinaus ist, wie von SMOTE erwähnt, der Ansatz der Generierung
synthetische Beispiele aus der Minderheitenklasse zu generieren, oft mit dem Entfernen einer Ran-
einer Stichprobe von Beispielen aus der Minderheitenklasse kombiniert.

Downsampling wird auch häufig mit dem Entwurfsmuster Ensemble kombiniert. Bei diesem
Ansatz wird eine Zufallsstichprobe der Mehrheitsklasse nicht vollständig entfernt, sondern es werden
verschiedene Teilmengen davon, um mehrere Modelle zu trainieren und diese Modelle dann zu einem Ensemble zusammenzufügen. Um
Beispiel: Wir haben einen Datensatz mit 100 Beispielen der Minderheitenklasse und 1.000
Beispielen der Mehrheit. Anstatt 900 Beispiele aus unserer Mehrheitsklasse zu entfernen, um
den Datensatz perfekt auszugleichen, würden wir die Beispiele der Mehrheitsklasse nach dem Zufallsprinzip in 10
Gruppen mit jeweils 100 Beispielen. Dann würden wir 10 Klassifikatoren trainieren, jeder mit denselben 100

134 | Kapitel 3: Entwurfsmuster für die Problemdarstellung

Beispiele aus unserer Minderheitenklasse und 100 verschiedene, zufällig ausgewählte Werte aus
unserer Mehrheitsklasse. Die in Abbildung 3-11 dargestellte Bagging-Technik würde sich für diesen Ansatz
für diesen Ansatz.

Zusätzlich zur Kombination dieser datenzentrierten Ansätze können wir auch die Dresch-
hold für unseren Klassifikator anpassen, um je nach Anwendungsfall die Präzision oder den Rückruf zu optimieren. Wenn
wir mehr Wert darauf legen, dass unser Modell korrekt ist, wenn es eine positive Klassenvorhersage macht,
würden wir unsere Vorhersageschwelle für die Wiedererkennung optimieren. Dies kann in jeder Situation angewendet werden
in der wir falsch-positive Ergebnisse vermeiden wollen. Wenn es jedoch kostspieliger ist, eine
eine potenziell positive Klassifizierung zu verpassen, selbst wenn wir falsch liegen könnten, optimieren wir unser
Modell für die Rückrufquote.

Auswahl einer Modellarchitektur

Abhängig von unserer Vorhersageaufgabe gibt es verschiedene Modellarchitekturen, die in Betracht gezogen werden können
bei der Lösung von Problemen mit dem Rebalancing-Entwurfsmuster. Wenn wir mit
tabellarischen Daten arbeiten und ein Klassifizierungsmodell zur Erkennung von Anomalien erstellen, hat die Forschung
hat die Forschung gezeigt, dass Entscheidungsbaummodelle für diese Art von Aufgaben gut geeignet sind. Baumbasierte
Modelle funktionieren auch gut bei Problemen mit kleinen und unausgewogenen Datensätzen.
XGBoost, scikit-learn und TensorFlow verfügen alle über Methoden zur Implementierung von Entscheidungsbaummodellen.
Baummodelle.

Wir können einen binären Klassifikator in XGBoost mit dem folgenden Code implementieren:

# Erstellen des Modells
model = xgb.XGBClassifier(
objective='binary:logistic'
)
# Trainieren des Modells
model.fit(
train_data,
train_labels
)
Wir können Downsampling und Klassengewichte in jedem dieser Frameworks verwenden, um
unser Modell mit Hilfe des Rebalancing-Entwurfsmusters weiter zu optimieren. Um zum Beispiel
gewichtete Klassen zu unserem obigen XGBClassifier hinzuzufügen, fügen wir einen scale_pos_weight-Param-
eter hinzufügen, der auf der Grundlage des Gleichgewichts der Klassen in unserem Datensatz berechnet wird.

Wenn es darum geht, Anomalien in Zeitreihendaten zu erkennen, eignen sich LSTM-Modelle (Long Short Memory)
Modelle gut geeignet, um in Sequenzen vorhandene Muster zu erkennen. Clustering-Modelle sind
auch eine Option für tabellarische Daten mit unausgewogenen Klassen. Für unausgewogene Datensätze mit
Bilddaten verwenden Sie Deep-Learning-Architekturen mit Downsampling, gewichteten Klassen,
Upsampling oder einer Kombination dieser Techniken. Bei Textdaten ist die Erzeugung
synthetische Daten zu generieren, weniger einfach, und es ist am besten, sich auf Downsampling
gewichtete Klassen.

Entwurfsmuster 10: Neugewichtung | 135
Unabhängig von der Datenmodalität, mit der wir arbeiten, ist es sinnvoll, mit verschiedenen Modellarchitekturen zu experimentieren.
Modellarchitekturen zu experimentieren, um herauszufinden, welche für unsere unausgewogenen Daten am besten geeignet ist.

Wichtigkeit der Erklärbarkeit

Bei der Entwicklung von Modellen zur Erkennung seltener Vorkommnisse in Daten, wie z. B. Anomalien, ist es
besonders wichtig, zu verstehen, wie unser Modell Vorhersagen trifft. Dies kann
überprüfen, ob das Modell die richtigen Signale für seine Vorhersagen aufnimmt
und hilft dabei, den Endnutzern das Verhalten des Modells zu erklären. Es gibt ein paar Werkzeuge, die
Modelle zu interpretieren und Vorhersagen zu erklären, darunter das Open-Source-Framework
work SHAP, das What-If Tool und Explainable AI auf Google Cloud.

Modellerklärungen können viele Formen annehmen, eine davon sind die sogenannten Attributionswerte.
Die Attributionswerte sagen uns, wie stark jedes Merkmal in unserem Modell die Vorhersage des Modells
Vorhersage beeinflusst hat. Positive Attributionswerte bedeuten, dass ein bestimmtes Merkmal die Vorhersage unseres
die Vorhersage des Modells nach oben, und negative Attributionswerte bedeuten, dass das Merkmal die
die Vorhersage des Modells nach unten. Je höher der absolute Wert eines Merkmals ist, desto größer
Auswirkung auf die Vorhersage unseres Modells hatte. Bei Bild- und Textmodellen können Attributionen
die Pixel oder Wörter anzeigen, die die Vorhersage Ihres Modells am meisten beeinflusst haben. Bei tabellarischen
Modellen liefern Attribute numerische Werte für jedes Merkmal, die dessen Gesamteffekt
Wirkung auf die Vorhersage des Modells.

Nach dem Training eines TensorFlow-Modells auf dem synthetischen Betrugserkennungsdatensatz von
Kaggle trainiert und auf Explainable AI in der Google Cloud bereitgestellt wurde, lassen Sie uns einen Blick auf einige
Beispiele für Attributionen auf Instanzebene. In Abbildung 3-21 sehen wir zwei Beispieltransaktionen
Transaktionen, die unser Modell korrekt als Betrug identifiziert hat, zusammen mit ihren
Attributionen.

Im ersten Beispiel, bei dem das Modell eine 99-prozentige Betrugswahrscheinlichkeit vorhersagte, war der alte Kontostand vor der
der alte Kontostand auf dem Ursprungskonto vor der Transaktion der größte Indikator für
für Betrug. Im zweiten Beispiel war unser Modell zu 89 % sicher in seiner Vorhersage von
Betrugsvorhersage, wobei der Betrag der Transaktion das größte Betrugssignal darstellte.
Der Saldo des Ursprungskontos hat jedoch dazu geführt, dass unser Modell bei der Vorhersage von Betrug weniger sicher war.
Betrugsvorhersage und erklärt, warum die Vorhersagesicherheit leicht um 10
Prozentpunkte.

Erklärungen sind für jede Art von maschinellem Lernmodell wichtig, aber wir können sehen
dass sie besonders nützlich für Modelle sind, die dem Rebalancing-Entwurfsmuster folgen.
Beim Umgang mit unausgewogenen Daten ist es wichtig, über die Genauigkeits- und Fehlermetriken des Modells hinaus zu
und Fehlermetriken unseres Modells zu schauen, um zu überprüfen, ob es sinnvolle Signale in unseren Daten aufgreift.

136 | Kapitel 3: Entwurfsmuster für die Problemdarstellung

Abbildung 3-21. Merkmalszuweisungen von Explainable AI für zwei korrekt klassifizierte Frau-
dulten Transaktionen.

Zusammenfassung
In diesem Kapitel wurden verschiedene Möglichkeiten zur Darstellung einer Vorhersageaufgabe unter dem Aspekt
der Modellarchitektur und der Modellausgabe. Wenn Sie darüber nachdenken, wie Sie Ihr
Modell angewendet werden soll, kann Ihnen bei der Entscheidung helfen, welche Art von Modell Sie erstellen und wie Sie die
Ausgabe für die Vorhersage. Vor diesem Hintergrund haben wir mit dem Reframing-Designpfad begonnen.
der untersucht, wie Sie Ihr Problem von einer Regressionsaufgabe in eine Klassifikationsaufgabe
Klassifikationsaufgabe (oder umgekehrt), um die Qualität Ihres Modells zu verbessern. Sie können dies tun, indem Sie

Zusammenfassung | 137
Umformatierung der Beschriftungsspalte in Ihren Daten. Als Nächstes haben wir uns mit dem Multilabel-Design
Entwurfsmuster, das sich mit Fällen befasst, in denen eine Eingabe in Ihr Modell mit
mehr als einem Label zugeordnet werden kann. Um diesen Fall zu behandeln, verwenden Sie die Sigmoid-Aktivierungsfunktion auf Ihrer
Ausgabeschicht mit binärem Kreuzentropieverlust.

Während sich die Muster Reframing und Multilabel auf die Formatierung der Modellausgabe konzentrieren,
das Entwurfsmuster Ensemble die Modellarchitektur und umfasst verschiedene Methoden
Methoden zur Kombination mehrerer Modelle, um die Ergebnisse des maschinellen Lernens aus einem
einzelnen Modells zu verbessern. Insbesondere umfasst das Ensemble-Muster Bagging, Boosting und
Stacking - alles verschiedene Techniken zur Zusammenfassung mehrerer Modelle in einem ML-System.
tem. Das Kaskadenmuster ist ebenfalls ein Ansatz auf Modellebene und beinhaltet die Aufteilung eines
ein maschinelles Lernproblem in mehrere kleinere Probleme auf. Im Gegensatz zu Ensemble
Ensemble-Modellen erfordert das Kaskadenmuster, dass die Ausgaben eines Ausgangsmodells in die
nachgelagerte Modelle. Aufgrund der Komplexität, die Kaskadenmodelle erzeugen können, sollten Sie
sollten Sie sie nur verwenden, wenn Sie ein Szenario haben, in dem die anfänglichen Klassifizierungslabels
ungleich und gleich wichtig sind.

Als Nächstes haben wir uns das Entwurfsmuster "Neutrale Klasse" angesehen, das die Problemrepräsentation auf
Problemdarstellung auf der Ausgabenebene behandelt. Dieses Muster verbessert einen binären Klassifikator durch Hinzufügen einer
dritte "neutrale" Klasse. Dies ist in Fällen nützlich, in denen Sie beliebige oder weniger
oder weniger polarisierende Klassifizierungen, die nicht in eine der eindeutigen binären Kategorien fallen
gorien fallen. Schließlich bietet das Entwurfsmuster Rebalancing Lösungen für Fälle, in denen Sie
einen inhärent unausgewogenen Datensatz haben. Dieses Muster schlägt die Verwendung von Downsampling vor,
gewichteter Klassen oder spezifischer Reframing-Techniken vor, um Datensätze mit unausgewogenen
unausgewogenen Label-Klassen zu lösen.

In den Kapiteln 2 und 3 ging es um die ersten Schritte zur Strukturierung Ihres Problems des maschinellen Lernens
Problem, insbesondere die Formatierung der Eingabedaten, die Optionen der Modellarchitektur und die
Darstellung der Ausgabe. Im nächsten Kapitel werden wir uns mit dem nächsten Schritt im Arbeitsablauf des maschinellen
Arbeitsablauf des maschinellen Lernens - Entwurfsmuster für das Training von Modellen.

138 | Kapitel 3: Entwurfsmuster für die Problemdarstellung

KAPITEL 4

Modell-Trainingsmuster
Modelle für maschinelles Lernen werden in der Regel iterativ trainiert, und dieser iterative Prozess wird
wird informell als Trainingsschleife bezeichnet. In diesem Kapitel wird erörtert, wie die typische Trainingsschleife
Trainingsschleife aussieht, und katalogisieren eine Reihe von Situationen, in denen Sie möglicherweise
etwas anders vorgehen wollen.

Typische Trainingsschleife
Modelle des maschinellen Lernens können mit verschiedenen Arten der Optimierung trainiert werden. Entscheidungs-
sionsbäume werden oft Knoten für Knoten auf der Grundlage eines Informationsgewinns aufgebaut. Bei
genetischen Algorithmen werden die Modellparameter als Gene dargestellt, und die Optimierungs
Optimierungsmethode basiert auf Techniken, die auf der Evolutionstheorie beruhen. Allerdings ist der
gängigste Ansatz zur Bestimmung der Parameter von Modellen des maschinellen Lernens
ist der Gradientenabstieg.

Stochastischer Gradientenabstieg
Bei großen Datensätzen wird der Gradientenabstieg auf Mini-Batches der Eingabedaten angewendet, um
von linearen Modellen und Boosted Trees bis hin zu tiefen neuronalen Netzen
(DNNs) und Support-Vektor-Maschinen (SVMs) zu trainieren. Dies wird als stochastischer Gradienten
Gradientenabstieg (SGD), und Erweiterungen von SGD (wie Adam und Adagrad) sind die de facto
Optimierer, die in modernen maschinellen Lernsystemen verwendet werden.

Da SGD ein iteratives Training auf kleinen Stapeln des Trainingsdatensatzes
Da SGD das Training iterativ an kleinen Stapeln des Trainingsdatensatzes durchführt, erfolgt das Training eines maschinellen Lernmodells in einer Schleife. SGD findet eine Mini-
aber keine geschlossene Lösung, so dass wir erkennen müssen, ob das Modell konvergiert.
Konvergenz des Modells stattgefunden hat. Aus diesem Grund muss der Fehler (der so genannte Verlust) im Trainingsdatensatz
Trainingsdatensatz überwacht werden. Eine Überanpassung kann auftreten, wenn die Modellkomplexität höher ist
als es die Größe und Abdeckung des Datensatzes zulassen. Leider müssen Sie

139
Sie können nicht wissen, ob die Modellkomplexität für einen bestimmten Datensatz zu hoch ist, bis
Sie das Modell tatsächlich auf diesem Datensatz trainieren. Daher muss die Bewertung
innerhalb der Trainingsschleife durchgeführt werden, und die Fehlermetriken auf einem zurückgehaltenen Teil der Trainingsdaten
den so genannten Validierungsdatensatz, müssen ebenfalls überwacht werden. Da die Trainings- und
Validierungsdatensätze in der Trainingsschleife verwendet wurden, ist es notwendig, einen weiteren
ein weiterer Teil des Trainingsdatensatzes, der so genannte Testdatensatz, zurückgehalten werden, um die tatsächlichen
Fehlermetriken, die bei neuen und ungesehenen Daten zu erwarten wären. Diese Auswertung wird
am Ende durchgeführt.

Keras-Trainingsschleife
Die typische Trainingsschleife in Keras sieht wie folgt aus:

model = keras.Model(...)
model.compile(optimizer=keras.optimizers. Adam (),
loss=keras.losses.categorical_ crossentropy (),
metrics=[ 'Genauigkeit' ])
history = model.fit(x_train, y_train,
batch_size=64,
epochs=3,
validation_data=(x_val, y_val))
Ergebnisse = model.evaluate(x_test, y_test, batch_size=128))
model.save(...)
Hier verwendet das Modell den Adam-Optimierer zur Durchführung von SGD auf der Kreuzentropie über
über den Trainingsdatensatz aus und gibt die endgültige Genauigkeit für den Testdatensatz an.
Die Modellanpassung durchläuft den Trainingsdatensatz dreimal (jeder Durchlauf durch den
Trainingsdatensatz wird als Epoche bezeichnet), wobei das Modell jeweils Stapel von 64
Trainingsbeispielen auf einmal. Am Ende jeder Epoche werden die Fehlermetriken auf dem Validierungsdatensatz berechnet.
auf dem Validierungsdatensatz berechnet und zur Historie hinzugefügt. Am Ende der Anpassungsschleife
wird das Modell auf dem Testdatensatz ausgewertet, gespeichert und potenziell zur
eingesetzt, wie in Abbildung 4-1 dargestellt.

Abbildung 4-1. Eine typische Trainingsschleife, bestehend aus drei Epochen. Jede Epoche wird in
Stücken von Beispielen der Größe batch_size verarbeitet. Am Ende der dritten Epoche wird das Modell anhand des
dem Testdatensatz ausgewertet und für eine mögliche Bereitstellung als Webdienst gespeichert.

Anstatt die vorgefertigte fit()-Funktion zu verwenden, könnten wir auch eine eigene Trainingsschleife schreiben
Schleife schreiben, die explizit über die Batches iteriert, aber das ist für keines der
den in diesem Kapitel besprochenen Entwurfsmustern.

140 | Kapitel 4: Muster der Modellausbildung

Ausbildung von Entwurfsmustern
Die in diesem Kapitel behandelten Entwurfsmuster haben alle mit der Modifizierung der typischen
Trainingsschleife in irgendeiner Weise zu verändern. In Useful Overfitting verzichten wir auf die Verwendung eines Validierungs- oder
Testdatensatz, weil wir den Trainingsdatensatz absichtlich überanpassen wollen. Unter
Checkpoints wird der vollständige Zustand des Modells in regelmäßigen Abständen gespeichert, so dass wir Zugang zu
teilweise trainierte Modelle. Wenn wir Checkpoints verwenden, verwenden wir normalerweise auch virtuelle
Epochen, in denen wir die innere Schleife der fit()-Funktion nicht mit dem vollständigen
den gesamten Trainingsdatensatz, sondern auf einer festen Anzahl von Trainingsbeispielen auszuführen. Beim Transfer
Lernen nehmen wir einen Teil eines zuvor trainierten Modells, frieren die Gewichte ein und integrieren
Gewichte ein und integrieren diese nicht trainierbaren Schichten in ein neues Modell, das das gleiche Problem löst, aber auf
einem kleineren Datensatz löst. Bei der Verteilungsstrategie wird die Trainingsschleife in großem Umfang über
mehreren Arbeitern durchgeführt, oft mit Caching, Hardwarebeschleunigung und Parallelisierung.
Beim Hyperparameter-Tuning schließlich wird die Trainingsschleife selbst in ein Optimierungsverfahren
Optimierungsverfahren eingefügt, um den optimalen Satz von Modellhyperparametern zu finden.

Entwurfsmuster 11: Nützliche Überanpassung
Nützliche Überanpassung ist ein Entwurfsmuster, bei dem wir auf die Verwendung von Generalisierungsmech- nismen verzichten, weil wir den Trainingsdatensatz absichtlich überanpassen wollen.
anismen verzichten, weil wir absichtlich eine Überanpassung des Trainingsdatensatzes vornehmen wollen. In Situationen
Situationen, in denen Overfitting von Vorteil sein kann, empfiehlt dieses Entwurfsmuster, dass wir das
maschinelles Lernen ohne Regularisierung, Dropout oder einen Validierungsdatensatz zum frühen
Stoppen.

Problem
Das Ziel eines maschinellen Lernmodells ist die Generalisierung und die Erstellung zuverlässiger Vorhersagen
auf neue, ungesehene Daten. Wenn Ihr Modell die Trainingsdaten übererfüllt (z. B. weil es den
es den Trainingsfehler über den Punkt hinaus, an dem der Validierungsfehler zu steigen beginnt, weiter
ansteigt), leidet seine Verallgemeinerungsfähigkeit und damit auch Ihre zukünftigen Vorhersagen.
Einführende Lehrbücher zum maschinellen Lernen empfehlen, eine Überanpassung durch frühzeitiges
Abbruch- und Regularisierungstechniken.

Betrachten wir jedoch eine Situation, in der das Verhalten von physikalischen oder dynamischen Systemen simuliert wird
Systemen, wie sie in der Klimawissenschaft, der Computerbiologie oder im
Finanzwesen. In solchen Systemen lässt sich die Zeitabhängigkeit von Beobachtungen durch eine
mathematische Funktion oder eine Reihe von partiellen Differentialgleichungen (PDEs) beschrieben werden. Obwohl die
Gleichungen, die viele dieser Systeme steuern, formal ausgedrückt werden können, gibt es keine
eine geschlossene Form der Lösung. Stattdessen wurden klassische numerische Methoden entwickelt
entwickelt, um Lösungen für diese Systeme zu approximieren. Leider sind diese Methoden für viele reale
Leider sind diese Methoden für viele reale Anwendungen zu langsam, um in der Praxis eingesetzt werden zu können.

Entwurfsmuster 11: Nützliche Überanpassung | 141
Betrachten Sie die in Abbildung 4-2 dargestellte Situation. Aus der physischen Umgebung gesammelte Beobachtungen
Umgebung werden als Eingaben (oder anfängliche Startbedingungen) für ein physikbasiertes
Modell, das iterative, numerische Berechnungen durchführt, um den genauen Zustand des Systems zu berechnen.
des Systems zu berechnen. Angenommen, alle Beobachtungen haben eine endliche Anzahl von Möglichkeiten (zum Beispiel
z.B. die Temperatur liegt zwischen 60°C und 80°C in Schritten von 0,01°C). Es ist
ist es dann möglich, einen Trainingsdatensatz für das maschinelle Lernsystem zu erstellen, der aus dem
bestehend aus dem gesamten Eingaberaum, zu erstellen und die Bezeichnungen anhand des physikalischen Modells zu berechnen.

Abbildung 4-2. Eine Situation, in der eine Überanpassung akzeptabel ist, ist, wenn der gesamte Bereich
Raum der Beobachtungen tabelliert werden kann und ein physikalisches Modell zur Berechnung der
genaue Lösung zu berechnen, verfügbar ist.

Das ML-Modell muss diese genau berechnete und sich nicht überschneidende Nachschlagetabelle
Tabelle von Eingaben zu Ausgaben lernen. Die Aufteilung eines solchen Datensatzes in einen Trainingsdatensatz und einen
Evaluierungsdatensatz ist kontraproduktiv, weil wir dann erwarten würden, dass das Modell
Teile des Eingaberaums zu lernen, die es im Trainingsdatensatz noch nicht gesehen hat.

Lösung
In diesem Szenario gibt es keine "ungesehenen" Daten, auf die verallgemeinert werden muss, da alle
möglichen Eingaben tabellarisch erfasst wurden. Beim Aufbau eines maschinellen Lernmodells zum
ein solches physikalisches Modell oder dynamisches System zu lernen, gibt es keine Überanpassung.
Das grundlegende Trainingsparadigma des maschinellen Lernens ist etwas anders. Hier gibt es
ein physikalisches Phänomen, das man zu erlernen versucht und das durch eine unter-
liegende PDE oder ein System von PDEs. Maschinelles Lernen bietet lediglich eine datengesteuerte

142 | Kapitel 4: Muster der Modellausbildung

Ansatz zur Annäherung an die genaue Lösung, und Konzepte wie Überanpassung müssen
neu bewertet werden.

So wird zum Beispiel ein Ray-Tracing-Ansatz verwendet, um die Satellitenbilder zu simulieren, die
die sich aus der Ausgabe von numerischen Wettervorhersagemodellen ergeben würden. Dabei wird
berechnet, wie viel eines Sonnenstrahls von den vorhergesagten Hydrometeoren
(Regen, Schnee, Hagel, Eispellets usw.) auf jeder atmosphärischen Ebene absorbiert wird. Es gibt eine endliche
Anzahl möglicher Hydrometeor-Typen und eine endliche Anzahl von Höhen, die das
numerische Modell vorhersagt. Das Strahlenverfolgungsmodell muss also optische Gleichungen anwenden auf
eine große, aber endliche Anzahl von Eingaben anwenden.

Die Gleichungen des Strahlungstransfers regeln das komplexe dynamische System, wie sich
wie sich elektromagnetische Strahlung in der Atmosphäre ausbreitet, und Vorwärts-Strahlungstrans- fer-Modelle
Modelle für den Strahlungstransfer sind ein wirksames Mittel, um auf den zukünftigen Zustand von Satellitenbildern zu schließen.
Die klassischen numerischen Methoden zur Berechnung der Lösungen dieser Gleichungen sind jedoch
können jedoch einen enormen Rechenaufwand erfordern und sind zu langsam, um in der Praxis eingesetzt zu werden.

Hier kommt das maschinelle Lernen ins Spiel. Mit Hilfe des maschinellen Lernens ist es möglich, ein Modell zu erstellen, das
Lösungen des Vorwärtsstrahlungsübertragungsmodells annähert (siehe Abbildung 4-3). Diese
ML-Näherung kann nahe genug an die Lösung des Modells herangeführt werden, die
die ursprünglich mit klassischeren Methoden erreicht wurde. Der Vorteil ist, dass die Inferenz
mit Hilfe der erlernten ML-Näherung (die lediglich eine geschlossene Formel berechnen muss)
nur einen Bruchteil der Zeit in Anspruch nimmt, die für die Durchführung der Strahlenverfolgung (die
numerische Methoden erfordern würde). Zugleich ist der Trainingsdatensatz zu groß (mehrere
tiple Terabytes) und zu unhandlich, um als Nachschlagetabelle in der Produktion verwendet zu werden.

Abbildung 4-3. Architektur für die Verwendung eines neuronalen Netzes zur Modellierung der Lösung einer partiellen
Differentialgleichung zur Lösung von I(r,t,n).

Es besteht ein wichtiger Unterschied zwischen dem Training eines ML-Modells zur Annäherung an die
Lösung eines dynamischen Systems wie diesem und dem Training eines ML-Modells zur Vorhersage des Babygewichts
Gewicht auf der Grundlage von Geburtsdaten, die im Laufe der Jahre gesammelt wurden. Bei dem dynamischen System
ist eine Reihe von Gleichungen, die den Gesetzen der elektromagnetischen Strahlung unterliegen - es gibt keine
unbeobachtete Variable, kein Rauschen und keine statistische Variabilität. Für einen gegebenen Satz von Eingaben,

Entwurfsmuster 11: Nützliche Überanpassung | 143
gibt es nur eine genau berechenbare Ausgabe. Es gibt keine Überschneidungen zwischen verschiedenen
Beispielen im Trainingsdatensatz. Aus diesem Grund können wir die Bedenken bezüglich der Gen-
eralisierung. Wir wollen, dass unser ML-Modell die Trainingsdaten so perfekt wie möglich abbildet, um
"Überanpassung".

Dies steht im Gegensatz zu dem typischen Ansatz beim Training eines ML-Modells, bei dem Überlegungen
Verzerrung, Varianz und Generalisierungsfehler eine wichtige Rolle spielen. Traditionelles Training
besagt, dass es möglich ist, dass ein Modell die Trainingsdaten "zu gut" lernt und dass das Trainieren
Modell so zu trainieren, dass die Zugverlustfunktion gleich Null ist, ist eher eine rote Fahne
als ein Grund zum Feiern. Eine derartige Überanpassung des Trainingsdatensatzes führt dazu, dass das
Modell dazu veranlasst, falsche Vorhersagen für neue, noch nicht gesehene Datenpunkte zu treffen. Der Unterschied hier
ist, dass wir im Voraus wissen, dass es keine unbekannten Daten geben wird, so dass das Modell sich einer Lösung einer
eine Lösung einer PDE über das gesamte Eingabespektrum. Wenn Ihr neuronales Netz in der Lage ist
in der Lage ist, einen Satz von Parametern zu lernen, bei dem die Verlustfunktion gleich Null ist, dann bestimmt dieser
dann bestimmt dieser Parametersatz die tatsächliche Lösung der betreffenden PDE.

Warum es funktioniert
Wenn alle möglichen Eingaben tabellarisch erfasst werden können, wird, wie die gepunktete Kurve in
Abbildung 4-4 zu sehen ist, wird ein Overfit-Modell immer noch dieselben Vorhersagen machen wie das "wahre" Modell, wenn
alle möglichen Eingabepunkte trainiert werden. Überanpassung ist also kein Problem. Wir müssen
darauf achten, dass die Rückschlüsse auf gerundete Werte der Eingaben gezogen werden, wobei die
Rundung durch die Auflösung bestimmt wird, mit der der Eingaberaum gerastert wurde.

Abbildung 4-4. Eine Überanpassung ist nicht zu befürchten, wenn alle möglichen Eingabepunkte für
trainiert werden, da die Vorhersagen bei beiden Kurven gleich sind.

Ist es möglich, eine Modellfunktion zu finden, die den wahren Bezeichnungen beliebig nahe kommt? Eine
warum dies funktioniert, ergibt sich aus der Uniform Approximation Theo-
rem des Deep Learning, die, grob gesagt, besagt, dass jede Funktion (und ihre Ableitungen)
durch ein neuronales Netzwerk mit mindestens einer versteckten Schicht und einer beliebigen Aktivierungsfunktion approximiert werden kann.
"Aktivierungsfunktion (z. B. Sigmoid) approximiert werden kann. Das bedeutet, dass, egal welche
Funktion gegeben ist, solange sie sich relativ gut verhält, gibt es ein neuronales

144 | Kapitel 4: Muster der Modellausbildung

1 Es kann natürlich sein, dass wir das Netz nicht mit Hilfe des Gradientenabstiegs lernen können, nur weil es
ein solches neuronales Netz existiert (aus diesem Grund ist es hilfreich, die Modellarchitektur durch Hinzufügen von Schichten zu ändern - es macht
die Verlustlandschaft für SGD zugänglicher).
Netzwerk mit nur einer versteckten Schicht, das diese Funktion so gut wie möglich approximiert
wollen.^1

Deep Learning-Ansätze zum Lösen von Differentialgleichungen oder komplexen dynamischen
Systeme zielen darauf ab, eine Funktion, die implizit durch eine Differentialgleichung oder ein
System von Gleichungen definiert ist, mit Hilfe eines neuronalen Netzes darzustellen.

Eine Überanpassung ist sinnvoll, wenn die beiden folgenden Bedingungen erfüllt sind:

Es gibt kein Rauschen, so dass die Beschriftungen für alle Instanzen korrekt sind.
Sie haben den gesamten Datensatz zur Verfügung (Sie haben alle Beispiele, die es
gibt). In diesem Fall wird aus der Überanpassung eine Interpolation des Datensatzes.
Kompromisse und Alternativen
Wir haben die Überanpassung als nützlich eingeführt, wenn die Eingabemenge vollständig aufgelistet werden kann
aufgelistet werden kann und die genaue Bezeichnung für jeden Satz von Eingaben berechnet werden kann. Wenn der gesamte Eingabe
Raum tabellarisch erfasst werden kann, ist Overfitting kein Problem, da es keine ungesehenen Daten gibt.
Das Entwurfsmuster Nützliche Überanpassung ist jedoch über diesen engen Anwendungsfall hinaus nützlich.
In vielen realen Situationen, selbst wenn eine oder mehrere dieser Bedingungen gelockert werden müssen
gelockert werden müssen, bleibt das Konzept, dass Overfitting nützlich sein kann, gültig.

Interpolation und Chaostheorie

Das Modell des maschinellen Lernens funktioniert im Wesentlichen als eine Annäherung an eine
Tabelle von Eingaben zu Ausgaben. Wenn die Nachschlagetabelle klein ist, verwenden Sie sie einfach als Nachschlagetabelle!
Es besteht keine Notwendigkeit, sie durch ein maschinelles Lernmodell zu approximieren. Eine ML-Approximation
Approximation ist in Situationen nützlich, in denen die Nachschlagetabelle zu groß ist, um sie effektiv zu nutzen. Sie
Wenn die Nachschlagetabelle zu unhandlich ist, ist es besser, sie als Trainingsdatensatz für ein maschinelles Lernmodell zu behandeln.
Trainingsdatensatz für ein maschinelles Lernmodell zu behandeln, das die Lookup-Tabelle approximiert.

Beachten Sie, dass wir davon ausgegangen sind, dass die Beobachtungen eine endliche Anzahl von Möglichkeiten haben.
tionen haben. Zum Beispiel haben wir angenommen, dass die Temperatur in 0,01°C-Schritten gemessen wird
gemessen wird und zwischen 60°C und 80°C liegt. Dies wird der Fall sein, wenn die Beobachtungen
mit digitalen Instrumenten durchgeführt werden. Wenn dies nicht der Fall ist, wird das ML-Modell benötigt, um zwischen
zwischen den Einträgen in der Lookup-Tabelle zu interpolieren.

Modelle des maschinellen Lernens interpolieren, indem sie ungesehene Werte mit dem Abstand der
dieser ungesehenen Werte von den Trainingsbeispielen. Eine solche Interpolation funktioniert nur, wenn das
zugrunde liegende System nicht chaotisch ist. In chaotischen Systemen, selbst wenn das System deterministisch ist
tisch ist, können kleine Unterschiede in den Ausgangsbedingungen zu dramatisch unterschiedlichen Ergebnissen führen.

Entwurfsmuster 11: Nützliches Overfitting | 145
In der Praxis hat jedoch jedes spezifische chaotische Phänomen eine bestimmte Auflösungs
Auflösungsschwelle, ab der es für Modelle möglich ist, sie über kurze Zeiträume vorherzusagen.
len. Vorausgesetzt, die Lookup-Tabelle ist fein genug und die Grenzen der
Auflösungsvermögens verstanden werden, können nützliche Näherungen entstehen.

Monte-Carlo-Methoden

In der Realität ist es möglicherweise nicht möglich, alle möglichen Eingaben zu erfassen.
Monte-Carlo-Ansatz, bei dem der Eingaberaum abgetastet wird, um den Eingabesatz zu erstellen, insbesondere
insbesondere dann, wenn nicht alle möglichen Kombinationen von Eingaben physikalisch möglich sind.

In solchen Fällen ist eine Überanpassung technisch möglich (siehe Abbildung 4-5, wo die nicht ausgefüllten Kreise
Kreise durch falsche Schätzungen approximiert werden, die durch gekreuzte Kreise dargestellt sind).

Abbildung 4-5. Wenn der Eingaberaum abgetastet und nicht tabelliert wird, müssen Sie darauf achten, dass
Modellkomplexität zu begrenzen.

Aber auch hier können Sie sehen, dass das ML-Modell zwischen bekannten Antworten interpoliert.
bekannten Antworten interpoliert. Die Berechnung ist immer deterministisch, und nur die Eingabepunkte
Punkte, die einer zufälligen Auswahl unterliegen. Daher enthalten diese bekannten Antworten kein
Rauschen enthalten, und da es keine unbeobachteten Variablen gibt, sind die Fehler an nicht
Punkte streng durch die Modellkomplexität begrenzt. Hier geht die Gefahr der Überanpassung
von der Komplexität des Modells und nicht von der Anpassung an das Rauschen. Überanpassung ist nicht so
wenn die Größe des Datensatzes größer ist als die Anzahl der freien Parameter.
Parameter ist. Daher bietet die Verwendung einer Kombination aus Modellen mit geringer Komplexität und einer milden Regula-
ularisierung einen praktischen Weg, um eine inakzeptable Überanpassung im Falle einer
Monte-Carlo-Auswahl des Eingaberaums.

Datengesteuerte Diskretisierungen

Obwohl die Ableitung einer geschlossenen Lösung für einige PDEs möglich ist, ist die Bestimmung
ist die Bestimmung von Lösungen mit numerischen Methoden üblicher. Numerische Methoden für PDEs
sind bereits ein umfangreiches Forschungsgebiet, und es gibt viele Bücher, Kurse und Fachzeitschriften
die sich mit diesem Thema befassen. Ein gängiger Ansatz ist die Verwendung von Finite-Differenzen-Methoden,
ähnlich den Runge-Kutta-Verfahren, zur Lösung gewöhnlicher Differentialgleichungen. Dies geschieht
Dies geschieht in der Regel durch Diskretisierung des Differentialoperators der PDE und die Suche nach einem

146 | Kapitel 4: Muster der Modellausbildung

Lösung des diskreten Problems auf einem räumlich-zeitlichen Gitter des ursprünglichen Bereichs.
Wenn jedoch die Dimension des Problems groß wird, versagt dieser gitterbasierte
Fluch der Dimensionalität, da der Maschenabstand des Gitters klein genug sein
des Gitters klein genug sein muss, um die kleinste Merkmalsgröße der Lösung zu erfassen.
tion zu erfassen. Um also eine 10-fach höhere Auflösung eines Bildes zu erreichen, ist 10.000-fach mehr
Rechenleistung, da das Maschengitter in vier Dimensionen skaliert werden muss, um
Raum und Zeit.

Es ist jedoch möglich, maschinelles Lernen (anstelle von Monte-Carlo-Methoden) einzusetzen
die Stichprobenpunkte auszuwählen, um datengesteuerte Diskretisierungen von PDEs zu erstellen. In dem
Papier "Learning data-driven discretizations for PDEs" demonstrieren Bar-Sinai et al.
die Wirksamkeit dieses Ansatzes. Die Autoren verwenden ein niedrig aufgelöstes Gitter von Fixpunkten
Punkte, um eine Lösung durch eine stückweise Polynominterpolation mit Hilfe von Standard-Finite-Differenzen-Methoden zu approximieren.
Die Autoren verwenden ein niedrig aufgelöstes Gitter von Fixpunkten, um eine Lösung durch eine stückweise polynomiale Interpolation mit Hilfe von Standard-Finite-Differenzen-Methoden zu approximieren. Die
Lösung aus dem neuronalen Netz übertrifft die numerische Simulation bei weitem
tion bei der Minimierung des absoluten Fehlers und erreicht an einigen Stellen eine 10^2
Verbesserung. Die Erhöhung der Auflösung erfordert zwar wesentlich mehr
Finite-Differenzen-Methoden wesentlich mehr Rechenleistung erfordert, kann das neuronale Netz seine
hohe Leistung mit nur marginalen zusätzlichen Kosten aufrechtzuerhalten. Techniken wie die Deep
Galerkin-Methode können dann mit Hilfe von Deep Learning eine netzfreie Approximation
der Lösung der gegebenen PDE liefern. Auf diese Weise wird das Lösen der PDE auf ein verkettetes
Optimierungsproblem reduziert (siehe "Entwurfsmuster 8: Kaskade" auf Seite 108).

Tiefes Galerkin-Verfahren
Die Deep-Galerkin-Methode ist ein Deep-Learning-Algorithmus zum Lösen von partiellen Differenzengleichungen.
tialgleichungen. Der Algorithmus ähnelt im Geiste den Galerkin-Methoden, die im Bereich der
der numerischen Analyse verwendet werden, wobei die Lösung durch ein neuronales Netz
anstelle einer linearen Kombination von Basisfunktionen approximiert wird.
Unbegrenzte Gebiete

Sowohl die Monte-Carlo-Methode als auch die datengesteuerte Diskretisierung gehen davon aus, dass
dass die Abtastung des gesamten Eingaberaums, wenn auch unvollkommen, möglich ist. Aus diesem Grund wurde das ML-Modell
als eine Interpolation zwischen bekannten Punkten behandelt.

Die Verallgemeinerung und das Problem der Überanpassung sind immer dann schwer zu ignorieren, wenn wir
nicht in der Lage sind, Punkte im gesamten Bereich der Funktion zu erfassen - zum Beispiel bei
Funktionen mit unbeschränkten Bereichen oder Projektionen entlang einer Zeitachse in die Zukunft.
In diesen Fällen ist es wichtig, Überanpassung, Unteranpassung und
Verallgemeinerungsfehler. Es hat sich nämlich gezeigt, dass Techniken wie die Deep
Galerkin-Methode in gut abgetasteten Regionen gut funktionieren, eine auf diese Weise gelernte Funktion
Funktion, die auf diese Weise erlernt wird, nicht gut auf Regionen außerhalb des Bereichs verallgemeinert, die nicht

Entwurfsmuster 11: Nützliches Overfitting | 147
in der Trainingsphase abgetastet. Dies kann bei der Verwendung von ML zur Lösung von PDEs problematisch sein
die auf unbeschränkten Gebieten definiert sind, problematisch sein, da es unmöglich wäre, eine
repräsentative Stichprobe für das Training zu erfassen.

Destillieren von Wissen über neuronale Netze

Eine weitere Situation, in der Overfitting gerechtfertigt ist, ist das Destillieren oder Übertragen
von einem großen Modell für maschinelles Lernen in ein kleineres Modell. Die Destillation von Wissen
Wissenstransfer ist sinnvoll, wenn die Lernkapazität des großen Modells nicht voll ausgeschöpft wird. Wenn
Wenn dies der Fall ist, ist die Rechenkomplexität des großen Modells möglicherweise nicht notwendig.
erforderlich. Es ist jedoch auch der Fall, dass das Training kleinerer Modelle schwieriger ist. Während das
kleinere Modell hat zwar genug Kapazität, um das Wissen darzustellen, aber möglicherweise nicht
genug Kapazität, um das Wissen effizient zu lernen.

Die Lösung besteht darin, das kleinere Modell auf einer großen Menge von generierten Daten zu trainieren, die von dem
die von dem größeren Modell beschriftet wurden. Das kleinere Modell lernt die weiche Ausgabe des größeren
Modells anstelle von tatsächlichen Kennzeichnungen auf realen Daten. Dies ist ein einfacheres Problem, das
das vom kleineren Modell gelernt werden kann. Wie bei der Approximation einer numerischen Funktion durch ein
durch ein Modell für maschinelles Lernen ist es das Ziel, dass das kleinere Modell die
Vorhersagen des größeren maschinellen Lernmodells genau wiedergibt. Dieser zweite Trainingsschritt kann
Nützliches Overfitting anwenden.

Überanpassung einer Charge

In der Praxis erfordert das Training neuronaler Netze eine Menge Experimente, und ein Praktiker muss viele
Praktiker muss viele Entscheidungen treffen, von der Größe und Architektur des Netzes bis hin zur
Wahl der Lernrate, der Initialisierung der Gewichte oder anderer Hyperparameter.

Die Überanpassung einer kleinen Gruppe ist eine gute Überprüfung sowohl des Modellcodes als auch der
die Dateneingabe-Pipeline. Nur weil das Modell kompiliert wird und der Code ohne Fehler läuft
ohne Fehler läuft, bedeutet das nicht, dass Sie das berechnet haben, was Sie zu haben glauben oder dass das Trainingsziel
Trainingsziel richtig konfiguriert ist. Ein ausreichend komplexes Modell sollte in der Lage sein
mit einer ausreichend kleinen Datenmenge zu übertreffen, vorausgesetzt, alles ist richtig konfiguriert. Wenn Sie also
nicht in der Lage sind, einen kleinen Stapel mit einem beliebigen Modell zu übertreffen, sollten Sie Ihren Modell
Code, die Eingabe-Pipeline und die Verlustfunktion auf Fehler oder einfache Bugs zu überprüfen. Die Überanpassung eines
Stapel ist eine nützliche Technik für das Training und die Fehlersuche bei neuronalen Netzen.

148 | Kapitel 4: Muster der Modellausbildung

Die Überanpassung geht über eine Charge hinaus. Aus einer eher ganzheitlichen Perspektive
Perspektive folgt die Überanpassung den allgemeinen Ratschlägen, die in Bezug auf
in Bezug auf Deep Learning und Regularisierung. Das am besten passende Modell
ist ein großes Modell, das richtig reguliert wurde. Kurz gesagt, wenn Ihr
tiefes neuronales Netzwerk nicht in der Lage ist, Ihre Trainingsdaten zu
zu überanpassen, sollten Sie ein größeres Modell verwenden. Sobald Sie dann ein großes
Modell, das den Trainingsdatensatz übererfüllt, können Sie die Regularisierung anwenden, um
die Validierungsgenauigkeit zu verbessern, auch wenn die Trainingsgenauigkeit
abnehmen kann.
Sie können Ihren Keras-Modellcode auf diese Weise mit dem tf.data.Dataset testen, das Sie für Ihre
für Ihre Eingabe-Pipeline geschrieben haben. Wenn Ihre Trainingsdaten-Eingabe-Pipeline zum Beispiel
trainds heißt, verwenden wir batch(), um einen einzelnen Datenstapel zu ziehen. Sie finden den vollständigen
Code für dieses Beispiel finden Sie im Repository zu diesem Buch:

BATCH_SIZE = 256
single_batch = trainds.batch(BATCH_SIZE).take(1)
Wenn Sie dann das Modell trainieren, rufen Sie nicht den gesamten trainds-Datensatz innerhalb der
fit()-Methode den gesamten trainds-Datensatz aufzurufen, sondern den von uns erstellten einzelnen Stapel zu verwenden:

model.fit(single_batch.repeat(),
validation_data=evalds,
...)
Beachten Sie, dass wir repeat() anwenden, damit uns nicht die Daten ausgehen, wenn wir mit diesem
einzelnen Stapel. Dadurch wird sichergestellt, dass wir beim Training immer wieder denselben Stapel verwenden.
trainieren. Alles andere (der Validierungsdatensatz, der Modellcode, die konstruierten Merkmale usw.)
usw.) bleibt gleich.

Anstatt eine beliebige Stichprobe des Trainingsdatensatzes zu wählen, empfehlen wir
empfehlen wir, eine Überanpassung auf einem kleinen Datensatz vorzunehmen, bei dem jedes
Beispiele sorgfältig auf korrekte Bezeichnungen überprüft wurden. Entwerfen Sie
Architektur Ihres neuronalen Netzes so aus, dass es in der Lage ist, diesen
Datenstapel genau zu lernen und den Verlust auf Null zu bringen. Dann nehmen Sie das gleiche Netz
Netzes und trainieren Sie es mit dem gesamten Trainingsdatensatz.
Entwurfsmuster 12: Kontrollpunkte
In Checkpoints speichern wir den vollständigen Zustand des Modells in regelmäßigen Abständen, so dass wir teilweise trainierte Modelle zur Verfügung haben.
teilweise trainierte Modelle zur Verfügung. Diese teilweise trainierten Modelle können als endgültiges Modell dienen
Modell dienen (im Falle eines vorzeitigen Abbruchs) oder als Ausgangspunkte für das weitere Training
(im Falle eines Maschinenausfalls und einer Feinabstimmung).

Entwurfsmuster 12: Kontrollpunkte | 149
Problem
Je komplexer ein Modell ist (z.B. je mehr Schichten und Knoten ein neuronales Netz
Netzes), desto größer ist der Datensatz, der benötigt wird, um es effektiv zu trainieren. Dies ist darauf zurückzuführen, dass
komplexere Modelle in der Regel mehr einstellbare Parameter haben. Je größer das Modell
nimmt auch die Zeit zu, die für die Anpassung eines Stapels von Beispielen benötigt wird. Je größer die Datenmenge
zunimmt (und unter der Annahme, dass die Stapelgrößen fest sind), nimmt auch die Anzahl der Stapel zu.
In Bezug auf die Rechenkomplexität bedeutet dieser doppelte Schlag daher, dass
dass das Training sehr lange dauern wird.

Zum Zeitpunkt der Erstellung dieses Artikels dauert das Training eines Übersetzungsmodells Englisch-Deutsch auf einem
Tensor Processing Unit (TPU)-Pod auf einem relativ kleinen Datensatz in etwa
zwei Stunden. Bei echten Datensätzen, wie sie für das Training intelligenter Geräte verwendet werden, kann das Training
mehrere Tage dauern.

Wenn eine Ausbildung so lange dauert, ist die Wahrscheinlichkeit eines Maschinenausfalls
unangenehm hoch. Wenn es ein Problem gibt, würden wir gerne von einem Zwischenpunkt aus weitermachen
einem Zwischenpunkt fortsetzen können, anstatt von Anfang an.

Lösung
Am Ende jeder Epoche können wir den Zustand des Modells speichern. Wenn die Trainingsschleife dann aus irgendeinem Grund
aus irgendeinem Grund unterbrochen wird, können wir zum gespeicherten Modellzustand zurückkehren und neu starten. Wie-
Dabei muss jedoch sichergestellt werden, dass der Zwischenzustand des Modells gespeichert wird und nicht
nur das Modell. Was ist damit gemeint?

Sobald das Training abgeschlossen ist, speichern oder exportieren wir das Modell, damit wir es für
Inferenz einsetzen können. Ein exportiertes Modell enthält nicht den gesamten Modellzustand, sondern nur die Informationen, die
Informationen, die zur Erstellung der Vorhersagefunktion erforderlich sind. Bei einem Entscheidungsbaum zum Beispiel
Entscheidungsbaum wären dies die endgültigen Regeln für jeden Zwischenknoten und der vorhergesagte Wert für
jeden der Blattknoten. Bei einem linearen Modell wären dies die endgültigen Werte der Gewichte
und Verzerrungen. Bei einem vollständig verknüpften neuronalen Netz müssen wir auch die Aktivierungsfunktionen
Funktionen und die Gewichte der versteckten Verbindungen hinzufügen.

Welche Daten über den Zustand des Modells benötigen wir bei der Wiederherstellung von einem Prüfpunkt aus, die ein
exportierte Modell nicht enthält? Ein exportiertes Modell enthält nicht, welche Epoche
und die Stapelnummer, die das Modell gerade verarbeitet, was natürlich wichtig ist, um
um das Training wieder aufzunehmen. Aber es gibt noch mehr Informationen, die eine Modelltrainingsschleife
enthalten kann. Um den Gradientenabstieg effektiv durchführen zu können, kann der Optimierer
die Lernrate nach einem Zeitplan ändern. Dieser Zustand der Lernrate ist nicht in einem
exportierten Modell. Außerdem kann das Modell ein stochastisches Verhalten aufweisen, wie
wie z.B. Ausfälle. Auch dies wird im exportierten Modellstatus nicht erfasst. Modelle wie rekurrierende
rent neuronale Netze enthalten eine Historie der vorherigen Eingabewerte. Im Allgemeinen kann der vollständige
Modellzustand ein Vielfaches der Größe des exportierten Modells betragen.

150 | Kapitel 4: Muster der Modellausbildung

Das Speichern des vollständigen Modellstatus, so dass das Modelltraining ab einem bestimmten Punkt wieder aufgenommen werden kann, wird als
Checkpointing, und die gespeicherten Modelldateien werden als Checkpoints bezeichnet. Wie oft sollten wir
Checkpoint? Der Zustand des Modells ändert sich aufgrund des Gradientenabstiegs nach jedem Batch.
Wenn wir also keine Arbeit verlieren wollen, sollten wir den Checkpoint nach jedem
Stapel prüfen. Checkpoints sind jedoch riesig, und diese E/A würde einen erheblichen Overhead verursachen.
Stattdessen bieten Modell-Frameworks in der Regel die Option, den Checkpoint am Ende jeder
jeder Epoche. Dies ist ein vernünftiger Kompromiss zwischen nie einem Checkpoint und einem Check
Checkpointing nach jedem Batch.

Um ein Modell in Keras zu überprüfen, stellen Sie einen Callback für die fit()-Methode bereit:

checkpoint_path = '{}/checkpoints/taxi'.format(OUTDIR)
cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,
save_weights_only=False,
verbose=1)
history = model.fit(x_train, y_train,
batch_size=64,
epochs=3,
validation_data=(x_val, y_val),
verbose=2,
callbacks=[cp_callback])
Durch Hinzufügen des Checkpointing wird die Trainingsschleife wie in
Abbildung 4-6.

Abbildung 4-6. Checkpointing speichert den vollständigen Modellzustand am Ende jeder Epoche.

Entwurfsmuster 12: Kontrollpunkte | 151
Prüfpunkte in PyTorch
Zum Zeitpunkt der Erstellung dieses Artikels unterstützt PyTorch Checkpoints nicht direkt. Es unterstützt jedoch
unterstützt jedoch die Externalisierung des Zustands der meisten Objekte. Um Prüfpunkte in PyTorch zu implementieren
PyTorch zu implementieren, bitten Sie darum, dass die Epoche, der Modellstatus, der Zustand des Optimierers und alle anderen Informationen
die zur Wiederaufnahme des Trainings benötigt werden, zusammen mit dem Modell serialisiert werden:
torch.save({
'epoch': epoch,
'model_state_dict': model.state_dict() ,
'optimizer_state_dict': optimizer.state_dict() ,
'loss': loss,
}, PATH)
Wenn Sie von einem Checkpoint laden, müssen Sie die notwendigen Klassen erstellen und dann
aus dem Prüfpunkt laden:
model = ...
optimizer = ...
checkpoint = torch.load(PATH)
model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(Prüfpunkt['optimizer_state_dict'])
epoch = checkpoint['epoch']
loss = checkpoint['loss']
Dies ist eine niedrigere Ebene als TensorFlow, bietet aber die Flexibilität, mehrere
Modelle in einem Checkpoint zu speichern und zu wählen, welche Teile des Modellstatus geladen oder nicht
laden.
Warum es funktioniert
TensorFlow und Keras setzen das Training automatisch von einem Checkpoint aus fort, wenn Check-
Punkte im Ausgabepfad gefunden werden. Um das Training von Grund auf neu zu beginnen, müssen Sie daher
müssen Sie daher von einem neuen Ausgabeverzeichnis aus starten (oder vorherige Checkpoints aus dem
Ausgabeverzeichnis löschen). Dies funktioniert, weil Frameworks für maschinelles Lernen in Unternehmen
das Vorhandensein von Prüfpunktdateien anerkennen.

Auch wenn Checkpoints in erster Linie zur Unterstützung der Ausfallsicherheit konzipiert sind, eröffnet die Verfügbarkeit
von teilweise trainierten Modellen eine Reihe weiterer Anwendungsfälle eröffnet. Dies liegt daran, dass die
Modelle in der Regel besser verallgemeinerbar sind als die in späteren
späteren Iterationen. Eine gute Intuition, warum dies der Fall ist, kann aus dem Tensor-
Flow Spielplatz, wie in Abbildung 4-7 gezeigt.

152 | Kapitel 4: Muster der Modellausbildung

Abbildung 4-7. Ausgangspunkt des Spiral-Klassifikationsproblems. Sie können zu diesem Aufbau gelangen
indem Sie diesen Link in einem Webbrowser öffnen.

Auf dem Spielplatz versuchen wir, einen Klassifikator zu bauen, der zwischen blauen Punkten
und orangefarbenen Punkten zu unterscheiden (wenn Sie dies im gedruckten Buch lesen, folgen Sie bitte
den Link in einem Webbrowser ansteuern). Die beiden Eingabemerkmale sind x 1 und x 2, die
sind die Koordinaten der Punkte. Basierend auf diesen Merkmalen muss das Modell Folgendes ausgeben
die Wahrscheinlichkeit, dass der Punkt blau ist. Das Modell beginnt mit zufälligen Gewichten und der
Hintergrund der Punkte zeigt die Modellvorhersage für jeden Koordinatenpunkt. Wie Sie
Da die Gewichte zufällig sind, tendiert die Wahrscheinlichkeit dazu, für alle Pixel in der Nähe des
ten Wert für alle Pixel.

Starten Sie das Training, indem Sie auf den Pfeil oben links im Bild klicken, und Sie sehen, wie das
Modell langsam beginnt, mit aufeinanderfolgenden Epochen zu lernen, wie in Abbildung 4-8 dargestellt.

Abbildung 4-8. Was das Modell mit fortschreitendem Training lernt. Die Diagramme oben zeigen den
Trainingsverlust und der Validierungsfehler, während die Bilder zeigen, wie das Modell in diesem Stadium
die Farbe eines Punktes an jeder Koordinate im Gitter vorhersagen würde.

Entwurfsmuster 12: Kontrollpunkte | 153
Den ersten Hinweis auf das Lernen sehen wir in Abbildung 4-8(b), und wir sehen, dass das Modell
Abbildung 4-8(c) die High-Level-Sicht auf die Daten gelernt hat. Von da an passt das Modell
die Grenzen an, um mehr und mehr blaue Punkte in die zentrale Region zu bringen und
die orangefarbenen Punkte draußen bleiben. Dies ist hilfreich, aber nur bis zu einem gewissen Punkt. Bis zu dem Zeitpunkt, an dem wir zu
Abbildung 4-8(e) kommen, beginnt die Anpassung der Gewichte, zufällige Störungen in den
den Trainingsdaten widerspiegeln, und diese sind im Validierungsdatensatz kontraproduktiv.

Wir können also die Ausbildung in drei Phasen unterteilen. In der ersten Phase, zwischen
(a) und (c), lernt das Modell die Organisation der Daten auf hoher Ebene. In der
zweiten Phase, zwischen den Stufen (c) und (e), lernt das Modell die Details. In der
Phase, der Stufe (f), ist das Modell überangepasst. Ein teilweise trainiertes
Modell aus dem Ende von Phase 1 oder aus Phase 2 hat einige Vorteile, weil es
weil es die Organisation auf hoher Ebene gelernt hat, aber nicht in den Details gefangen ist.

Kompromisse und Alternativen
Neben der Ausfallsicherheit ermöglicht uns die Speicherung von Zwischenprüfpunkten auch die Implementierung von
frühzeitiges Anhalten und Feinabstimmung zu ermöglichen.

Vorzeitiges Aufhören

Im Allgemeinen gilt: Je länger Sie trainieren, desto geringer ist der Verlust im Trainingsdatensatz. Allerdings,
irgendwann kann es jedoch sein, dass der Fehler im Validierungsdatensatz nicht mehr abnimmt. Wenn Sie
Überanpassung des Trainingsdatensatzes beginnt, kann der Validierungsfehler sogar ansteigen
ansteigen, wie in Abbildung 4-9 dargestellt.

Abbildung 4-9. Normalerweise sinkt der Trainingsverlust, je länger man trainiert, aber sobald
Überanpassung beginnt, steigt der Validierungsfehler bei einem zurückgehaltenen Datensatz an.

154 | Kapitel 4: Muster der Modellausbildung

In solchen Fällen kann es hilfreich sein, den Validierungsfehler am Ende jeder Epoche zu betrachten
Epoche zu betrachten und den Trainingsprozess zu stoppen, wenn der Validierungsfehler größer ist als der der
vorherigen Epoche. In Abbildung 4-9 ist dies am Ende der vierten Epoche der Fall, dargestellt durch
die dicke gestrichelte Linie. Dies wird als Frühstopp bezeichnet.

Hätten wir am Ende eines jeden Stapels einen Kontrollpunkt gesetzt, hätten wir vielleicht
in der Lage gewesen, das wahre Minimum zu erfassen, das vielleicht
ein wenig vor oder nach der Epochengrenze gelegen hätte. Siehe die Diskussion
über virtuelle Epochen in diesem Abschnitt für eine häufigere Art des
Kontrollpunkt.
Wenn wir viel häufiger einen Checkpoint durchführen, kann es hilfreich sein, wenn
frühes Anhalten nicht übermäßig empfindlich auf kleine Störungen im
Validierungsfehler reagiert. Stattdessen können wir das frühzeitige Anhalten erst anwenden, wenn der
Validierungsfehler sich für mehr als N Prüfpunkte nicht verbessert hat.
Auswahl der Prüfpunkte. Während das frühzeitige Anhalten durch Anhalten des Trainings implementiert werden kann
Training abbricht, sobald der Validierungsfehler ansteigt, empfehlen wir, länger zu trainieren
und den optimalen Lauf als Nachbearbeitungsschritt auszuwählen. Der Grund, warum wir empfehlen, das Training
Phase 3 zu trainieren (eine Erklärung der drei Phasen der Trainingsschleife finden Sie im Abschnitt "Warum es funktioniert")
der drei Phasen der Trainingsschleife) ist, dass es nicht ungewöhnlich ist, dass der Validierungsfehler
Fehler kurzzeitig ansteigt und dann wieder abfällt. Dies liegt in der Regel daran, dass das
Training sich zunächst auf häufigere Szenarien konzentriert (Phase 1) und dann beginnt, sich auf
auf die selteneren Situationen konzentriert (Phase 2). Da seltene Situationen möglicherweise nur unvollständig
zwischen den Trainings- und Validierungsdatensätzen unvollkommen abgetastet werden können, sind gelegentliche Erhöhungen des Validierungs
Fehler während des Trainingslaufs in Phase 2 zu erwarten. Darüber hinaus gibt es Situationen
Außerdem gibt es Situationen, die für große Modelle typisch sind, in denen ein tiefer doppelter Abstieg zu erwarten ist, so dass es
ist es unerlässlich, vorsichtshalber etwas länger zu trainieren.

In unserem Beispiel werden wir, anstatt das Modell am Ende des Trainingslaufs zu exportieren, den
laden wir den vierten Kontrollpunkt und exportieren stattdessen unser endgültiges Modell von dort. Dies wird
Prüfpunktauswahl genannt und kann in TensorFlow mit BestExporter erreicht werden.

Regularisierung. Anstelle des vorzeitigen Abbruchs oder der Checkpoint-Auswahl kann es hilfreich sein
kann es hilfreich sein, eine L2-Regularisierung zu Ihrem Modell hinzuzufügen, damit der Validierungsfehler nicht
nicht zunimmt und das Modell nie in Phase 3 gelangt. Stattdessen sollten sowohl der Trainingsverlust als auch der
Validierungsfehler ein Plateau erreichen, wie in Abbildung 4-10 dargestellt. Wir bezeichnen eine solche Trainings
Schleife (bei der sowohl die Trainings- als auch die Validierungsmetrik ein Plateau erreichen) als gut
Trainingsschleife.

Entwurfsmuster 12: Kontrollpunkte | 155
Abbildung 4-10. In der idealen Situation erhöht sich der Validierungsfehler nicht. Stattdessen werden sowohl der
Trainingsverlust und der Validierungsfehler ein Plateau.

Wenn kein vorzeitiger Abbruch erfolgt und nur der Trainingsverlust zur Entscheidung über die Konvergenz
dann können wir vermeiden, dass wir einen separaten Testdatensatz anlegen müssen. Auch wenn wir
Selbst wenn wir nicht vorzeitig aufhören, kann es hilfreich sein, den Fortschritt des Modelltrainings anzuzeigen
hilfreich sein, insbesondere wenn das Modell eine lange Zeit zum Trainieren benötigt. Obwohl die Leistung
und der Fortschritt der Modellschulung normalerweise auf dem Validierungsdatensatz
während der Trainingsschleife überwacht wird, dient dies nur der Visualisierung. Da wir keine
keine Maßnahmen aufgrund der angezeigten Metriken ergreifen müssen, können wir die Visualisierung auf
dem Testdatensatz durchführen.

Der Grund dafür, dass die Verwendung der Regularisierung besser ist als das frühzeitige Stoppen, ist, dass die Regu-
larisierung es Ihnen ermöglicht, den gesamten Datensatz zu verwenden, um die Gewichte des Modells zu ändern,
wohingegen beim frühen Abbruch 10 % bis 20 % des Datensatzes verschwendet werden müssen, nur um
zu entscheiden, wann das Training beendet werden soll. Andere Methoden zur Begrenzung der Überanpassung (wie Dropout
und die Verwendung von Modellen mit geringerer Komplexität) sind ebenfalls gute Alternativen zum frühen Abbruch.
Jüngste Forschungsergebnisse deuten darauf hin, dass Doppelabstieg bei einer Vielzahl von
Problemen des maschinellen Lernens vorkommt und es daher besser ist, länger zu trainieren, als eine
suboptimale Lösung durch frühzeitiges Abbrechen zu riskieren.

Zwei Spaltungen. Stehen die Ratschläge im Abschnitt über die Regularisierung nicht im Widerspruch zu den Ratschlägen in
den vorangegangenen Abschnitten über das frühzeitige Anhalten oder die Auswahl von Kontrollpunkten? Nicht wirklich.

Wir empfehlen Ihnen, Ihre Daten in zwei Teile aufzuteilen: einen Trainingsdatensatz und einen
Auswertungsdatensatz. Der Auswertungsdatensatz spielt die Rolle des Testdatensatzes während

156 | Kapitel 4: Muster der Modellausbildung

(wo es keinen Validierungsdatensatz gibt) und spielt die Rolle des Validierungs
dungsdatensatz in der Produktion (wo es keinen Testdatensatz gibt).

Je größer Ihr Trainingsdatensatz ist, desto komplexere Modelle können Sie verwenden und desto
desto genauer kann das Modell werden. Die Verwendung der Regularisierung anstelle des frühen Abbruchs
oder Checkpoint-Auswahl können Sie einen größeren Trainingsdatensatz verwenden. In der Versuchs- und
der Experimentierphase (wenn Sie verschiedene Modellarchitekturen, Trainingstechniken und
Trainingstechniken und Hyperparametern erforschen), empfehlen wir, das frühe Stoppen zu deaktivieren und mit
mit größeren Modellen zu trainieren (siehe auch "Entwurfsmuster 11: Nützliches Overfitting" auf Seite
141 ). Damit soll sichergestellt werden, dass das Modell genügend Kapazität hat, um die Vorhersagemuster zu erlernen.
tern. Überwachen Sie während dieses Prozesses die Fehlerkonvergenz auf dem Trainingssplit. Am Ende
können Sie den Evaluierungsdatensatz verwenden, um festzustellen, wie gut Ihr
Modell bei Daten abschneidet, auf die es während des Trainings nicht gestoßen ist.

Wenn Sie das Modell für den Einsatz in der Produktion trainieren, müssen Sie sich darauf vorbereiten, dass Sie in der Lage sind
eine kontinuierliche Bewertung und ein erneutes Training des Modells durchzuführen. Aktivieren Sie das frühe Anhalten oder die
Punktauswahl und überwachen Sie die Fehlermetrik im Evaluierungsdatensatz. Wählen Sie
zwischen frühem Stoppen und Kontrollpunktauswahl, je nachdem, ob Sie
Kosten kontrollieren müssen (in diesem Fall würden Sie sich für frühes Anhalten entscheiden) oder die
Modellgenauigkeit (in diesem Fall würden Sie die Checkpoint-Auswahl wählen).

Feinabstimmung

In einer gut funktionierenden Trainingsschleife verhält sich der Gradientenabstieg so, dass man
auf der Grundlage der meisten Daten schnell in die Nähe des optimalen Fehlers gelangt,
und sich dann langsam dem niedrigsten Fehler annähert, indem man die Eckfälle optimiert.

Stellen Sie sich nun vor, dass Sie das Modell in regelmäßigen Abständen anhand neuer Daten neu trainieren müssen. Sie wollen typischerweise
Sie wollen die neuen Daten in den Vordergrund stellen, nicht die Eckfälle vom letzten Monat. Sie sind
Sie sind oft besser beraten, wenn Sie das Training nicht am letzten Kontrollpunkt, sondern am Kontrollpunkt
Punkt, der in Abbildung 4-11 durch die blaue Linie markiert ist. Dies entspricht dem Beginn von Phase 2
in unserer Diskussion über die Phasen des Modelltrainings, die weiter oben in "Warum es funktioniert"
auf Seite 169. So können Sie sicherstellen, dass Sie eine allgemeine Methode haben, die Sie dann
ein paar Epochen lang nur mit den neuen Daten feinabstimmen können.

Wenn Sie an dem durch die dicke gestrichelte vertikale Linie markierten Kontrollpunkt weitermachen, befinden Sie sich
befinden Sie sich in der vierten Epoche, so dass die Lernrate recht niedrig sein wird. Daher werden die
neuen Daten das Modell nicht dramatisch verändern. Allerdings verhält sich das Modell
optimal (im Kontext des größeren Modells) verhalten, weil Sie es auf diesen
auf diesem kleineren Datensatz geschärft haben. Dies wird als Feinabstimmung bezeichnet. Die Feinabstimmung wird auch in
in "Entwurfsmuster 13: Transfer Learning" auf Seite 161 behandelt.

Entwurfsmuster 12: Kontrollpunkte | 157
Abbildung 4-11. Wiederaufnahme des Trainings von einem Kontrollpunkt aus, bevor der Trainingsverlust ein Plateau erreicht.
Trainieren Sie in den folgenden Iterationen nur mit neuen Daten.

Die Feinabstimmung funktioniert nur so lange, wie Sie die Modellarchitektur nicht
Architektur.
Es ist nicht notwendig, immer von einem früheren Kontrollpunkt zu beginnen. In einigen Fällen kann der letzte
Kontrollpunkt (der für das Modell verwendet wird) als Warmstart für eine weitere
Iteration der Modellschulung verwendet werden. Dennoch bietet der Start von einem früheren Kontrollpunkt tendenziell eine
bessere Verallgemeinerung.

Die Neudefinition einer Epoche

Tutorials zum maschinellen Lernen enthalten oft Code wie diesen:

model.fit(X_train, y_train,
batch_size=100,
epochs=15)
Dieser Code geht davon aus, dass Sie einen Datensatz haben, der in den Speicher passt, und folglich, dass
Ihr Modell 15 Epochen durchlaufen kann, ohne dass die Gefahr eines Maschinenausfalls besteht.
auszufallen. Beide Annahmen sind unvernünftig - ML-Datensätze umfassen mehrere Terabyte, und
und wenn das Training Stunden dauern kann, ist die Wahrscheinlichkeit eines Maschinenausfalls hoch.

Um den vorhergehenden Code belastbarer zu machen, liefern Sie einen TensorFlow-Datensatz (nicht nur ein
NumPy-Array), weil der TensorFlow-Datensatz ein Out-of-Memory-Datensatz ist. Er pro-
Es bietet Iterationsfähigkeit und träges Laden. Der Code sieht nun wie folgt aus:

158 | Kapitel 4: Muster der Modellausbildung

cp_callback = tf.keras.callbacks.ModelCheckpoint(...)
history = model.fit(trainds,
validation_data=evalds,
epochs=15,
batch_size=128,
callbacks=[cp_callback])
Die Verwendung von Epochen bei großen Datensätzen ist jedoch nach wie vor eine schlechte Idee. Epochen mögen leicht zu verstehen sein
zu verstehen, aber die Verwendung von Epochen führt zu schlechten Effekten in realen ML-Modellen. Um
stellen Sie sich vor, Sie hätten einen Trainingsdatensatz mit einer Million Beispielen. Es kann
Es kann verlockend sein, diesen Datensatz einfach 15 Mal durchzugehen (zum Beispiel), indem man die
Anzahl der Epochen auf 15 setzt. Dabei gibt es mehrere Probleme:

Die Anzahl der Epochen ist eine ganze Zahl, aber der Unterschied in der Trainingszeit zwischen
der 14,3-fachen und der 15-fachen Verarbeitung des Datensatzes kann Stunden betragen. Wenn das Modell
konvergiert hat, nachdem es 14,3 Millionen Beispiele gesehen hat, sollten Sie es beenden und
nicht die Rechenressourcen verschwenden, die für die Verarbeitung von weiteren 0,7 Millionen
Beispiele.
Der Checkpoint wird einmal pro Epoche gesetzt, und die Wartezeit von einer Million Beispielen zwischen
Checkpoints zu warten, könnte viel zu lang sein. Um die Ausfallsicherheit zu erhöhen, sollten Sie den Checkpoint
öfters überprüfen.
Datensätze wachsen mit der Zeit. Wenn Sie 100.000 weitere Beispiele erhalten und das Modell trainieren
Modell trainieren und einen höheren Fehler erhalten, liegt es dann daran, dass Sie vorzeitig aufhören müssen, oder sind die
neuen Daten in irgendeiner Weise fehlerhaft? Das kann man nicht sagen, weil das vorherige Training auf
15 Millionen Beispielen und das neue Training auf 16,5 Millionen Beispielen basiert.
Beim verteilten Training mit Parameter-Servern (siehe "Entwurfsmuster 14: Verteilungs
Strategie" auf Seite 175) mit Datenparallelität und richtigem Shuffling ist das Konzept einer
Epoche nicht mehr klar. Wegen potenziell nachhinkender Arbeiter können Sie
können Sie das System nur anweisen, mit einer bestimmten Anzahl von Mini-Batches zu trainieren.
Schritte pro Epoche. Anstatt für 15 Epochen zu trainieren, könnte man beschließen, für
143.000 Schritte zu trainieren, wobei die batch_size 100 beträgt:

NUM_STEPS = 143000
BATCH_SIZE = 100
NUM_CHECKPOINTS = 15
cp_callback = tf.keras.callbacks.ModelCheckpoint(...)
history = model.fit(trainds,
validation_data=evalds,
epochs=NUM_CHECKPOINTS,
steps_per_epoch=NUM_STEPS // NUM_CHECKPOINTS,
batch_size=BATCH_SIZE,
callbacks=[cp_callback])
Entwurfsmuster 12: Prüfpunkte | 159
Jeder Schritt beinhaltet Gewichtungsaktualisierungen auf der Grundlage eines einzelnen Mini-Batches von Daten, und dies
ermöglicht es uns, bei 14,3 Epochen anzuhalten. Dies gibt uns viel mehr Granularität, aber wir müssen
eine "Epoche" als 1/15 der Gesamtzahl der Schritte definieren:

steps_per_epoch=NUM_STEPS // NUM_CHECKPOINTS,
Dies ist notwendig, um die richtige Anzahl von Kontrollpunkten zu erhalten. Es funktioniert, solange wir sicherstellen
sicherstellen, dass die Züge unendlich oft wiederholt werden:

trainds = trainds.repeat()
Die Wiederholung() ist erforderlich, weil wir num_epochs nicht mehr festlegen, so dass die Anzahl der
Epochen standardmäßig auf eins gesetzt. Ohne repeat() wird das Modell beendet, sobald die Trainings
Muster nach einmaligem Lesen des Datensatzes erschöpft sind.

Umlernen mit mehr Daten. Was passiert, wenn wir 100.000 weitere Beispiele erhalten? Ganz einfach!
Wir fügen sie zu unserem Data Warehouse hinzu, aktualisieren aber nicht den Code. Unser Code wird immer noch
143.000 Schritte verarbeiten, und er wird auch so viele Daten verarbeiten, nur dass 10 % der
der Beispiele, die er sieht, neuer sind. Wenn das Modell konvergiert, großartig. Wenn nicht, dann wissen wir
dass diese neuen Datenpunkte das Problem sind, weil wir nicht länger trainieren als
vorher. Indem wir die Anzahl der Schritte konstant halten, können wir die Auswirkungen
die Auswirkungen der neuen Daten vom Training mit mehr Daten zu trennen.

Sobald wir 143.000 Schritte trainiert haben, starten wir das Training erneut und lassen es etwas länger laufen
(z. B. 10.000 Schritte), und solange das Modell konvergiert, trainieren wir weiter
trainieren wir es länger. Dann aktualisieren wir die Zahl 143.000 im obigen Code (in Wirklichkeit ist es ein
ein Parameter für den Code), um die neue Anzahl von Schritten wiederzugeben.

Das funktioniert so lange gut, bis Sie eine Abstimmung der Hyperparameter vornehmen wollen. Wenn Sie die
Hyperparametertuning durchführen, werden Sie die Chargengröße ändern wollen. Unglücklicherweise
Wenn Sie die Stapelgröße auf 50 ändern, trainieren Sie leider nur noch die Hälfte der
Zeit, da wir 143.000 Schritte trainieren und jeder Schritt nur noch halb so lang ist wie
vorher. Das ist natürlich nicht gut.

Virtuelle Epochen. Die Antwort ist, die Gesamtzahl der dem Modell angezeigten Trainingsbeispiele
Modells (nicht die Anzahl der Schritte; siehe Abbildung 4-12) konstant zu halten:

NUM_TRAINING_EXAMPLES = 1000 * 1000
STOP_PUNKT = 14.3
TOTAL_TRAINING_EXAMPLES = int(STOP_POINT * NUM_TRAINING_EXAMPLES)
BATCH_SIZE = 100
NUM_CHECKPOINTS = 15
steps_per_epoch = (TOTAL_TRAINING_EXAMPLES //
(BATCH_SIZE*NUM_CHECKPOINTS))
cp_callback = tf.keras.callbacks.ModelCheckpoint(...)
history = model.fit(trainds,
validation_data=evalds,
epochs=NUM_CHECKPOINTS,
steps_per_epoch=steps_per_epoch,
160 | Kapitel 4: Modell-Trainingsmuster

batch_size=BATCH_SIZE,
callbacks=[cp_callback])
Abbildung 4-12. Definieren einer virtuellen Epoche in Form der gewünschten Anzahl von Schritten zwischen
Kontrollpunkten.

Wenn Sie mehr Daten erhalten, trainieren Sie zunächst mit den alten Einstellungen, erhöhen dann die Anzahl
der Beispiele, um die neuen Daten zu berücksichtigen, und ändern Sie schließlich den STOP_POINT, um die
wie oft Sie die Daten durchlaufen müssen, um Konvergenz zu erreichen.

Dies ist nun auch bei der Abstimmung der Hyperparameter (die später in diesem Kapitel besprochen wird) sicher
und hat den Vorteil, dass die Anzahl der Schritte konstant bleibt.

Entwurfsmuster 13: Transfer-Lernen
Beim Transfer Learning nehmen wir einen Teil eines zuvor trainierten Modells, frieren die Gewichte ein,
und bauen diese nicht trainierbaren Schichten in ein neues Modell ein, das ein ähnliches
Problem löst, aber mit einem kleineren Datensatz.

Problem
Das Training benutzerdefinierter ML-Modelle auf unstrukturierten Daten erfordert extrem große Datensätze,
die nicht immer ohne weiteres verfügbar sind. Nehmen wir den Fall eines Modells, das erkennt
ob ein Röntgenbild eines Arms einen Knochenbruch enthält. Um eine hohe Genauigkeit zu erreichen, benötigen Sie
braucht man Hunderttausende von Bildern, wenn nicht mehr. Bevor Ihr Modell lernt, wie ein
wie ein gebrochener Knochen aussieht, muss es zunächst lernen, die Pixel, Kanten und Formen
Formen, die Teil der Bilder in Ihrem Datensatz sind. Das Gleiche gilt für Modelle, die auf
auf Textdaten trainiert werden. Nehmen wir an, wir bauen ein Modell, das Beschreibungen von Patientensympathien aufnimmt
und die möglichen Bedingungen vorhersagt, die mit diesen Symptomen verbunden sind. Zusätzlich zum
zu lernen, welche Wörter eine Erkältung von einer Lungenentzündung unterscheiden, muss das Modell auch
muss das Modell auch die grundlegende Sprachsemantik lernen und wie die Abfolge der Wörter eine
Bedeutung erzeugt. So müsste das Modell beispielsweise nicht nur lernen, das Vorhandensein des Wortes
des Wortes Fieber zu erkennen, sondern auch, dass die Wortfolge kein Fieber eine ganz andere Bedeutung hat als
hohes Fieber.

Entwurfsmuster 13: Transferlernen | 161
2 MLPerf v0.7 Training Closed ResNet. Abgerufen von http://www.mlperf.org 23. September 2020, Eintrag 0.7-67.
MLPerf-Name und -Logo sind Warenzeichen. Siehe http://www.mlperf.org für weitere Informationen.
Um zu sehen, wie viele Daten erforderlich sind, um hochpräzise Modelle zu trainieren, können wir einen Blick auf
ImageNet, eine Datenbank mit über 14 Millionen beschrifteten Bildern. ImageNet wird häufig verwendet
als Benchmark für die Bewertung von Frameworks für maschinelles Lernen auf verschiedener Hardware verwendet. Als
Beispiel: Die MLPerf Benchmark Suite verwendet ImageNet, um die Zeit zu vergleichen, die
die Zeit zu vergleichen, die verschiedene ML-Frameworks auf unterschiedlicher Hardware benötigen, um eine Klassifizierungsgenauigkeit von 75,9 % zu erreichen.
tionsgenauigkeit zu erreichen. In den MLPerf-Trainingsergebnissen der Version 0.7 benötigte ein TensorFlow-Modell, das auf einer
Google TPU v3 etwa 30 Sekunden, um diese Zielgenauigkeit zu erreichen.^2 Mit mehr
Trainingszeit können die Modelle eine noch höhere Genauigkeit auf ImageNet erreichen. Dies ist jedoch
ist jedoch weitgehend auf die Größe von ImageNet zurückzuführen. Die meisten Organisationen mit speziellen Vorhersageproblemen
Probleme haben nicht annähernd so viele Daten zur Verfügung.

Da es sich bei Anwendungsfällen wie den oben beschriebenen Bild- und Textbeispielen um besonders
spezialisierte Datendomänen betreffen, ist es auch nicht möglich, ein Allzweckmodell zu verwenden, um
erfolgreich Knochenbrüche zu identifizieren oder Krankheiten zu diagnostizieren. Ein Modell, das auf
ImageNet trainiert wurde, kann ein Röntgenbild vielleicht als Röntgenbild oder medizinische Bildgebung bezeichnen, ist aber
aber es ist unwahrscheinlich, dass es einen gebrochenen Oberschenkelknochen erkennen kann. Da solche Modelle häufig
Kategorien trainiert werden, ist nicht zu erwarten, dass sie die Bedingungen
Bedingungen in den Bildern, die für unseren Datensatz spezifisch sind, zu verstehen. Um dies zu bewältigen,
brauchen wir eine Lösung, die es uns ermöglicht, ein benutzerdefiniertes Modell zu erstellen, das nur die Daten verwendet, die wir
verfügbaren Daten und mit den für uns wichtigen Bezeichnungen zu erstellen.

Lösung
Mit dem Entwurfsmuster "Transfer Learning" können wir ein Modell, das für ähnliche Daten trainiert wurde
Daten für eine ähnliche Aufgabe trainiert wurde, auf eine spezielle Aufgabe mit unseren
eigenen Daten. Mit "derselben Art von Daten" meinen wir die gleiche Datenmodalität - Bilder,
Text, und so weiter. Abgesehen von der breiten Kategorie wie Bilder ist es auch ideal, ein Modell zu verwenden
Modell zu verwenden, das zuvor mit denselben Arten von Bildern trainiert wurde. Verwenden Sie zum Beispiel ein
Modell, das zuvor auf Fotos trainiert wurde, wenn Sie es für die Klassifizierung von Fotos
Fotoklassifizierung verwendet werden soll, und ein Modell, das auf Fernerkundungsbildern trainiert wurde
Fernerkundungsbildern trainiert wurde, wenn Sie es zur Klassifizierung von Satellitenbildern verwenden wollen. Mit ähnlicher Aufgabe meinen wir
beziehen wir uns auf das zu lösende Problem. Um Transfer-Lernen für die Bildklassifizierung durchzuführen
ist es zum Beispiel besser, mit einem Modell zu beginnen, das für die Bildklassifizierung
Bildklassifizierung trainiert wurde und nicht für die Objekterkennung.

Fahren wir mit dem Beispiel fort und nehmen wir an, dass wir einen binären Klassifikator erstellen wollen, um festzustellen
ob das Bild einer Röntgenaufnahme einen Knochenbruch enthält. Wir haben nur 200 Bilder von
jeder Klasse: gebrochen und nicht gebrochen. Das ist nicht genug, um ein qualitativ hochwertiges Modell zu trainieren
von Grund auf zu trainieren, aber es reicht für das Transferlernen. Um dieses Problem mit Transfer
zu lösen, müssen wir ein Modell finden, das bereits auf einem großen Datensatz trainiert wurde, um

162 | Kapitel 4: Muster der Modellausbildung

3 Jia Deng et al., "ImageNet: A Large-Scale Hierarchical Image Database," IEEE Computer Society Conference
on Computer Vision and Pattern Recognition (CVPR) (2009): 248-255.
Bildklassifizierung durchführen. Wir entfernen dann die letzte Schicht aus diesem Modell, frieren die
Gewichte des Modells einfrieren und das Training mit unseren 400 Röntgenbildern fortsetzen. Wir würden idealerweise
Wir würden idealerweise ein Modell finden, das mit einem Datensatz trainiert wurde, der ähnliche Bilder wie unsere Röntgenbilder enthält, z. B. Bilder
die in einem Labor oder unter anderen kontrollierten Bedingungen aufgenommen wurden. Wir können jedoch immer noch Transfer
lernen, wenn die Datensätze unterschiedlich sind, solange die Vorhersageaufgabe dieselbe ist. In diesem
Fall machen wir eine Bildklassifizierung.

Sie können das Transfer-Lernen nicht nur für die Bildklassifizierung, sondern auch für viele andere Vorhersageaufgaben verwenden.
Bildklassifizierung verwenden, solange es ein bereits trainiertes Modell gibt, das der Aufgabe entspricht, die Sie
Aufgabe entspricht, die Sie mit Ihrem Datensatz durchführen möchten. Transfer Learning wird zum Beispiel auch häufig
in der Erkennung von Bildobjekten, der Übertragung von Bildstilen, der Bilderzeugung, der Textklassifizierung
Textklassifizierung, maschinelle Übersetzung und mehr.

Transferlernen funktioniert, weil es uns erlaubt, auf den Schultern von
auf den Schultern von Giganten stehen und Modelle nutzen, die bereits auf
extrem großen, beschrifteten Datensätzen trainiert wurden. Wir sind in der Lage, das Transfer-Lernen
Dank jahrelanger Forschung und Arbeit, die andere in die Erstellung dieser
diese Datensätze für uns erstellt haben, was den Stand der Technik beim
Transfer-Lernen. Ein Beispiel für einen solchen Datensatz ist das ImageNet
Projekt, das 2006 von Fei-Fei Li ins Leben gerufen und 2009 veröffentlicht wurde. Image-
Net^3 war für die Entwicklung des Transferlernens von entscheidender Bedeutung und
und ebnete den Weg für andere große Datensätze wie COCO und Open
Images.
Die Idee hinter dem Transfer-Lernen ist, dass Sie die Gewichte und Schichten eines Modells nutzen können, das in demselben
Modells, das in demselben Bereich wie die Vorhersageaufgabe trainiert wurde. Bei den meisten Deep Learning
Modellen enthält die letzte Schicht das Klassifizierungslabel oder die Ausgabe, die für Ihre Vorhersageaufgabe
Vorhersageaufgabe. Beim Transfer-Lernen entfernen wir diese Schicht, frieren die trainierten Gewichte des Modells ein
Gewichte des Modells ein und ersetzen die letzte Schicht durch die Ausgabe für unsere spezielle Vorhersageaufgabe
bevor wir mit dem Training fortfahren. In Abbildung 4-13 sehen Sie, wie dies funktioniert.

Normalerweise wird die vorletzte Schicht des Modells (die Schicht vor der Ausgabeschicht des Modells)
Schicht) als Engpass-Schicht gewählt. Als nächstes werden wir die Engpass-Schicht erklären, zusammen
zusammen mit verschiedenen Möglichkeiten, Transfer Learning in TensorFlow zu implementieren.

Entwurfsmuster 13: Transfer Learning | 163
Abbildung 4-13. Beim Transferlernen wird ein Modell auf einem großen Datensatz trainiert. Die "Spitze"
des Modells (in der Regel nur die Ausgabeschicht) wird entfernt und die Gewichte der übrigen Schichten werden
ihre Gewichte eingefroren. Die letzte Schicht des verbleibenden Modells wird als Bottleneck-Schicht bezeichnet.

Engpass-Schicht

Bezogen auf ein Gesamtmodell stellt die Engpassschicht die Eingabe (in der Regel ein
Bild- oder Textdokument) im Raum mit der niedrigsten Dimensionalität dar. Genauer gesagt, wenn
wir Daten in unser Modell einspeisen, sehen die ersten Schichten diese Daten fast in ihrer ursprünglichen Form.
Um zu sehen, wie das funktioniert, fahren wir mit einem Beispiel aus der medizinischen Bildgebung fort, aber dieses Mal
erstellen wir ein Modell mit einem kolorektalen Histologiedatensatz, um die Histologiebilder
in eine von acht Kategorien einzuordnen.

Um das Modell zu untersuchen, das wir für das Transferlernen verwenden werden, laden wir die VGG
Modellarchitektur, die zuvor mit dem ImageNet-Datensatz trainiert wurde:

vgg_model_withtop = tf.keras.applications.VGG19(
include_top=True,
weights='imagenet',
)
164 | Kapitel 4: Modell-Trainings-Muster

Beachten Sie, dass wir include_top=True eingestellt haben, was bedeutet, dass wir das gesamte VGG
Modell, einschließlich der Ausgabeschicht. Bei ImageNet klassifiziert das Modell Bilder in
1.000 verschiedene Klassen ein, also ist die Ausgabeschicht ein Array mit 1.000 Elementen. Schauen wir uns die
Ausgabe von model.summary() an, um zu verstehen, welche Schicht als Engpass verwendet wird.
Der Kürze halber haben wir hier einige der mittleren Schichten weggelassen:

Modell: "vgg19"
_________________________________________________________________
Ebene (Typ) Ausgang Form Param #
=================================================================
input_3 (InputLayer) [(None, 224, 224, 3)] 0
_________________________________________________________________
block1_conv1 (Conv2D) (Keine, 224, 224, 64) 1792
...mehr Schichten hier...
_________________________________________________________________
block5_conv3 (Conv2D) (Keine, 14, 14, 512) 2359808
_________________________________________________________________
block5_conv4 (Conv2D) (Keine, 14, 14, 512) 2359808
_________________________________________________________________
block5_pool (MaxPooling2D) (Keine, 7, 7, 512) 0
_________________________________________________________________
flatten (Abflachen) (Keine, 25088) 0
_________________________________________________________________
fc1 (Dense) (Keine, 4096) 102764544
_________________________________________________________________
fc2 (Dicht) (Keine, 4096) 16781312
_________________________________________________________________
Vorhersagen (Dicht) (Keine, 1000) 4097000
=================================================================
Params insgesamt: 143,667,240
Trainierbare Parameter: 143,667,240
Nicht trainierbare Parameter: 0
_________________________________________________________________
Wie Sie sehen können, akzeptiert das VGG-Modell Bilder als ein 224×224×3-Pixel-Array. Dieses 128-
Element-Array wird dann durch aufeinanderfolgende Schichten geleitet (von denen jede die
Dimensionalität des Feldes ändern kann), bis es in der Ebene "flatten" zu einem 25.088×1-dimensionalen Feld
der Schicht namens flatten. Schließlich wird es in die Ausgabeschicht eingespeist, die ein 1.000-
Element-Array zurückgibt (für jede Klasse im ImageNet). In diesem Beispiel wählen wir die
block5_pool-Schicht als Engpass-Schicht, wenn wir dieses Modell für das Training auf
medizinischen Histologiebildern zu trainieren. Die Engpass-Schicht erzeugt ein 7×7×512-dimensionales
Array, das eine niedrigdimensionale Darstellung des Eingabebildes ist. Sie hat
genug Informationen aus dem Eingangsbild erhalten, um es zu klassifizieren. Wenn wir
Wenn wir dieses Modell auf unsere Aufgabe der Klassifizierung medizinischer Bilder anwenden, hoffen wir, dass die Informa
Informationsdestillation ausreichen wird, um eine erfolgreiche Klassifizierung in unserem
Datensatz zu klassifizieren.

Entwurfsmuster 13: Transfer Learning | 165
Der Histologie-Datensatz enthält Bilder in Form von (150,150,3)-dimensionalen Arrays. Diese
150×150×3 Darstellung ist die höchste Dimensionalität. Zur Verwendung des VGG-Modells mit
mit unseren Bilddaten zu verwenden, können wir es mit dem Folgenden laden:

vgg_model = tf.keras.applications.VGG19(
include_top=False,
weights='imagenet',
input_shape=((150,150,3))
)
vgg_model.trainable = False
Durch die Einstellung include_top=False legen wir fest, dass die letzte VGG-Schicht, die wir laden wollen
die Engpass-Schicht ist. Die input_shape, die wir übergeben haben, entspricht der Eingabeform
unserer Histologie-Bilder. Eine Zusammenfassung der letzten paar Schichten dieses aktualisierten VGG-Modells
sieht wie folgt aus:

block5_conv3 (Conv2D) (Keine, 9, 9, 512) 2359808
_________________________________________________________________
block5_conv4 (Conv2D) (Keiner, 9, 9, 512) 2359808
_________________________________________________________________
block5_pool (MaxPooling2D) (Keine, 4, 4, 512) 0
=================================================================
Parameter insgesamt: 20.024.384
Trainierbare Parameter: 0
Nicht trainierbare Parameter: 20.024.384
_________________________________________________________________
Die letzte Schicht ist nun unsere Engpass-Schicht. Sie werden feststellen, dass die Größe von
block5_pool (4,4,512) ist, während sie vorher (7,7,512) war. Das liegt daran, dass wir VGG mit einer
VGG mit einem input_shape-Parameter instanziiert haben, um der Größe der Bilder in
unseres Datensatzes zu berücksichtigen. Es ist auch erwähnenswert, dass die Einstellung include_top=False hart kodiert ist, um
block5_pool als Engpassschicht zu verwenden, aber wenn Sie dies anpassen möchten, können Sie
können Sie das vollständige Modell laden und alle zusätzlichen Ebenen löschen, die Sie nicht verwenden möchten.

Bevor dieses Modell trainiert werden kann, müssen wir noch einige Schichten hinzufügen, die speziell auf unsere Daten und unsere Klassifizierungsaufgabe zugeschnitten sind.
für unsere Daten und Klassifizierungsaufgabe. Es ist auch wichtig zu beachten, dass aufgrund der Einstellung
trainable=False gesetzt haben, gibt es 0 trainierbare Parameter im aktuellen Modell.

Als allgemeine Faustregel gilt, dass die Engpassschicht in der Regel die letzte Schicht ist,
Ebene mit der niedrigsten Dimension, die vor einer Glättungsoperation geglättet wird.
166 | Kapitel 4: Muster für das Modelltraining

Da sie beide Merkmale in reduzierter Dimensionalität darstellen, sind Bottleneck-Schichten
konzeptionell ähnlich wie Einbettungen. Zum Beispiel ist in einem Autoencoder-Modell mit einer
Encoder-Decoder-Architektur ist die Engpassschicht eine Einbettung. In diesem Fall dient die
Engpass als mittlere Schicht des Modells und bildet die ursprünglichen Eingabedaten auf eine
eine Repräsentation niedrigerer Dimensionalität, die der Decoder (die zweite Hälfte des Netzes)
(die zweite Hälfte des Netzes) verwendet, um die Eingabedaten wieder auf ihre ursprüngliche, höherdimensionale Darstellung abzubilden.
Ein Diagramm der Bottleneck-Schicht in einem Autoencoder finden Sie in Abbildung 2-13 in
Kapitel 2.

Eine Einbettungsschicht ist im Wesentlichen eine Nachschlagetabelle mit Gewichten, die ein bestimmtes Merkmal auf eine Dimension im Vektorraum abbildet.
Merkmal auf eine Dimension im Vektorraum abbildet. Der Hauptunterschied besteht darin, dass die Gewichte in einer
Gewichte in einer Einbettungsschicht trainiert werden können, während die Gewichte aller Schichten, die bis zur
Engpassschicht ihre Gewichte eingefroren haben. Mit anderen Worten: Das gesamte Netz bis zur
und einschließlich der Engpassschicht ist nicht trainierbar, und die Gewichte in den Schichten nach
der Engpassschicht sind die einzigen trainierbaren Schichten des Modells.

Es ist auch erwähnenswert, dass vortrainierte Einbettungen in
dem Entwurfsmuster Transfer Learning verwendet werden können. Wenn Sie ein Modell erstellen, das
eine Einbettungsschicht enthält, können Sie entweder eine vorhandene
(vortrainierte) Einbettungssuche verwenden oder eine eigene Einbettungsschicht
Schicht von Grund auf trainieren.
Zusammenfassend lässt sich sagen, dass Transfer Learning eine Lösung ist, mit der Sie ein ähnliches Problem auf einem kleineren Datensatz lösen können.
Problem auf einem kleineren Datensatz zu lösen. Beim Transfer-Lernen wird immer eine Engpass-Schicht verwendet
mit nicht trainierbaren, eingefrorenen Gewichten. Einbettungen sind eine Art der Datendarstellung.
Letztlich kommt es auf den Zweck an. Wenn der Zweck darin besteht, ein ähnliches Modell zu trainieren, würde man
würde man Transfer-Lernen verwenden. Wenn der Zweck also darin besteht, ein Eingabebild prägnanter darzustellen
Bild prägnanter darzustellen, würde man eine Einbettung verwenden. Der Code könnte genau derselbe sein
gleich sein.

Umsetzung des Transferlernens

Sie können Transfer Learning in Keras mit einer dieser beiden Methoden implementieren:

Laden Sie ein vortrainiertes Modell selbst, entfernen Sie die Schichten nach dem Flaschenhals
Flaschenhals, und Hinzufügen einer neuen letzten Schicht mit Ihren eigenen Daten und Beschriftungen
Verwendung eines vortrainierten TensorFlow Hub Moduls als Grundlage für Ihre Transfer
Lernaufgabe
Beginnen wir mit der Frage, wie Sie ein vortrainiertes Modell laden und selbst verwenden können. Für
bauen wir auf dem Beispiel des VGG-Modells auf, das wir zuvor vorgestellt haben. Beachten Sie, dass es sich bei VGG
eine Modellarchitektur ist, während ImageNet die Daten sind, mit denen es trainiert wurde. Zusammen bilden diese
bilden sie das vortrainierte Modell, das wir für das Transfer-Lernen verwenden werden. Hier verwenden wir
Transfer-Lernen, um kolorektale Histologie-Bilder zu klassifizieren. Während das Original

Entwurfsmuster 13: Transfer Learning | 167
ImageNet-Datensatz 1.000 Bezeichnungen enthält, wird unser resultierendes Modell nur 8 mögliche Klassen zurückgeben.
Klassen zurück, die wir spezifizieren werden, im Gegensatz zu den Tausenden von Bezeichnungen in
ImageNet.

Das Laden eines zuvor trainierten Modells und dessen Verwendung zur Klassifizierung
Modells zu erhalten, ist kein Transfer-Lernen.
lich. Beim Transferlernen geht man einen Schritt weiter und ersetzt die letzten
Schichten des Modells durch Ihre eigene Vorhersageaufgabe.
Das VGG-Modell, das wir geladen haben, wird unser Basismodell sein. Wir müssen ein paar Schichten hinzufügen
um die Ausgabe unserer Engpassschicht zu glätten und diese geglättete Ausgabe in ein 8-
Element-Softmax-Array einspeisen:

global_avg_layer = tf.keras.layers.GlobalAveragePooling2D()
feature_batch_avg = global_avg_layer(feature_batch)
vorhersage_schicht = tf.keras.layers.Dense(8, activation='softmax')
prediction_batch = prediction_layer(feature_batch_avg)
Schließlich können wir die Sequential, API verwenden, um unser neues Transfer-Learning-Modell zu erstellen als
einen Stapel von Schichten:

histology_model = keras.Sequential([
vgg_model,
global_avg_layer,
vorhersage_schicht
])
Sehen wir uns die Ausgabe von model.summary() für unser Transfer-Learning-Modell an:

_________________________________________________________________
Ebene (Typ) Ausgabeform Param #
=================================================================
vgg19 (Modell) (Keine, 4, 4, 512) 20024384
_________________________________________________________________
global_average_pooling2d (Gl (Keine, 512) 0
_________________________________________________________________
dense (Dense) (Keine, 8) 4104
=================================================================
Parameter insgesamt: 20.028.488
Trainierbare Parameter: 4,104
Nicht trainierbare Parameter: 20.024.384
_________________________________________________________________
Wichtig ist hier, dass die einzigen trainierbaren Parameter die nach unserer
Engpass-Schicht. In diesem Beispiel sind die Engpass-Schicht die Merkmalsvektoren aus dem
VGG-Modell. Nachdem wir dieses Modell kompiliert haben, können wir es mit unserem Datensatz von Histol-
histologischen Bildern trainieren.

168 | Kapitel 4: Muster der Modellausbildung

Vorgefertigte Einbettungen

Wir können zwar selbst ein vortrainiertes Modell laden, aber wir können auch Transfer
Transfer Learning implementieren, indem wir die vielen vortrainierten Modelle nutzen, die in TF Hub, einer Bibliothek
von vortrainierten Modellen (Module genannt). Diese Module decken eine Vielzahl von Daten
Domänen und Anwendungsfälle, einschließlich Klassifizierung, Objekterkennung, maschinelle Übersetzung
und mehr. In TensorFlow können Sie diese Module als eine Schicht laden und dann Ihre eigene
Klassifizierungsschicht darüber legen.

Um zu sehen, wie TF Hub funktioniert, wollen wir ein Modell erstellen, das Filmkritiken entweder als
positiv oder negativ klassifiziert. Zunächst laden wir ein vortrainiertes Einbettungsmodell, das auf einem
großen Korpus von Nachrichtenartikeln trainiert. Wir können dieses Modell als hub.KerasLayer instanziieren:

hub_layer = hub.KerasLayer(
"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1",
input_shape=[], dtype=tf.string, trainable=True)
Wir können weitere Schichten darauf stapeln, um unseren Klassifikator zu erstellen:

model = keras.Sequential([
hub_layer,
keras.layers.Dense(32, activation='relu'),
keras.layers.Dense(1, activation='sigmoid')
])
Wir können dieses Modell nun trainieren, indem wir ihm unseren eigenen Textdatensatz als Eingabe übergeben. Die resultierende
Vorhersage wird ein 1-Element-Array sein, das angibt, ob unser Modell den gegebenen Text
Text positiv oder negativ ist.

Warum es funktioniert
Um zu verstehen, warum Transferlernen funktioniert, sollten wir uns zunächst eine Analogie ansehen. Wenn Kinder
Wenn Kinder ihre erste Sprache lernen, werden sie mit vielen Beispielen konfrontiert und korrigiert
wenn sie etwas falsch identifizieren. Wenn sie zum Beispiel zum ersten Mal lernen, eine Katze zu identifizieren
Katze zu identifizieren, sehen sie, wie ihre Eltern auf die Katze zeigen und das Wort Katze sagen, und diese Wiederholung
stärkt die Bahnen in ihrem Gehirn. In ähnlicher Weise werden sie korrigiert, wenn sie Katze sagen
sagen und damit ein Tier meinen, das keine Katze ist. Wenn das Kind dann lernt, einen Hund zu identifizieren
Hund zu identifizieren, muss es nicht bei Null anfangen. Es kann einen ähnlichen Erkennungsprozess anwenden
Erkennungsprozess anwenden, den es für die Katze verwendet hat, ihn aber auf eine etwas andere Aufgabe anwenden. Auf diese Weise hat das
hat das Kind eine Grundlage für das Lernen geschaffen. Es lernt nicht nur neue Dinge, sondern auch
auch gelernt, wie man neue Dinge lernt. Die Anwendung dieser Lernmethoden auf andere
Die Anwendung dieser Lernmethoden auf verschiedene Bereiche entspricht in etwa der Funktionsweise des Transferlernens.

Wie sieht das bei neuronalen Netzen aus? In einem typischen faltigen neuronalen Netz
(CNN) ist das Lernen hierarchisch aufgebaut. Die ersten Schichten lernen das Erkennen von Kanten und
Formen in einem Bild zu erkennen. Im Beispiel der Katze könnte dies bedeuten, dass das Modell
Bereiche in einem Bild identifizieren kann, in denen der Rand des Katzenkörpers auf den Hintergrund trifft.
Die nächsten Schichten des Modells beginnen, Gruppen von Kanten zu verstehen, z. B. dass

Entwurfsmuster 13: Transfer Learning | 169
es gibt zwei Kanten, die sich in der oberen linken Ecke des Bildes treffen. Die letzten Schichten eines CNN
Schichten eines CNN können dann diese Gruppen von Kanten zusammensetzen und ein Verständnis für
verschiedene Merkmale des Bildes. Im Beispiel der Katze könnte das Modell zwei dreieckige
zwei dreieckige Formen im oberen Teil des Bildes und zwei ovale Formen darunter
zu erkennen. Als Menschen wissen wir, dass diese dreieckigen Formen Ohren sind und die ovalen Formen
Augen sind.

Wir können diesen Prozess in Abbildung 4-14 veranschaulichen, die aus der Forschung von Zeiler und Fergus stammt
CNNs zu dekonstruieren, um die verschiedenen Merkmale zu verstehen, die in jeder
in jeder Schicht des Modells aktiviert wurden. Für jede Schicht in einem fünfschichtigen CNN zeigt dies ein
die Merkmalskarte eines Bildes für eine bestimmte Schicht neben dem eigentlichen Bild. So können wir sehen, wie
die Wahrnehmung eines Bildes durch das Modell fortschreitet, während es sich durch das Netzwerk bewegt.
Die Schichten 1 und 2 erkennen nur Kanten, Schicht 3 beginnt, Objekte zu erkennen, und die Schichten 4
und 5 können Brennpunkte innerhalb des gesamten Bildes erkennen.

Bedenken Sie jedoch, dass es sich für unser Modell lediglich um Gruppierungen von Pixelwerten handelt. Es
weiß nicht, dass die dreieckigen und ovalen Formen Ohren und Augen sind - es weiß nur, dass es
Es weiß nur, dass es bestimmte Gruppierungen von Merkmalen mit den Bezeichnungen assoziiert, auf die es trainiert wurde. Auf diese Weise
Auf diese Weise unterscheidet sich der Lernprozess des Modells, welche Gruppen von Merkmalen eine Katze ausmachen, nicht
nicht viel anders als das Lernen von Merkmalsgruppen, die Teil anderer Objekte sind, wie
einem Tisch, einem Berg oder sogar einem Prominenten. Für ein Modell sind das alles nur verschiedene Kombi
nationen von Pixelwerten, Kanten und Formen.

170 | Kapitel 4: Muster der Modellausbildung

Abbildung 4-14. Die Forschung von Zeiler und Fergus (2013) zur Dekonstruktion von CNNs hilft uns
zu veranschaulichen, wie ein CNN Bilder auf jeder Schicht des Netzwerks sieht.

Entwurfsmuster 13: Transferlernen | 171
Kompromisse und Alternativen
Bisher haben wir noch keine Methoden zur Änderung der Gewichte unseres ursprünglichen Modells
bei der Implementierung von Transfer Learning. Hier werden wir zwei Ansätze dafür untersuchen:
Merkmalsextraktion und Feinabstimmung. Wir werden auch erörtern, warum sich das Transfer-Lernen in erster Linie auf Bild- und
auf Bild- und Textmodelle konzentriert und die Beziehung zwischen Textsenseneinbettungen und Transferlernen betrachtet.
Einbettung von Texten und Transferlernen.

Feinabstimmung versus Merkmalsextraktion

Die Merkmalsextraktion beschreibt einen Ansatz des Transfer-Lernens, bei dem man die
Gewichte aller Schichten vor der Engpass-Schicht einfrieren und die folgenden Schichten auf Ihren
eigenen Daten und Beschriftungen trainieren. Eine andere Möglichkeit ist die Feinabstimmung der Gewichte der bereits
Schichten des trainierten Modells. Bei der Feinabstimmung können Sie entweder die Gewichte der einzelnen
Schicht des vortrainierten Modells aktualisieren, oder nur einige der Schichten direkt vor dem Engpass.
Das Training eines Transfer-Learning-Modells mit Feinabstimmung dauert in der Regel länger als die Merkmalsextraktion.
als die Extraktion von Merkmalen. In unserem obigen Beispiel zur Textklassifizierung haben wir
trainable=True gesetzt haben, als wir unsere TF-Hub-Schicht initialisiert haben. Dies ist ein Beispiel für die Fein
Abstimmung.

Bei der Feinabstimmung ist es üblich, die Gewichte der ersten Schichten des Modells unberührt zu lassen.
zen, da diese Schichten darauf trainiert wurden, grundlegende Merkmale zu erkennen, die oft
die häufig bei vielen Bildtypen vorkommen. Um zum Beispiel ein MobileNet-Modell fein abzustimmen
würde man trainable=False nur für eine Teilmenge der Schichten des Modells festlegen, anstatt alle
jede Schicht nicht trainierbar zu machen. Um zum Beispiel nach der 100. Schicht eine Feinabstimmung vorzunehmen, könnte man
ausführen:

base_model = tf.keras.applications.MobileNetV2(input_shape=(160,160,3),
include_top=False,
weights='imagenet')
for layer in base_model.layers[:100]:
layer.trainable = False
Ein empfohlener Ansatz, um zu bestimmen, wie viele Schichten eingefroren werden sollen, ist die sogenannte
schrittweise Feinabstimmung bekannt und beinhaltet das iterative Aufheben des Einfrierens von Schichten nach jedem Trainingslauf
trainieren, um die ideale Anzahl von Schichten für die Feinabstimmung zu finden. Dies funktioniert am besten und ist am
am effizientesten, wenn Sie die Lernrate niedrig halten (0,001 ist üblich) und die Anzahl der
Trainingsiterationen relativ klein ist. Um eine progressive Feinabstimmung durchzuführen, beginnen Sie mit
nur die letzte Schicht des übertragenen Modells auftauen (die Schicht, die dem Ergebnis am nächsten ist)
put) und berechnen Sie den Verlust Ihres Modells nach dem Training. Dann tauen Sie nach und nach weitere Schichten auf
Schichten auf, bis Sie die Eingabeschicht erreichen oder bis der Verlust ein Plateau erreicht hat. Verwenden Sie dies, um
die Anzahl der Schichten für die Feinabstimmung.

Wie sollten Sie entscheiden, ob Sie eine Feinabstimmung vornehmen oder alle Schichten Ihres zuvor
trainierten Modells? Wenn Sie einen kleinen Datensatz haben, ist es in der Regel am besten, die

172 | Kapitel 4: Muster der Modellausbildung

4 Für weitere Informationen siehe "CS231n Convolutional Neural Networks for Visual Recognition".
vortrainierten Modells als Merkmalsextraktor zu verwenden, anstatt eine Feinabstimmung vorzunehmen. Wenn Sie ein Modell neu trainieren
die Gewichte eines Modells, das wahrscheinlich mit Tausenden oder Millionen von Beispielen trainiert wurde,
kann die Feinabstimmung dazu führen, dass sich das aktualisierte Modell zu sehr an Ihren kleinen Datensatz anpasst und die
die allgemeineren Informationen, die aus diesen Millionen von Beispielen gelernt wurden. Obwohl es
von Ihren Daten und Ihrer Vorhersageaufgabe abhängt, meinen wir mit "kleinem Datensatz" hier
Datensätze mit Hunderten oder ein paar Tausend Trainingsbeispielen.

Ein weiterer Faktor, der bei der Entscheidung über die Feinabstimmung zu berücksichtigen ist, ist, wie ähnlich
wie ähnlich Ihre Vorhersageaufgabe der des ursprünglich trainierten Modells ist, das Sie verwenden.
Wenn die Vorhersageaufgabe ähnlich ist oder eine Fortsetzung des vorherigen Trainings darstellt, wie es bei unserem
wie bei unserem Modell für die Stimmungsanalyse von Filmrezensionen, kann die Feinabstimmung zu
genaueren Ergebnissen führen. Wenn die Aufgabe eine andere ist oder die Datensätze sich deutlich unterscheiden,
ist es am besten, alle Schichten des zuvor trainierten Modells einzufrieren, anstatt sie fein abzustimmen.
Tabelle 4-1 fasst die wichtigsten Punkte zusammen.^4

Tabelle 4-1. Kriterien für die Wahl zwischen Merkmalsextraktion und Feinabstimmung

Kriterium Merkmalsextraktion Feinabstimmung
Wie groß ist der Datensatz? Klein Groß
Ist Ihre Vorhersageaufgabe die gleiche wie die des vortrainierten
Modells?
Unterschiedliche Aufgaben Gleiche Aufgabe, oder ähnliche Aufgabe mit gleicher
Klassenverteilung der Bezeichnungen
Budget für Trainingszeit und Rechenkosten Niedrig Hoch
In unserem Textbeispiel wurde das vortrainierte Modell auf einem Nachrichtenkorpus trainiert.
unser Anwendungsfall war die Stimmungsanalyse. Da diese Aufgaben unterschiedlich sind, sollten wir das
das ursprüngliche Modell als Merkmalsextraktor verwenden, anstatt es zu verfeinern. Ein Beispiel für unterschiedliche
Vorhersageaufgaben in einer Bilddomäne könnte die Verwendung unseres MobileNet-Modells
das auf ImageNet trainiert wurde, als Grundlage für das Transfer-Lernen auf einen Datensatz mit medizinischen
Bilder. Obwohl es bei beiden Aufgaben um die Klassifizierung von Bildern geht, ist die Art der Bilder in
die Art der Bilder in jedem Datensatz sehr unterschiedlich.

Schwerpunkt auf Bild- und Textmodellen

Sie haben vielleicht bemerkt, dass sich alle Beispiele in diesem Abschnitt auf Bild- und
Textdaten beziehen. Das liegt daran, dass das Transfer-Lernen in erster Linie für Fälle gedacht ist, in denen man
eine ähnliche Aufgabe auf dieselbe Datendomäne anwenden kann. Modelle, die mit Tabellendaten trainiert wurden, decken jedoch
decken jedoch eine potenziell unendliche Anzahl möglicher Vorhersageaufgaben und Datentypen ab. Sie
können ein Modell auf Tabellendaten trainieren, um vorherzusagen, wie die Preise für Eintrittskarten für Ihre
Veranstaltung, die Wahrscheinlichkeit, dass jemand einen Kredit nicht zurückzahlt, die Einnahmen Ihres Unternehmens
Ihres Unternehmens im nächsten Quartal, die Dauer einer Taxifahrt und so weiter. Die spezifischen Daten für diese Aufgaben
sind ebenfalls unglaublich vielfältig, wobei das Ticketproblem von Informationen über

Entwurfsmuster 13: Transfer Learning | 173
Künstler und Veranstaltungsorte, das Kreditproblem auf das persönliche Einkommen und die Taxidauer auf
städtische Verkehrsmuster. Aus diesen Gründen gibt es inhärente Herausforderungen beim Übertragen
von einem tabellarischen Modell auf ein anderes zu übertragen.

Obwohl das Transfer-Lernen bei tabellarischen Daten noch nicht so weit verbreitet ist wie bei Bildern und
Textbereichen üblich ist, stellt eine neue Modellarchitektur namens TabNet neue Forschungsergebnisse in diesem
Bereich. Die meisten tabellarischen Modelle erfordern im Vergleich zu Bild- und Textmodellen
mit Bild- und Textmodellen. TabNet setzt eine Technik ein, die zunächst unüberwachtes
Repräsentationen für tabellarische Merkmale erlernt und dann diese Repräsentationen fein
erlernten Repräsentationen, um Vorhersagen zu erstellen. Auf diese Weise automatisiert TabNet das Fea-
ture engineering für tabellarische Modelle.

Einbettung von Wörtern und Sätzen

In unserer bisherigen Diskussion über Texteinbettungen haben wir uns hauptsächlich auf Worteinbettungen bezogen.
Einbettungen. Eine andere Art der Texteinbettung ist die Satzeinbettung. Während Worteinbettungen
Einbettungen einzelne Wörter in einem Vektorraum darstellen, stellen Satzeinbettungen
ganze Sätze. Folglich sind Worteinbettungen kontextunabhängig. Schauen wir uns das
wie sich das auf den folgenden Satz auswirkt:

"Ich habe dir frisch gebackene Kekse auf die linke Seite des Küchentischs gelegt."

Beachten Sie, dass das Wort links in diesem Satz zweimal vorkommt, zuerst als Verb und dann als
Adjektiv. Wenn wir für diesen Satz Worteinbettungen generieren würden, bekämen wir für jedes Wort ein sepa-
raten-Array für jedes Wort. Mit Worteinbettungen wäre das Array für beide Instanzen des
Instanzen des Wortes links das gleiche sein. Bei der Verwendung von Einbettungen auf Satzebene erhalten wir jedoch einen
einen einzigen Vektor, der den gesamten Satz repräsentiert. Es gibt verschiedene Ansätze zur Generierung
Satzeinbettungen zu generieren - von der Durchschnittsbildung der Worteinbettungen eines Satzes bis hin zum Training
bis hin zum Trainieren eines überwachten Lernmodells auf einem großen Textkorpus, um die Einbettungen zu erzeugen.

Wie verhält sich dies zum Transferlernen? Die letztgenannte Methode - das Trainieren eines überwachten
Lernmodells zur Erzeugung von Einbettungen auf Satzebene - ist eigentlich eine Form des Transfer
Lernens. Dieser Ansatz wird von Googles Universal Sentence Encoder (verfügbar in
verfügbar in TF Hub) und BERT. Diese Methoden unterscheiden sich von den Worteinbettungen dadurch, dass sie
dass sie über das einfache Nachschlagen von Gewichten für einzelne Wörter hinausgehen. Stattdessen wurden sie
ein Modell auf einem großen Datensatz mit unterschiedlichen Texten trainiert, um die Bedeutung
Bedeutung zu verstehen, die durch Wortfolgen vermittelt wird. Auf diese Weise sind sie so konzipiert, dass sie
auf verschiedene natürlichsprachliche Aufgaben übertragen werden und können daher zum Aufbau von Modellen verwendet werden, die
Transferlernen implementieren.

174 | Kapitel 4: Muster der Modellausbildung

Entwurfsmuster 14: Verteilungsstrategie
Bei der Verteilungsstrategie wird die Trainingsschleife in großem Umfang über mehrere Worker ausgeführt.
Worker ausgeführt, oft mit Caching, Hardwarebeschleunigung und Parallelisierung.

Problem
Heutzutage ist es üblich, dass große neuronale Netze Millionen von Parametern haben
und werden anhand großer Datenmengen trainiert. Es hat sich sogar gezeigt, dass die Erhöhung
Skalierung des Deep Learning in Bezug auf die Anzahl der Trainingsbeispiele, die Anzahl der
der Modellparameter oder beides die Modellleistung drastisch verbessert. Allerdings
Mit zunehmender Größe der Modelle und Daten steigt jedoch der Rechen- und Speicherbedarf
und Speicherbedarf proportional an, so dass die Zeit, die für das Training dieser Modelle benötigt wird, eines der größten
größten Probleme des Deep Learning.

GPUs sorgen für einen erheblichen Rechenschub und bringen die Trainingszeit von
tiefe neuronale Netze mittlerer Größe in Reichweite. Für sehr große Modelle, die auf
Datenmengen trainiert werden, reichen einzelne GPUs nicht aus, um die Trainingszeit
Zeit erträglich zu machen. Zum Zeitpunkt der Erstellung dieses Dokuments wurde beispielsweise das Training von ResNet-50 auf dem Bench
mark ImageNet-Datensatz für 90 Epochen auf einem einzelnen NVIDIA M40-Grafikprozessor 10^18
einfache Präzisionsoperationen und dauert 14 Tage. Da die KI immer mehr zur Lösung von
um Probleme in komplexen Domänen zu lösen, und Open-Source-Bibliotheken wie Tensorflow
und PyTorch die Erstellung von Deep-Learning-Modellen zugänglicher machen, sind große neuronale Netze
vergleichbar mit ResNet-50 zur Norm geworden.

Das ist ein Problem. Wenn es zwei Wochen dauert, Ihr neuronales Netz zu trainieren, dann müssen Sie
zwei Wochen warten, bevor Sie neue Ideen ausprobieren oder mit den Einstellungen experimentieren können.
Einstellungen. Außerdem ist es für einige komplexe Probleme wie medizinische Bildgebung, autonomes Fahren oder Sprachübersetzung
autonomes Fahren oder Sprachübersetzung ist es nicht immer möglich, das Problem in kleinere
in kleinere Komponenten aufzuteilen oder nur mit einer Teilmenge der Daten zu arbeiten. Nur mit
der gesamten Daten kann man beurteilen, ob etwas funktioniert oder nicht.

Trainingszeit bedeutet im wahrsten Sinne des Wortes Geld. In der Welt des serverlosen maschinellen Lernens
maschinellen Lernens ist es möglich, Trainingsaufträge über einen Cloud-Service zu erteilen, bei dem die Trainingszeit in Rechnung gestellt wird, anstatt einen eigenen teuren Grafikprozessor zu kaufen.
Trainingsaufträge über einen Cloud-Dienst zu erteilen, bei dem die Trainingszeit in Rechnung gestellt wird. Die Kosten für das Training
Die Kosten für das Training eines Modells, sei es für einen Grafikprozessor oder für einen serverlosen Trainingsservice, summieren sich schnell,
summieren sich schnell.

Gibt es eine Möglichkeit, das Training dieser großen neuronalen Netze zu beschleunigen?

Lösung
Eine Möglichkeit, die Ausbildung zu beschleunigen, sind Verteilungsstrategien in der Ausbildungsschleife.
Es gibt verschiedene Verteilungstechniken, aber die gemeinsame Idee ist, den Aufwand
des Modells auf mehrere Maschinen zu verteilen. Es gibt zwei Möglichkeiten, dies zu tun:

Entwurfsmuster 14: Verteilungsstrategie | 175
Datenparallelität und Modellparallelität. Bei der Datenparallelität werden die Berechnungen
Rechner aufgeteilt, und verschiedene Arbeiter trainieren auf verschiedenen Teilmengen der
Trainingsdaten. Bei der Modellparallelität wird das Modell aufgeteilt und verschiedene Arbeiter führen die
die Berechnungen für verschiedene Teile des Modells. In diesem Abschnitt werden wir uns auf die Daten
Parallelität und zeigen Implementierungen in TensorFlow unter Verwendung der tf.distrib
ute.Strategy Bibliothek. Wir werden die Modellparallelität in "Kompromisse und Alternativen
tives" auf Seite 183.

Um Datenparallelität zu implementieren, muss eine Methode vorhanden sein, mit der verschiedene Arbeiter
Gradienten zu berechnen und diese Informationen gemeinsam zu nutzen, um Aktualisierungen an den
Parameter vorzunehmen. Dadurch wird sichergestellt, dass alle Arbeiter konsistent sind und jeder Gradientenschritt
um das Modell zu trainieren. Grob gesagt kann die Datenparallelität entweder syn-
chronisch oder asynchron erfolgen.

Synchrone Ausbildung

Beim synchronen Training trainieren die Worker parallel auf verschiedenen Slices von Eingabedaten
und die Gradientenwerte werden am Ende eines jeden Trainingsschritts aggregiert. Dies wird durch
über einen All-Reduce-Algorithmus. Das bedeutet, dass jeder Worker, in der Regel eine GPU,
eine Kopie des Modells auf dem Gerät hat und für einen einzelnen stochastischen Gradientenabstieg (SGD)
Schritt wird ein Mini-Batch von Daten auf die einzelnen Worker aufgeteilt. Jedes Gerät
führt einen Vorwärtsdurchlauf mit seinem Teil des Ministapels durch und berechnet die Gradienten
für jeden Parameter des Modells. Diese lokal berechneten Gradienten werden dann
von jedem Gerät gesammelt und aggregiert (z. B. gemittelt), um eine einzige Gradientenaktualisierung
Aktualisierung für jeden Parameter zu erstellen. Ein zentraler Server hält die aktuellste Kopie der Modell
der Modellparameter und führt den Gradientenschritt entsprechend den von den
den mehreren Arbeitern. Sobald die Modellparameter entsprechend diesem
werden, wird das neue Modell zusammen mit einer weiteren Aufteilung des nächsten Mini-Batch an die Arbeiter zurückgeschickt.
zusammen mit einer weiteren Aufteilung des nächsten Ministapels an die Arbeiter zurückgeschickt, und der Prozess wird wiederholt. Abbildung 4-15 zeigt eine
typische All-Reduce-Architektur für synchrone Datenverteilung.

Wie bei jeder Parallelitätsstrategie führt dies zu zusätzlichem Overhead für die Verwaltung von Zeit und Kommunikation zwischen den Arbeitern.
und Kommunikation zwischen den Arbeitern. Große Modelle können E/A-Engpässe verursachen
Engpässe verursachen, da die Daten während des Trainings von der CPU zur GPU übertragen werden, und
können ebenfalls Verzögerungen verursachen.

In TensorFlow unterstützt tf.distribute.MirroredStrategy synchrones verteiltes
verteiltes Training über mehrere GPUs auf der gleichen Maschine. Jeder Modellparameter
wird über alle Worker gespiegelt und als eine einzige konzeptionelle Variable namens
GespiegelteVariable. Während des All-Reduce-Schrittes werden alle Gradiententensoren auf jedem Gerät verfügbar gemacht.
auf jedem Gerät verfügbar gemacht. Dies trägt dazu bei, den Overhead der Synchronisierung erheblich zu reduzieren.
Es gibt auch verschiedene andere Implementierungen des All-Reduce-Algorithmus,
von denen viele NVIDIA NCCL verwenden.

176 | Kapitel 4: Muster der Modellausbildung

Abbildung 4-15. Beim synchronen Training besitzt jeder Arbeiter eine Kopie des Modells und
Gradienten unter Verwendung eines Ausschnitts des Trainingsdaten-Ministapels.

Um diese gespiegelte Strategie in Keras zu implementieren, erstellen Sie zunächst eine Instanz der gespiegelten
Instanz der gespiegelten Verteilungsstrategie und verschieben dann die Erstellung und Kompilierung des Modells in den
den Bereich dieser Instanz. Der folgende Code zeigt die Verwendung von MirroredStrategy
beim Training eines dreischichtigen neuronalen Netzes verwendet wird:

mirrored_strategy = tf.distribute.MirroredStrategy()
mit mirrored_strategy.scope():
model = tf.keras.Sequential([tf.keras.layers.Dense(32, input_shape=(5,)),
tf.keras.layers.Dense(16, activation='relu'),
tf.keras.layers.Dense(1)])
model.compile(loss='mse', optimizer='sgd')
Durch die Erstellung des Modells in diesem Bereich werden die Parameter des Modells als
gespiegelte Variablen anstelle von regulären Variablen erstellt. Bei der Anpassung des Modells an den
auf den Datensatz kommt, wird alles genau so wie zuvor durchgeführt. Der Modellcode bleibt
derselbe! Um verteiltes Training zu ermöglichen, müssen Sie nur den Modellcode in den Bereich der Verteilungsstrategie
um verteiltes Training zu ermöglichen. Die MirroredStrategy übernimmt die Replikation der
Modellparameter auf den verfügbaren GPUs, die Aggregation von Gradienten und vieles mehr. Zum Trainieren
oder das Modell zu evaluieren, rufen wir einfach fit() oder evaluate() wie gewohnt auf:

model.fit(train_dataset, epochs=2)
model.evaluate(train_dataset)
Während des Trainings wird jeder Stapel der Eingabedaten gleichmäßig unter den
mehrere Arbeiter aufgeteilt. Wenn Sie zum Beispiel zwei GPUs verwenden, wird ein Stapel
Größe von 10 auf die 2 GPUs aufgeteilt, wobei jede 5 Trainingsbeispiele erhält

Entwurfsmuster 14: Verteilungsstrategie | 177
jedem Schritt. Es gibt auch andere synchrone Verteilungsstrategien in
Keras, wie CentralStorageStrategy und MultiWorkerMirroredStrategy.
Mit MultiWorkerMirroredStrategy kann die Verteilung nicht nur auf
GPUs auf einem einzigen Rechner, sondern auf mehreren Rechnern. Bei CentralStorageStrategy,
werden die Modellvariablen nicht gespiegelt, sondern auf der CPU platziert und die Operationen
Operationen werden auf alle lokalen GPUs repliziert. Die Aktualisierungen der Variablen finden also nur an einem
Ort.

Wenn Sie zwischen verschiedenen Verteilungsstrategien wählen, hängt die beste Option von folgenden Faktoren ab
Topologie Ihres Computers ab und davon, wie schnell die CPUs und GPUs miteinander kommunizieren
miteinander kommunizieren können. Tabelle 4-2 fasst zusammen, wie die verschiedenen hier beschriebenen Strategien
diese Kriterien vergleichen.

Tabelle 4-2. Die Wahl zwischen den Verteilungsstrategien hängt von der Topologie Ihres Computers ab und davon
wie schnell die CPUs und GPUs miteinander kommunizieren können

Schnellere CPU-GPU-Verbindung Schnellere GPU-GPU-Verbindung
Ein Rechner mit mehreren GPUs CentralStorageStrategy MirroredStrategy
Mehrere Rechner mit mehreren GPUs MultiWorkerMirroredStrategy MultiWorkerMirroredStrategy
Verteilte Datenparallelität in PyTorch
In PyTorch verwendet der Code immer DistributedDataParallel, unabhängig davon, ob Sie eine
GPU oder mehrere GPUs haben und ob das Modell auf einem Rechner oder mehreren
Maschinen ausgeführt wird. Stattdessen hängt es davon ab, wie und wo Sie die Prozesse starten und wie Sie die Abtastung, das Laden von Daten usw. verkabeln.
pling, das Laden von Daten und so weiter die Verteilungsstrategie bestimmt.
Zunächst initialisieren wir den Prozess und warten darauf, dass andere Prozesse starten und die Kommunikation einrichten.
Kommunikation mit:
torch.distributed.init_process_group(backend="nccl")
Zweitens geben Sie die Gerätenummer an, indem Sie einen Rang über die Befehlszeile abrufen.
Rang = 0 ist der Master-Prozess, und 1,2,3,... sind die Arbeiter:
device = torch.device("cuda:{}".format(local_rank))
Das Modell wird wie üblich in jedem der Prozesse erstellt, aber an dieses Gerät gesendet. A
verteilte Version des Modells, die ihren Teil des Stapels verarbeiten wird, wird mit
DistributedDataParallel erstellt:
model = model.to(device)
ddp_model = DistributedDataParallel(model, device_ids=[local_rank],
output_device=local_rank)
Die Daten selbst werden mithilfe eines DistributedSamplers gesplittet, und jeder Datenstapel wird auch
an das Gerät gesendet:
178 | Kapitel 4: Modell-Trainingsmuster

sampler = DistributedSampler(dataset=trainds)
loader = DataLoader(dataset=trainds, batch_size=batch_size,
sampler=sampler, num_workers=4)
...
for data in train_loader:
features, labels = data[0].to(device), data[1].to(device)
Wenn ein PyTorch-Trainer gestartet wird, werden ihm die Gesamtzahl der Knoten und sein eigener
Rang:
python -m torch.distributed.launch --nproc_per_node=4 \
--nnodes=16 --node_rank=3 --master_addr="192.168.0.1" \
--master_port=1234 my_pytorch.py
Wenn die Anzahl der Knoten eins ist, haben wir das Äquivalent von TensorFlow's
MirroredStrategy von TensorFlow, und wenn die Anzahl der Knoten mehr als eins ist, haben wir das Äquivalent
lent von TensorFlow's MultiWorkerMirroredStrategy. Wenn die Anzahl der Prozesse pro
Knoten und die Anzahl der Knoten beide eins sind, dann haben wir eine OneDeviceStrategy. Opti-
misierte Kommunikation für alle diese Fälle wird bereitgestellt, wenn sie vom Backend
(in diesem Fall NCCL) in init_process_group übergeben wird.
Asynchrone Ausbildung

Beim asynchronen Training trainieren die Arbeiter unabhängig voneinander auf verschiedenen Slices der Eingabedaten.
und die Modellgewichte und -parameter werden asynchron aktualisiert, typischerweise durch eine
typischerweise durch eine Parameter-Server-Architektur. Dies bedeutet, dass kein Arbeiter
auf Aktualisierungen des Modells von anderen Arbeitern wartet. In der Parameter-Server
Architektur gibt es einen einzigen Parameterserver, der die aktuellen Werte der
Modellgewichte verwaltet, wie in Abbildung 4-16.

Wie beim synchronen Training wird für jeden SGD-Schritt ein Mini-Batch an Daten auf die einzelnen Worker aufgeteilt.
Arbeiter für jeden SGD-Schritt aufgeteilt. Jedes Gerät führt einen Vorwärtsdurchlauf mit seinem Teil
Teil des Mini-Batch durch und berechnet Gradienten für jeden Parameter des Modells.
Diese Gradienten werden an den Parameter-Server gesendet, der die Parameteraktualisierung durchführt
der die Parameteraktualisierung durchführt und dann die neuen Modellparameter mit einem weiteren
Teil des nächsten Mini-Batch.

Der Hauptunterschied zwischen synchronem und asynchronem Training besteht darin, dass der
Parameter-Server keine vollständige Reduktion vornimmt. Stattdessen berechnet er die neuen Modell
Modellparameter in regelmäßigen Abständen auf der Grundlage der Gradientenaktualisierungen, die er seit der letzten
Berechnung erhalten hat. In der Regel erzielt die asynchrone Verteilung einen höheren Durchsatz als
synchrones Training, da ein langsamer Worker die Abfolge der Trainingsschritte nicht blockiert
schritte blockiert. Fällt ein einzelner Worker aus, wird das Training wie geplant mit den anderen Workern fortgesetzt.
Arbeitern fortgesetzt, während dieser Arbeiter neu startet. Infolgedessen können einige Splits des Mini-Batch während des Trainings verloren gehen
während des Trainings verloren gehen, was es zu schwierig macht, genau zu verfolgen, wie viele Epochen
Epochen von Daten verarbeitet wurden. Dies ist ein weiterer Grund, warum wir normalerweise virtuelle

Entwurfsmuster 14: Verteilungsstrategie | 179
Epochen beim Training großer verteilter Aufträge anstelle von Epochen zu verwenden; siehe "Entwurfsmuster 12:
Prüfpunkte" auf Seite 149 für eine Diskussion über virtuelle Epochen.

Abbildung 4-16. Beim asynchronen Training führt jeder Arbeiter einen Schritt des Gradientenabstiegs
mit einer Aufteilung des Mini-Batch durch. Keiner der Arbeiter wartet auf Aktualisierungen des Modells durch einen der
der anderen Arbeiter.

Da es keine Synchronisation zwischen den Gewichtungsaktualisierungen gibt, ist es außerdem möglich
ist es möglich, dass ein Arbeiter die Modellgewichte auf der Grundlage eines veralteten Modellstatus aktualisiert. Allerdings,
in der Praxis scheint dies jedoch kein Problem zu sein. Typischerweise werden große neuronale Netze
große neuronale Netze über mehrere Epochen hinweg trainiert, so dass diese kleinen Diskrepanzen am Ende vernachlässigbar
Ende.

In Keras implementiert ParameterServerStrategy asynchrones Parameterserver
Training auf mehreren Maschinen. Bei der Verwendung dieser Verteilung werden einige Maschinen als
als Worker und einige als Parameterserver gehalten. Die Parameterserver
halten jede Variable des Modells, und die Berechnungen werden auf den Workern durchgeführt, typischerweise
typischerweise GPUs.

Die Implementierung ist ähnlich wie bei anderen Verteilungsstrategien in Keras. Für
Beispiel: In Ihrem Code würden Sie einfach MirroredStrategy() durch
ParameterServerStrategy().

Eine weitere Verteilungsstrategie, die in Keras unterstützt wird, ist
ist die OneDeviceStrategy. Diese Strategie platziert alle Variablen
Variablen, die in ihrem Bereich erstellt wurden, auf dem angegebenen Gerät. Diese Strategie ist besonders
besonders nützlich, um Ihren Code zu testen, bevor Sie zu anderen
Strategien, die tatsächlich auf mehrere Geräte/Maschinen verteilen.
180 | Kapitel 4: Muster für das Modelltraining

5 Victor Campos et al., "Distributed training strategies for a computer vision deep learning algorithm on a dis- tributed GPU cluster.
tributed GPU cluster," International Conference on Computational Science, ICCS 2017, June 12-14, 2017.
6 Ibid.
Synchrones und asynchrones Training haben jeweils ihre Vor- und Nachteile.
und die Wahl zwischen beiden hängt oft von Hardware- und Netzwerkbeschränkungen ab.
Beschränkungen.

Das synchrone Training ist besonders anfällig für langsame Geräte oder schlechte Netzwerkverbindungen.
anfällig, da die Ausbildung ins Stocken gerät, wenn sie auf Aktualisierungen von allen Arbeitnehmern wartet. Das bedeutet
ist die synchrone Verteilung vorzuziehen, wenn sich alle Geräte auf einem einzigen Host befinden und es
schnelle Geräte (z. B. TPUs oder GPUs) mit starken Verbindungen vorhanden sind. Andererseits
ist die asynchrone Verteilung vorzuziehen, wenn es viele leistungsschwache oder unzuverlässige
Arbeiter gibt. Wenn ein einzelner Worker ausfällt oder bei der Rückgabe einer Gradientenaktualisierung ins Stocken gerät, wird er nicht
die Trainingsschleife. Die einzige Einschränkung sind E/A-Beschränkungen.

Warum es funktioniert
Große, komplexe neuronale Netze benötigen große Mengen an Trainingsdaten, um effizient zu sein.
tiv zu sein. Verteilte Trainingsverfahren erhöhen den Datendurchsatz dieser Modelle drastisch.
dieser Modelle und können die Trainingszeiten von Wochen auf Stunden reduzieren.
Stunden verkürzen. Die gemeinsame Nutzung von Ressourcen durch Worker und Parameter-Server-Aufgaben führt zu einer drastischen
matischen Anstieg des Datendurchsatzes. Abbildung 4-17 vergleicht den Durchsatz von Trainings
Trainingsdaten, in diesem Fall Bilder, mit verschiedenen Verteilungskonfigurationen.^5 Besonders bemerkenswert ist, dass
dass der Durchsatz mit der Anzahl der Arbeitsknoten zunimmt und dass, obwohl Parameter
Server Aufgaben ausführen, die nicht mit den Berechnungen auf den GPU-Workern zusammenhängen,
ist die Aufteilung der Arbeitslast auf mehrere Maschinen die vorteilhafteste Strategie.

Außerdem verkürzt die Parallelisierung der Daten die Zeit bis zur Konvergenz beim Training. In einer
ähnlichen Studie wurde gezeigt, dass die Erhöhung der Anzahl der Arbeiter zu einem viel schnelleren Minimumverlust führt.
^6 Abbildung 4-18 vergleicht die Zeit bis zum Minimum für verschiedene Verteilungsstrategien.
gien. Mit zunehmender Anzahl der Arbeiter nimmt die Zeit bis zum minimalen Trainingsverlust dramatisch ab.
Die Zeit bis zum minimalen Ausbildungsverlust nimmt drastisch ab, wobei sich die Zeit bei 8 Arbeitnehmern fast um das 5fache verkürzt, im Gegensatz zu nur 1 Arbeitnehmer.

Entwurfsmuster 14: Verteilungsstrategie | 181
Abbildung 4-17. Vergleich des Durchsatzes zwischen verschiedenen Verteilungskonfigurationen. Hier,
2W1PS bedeutet zwei Worker und einen Parameterserver.

Abbildung 4-18. Mit zunehmender Anzahl von GPUs nimmt die Zeit bis zur Konvergenz beim Training
sinkt.

182 | Kapitel 4: Muster der Modellausbildung

7 Jeffrey Dean et al. "Large Scale Distributed Deep Networks," NIPS Proceedings (2012).
Kompromisse und Alternativen
Neben der Datenparallelität gibt es noch weitere Aspekte der Verteilung zu berücksichtigen,
wie die Modellparallelität, andere Trainingsbeschleuniger (wie TPUs) und andere
Überlegungen (z. B. E/A-Beschränkungen und Stapelgröße).

Parallelität modellieren

In einigen Fällen ist das neuronale Netz so groß, dass es nicht in den Speicher eines einzelnen Geräts passt.
So hat die neuronale maschinelle Übersetzung von Google beispielsweise Milliarden von Parametern.
Um Modelle dieser Größe zu trainieren, müssen sie auf mehrere Geräte aufgeteilt werden,^7 wie
in Abbildung 4-19 dargestellt. Dies wird als Modellparallelisierung bezeichnet. Durch die Aufteilung von Teilen eines Netzes
Netzes und der damit verbundenen Berechnungen auf mehrere Kerne wird die Rechen- und
Speicherlast auf mehrere Geräte verteilt. Jedes Gerät arbeitet über
über denselben Mini-Batch von Daten während des Trainings, führt aber Berechnungen durch, die
nur auf ihre separaten Komponenten des Modells.

Abbildung 4-19. Die Modellparallelität teilt das Modell auf mehrere Geräte auf.

Entwurfsmuster 14: Verteilungsstrategie | 183
Modellparallelität oder Datenparallelität?
A priori ist kein Schema besser als das andere. Jedes hat seine eigenen Vorteile. Typischerweise,
bestimmt die Modellarchitektur, ob es besser ist, Datenparallelität oder
Modellparallelität.
Insbesondere verbessert die Modellparallelität die Effizienz, wenn der Rechenaufwand
Rechenaufwand pro Neuronenaktivität hoch ist, wie z. B. bei breiten Modellen mit vielen vollständig verknüpften
Schichten. Dies liegt daran, dass der Neuronenwert zwischen den verschiedenen Komponenten des Modells kommuniziert wird.
verschiedenen Komponenten des Modells kommuniziert wird. Außerhalb des Trainingsparadigmas bietet die Modellparallelität
einen zusätzlichen Vorteil für sehr große Modelle, bei denen eine geringe Latenzzeit erforderlich ist.
Die Aufteilung der Berechnung eines großen Modells auf mehrere Geräte kann die
kann die Gesamtberechnungszeit bei Online-Vorhersagen erheblich reduzieren.
Andererseits ist die Datenparallelität effizienter, wenn der Rechenaufwand pro
Datenparallelität ist hingegen effizienter, wenn der Rechenaufwand pro Gewicht hoch ist, z. B. wenn Faltungsschichten beteiligt sind. Dies ist
weil es die Modellgewichte (und ihre Gradientenaktualisierungen) sind, die zwischen
zwischen verschiedenen Arbeitern weitergegeben werden.
Je nach Umfang des Modells und des Problems kann es notwendig sein, beides zu nutzen.
beides. Mesh TensorFlow ist eine für verteiltes Deep Learning optimierte Bibliothek, die
die synchrone Datenparallelität mit Modellparallelität verbindet. Sie ist als eine
Schicht über TensorFlow implementiert und ermöglicht die einfache Aufteilung von Tensoren über verschiedene Dimen- sionen.
dimen- sionen. Die Aufteilung über die Stapelschicht ist gleichbedeutend mit Datenparallelität, während
Aufteilung über eine andere Dimension - zum Beispiel eine Dimension, die die Größe einer
einer verborgenen Schicht darstellt, Modellparallelität erreicht wird.
ASICs für bessere Leistung bei geringeren Kosten

Eine weitere Möglichkeit, den Trainingsprozess zu beschleunigen, besteht in der Beschleunigung der zugrunde liegenden Hard-
Hardware, z. B. durch den Einsatz anwendungsspezifischer integrierter Schaltungen (ASICs). Beim maschinellen
maschinellem Lernen bezieht sich dies auf Hardwarekomponenten, die speziell zur Optimierung der Leistung
Leistung bei den Arten von großen Matrixberechnungen zu optimieren, die den Kern der Trainingsschleife bilden.
TPUs in Google Cloud sind ASICs, die sowohl für das Training von Modellen als auch für Vorhersagen verwendet werden können.
Vorhersagen verwendet werden können. In ähnlicher Weise bietet Microsoft Azure das Azure FPGA (field-
programmierbare Gate Array) an, der ebenfalls ein benutzerdefinierter Chip für maschinelles Lernen ist, wie der
ASIC, mit dem Unterschied, dass er im Laufe der Zeit neu konfiguriert werden kann. Diese Chips sind in der Lage, die
die Zeit bis zur Genauigkeit beim Training großer, komplexer neuronaler Netzwerkmodelle erheblich verkürzen. A
Modell, das auf GPUs zwei Wochen zum Trainieren braucht, kann auf TPUs in wenigen Stunden konvergieren.

Die Verwendung von kundenspezifischen Chips für maschinelles Lernen hat noch weitere Vorteile. Zum Beispiel, da
Beschleuniger (GPUs, FPGAs, TPUs usw.) schneller geworden sind, ist E/A zu einem
Engpass beim ML-Training geworden. Viele Trainingsprozesse verschwenden Zyklen mit dem Warten auf
Daten zu lesen und an den Beschleuniger zu übertragen und auf die Ausführung von Gradientenaktualisierungen zu warten

184 | Kapitel 4: Muster der Modellausbildung

8 Priya Goyal et al., "Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour" (2017), arXiv:
1706.02677v2 [cs.CV].
all-reduce. TPU-Pods verfügen über Hochgeschwindigkeitsverbindungen, sodass wir uns in der Regel keine Gedanken über
Kommunikations-Overhead innerhalb eines Pods (ein Pod besteht aus Tausenden von TPUs). Unter
Außerdem ist viel Speicher auf der Festplatte verfügbar, was bedeutet, dass es möglich ist
Daten präemptiv abzurufen und die CPU seltener anzufordern. Infolgedessen können Sie
sollten Sie viel höhere Losgrößen verwenden, um die Vorteile von Chips mit hohem Speicherbedarf und hoher
Interconnect-Chips wie TPUs zu nutzen.

Was das verteilte Training betrifft, so ermöglicht TPUStrategy die Ausführung von verteilten Trainings
Aufträge auf TPUs auszuführen. Unter der Haube ist TPUStrategy dasselbe wie MirroredStrategy
obwohl TPUs ihre eigene Implementierung des All-Reduce-Algorithmus haben.

Die Verwendung von TPUStrategy ist ähnlich wie die Verwendung der anderen Verteilungsstrategien in Tensor-
Flow. Ein Unterschied ist, dass Sie zuerst einen TPUClusterResolver einrichten müssen, der auf den
den Standort der TPUs verweist. TPUs sind derzeit zur kostenlosen Verwendung in Google
Colab, und dort müssen Sie keine Argumente für tpu_address angeben:

cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(
tpu=tpu_address)
tf.config.experimental_connect_to_cluster(cluster_resolver)
tf.tpu.experimental.initialize_tpu_system(cluster_resolver)
tpu_strategy = tf.distribute.experimental.TPUStrategy(cluster_resolver)
Auswahl einer Losgröße

Ein weiterer wichtiger Faktor ist die Stapelgröße. Insbesondere bei synchroner Daten
Datenparallelität, wenn das Modell besonders groß ist, ist es besser, die Gesamtzahl der
Trainingsiterationen zu verringern, da bei jedem Trainingsschritt das aktualisierte Modell
auf verschiedene Arbeiter aufgeteilt werden muss, was zu einer Verlangsamung der Übertragungszeit führt. Daher ist es
Es ist daher wichtig, die Mini-Batch-Größe so weit wie möglich zu erhöhen, damit die gleiche Leistung mit weniger Schritten erreicht werden kann.
Leistung mit weniger Schritten erreicht werden kann.

Es hat sich jedoch gezeigt, dass sehr große Losgrößen die Konvergenzrate
die Konvergenzrate des stochastischen Gradientenabstiegs und die Qualität der endgültigen Lösung beeinträchtigen.
tion beeinträchtigen.^8 Abbildung 4-20 zeigt, dass eine Erhöhung der Batchgröße allein letztlich den
Top-1-Validierungsfehler zunimmt. Tatsächlich argumentieren sie, dass eine lineare Skalierung der Lernrate
Lernrate als Funktion der großen Batchgröße notwendig ist, um einen niedrigen Validierungsfehler zu
Validierungsfehler beizubehalten und gleichzeitig die Zeit für das verteilte Training zu verkürzen.

Entwurfsmuster 14: Verteilungsstrategie | 185
Abbildung 4-20. Es hat sich gezeigt, dass große Losgrößen die Qualität des endgültigen
endgültigen trainierten Modells auswirken.

Daher ist die Festlegung der Mini-Batch-Größe im Kontext des verteilten Trainings ein komplexer
Optimierungsbereich, da sie sowohl die statistische Genauigkeit (Generalisierung)
als auch die Hardware-Effizienz (Auslastung) des Modells beeinflusst. Verwandte Arbeiten, die sich auf diese
Optimierung konzentriert, führt eine schichtweise adaptive Großserienoptimierungstechnik ein
LAMB genannt, mit der die BERT-Trainingszeit von 3 Tagen auf nur 76 Minuten reduziert werden konnte.
76 Minuten reduzieren konnte.

Minimierung der E/A-Wartezeiten

GPUs und TPUs können Daten viel schneller verarbeiten als CPUs, und bei der Verwendung von dis
Bei verteilten Strategien mit mehreren Beschleunigern können die E/A-Pipelines nur schwer mithalten,
Dies stellt einen Engpass für ein effizienteres Training dar. Konkret bedeutet dies, dass vor Abschluss eines
Trainingsschritt abgeschlossen ist, stehen die Daten für den nächsten Schritt nicht zur Verarbeitung zur Verfügung. Dies ist in
Abbildung 4-21. Die CPU übernimmt die Eingabepipeline: Lesen der Daten aus dem Speicher, Vorverarbeitung
Aufbereitung und Weiterleitung an den Beschleuniger zur Berechnung. Da verteilte Strategien
Strategien das Training beschleunigen, wird es mehr denn je notwendig, effiziente Eingabepipelines zu haben, um die
Leitungen, um die verfügbare Rechenleistung voll auszuschöpfen.

Dies kann auf verschiedene Weise erreicht werden, einschließlich der Verwendung optimierter Dateiformate wie
TFRecords und der Aufbau von Datenpipelines mit der TensorFlow tf.data API. Die
tf.data API ermöglicht es, große Datenmengen zu handhaben und hat eingebaute Trans
formationen, die für die Erstellung flexibler, effizienter Pipelines nützlich sind. Zum Beispiel, tf.data.Data
set.prefetch die Vorverarbeitung und die Modellausführung eines Trainingsschritts überlappen, so dass

186 | Kapitel 4: Muster der Modellausbildung

dass, während das Modell den Trainingsschritt N ausführt, die Eingabepipeline Daten liest und
Daten für den Trainingsschritt N + 1 vorbereitet, wie in Abbildung 4-22 dargestellt.

Abbildung 4-21. Bei verteiltem Training auf mehreren verfügbaren GPU/TPUs ist es notwendig
effiziente Eingabepipelines zu haben.

Abbildung 4-22. Der Vorabruf überschneidet sich mit der Vorverarbeitung und der Modellausführung, d. h. während das
Modell einen Trainingsschritt ausführt, liest die Eingabepipeline Daten und bereitet sie
für den nächsten Schritt.

Entwurfsmuster 15: Hyperparameter-Abstimmung
Beim Hyperparameter-Tuning wird die Trainingsschleife selbst in ein Optimierungsverfahren eingefügt
Optimierungsverfahren eingefügt, um den optimalen Satz von Modell-Hyperparametern zu finden.

Problem
Beim maschinellen Lernen geht es bei der Modellschulung darum, den optimalen Satz von Haltepunkten
(im Falle von Entscheidungsbäumen), Gewichten (im Falle von neuronalen Netzen) oder Stützvektoren
Vektoren (im Fall von Support-Vektor-Maschinen). Wir bezeichnen diese als Modellparameter.
Um das Modell zu trainieren und die optimalen Modellparameter zu finden, müssen wir jedoch oft
ters zu finden, müssen wir oft eine Reihe von Dingen hart kodieren. Wir könnten zum Beispiel entscheiden, dass
dass die maximale Tiefe eines Baums 5 beträgt (im Falle von Entscheidungsbäumen) oder dass die Akti-
vationsfunktion ReLU sein wird (bei neuronalen Netzen) oder die Menge der Kernel wählen, die
wir verwenden werden (bei SVMs). Diese Parameter werden als Hyperparameter bezeichnet.

Modellparameter beziehen sich auf die Gewichte und Verzerrungen, die Ihr Modell erlernt hat. Sie haben keine
haben keine direkte Kontrolle über die Modellparameter, da sie weitgehend von Ihren
Trainingsdaten, der Modellarchitektur und vielen anderen Faktoren abhängen. Mit anderen Worten: Sie können-
Sie können die Modellparameter nicht manuell einstellen. Die Gewichte Ihres Modells werden mit ran-
Gewichte Ihres Modells werden mit laufenden Werten initialisiert und dann von Ihrem Modell optimiert, während es die Trainingsiterationen durchläuft.
Hyperparameter hingegen beziehen sich auf alle Parameter, die Sie als Modell

Entwurfsmuster 15: Hyperparameter-Tuning | 187
Konstrukteur, steuern kann. Dazu gehören Werte wie Lernrate, Anzahl der Epochen, Anzahl der
Anzahl der Schichten in Ihrem Modell und mehr.

Manuelle Abstimmung

Da Sie die Werte für die verschiedenen Hyperparameter manuell auswählen können, ist Ihr erster
Instinkt könnte ein Versuch-und-Irrtum-Ansatz sein, um die optimale Kombination von
Hyperparameterwerten zu finden. Dies mag für Modelle funktionieren, die in Sekunden oder Minuten trainiert werden,
aber bei größeren Modellen, die viel Trainingszeit und Infrastruktur benötigen, kann es schnell teuer werden.
Zeit und Infrastruktur benötigen. Stellen Sie sich vor, Sie trainieren ein Bildklassifizierungsmodell, das
Stunden für das Training auf GPUs benötigt. Sie entscheiden sich für ein paar Hyperparameterwerte, die Sie ausprobieren und
warten dann auf die Ergebnisse des ersten Trainingslaufs. Anhand dieser Ergebnisse optimieren Sie die
Hyperparameter, trainieren das Modell erneut, vergleichen die Ergebnisse mit denen des ersten Durchgangs und
und legen dann die besten Hyperparameterwerte fest, indem Sie den Trainingslauf mit den
besten Metriken.

Bei diesem Ansatz gibt es einige Probleme. Erstens: Sie haben fast einen Tag und
viele Rechenstunden auf diese Aufgabe verwendet. Zweitens kann man nicht wissen, ob man
die optimale Kombination von Hyperparameterwerten gefunden haben. Sie haben nur zwei
Kombinationen ausprobiert, und da Sie mehrere Werte auf einmal geändert haben, wissen Sie nicht
wissen Sie nicht, welcher Parameter den größten Einfluss auf die Leistung hatte. Selbst mit zusätzlichen
tionalen Versuchen werden bei diesem Ansatz schnell Zeit und Rechenressourcen verbraucht
und führt möglicherweise nicht zu den optimalsten Hyperparameterwerten.

Wir verwenden den Begriff Versuch hier für einen einzelnen Trainingslauf mit
einem Satz von Hyperparameterwerten.
Gittersuche und kombinatorische Explosion

Eine strukturiertere Version des zuvor beschriebenen Versuch-und-Irrtum-Ansatzes ist bekannt als
als Gittersuche. Bei der Implementierung der Hyperparameterabstimmung mit der Rastersuche
wählen wir eine Liste möglicher Werte, die wir für jeden zu optimierenden Hyperparameter ausprobieren
optimieren wollen. Beispiel: Im RandomForestRegressor()-Modell von scikit-learn, sagen wir
wollen wir die folgende Kombination von Werten für die Modell-Hyperparameter max_depth und
n_estimators ausprobieren:

grid_values = {
'max_depth': [5, 10, 100],
'n_estimators': [100, 150, 200]
}
188 | Kapitel 4: Muster für das Modelltraining

Bei der Rastersuche würden wir jede Kombination der angegebenen Werte ausprobieren und dann die
Kombination, die die beste Bewertungsmetrik für unser Modell ergibt. Wir wollen sehen, wie dies
mit einem Random-Forest-Modell funktioniert, das mit dem Datensatz "Boston Housing" trainiert wurde, der
mit scikit-learn vorinstalliert ist. Das Modell wird den Preis eines Hauses auf der Grundlage einer
einer Reihe von Faktoren. Wir können die Rastersuche ausführen, indem wir eine Instanz der Klasse
GridSearchCV-Klasse erstellen und das Modell trainieren, indem wir ihm die zuvor definierten Werte übergeben:

from sklearn.ensemble import RandomForestRegressor
from sklearn.datasets import load_boston
X, y = load_boston(return_X_y=True)
housing_model = RandomForestRegressor()
grid_search_housing = GridSearchCV(
housing_model, param_grid=grid_vals, scoring='max_error')
grid_search_housing.fit(X, y)
Beachten Sie, dass der Scoring-Parameter hier die Metrik ist, die wir optimieren wollen. Im Fall von
dieses Regressionsmodells wollen wir die Kombination von Hyperparametern verwenden, die
den geringsten Fehler für unser Modell ergibt. Um die beste Kombination von Werten aus der
der Rastersuche zu erhalten, können wir grid_search_housing.best_params_ ausführen. Dies liefert die
folgendes:

{'max_depth': 100, 'n_estimators': 150}
Wir möchten dies mit dem Fehler vergleichen, den wir beim Training eines Random-Forest-Regressor
Regressor-Modell ohne Hyperparameter-Tuning mit den Standardwerten von scikit-learn für diese
Parameter. Dieser Ansatz der Rastersuche funktioniert gut für das kleine Beispiel, das wir oben definiert haben
aber bei komplexeren Modellen müssen wir wahrscheinlich mehr als zwei Hyperparameter optimieren
Hyperparameter zu optimieren, die jeweils eine große Bandbreite an möglichen Werten aufweisen. Letztendlich führt die Gittersuche
zu einer kombinatorischen Explosion führen - wenn wir weitere Hyperparameter und Werte
Werte zu unserem Optionsraster hinzufügen, steigt die Anzahl der möglichen Kombinationen, die wir ausprobieren müssen, und
die Zeit, die benötigt wird, um sie alle auszuprobieren, erheblich an.

Ein weiteres Problem bei diesem Ansatz ist, dass bei der Auswahl der verschiedenen Kombinationen keine Logik angewandt wird.
verschiedenen Kombinationen. Die Gittersuche ist im Wesentlichen eine Brute-Force-Lösung, bei der jede
mögliche Kombination von Werten. Nehmen wir an, dass nach einem bestimmten Wert für max_depth der Fehler unseres
der Fehler unseres Modells zunimmt. Der Rastersuchalgorithmus lernt nicht aus früheren Versuchen,
Er würde also nicht wissen, dass er ab einem bestimmten Schwellenwert aufhören sollte, max_depth-Werte auszuprobieren. Er wird
wird einfach jeden Wert ausprobieren, den Sie vorgeben, unabhängig von den Ergebnissen.

Entwurfsmuster 15: Abstimmung der Hyperparameter | 189
scikit-learn unterstützt eine Alternative zur Rastersuche namens
RandomizedSearchCV, die eine zufällige Suche implementiert. Anstatt
alle möglichen Kombinationen von Hyperparametern aus einer Menge auszuprobieren,
bestimmen Sie die Anzahl der Stichproben, die Sie für jeden Hyperparameter zufällig
Werte für jeden Hyperparameter. Um die Zufallssuche in
scikit-learn zu implementieren, erstellt man eine Instanz von RandomizedSearchCV und
ein Diktat übergeben, ähnlich wie grid_values oben, mit Angabe von Bereichen
anstelle von spezifischen Werten. Die zufällige Suche läuft schneller als die Grid
Suche, da sie nicht jede Kombination in der Menge der möglichen Werte ausprobiert.
Werte ausprobiert, aber es ist sehr wahrscheinlich, dass der optimale Satz von Hyperparametern
nicht zu den zufällig ausgewählten Hyperparametern gehören wird.
Für eine robuste Abstimmung der Hyperparameter benötigen wir eine Lösung, die skaliert und aus früheren Versuchen lernt
früheren Versuchen lernt, um eine optimale Kombination von Hyperparameterwerten zu finden.

Lösung
Die keras-tuner-Bibliothek implementiert Bayes'sche Optimierung für die Hyperparametersuche
Suche direkt in Keras. Um keras-tuner zu verwenden, definieren wir unser Modell innerhalb einer Funktion
die ein Hyperparameter-Argument entgegennimmt, hier hp genannt. Wir können dann hp in der gesamten Funktion verwenden.
Funktion überall dort verwenden, wo wir einen Hyperparameter einbeziehen wollen, indem wir den
den Namen des Hyperparameters, den Datentyp, den zu durchsuchenden Wertebereich und wie viel
er jedes Mal erhöht werden soll, wenn wir einen neuen Parameter ausprobieren.

Anstatt den Hyperparameterwert bei der Definition einer Schicht in unserem Keras-Modell fest zu kodieren
Modell zu definieren, definieren wir ihn mit einer Hyperparameter-Variablen. Hier wollen wir die Anzahl der Neuronen
der Neuronen in der ersten versteckten Schicht unseres neuronalen Netzes einstellen:

keras.layers.Dense(hp.Int('first_hidden', 32, 256, step=32), activation='relu')
first_hidden ist der Name, den wir diesem Hyperparameter gegeben haben, 32 ist der Mindestwert
Wert, 256 ist der Maximalwert und 32 ist der Wert, um den der Wert innerhalb des
um den dieser Wert innerhalb des definierten Bereichs erhöht werden soll. Wenn wir ein MNIST-Klassifizierungsmodell erstellen
Klassifizierungsmodell erstellen würden, könnte die vollständige Funktion, die wir an keras-tuner übergeben würden, wie folgt aussehen
wie folgt aussehen:

def build_model(hp):
model = keras.Sequential([
keras.layers.Flatten(input_shape=(28, 28)),
keras.layers.Dense(
hp.Int('first_hidden', 32, 256, step=32), activation='relu'),
keras.layers.Dense(
hp.Int('second_hidden', 32, 256, step=32), activation='relu'),
keras.layers.Dense(10, activation='softmax')
])
model.compile(
optimizer=tf.keras.optimizers.Adam(
190 | Kapitel 4: Modell-Trainings-Muster

hp.Float('learning_rate', .005, .01, sampling='log')),
loss='sparse_categorical_crossentropy',
metrics=['accuracy'])
Modell zurückgeben
Die keras-tuner-Bibliothek unterstützt viele verschiedene Optimierungsalgorithmen. Hier,
instanziieren wir unseren Tuner mit Bayes'scher Optimierung und optimieren für die Validierung
Genauigkeit:

kerastuner als kt importieren
tuner = kt.BayesscheOptimierung(
build_model,
objective='val_accuracy',
max_versuche=10
)
Der Code zur Ausführung des Tuning-Jobs sieht ähnlich aus wie das Training unseres Modells mit fit(). Unter
können wir die Werte für die drei Hyperparameter sehen, die für jeden Versuch ausgewählt
die für jeden Versuch ausgewählt wurden. Wenn der Job abgeschlossen ist, können wir die Hyperparameter-Kombination
Kombination, die zum besten Versuch geführt hat. In Abbildung 4-23 sehen Sie die Beispielausgabe
für einen einzelnen Versuchslauf mit keras-tuner.

Abbildung 4-23. Ausgabe für einen Probelauf des Hyperparameter-Tunings mit keras-tuner. Unter
sind die vom Tuner ausgewählten Hyperparameter zu sehen, und im Abschnitt "Zusammenfassung" wird die
Abschnitt wird die resultierende Optimierungsmetrik angezeigt.

Zusätzlich zu den hier gezeigten Beispielen bietet keras-tuner noch weitere Funktionen
keras-tuner zur Verfügung, die wir noch nicht behandelt haben. Sie können damit mit verschiedenen
Anzahl von Schichten für Ihr Modell experimentieren, indem Sie einen hp.Int()-Parameter innerhalb einer Schleife definieren,
und Sie können auch einen festen Satz von Werten für einen Hyperparameter anstelle eines Bereichs angeben.

Entwurfsmuster 15: Abstimmung der Hyperparameter | 191
Für komplexere Modelle kann dieser hp.Choice()-Parameter verwendet werden, um
mit verschiedenen Typen von Schichten, wie BasicLSTMCell und BasicRNNCell, zu experimentieren. keras-tuner
läuft in jeder Umgebung, in der Sie ein Keras-Modell trainieren können.

Warum es funktioniert
Obwohl Gitter- und Zufallssuche effizienter sind als ein Versuch-und-Irrtum-Ansatz
der Hyperparameterabstimmung, aber sie werden schnell teuer für Modelle, die viel
viel Trainingszeit benötigen oder einen großen Hyperparameter-Suchraum haben.

Da sowohl die Modelle des maschinellen Lernens selbst als auch der Prozess der Hyperparametersuche
Optimierungsprobleme sind, wäre es folglich möglich, einen Ansatz zu verwenden
Ansatz verwenden können, der lernt, die optimale Hyperparameterkombination innerhalb eines bestimmten
Bereich möglicher Werte zu finden, so wie unsere Modelle aus Trainingsdaten lernen.

Wir können uns die Abstimmung der Hyperparameter als eine äußere Optimierungsschleife vorstellen (siehe
Abbildung 4-24), wobei die innere Schleife aus dem typischen Modelltraining besteht. Auch wenn wir
Obwohl wir neuronale Netze als das Modell darstellen, dessen Parameter optimiert werden, ist diese Lösung
Lösung auch auf andere Arten von maschinellen Lernmodellen anwendbar. Auch wenn der
Auch wenn der häufigere Anwendungsfall darin besteht, ein einziges bestes Modell aus allen potenziellen Hyperpara-
In einigen Fällen kann das Hyperparameter-Framework auch dazu verwendet werden, eine Familie von Modellen zu
eine Familie von Modellen zu erzeugen, die als Ensemble fungieren können (siehe die Diskussion des Musters Ensembles
in Kapitel 3).

Abbildung 4-24. Die Abstimmung der Hyperparameter kann als eine äußere Optimierungsschleife betrachtet werden.

192 | Kapitel 4: Muster der Modellausbildung

Nichtlineare Optimierung

Die Hyperparameter, die abgestimmt werden müssen, lassen sich in zwei Gruppen einteilen: diejenigen, die mit der
Modellarchitektur und diejenigen, die sich auf das Modelltraining beziehen. Modellarchitektur-Hyper-
Parameter, wie die Anzahl der Schichten in Ihrem Modell oder die Anzahl der Neuronen pro
Schicht, steuern die mathematische Funktion, die dem Modell für maschinelles Lernen zugrunde liegt.
Parameter für die Modellschulung, wie die Anzahl der Epochen, die Lernrate und die
Stapelgröße, steuern die Trainingsschleife und haben oft mit der Arbeitsweise des Gradientenoptimierers zu tun.
Abstiegsoptimierer arbeitet. Wenn man diese beiden Arten von Parametern berücksichtigt
ist es klar, dass die Gesamtmodellfunktion in Bezug auf diese Hyperparameter
im Allgemeinen nicht differenzierbar ist.

Die innere Trainingsschleife ist differenzierbar, und die Suche nach optimalen Parametern kann
durch stochastischen Gradientenabstieg durchgeführt werden. Ein einziger Schritt eines maschinellen Lern
Modells, das durch stochastischen Gradientenabstieg trainiert wird, kann nur ein paar Millisekunden dauern. Auf
Andererseits erfordert ein einziger Versuch beim Problem der Abstimmung der Hyperparameter das Training
ein komplettes Modell auf dem Trainingsdatensatz und kann mehrere Stunden dauern. Außerdem wird das
Außerdem muss das Optimierungsproblem für die Hyperparameter durch nichtlineare
Optimierungsmethoden gelöst werden, die auf nicht differenzierbare Probleme anwendbar sind.

Sobald wir uns entscheiden, nichtlineare Optimierungsmethoden zu verwenden, wird die Auswahl
der Metrik breiter. Diese Metrik wird anhand des Validierungsdatensatzes bewertet und
muss nicht mit dem Trainingsverlust identisch sein. Bei einem Klassifikationsmodell könnte Ihre
Bei einem Klassifizierungsmodell könnte Ihre Optimierungsmetrik die Genauigkeit sein, und Sie würden daher die Kombination von Hyperparametern finden wollen, die zu
Kombination von Hyperparametern zu finden, die zur höchsten Modellgenauigkeit führt, auch wenn der Verlust die
binäre Kreuzentropie. Bei einem Regressionsmodell möchten Sie vielleicht den Median
absoluten Fehler optimieren, selbst wenn der Verlust ein quadratischer Fehler ist. In diesem Fall sollten Sie die
Hyperparameter zu finden, die den niedrigsten mittleren quadratischen Fehler ergeben. Diese Metrik kann sogar
auf der Grundlage von Geschäftszielen gewählt werden. Zum Beispiel könnte man sich dafür entscheiden, den erwarteten
Einnahmen zu maximieren oder Verluste aufgrund von Betrug zu minimieren.

Bayessche Optimierung

Die Bayes'sche Optimierung ist eine Technik zur Optimierung von Black-Box-Funktionen, die ursprünglich
die in den 1970er Jahren von Jonas Mockus entwickelt wurde. Die Technik wurde auf viele Bereiche angewandt
und wurde 2012 erstmals auf die Abstimmung von Hyperparametern angewandt. Hier werden wir uns konzentrieren auf
Bayes'sche Optimierung im Zusammenhang mit der Abstimmung von Hyperparametern. In diesem Zusammenhang ist ein
Machine-Learning-Modell unsere Black-Box-Funktion, da ML-Modelle eine Reihe von
eine Reihe von Ausgaben aus den von uns bereitgestellten Eingaben erzeugen, ohne dass wir die internen Details des
des Modells selbst zu kennen. Der Prozess des Trainings unseres ML-Modells wird als Aufruf der
Zielfunktion bezeichnet.

Das Ziel der Bayes'schen Optimierung ist es, unser Modell so wenig wie möglich direkt zu trainieren.
wie möglich zu trainieren, da dies kostspielig ist. Denken Sie daran, dass wir jedes Mal, wenn wir eine neue Kombination von
Hyperparametern für unser Modell ausprobieren, müssen wir das gesamte Training des Modells durchlaufen

Entwurfsmuster 15: Hyperparameter-Abstimmung | 193
Zyklus. Bei einem kleinen Modell wie dem von scikit-learn, das wir oben trainiert haben, mag dies trivial erscheinen.
trainiert haben, aber für viele Produktionsmodelle erfordert der Trainingsprozess erhebliche
Infrastruktur und Zeit.

Anstatt unser Modell jedes Mal zu trainieren, wenn wir eine neue Kombination von Hyperparametern ausprobieren
Bayes'sche Optimierung eine neue Funktion, die unser Modell nachbildet, aber viel
viel billiger auszuführen ist. Diese Funktion wird als Surrogatfunktion bezeichnet - die Eingaben für diese Funktion
Funktion sind Ihre Hyperparameterwerte und die Ausgabe ist Ihre Optimierungsmetrik.
Die Surrogatfunktion wird viel häufiger aufgerufen als die Zielfunktion,
mit dem Ziel, eine optimale Kombination von Hyperparametern zu finden, bevor ein
vor Abschluss eines Trainingslaufs für Ihr Modell. Bei diesem Ansatz wird mehr Rechenzeit für die
Auswahl der Hyperparameter für jeden Versuch im Vergleich zur Rastersuche. Allerdings
da dies jedoch wesentlich billiger ist als die Ausführung unserer Zielfunktion bei jedem
verschiedene Hyperparameter ausprobieren, ist der Bayes'sche Ansatz der Verwendung einer Surrogatfunktion
bevorzugt. Gängige Ansätze zur Generierung der Surrogatfunktion sind ein Gaus-
sischen Prozess oder einen baumstrukturierten Parzen-Schätzer.

Bis jetzt haben wir die verschiedenen Teile der Bayes'schen Optimierung behandelt, aber wie
arbeiten sie zusammen? Zunächst müssen wir die Hyperparameter auswählen, die wir optimieren wollen
und einen Wertebereich für jeden Hyperparameter festlegen. Dieser Teil des Prozesses ist
manuell und definiert den Raum, in dem unser Algorithmus nach optimalen Werten sucht.
Werte. Wir müssen auch unsere Zielfunktion definieren, die den Code darstellt, der unseren
Modell-Trainingsprozess aufruft. Von dort aus entwickelt die Bayes'sche Optimierung eine Surrogat
Surrogatfunktion, um unseren Modellbildungsprozess zu simulieren, und verwendet diese Funktion, um die
die beste Kombination von Hyperparametern für unser Modell zu bestimmen. Erst wenn diese
Surrogat zu einer seiner Meinung nach guten Kombination von Hyperparametern kommt, führen wir
einen vollständigen Trainingslauf (Versuch) mit unserem Modell durchführen. Die Ergebnisse werden dann wieder an die
Surrogatfunktion zurückgeführt und der Prozess wird für die von uns festgelegte Anzahl von Versuchen wiederholt.
festgelegt.

Kompromisse und Alternativen
Genetische Algorithmen sind eine Alternative zu Bayes'schen Methoden für die Abstimmung von Hyperparametern.
Sie erfordern jedoch in der Regel viel mehr Modell-Trainingsläufe als Bayes'sche Methoden.
Wir zeigen Ihnen auch, wie Sie einen verwalteten Dienst für die Optimierung der Hyperparameter verwenden können.
Optimierung von Hyperparametern für Modelle, die mit verschiedenen ML-Frameworks erstellt wurden.

Vollständig verwaltete Hyperparameter-Abstimmung

Der keras-tuner-Ansatz eignet sich möglicherweise nicht für große maschinelle Lernprobleme
weil wir die Versuche parallel durchführen möchten und die Wahrscheinlichkeit von Maschinenfehlern
Und die Wahrscheinlichkeit von Maschinenfehlern und anderen Fehlern steigt, wenn sich die Zeit für die Modellschulung in Stunden ausdehnt.
Daher ist ein vollständig verwalteter, belastbarer Ansatz, der eine Black-Box-Optimierung bietet
für die Abstimmung der Hyperparameter nützlich. Ein Beispiel für einen verwalteten Dienst, der Folgendes implementiert

194 | Kapitel 4: Muster der Modellausbildung

Bayes'sche Optimierung ist der Hyperparameter-Abstimmungsdienst, der von Google
Cloud AI-Plattform. Dieser Dienst basiert auf Vizier, dem Black-Box-Optimierungstool
das intern bei Google verwendet wird.

Die zugrunde liegenden Konzepte des Cloud-Dienstes funktionieren ähnlich wie bei keras-tuner: Sie
Sie geben den Namen, den Typ, den Bereich und die Skala jedes Hyperparameters an, und diese Werte werden
in den Trainingscode Ihres Modells übernommen. Wir werden Ihnen zeigen, wie Sie Hyperparameter
Abstimmung in AI Platform anhand eines PyTorch-Modells, das auf den BigQuery-Natalitätsdaten
Satz trainiert wurde, um das Geburtsgewicht eines Babys vorherzusagen.

Der erste Schritt besteht darin, eine config.yaml-Datei zu erstellen, in der die Hyperparameter angegeben sind, die
die der Job optimieren soll, zusammen mit einigen anderen Metadaten zu Ihrem Job. Ein Vorteil der
Cloud-Dienstes ist, dass Sie Ihren Optimierungsauftrag skalieren können, indem Sie ihn auf GPUs
oder TPUs ausgeführt und auf mehrere Parameterserver verteilt werden kann. In dieser Konfigurationsdatei können Sie
die Gesamtzahl der Hyperparameterversuche an, die Sie durchführen möchten, und wie viele
dieser Versuche Sie parallel ausführen möchten. Je mehr Versuche Sie parallel ausführen, desto schneller
desto schneller wird Ihr Auftrag ausgeführt. Die parallele Ausführung von weniger Versuchen hat jedoch den Vorteil, dass der
Dienst in der Lage ist, aus den Ergebnissen jeder abgeschlossenen Prüfung zu lernen, um die
nächsten Versuche zu optimieren.

Für unser Modell könnte eine Beispielkonfigurationsdatei, die GPUs nutzt, wie folgt aussehen
folgend aussehen. In diesem Beispiel werden wir drei Hyperparameter einstellen - die Lernrate unseres Modells,
den Impulswert des Optimierers und die Anzahl der Neuronen in der versteckten Schicht unseres Modells
Schicht. Wir legen auch unsere Optimierungsmetrik fest. In diesem Beispiel besteht unser Ziel darin
den Verlust unseres Modells auf unserer Validierungsmenge zu minimieren:

trainingInput:
scaleTier: BASIC_GPU
ParameterServerTyp: großes_Modell
workerCount: 9
parameterServerCount: 3
Hyperparameter:
Ziel: MINIMIZE
maxTrials: 10
maxParallelVersuche: 5
hyperparameterMetricTag: val_error
enableTrialEarlyStopping: TRUE
params:
parameterName: lr
Typ: DOUBLE
minWert: 0.0001
maxWert: 0.1
scaleType: UNIT_LINEAR_SCALE
parameterName: momentum
Typ: DOUBLE
minWert: 0.0
maxWert: 1.0
scaleType: UNIT_LINEAR_SCALE
parameterName: hidden-layer-size
Entwurfsmuster 15: Hyperparameter-Abstimmung | 195
type: INTEGER
minWert: 8
maxValue: 32
scaleType: UNIT_LINEAR_SCALE
Anstatt diese Werte in einer Konfigurationsdatei zu definieren, können Sie dies auch
die AI Platform Python API verwenden.
Dazu müssen wir einen Argument-Parser in unseren Code einfügen, der die
die Argumente, die wir in der obigen Datei definiert haben, und verweisen dann auf diese Hyperparameter
verweisen, wo sie in unserem Modellcode erscheinen.

Als Nächstes erstellen wir unser Modell mithilfe der nn.Sequential API von PyTorch mit dem SGD-Opti-
mizer. Da unser Modell das Gewicht des Babys als Fließkommazahl vorhersagt, wird es ein Regressionsmodell sein.
Modell. Wir spezifizieren jeden unserer Hyperparameter mit der args-Variable, die die in unserem Argument definierten Variablen enthält.
die die in unserem Argumentparser definierten Variablen enthält:

torch.nn as nn importieren
model = nn.Sequential(nn.Linear(num_features, args.hidden_layer_size),
nn.ReLU(),
nn.Linear(args.hidden_layer_size, 1))
optimizer = torch.optim.SGD(model.parameters(), lr=args.lr,
momentum=args.momentum)
Am Ende unseres Modelltrainingscodes erstellen wir eine Instanz von HyperTune() und
und teilen ihr die zu optimierende Metrik mit. Dies wird den resultierenden Wert unserer
Optimierungsmetrik nach jedem Trainingslauf. Es ist wichtig, dass die gewählte Optimierungs
Optimierungsmetrik auf unseren Test- oder Validierungsdatensätzen berechnet wird und nicht auf unserem
Trainingsdatensatz:

hypertune importieren
hpt = hypertune.HyperTune()
val_mse = 0
num_batches = 0
kriterium = nn.MSELoss()
mit torch.no_grad():
for i, (data, label) in enumerate(validation_dataloader):
num_batches += 1
y_pred = model(daten)
mse = Kriterium(y_pred, label.view(-1,1))
val_mse += mse.item()
196 | Kapitel 4: Modell-Trainingsmuster

avg_val_mse = (val_mse / num_batches)
hpt.report_hyperparameter_tuning_metric(
hyperparameter_metric_tag='val_mse',
metric_value=avg_val_mse,
global_step=epochs
)
Sobald wir unseren Trainingsauftrag an AI Platform übermittelt haben, können wir die Protokolle in der
Cloud-Konsole überwachen. Nach Abschluss jedes Versuchs können Sie die für jeden Hyperparameter gewählten Werte
jeden Hyperparameter und den resultierenden Wert Ihrer Optimierungsmetrik, wie in
Abbildung 4-25.

Abbildung 4-25. Ein Beispiel für die HyperTune-Zusammenfassung in der AI Platform-Konsole. Dies ist
für ein PyTorch-Modell zur Optimierung von drei Modellparametern mit dem Ziel der Minimierung des
des mittleren quadratischen Fehlers auf dem Validierungsdatensatz.

Standardmäßig verwendet AI Platform Training die Bayes'sche Optimierung für Ihren Tuning-Auftrag,
Sie können aber auch angeben, ob Sie stattdessen Gitter- oder Zufallssuchalgorithmen verwenden möchten.
Der Cloud-Service optimiert auch die Suche nach Hyperparametern über Trainingsaufträge hinweg. Wenn
Wenn wir einen weiteren Trainingsauftrag ähnlich dem obigen ausführen, aber mit ein paar Änderungen an unseren
Hyperparametern und dem Suchraum, werden die Ergebnisse des letzten Auftrags verwendet, um effizient
Werte für den nächsten Satz von Versuchen auszuwählen.

Wir haben hier ein PyTorch-Beispiel gezeigt, aber Sie können AI Platform Training für
Hyperparameter-Tuning in jedem Framework für maschinelles Lernen nutzen, indem Sie Ihren Trainingscode
Code packen und eine setup.py-Datei bereitstellen, die alle Bibliotheksabhängigkeiten installiert.

Genetische Algorithmen

Wir haben verschiedene Algorithmen für die Optimierung von Hyperparametern erforscht: manuelle Suche,
Gittersuche, Zufallssuche und Bayes'sche Optimierung. Eine weitere, weniger verbreitete Variante
native ist der genetische Algorithmus, der sich grob an Charles Darwins

Entwurfsmuster 15: Hyperparameter-Abstimmung | 197
Evolutionstheorie der natürlichen Selektion. Diese Theorie, auch bekannt als "Überleben des
fittest" bekannt ist, besagt, dass die leistungsstärksten ("fittesten") Mitglieder einer Population
überleben und ihre Gene an künftige Generationen weitergeben, während weniger fitte Mitglieder dies nicht tun.
Genetische Algorithmen sind auf verschiedene Arten von Optimierungsproblemen angewandt worden,
einschließlich der Abstimmung von Hyperparametern.

Bei der Suche nach Hyperparametern funktioniert ein genetischer Ansatz, indem zunächst eine
Fitnessfunktion. Diese Funktion misst die Qualität eines bestimmten Versuchs und kann typischerweise
Diese Funktion misst die Qualität eines bestimmten Versuchs und kann typischerweise durch die Optimierungsmetrik Ihres Modells (Genauigkeit, Fehler usw.) definiert werden.
Nachdem Sie Ihre Fitnessfunktion definiert haben, wählen Sie zufällig einige Kombinationen von
Hyperparameter aus dem Suchraum und führen für jede dieser Kombinationen einen Versuch durch.
Kombinationen durch. Anschließend nehmen Sie die Hyperparameter aus den Versuchen, die am besten abgeschnitten haben, und verwenden
Sie diese Werte, um Ihren neuen Suchraum zu definieren. Dieser Suchraum wird zu Ihrer neuen
"Population", aus der Sie neue Wertekombinationen für die nächste Versuchsreihe generieren.
nächsten Versuchsreihe zu verwenden. Sie setzen diesen Prozess fort, indem Sie die Anzahl der Versuche einschränken
bis Sie zu einem Ergebnis gekommen sind, das Ihren Anforderungen entspricht.

Da sie die Ergebnisse früherer Versuche zur Verbesserung nutzen, sind genetische Algorithmen
"intelligenter" als manuelle, Gitter- und Zufallssuche. Wenn jedoch der Hyperparamter-Suchraum
ter Suchraum groß ist, steigt die Komplexität der genetischen Algorithmen. Anstatt eine
eine Surrogatfunktion als Ersatz für das Modelltraining wie bei der Bayes'schen Optimierung zu verwenden
Optimierung, müssen genetische Algorithmen das Modell für jede mögliche Kombination von
Hyperparameter-Werten trainiert werden. Außerdem sind genetische Algorithmen zum Zeitpunkt der Erstellung
weniger verbreitet und es gibt weniger ML-Frameworks, die sie von Haus aus für die
für die Abstimmung von Hyperparametern unterstützen.

Zusammenfassung
Dieses Kapitel konzentrierte sich auf Entwurfsmuster, die die typische SGD-Trainingsschleife des
maschinellen Lernens modifizieren. Wir begannen mit der Betrachtung des Musters Useful Overfitting, das
Situationen behandelt, in denen Overfitting von Vorteil ist. Zum Beispiel bei der Verwendung von daten
Methoden wie dem maschinellen Lernen zur Annäherung an Lösungen für komplexe dynamische Systeme oder
dynamischen Systemen oder PDEs, bei denen der gesamte Eingaberaum abgedeckt werden kann, ist die Überanpassung
Trainingsmenge das Ziel. Überanpassung ist auch eine nützliche Technik bei der Entwicklung und
Fehlersuche in ML-Modellarchitekturen. Als Nächstes haben wir uns mit Modellprüfpunkten beschäftigt und wie man
sie beim Training von ML-Modellen verwenden. Bei diesem Entwurfsmuster wird der vollständige Zustand des
des Modells in regelmäßigen Abständen während des Trainings. Diese Checkpoints können als das endgültige
Modell verwendet werden, wie im Fall des vorzeitigen Abbruchs, oder als Startpunkte im Fall von
Trainingsfehlern oder zur Feinabstimmung.

Das Entwurfsmuster Transfer Learning umfasst die Wiederverwendung von Teilen eines zuvor trainierten
Modells. Transfer-Lernen ist eine nützliche Methode, um die gelernten Merkmalsextraktionsschichten
des zuvor trainierten Modells zu nutzen, wenn Ihr eigener Datensatz begrenzt ist. Es kann auch verwendet werden, um
ein bereits trainiertes Modell, das auf einem großen allgemeinen Datensatz trainiert wurde, auf Ihre

198 | Kapitel 4: Muster der Modellausbildung

spezialisiertere Datensätze. Anschließend haben wir das Entwurfsmuster der Verteilungsstrategie erörtert.
Das Training großer, komplexer neuronaler Netze kann sehr viel Zeit in Anspruch nehmen.
Verteilungsstrategien bieten verschiedene Möglichkeiten, wie die Trainingsschleife modifiziert werden kann
Trainingsschleife so modifiziert werden kann, dass sie mit Hilfe von Parallelisierung und Hardwarebeschleunigern über mehrere
Beschleuniger.

Schließlich wurde im Entwurfsmuster Hyperparameter-Tuning erörtert, wie die SGD-Trainingsschleife
Schleife selbst im Hinblick auf die Modellhyperparameter optimiert werden kann. Wir sahen einige
nützliche Bibliotheken, die zur Implementierung von Hyperparameter-Tuning für Modelle, die mit
die mit Keras und PyTorch erstellt wurden.

Das nächste Kapitel befasst sich mit Entwurfsmustern im Zusammenhang mit der Widerstandsfähigkeit (bei einer großen Anzahl von
Anfragen, spitzem Datenverkehr oder Änderungsmanagement) bei der Einführung von Modellen in die Produktion.

Zusammenfassung | 199
KAPITEL 5

Entwurfsmuster für robustes Serving
Der Zweck eines maschinellen Lernmodells besteht darin, Rückschlüsse auf Daten zu ziehen, die es
die es während des Trainings noch nicht gesehen hat. Sobald ein Modell trainiert wurde, wird es daher normalerweise
in einer Produktionsumgebung eingesetzt und verwendet, um Vorhersagen als Reaktion auf
eingehende Anfragen. Von Software, die in Produktionsumgebungen eingesetzt wird, wird erwartet
wird erwartet, dass sie belastbar ist und nur wenige menschliche Eingriffe erfordert, um sie
laufen zu lassen. Die Entwurfsmuster in diesem Kapitel lösen Probleme im Zusammenhang mit der Ausfallsicherheit
unter verschiedenen Umständen, die sich auf ML-Modelle in der Produktion beziehen.

Mit dem Entwurfsmuster der zustandslosen Serving-Funktion kann die Serving-Infrastruktur
skalieren und Tausende oder sogar Millionen von Vorhersageanfragen pro Sekunde verarbeiten. Das
Batch Serving Design Pattern ermöglicht der Serving-Infrastruktur die asynchrone
gelegentliche oder periodische Anfragen für Millionen bis Milliarden von Vorhersagen asynchron zu verarbeiten. Diese Muster
Diese Patterns sind über die Ausfallsicherheit hinaus nützlich, da sie die Kopplung zwischen Erstellern und
Nutzern von Modellen des maschinellen Lernens.

Das Entwurfsmuster Continued Model Evaluation behandelt das häufige Problem, dass
zu erkennen, wann ein eingesetztes Modell nicht mehr zweckdienlich ist. Das Two-Phase Predic-
Das Two-Phase Predic- tions Design Pattern bietet eine Möglichkeit, das Problem zu lösen, dass die Modelle
Modelle ausgereift und leistungsfähig zu halten, wenn sie auf verteilten Geräten eingesetzt werden müssen. Das
Entwurfsmuster Keyed Predictions ist eine Notwendigkeit für die skalierbare Implementierung mehrerer
in diesem Kapitel besprochenen Entwurfsmuster skalierbar zu implementieren.

Entwurfsmuster 16: Zustandslose Serving-Funktion
Das Entwurfsmuster "Stateless Serving Function" ermöglicht es einem ML-Produktionssystem
System die synchrone Verarbeitung von Tausenden bis Millionen von Vorhersageanfragen pro Sek.
ond. Das ML-Produktionssystem wird um eine zustandslose Funktion herum entworfen, die die
Architektur und Gewichte eines trainierten Modells erfasst.

201
Zustandslose Funktionen
Eine zustandslose Funktion ist eine Funktion, deren Ausgaben ausschließlich durch ihre Eingaben bestimmt werden.
Diese Funktion ist zum Beispiel zustandslos:
def zustandslos_fn(x):
return 3*x + 15
Eine andere Möglichkeit, sich eine zustandslose Funktion vorzustellen, ist die eines unveränderlichen Objekts, bei dem die
Gewichte und Verzerrungen als Konstanten gespeichert sind:
class Stateless :
def __init__ (self):
self.weight = 3
self.bias = 15
def __call__ (self, x):
return self.weight*x + self.bias
Eine Funktion, die einen Zähler darüber führt, wie oft sie aufgerufen wurde, und
einen anderen Wert zurückgibt, je nachdem, ob der Zähler ungerade oder gerade ist, ist ein
Beispiel für eine Funktion, die zustandsabhängig und nicht zustandslos ist:
class State :
def __init__ (self):
self.counter = 0
def __call__ (self, x):
self.counter += 1
wenn self.counter % 2 == 0:
return 3*x + 15
else :
return 3*x - 15
Der Aufruf von stateless_fn(3) oder Stateless()(3) gibt immer 24 zurück, während
a = Zustand()
und der anschließende Aufruf von
a(3)
einen Wert zurückgibt, der zwischen -6 und 24 schwankt. Der Zähler ist in diesem Fall der Zustand der Funktion
der Funktion, und die Ausgabe hängt sowohl von der Eingabe (x) als auch vom Zustand (Zähler) ab.
Der Zustand wird in der Regel mit Klassenvariablen (wie in unserem Beispiel) oder mit
globale Variablen.
Da zustandslose Komponenten keinen Zustand haben, können sie von mehreren Cli-
ents. Server erstellen in der Regel einen Instanzpool von zustandslosen Komponenten und verwenden sie
um eingehende Client-Anfragen zu bedienen. Auf der anderen Seite müssen zustandsabhängige Komponenten
zustandsabhängige Komponenten hingegen müssen den Konversationsstatus eines jeden Clients repräsentieren. Der Lebenszyklus zustandsloser Kompo-
Komponenten muss vom Server verwaltet werden. Zum Beispiel müssen sie bei der ersten Anfrage initialisiert
der ersten Anfrage initialisiert und zerstört werden, wenn der Client sich beendet oder eine Zeitüberschreitung eintritt. Aufgrund dieser
Aufgrund dieser Faktoren sind zustandslose Komponenten hoch skalierbar, während zustandsabhängige Komponenten
202 | Kapitel 5: Entwurfsmuster für ausfallsichere Dienste

sind teuer und schwer zu verwalten. Beim Entwurf von Unternehmensanwendungen achten die Archi-
tekten darauf, die Anzahl der zustandsabhängigen Komponenten zu minimieren. Webanwendungen,
werden zum Beispiel oft auf der Grundlage von REST-APIs entwickelt, die bei jedem Aufruf eine
Übertragung von Zuständen vom Client zum Server bei jedem Aufruf.
In einem Modell für maschinelles Lernen wird während des Trainings eine Menge an Zuständen erfasst. Dinge
Dinge wie die Epochenzahl und die Lernrate sind Teil des Zustands eines Modells und müssen
gespeichert werden, da die Lernrate in der Regel mit jeder aufeinander folgenden Epoche abnimmt.
Epoche. Indem wir sagen, dass das Modell als zustandslose Funktion exportiert werden muss, verlangen wir
dass die Entwickler des Modellrahmens diese zustandsabhängigen Variablen im Auge behalten und
sie nicht in die exportierte Datei aufzunehmen.
Wenn zustandslose Funktionen verwendet werden, vereinfacht dies den Servercode und macht ihn
skalierbar, kann aber den Client-Code komplizierter machen. Zum Beispiel sind einige Modellfunktio
tionen von Natur aus zustandsabhängig. Ein Modell zur Rechtschreibkorrektur, das ein Wort nimmt und
die korrigierte Form zurückgibt, muss zustandsbehaftet sein, da es die vorherigen Wörter kennen muss
Es muss die vorherigen Wörter kennen, um ein Wort wie "there" je nach Kontext in "their" zu korrigieren.
Modelle, die mit Sequenzen arbeiten, behalten den Verlauf bei, indem sie spezielle Strukturen wie
rekurrente neuronale Netzeinheiten. In solchen Fällen ist es erforderlich, das Modell als zustandslose Funktion zu exportieren.
Funktion zu exportieren, ist es erforderlich, die Eingabe von einem einzelnen Wort in einen Satz zu ändern.
satz. Das bedeutet, dass die Kunden eines Rechtschreibkorrekturmodells den Zustand des Modells verwalten müssen.
(um eine Folge von Wörtern zu sammeln und sie in Sätze aufzuteilen) und sie mit jeder
jeder Anfrage mitsenden. Die daraus resultierende Komplexität auf der Client-Seite wird am deutlichsten, wenn der
Client für die Rechtschreibprüfung ein vorheriges Wort aufgrund eines später hinzugefügten Kontexts ändern muss.
der später hinzugefügt wird.
Problem
Nehmen wir ein Textklassifizierungsmodell, das als Trainingsdaten Filmkritiken aus der
der Internet Movie Database (IMDb) verwendet. Für die erste Schicht des Modells werden wir eine
vortrainierte Einbettung, die den Text auf 20-dimensionale Einbettungsvektoren abbildet (für den
vollständigen Code finden Sie im serving_function.ipynb-Notebook im GitHub-Repository für dieses
Buch):

model = tf.keras.Sequential()
embedding = (
"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim-with-oov/1")
hub_layer = hub.KerasLayer(einbettung, input_shape=[],
dtype=tf.string, trainable=True, name='full_text')
model.add(hub_layer)
model.add(tf.keras.layers.Dense(16, activation='relu', name='h1_dense'))
model.add(tf.keras.layers.Dense(1, name='positive_review_logits'))
Die Einbettungsschicht wird vom TensorFlow Hub bezogen und als trainiert markiert
markiert, so dass wir eine Feinabstimmung durchführen können (siehe "Design Pattern 13: Transfer Learning" auf Seite 161 in Kapitel 4)
auf Seite 161 in Kapitel 4) auf das Vokabular der IMDb-Rezensionen durchführen können. Die anschließende

Entwurfsmuster 16: Zustandslose Servierfunktion | 203
Schichten sind die eines einfachen neuronalen Netzes mit einer verborgenen Schicht und einer Ausgangslogits
Schicht. Dieses Modell kann dann auf den Datensatz der Filmkritiken trainiert werden, um zu lernen
diktieren, ob eine Rezension positiv oder negativ ist oder nicht.

Sobald das Modell trainiert ist, können wir es verwenden, um Rückschlüsse auf die Posi
tive eine Überprüfung ist:

review1 = 'Der Film basiert auf einem preisgekrönten Roman.'
review2 = 'Der Film ist rasant und hat einige tolle Actionszenen.'
review3 = 'Der Film war sehr langweilig. Ich bin auf halbem Wege ausgestiegen.'
logits = model.predict (x=tf.constant([review1, review2, review3]))
Das Ergebnis ist ein 2D-Array, das etwa so aussehen könnte:

[[ 0.6965847]
[ 1.61773 ]
[-0.7543597]]
Es gibt mehrere Probleme bei der Durchführung von Inferenzen durch den Aufruf von model.predict()
auf ein speicherinternes Objekt (oder ein trainierbares Objekt, das in den Speicher geladen wurde) wie im
Code-Schnipsel beschrieben:

Wir müssen das gesamte Keras-Modell in den Speicher laden. Die Texteinbettungsschicht,
die so eingerichtet wurde, dass sie trainierbar ist, kann recht groß sein, da sie die
Einbettungen für das gesamte Vokabular der englischen Wörter speichern muss. Deep-Learning-Modelle mit
vielen Schichten können ebenfalls recht groß sein.
Die vorangegangene Architektur schränkt die erreichbare Latenzzeit ein
da die Aufrufe der predict()-Methode nacheinander gesendet werden müssen.
Auch wenn die bevorzugte Programmiersprache des Datenwissenschaftlers Python ist,
ist es wahrscheinlich, dass die Modellinferenz von Programmen aufgerufen wird, die von Entwicklern
die andere Sprachen bevorzugen, oder auf mobilen Plattformen wie Android oder iOS, für die
andere Sprachen erfordern.
Die Modelleingabe und -ausgabe, die für das Training am effektivsten ist, ist möglicherweise nicht benutzer
freundlich. In unserem Beispiel war die Modellausgabe logits, weil sie sich besser für den gra-
Abstieg geeignet ist. Aus diesem Grund ist die zweite Zahl im Ausgabefeld größer als
Was die Kunden in der Regel wollen, ist der Sigmoid dieses Wertes, so dass der Ausgabebereich
0 bis1 ist und in einem benutzerfreundlicheren Format als Wahrscheinlichkeit interpretiert werden kann.
Wir werden diese Nachbearbeitung auf dem Server durchführen wollen, damit der Client-Code
so einfach wie möglich ist. Ebenso kann das Modell anhand von kom- plexen, binären Datensätzen trainiert worden sein.
gepressten, binären Datensätzen trainiert worden, während wir in der Produktion in der Lage sein wollen
selbstbeschreibende Eingabeformate wie JSON verarbeiten können.
204 | Kapitel 5: Entwurfsmuster für belastbare Dienste

Lösung
Die Lösung besteht aus den folgenden Schritten:

Exportieren Sie das Modell in ein Format, das den mathematischen Kern des Modells wiedergibt
und programmiersprachenunabhängig ist.
Im Produktionssystem wird die Formel, die aus den "Vorwärts"-Berechnungen des
des Modells besteht, als zustandslose Funktion wiederhergestellt.
Die zustandslose Funktion wird in einem Framework bereitgestellt, das einen REST-Endpunkt
Punkt bereitstellt.
Modell-Export

Der erste Schritt der Lösung besteht darin, das Modell in ein Format zu exportieren (TensorFlow verwendet
SavedModel, aber ONNX ist eine andere Wahl), das den mathematischen Kern des Modells erfasst
des Modells erfasst. Der gesamte Modellzustand (Lernrate, Dropout, Kurzschluss, etc.) muss nicht
muss nicht gespeichert werden, sondern nur die mathematische Formel, die zur Berechnung der Ausgabe
aus den Eingaben zu berechnen. Üblicherweise sind die trainierten Gewichtungswerte Konstanten in der mathematischen Formel.
cal-Formel.

In Keras wird dies erreicht durch:

model.save('export/mymodel')
Das SavedModel-Format basiert auf Protokollpuffern für einen plattformneutralen, effizienten
Wiederherstellungsmechanismus. Mit anderen Worten, die Methode model.save() schreibt das Modell
als Protokollpuffer (mit der Erweiterung .pb) und externalisiert die trainierten Gewichte,
Vokabularien usw. in andere Dateien in einer Standardverzeichnisstruktur:

export/.../Variablen/Variablen.data-00000-of-00001
export/.../Assets/Tokens.txt
export/.../gespeichertes_Modell.pb
Inferenz in Python

In einem Produktionssystem wird die Formel des Modells aus dem Protokollpuffer und
anderen zugehörigen Dateien als zustandslose Funktion wiederhergestellt, die einem bestimmten Modellsigna-
mit den Namen der Eingabe- und Ausgabevariablen und den Datentypen entspricht.

Wir können das TensorFlow saved_model_cli Werkzeug benutzen, um die exportierten Dateien zu untersuchen, um
um die Signatur der zustandslosen Funktion zu bestimmen, die wir im Serving verwenden können:

saved_model_cli show --dir ${export_path} \
--tag_set serve --signature_def serving_default
Dies gibt aus:

Entwurfsmuster 16: Zustandslose Serving-Funktion | 205
Das angegebene SavedModel SignatureDef enthält die folgende(n) Eingabe(n):
inputs['full_text_input'] tensor_info:
dtype: DT_STRING
Form: (-1)
name: serving_default_full_text_input:0
Das angegebene SavedModel SignatureDef enthält die folgende(n) Ausgabe(n):
outputs['positive_review_logits'] tensor_info:
dtype: DT_FLOAT
Form: (-1, 1)
Name: StatefulPartitionedCall_2:0
Der Name der Methode ist: tensorflow/serving/predict
Die Signatur gibt an, dass die Vorhersagemethode ein Ein-Element-Array als Eingabe nimmt
(genannt full_text_input), das ein String ist, und eine Fließkommazahl ausgibt
ausgibt, deren Name positive_review_logits ist. Diese Namen stammen von den Namen, die wir
den Keras-Schichten zugewiesen haben:

hub_layer = hub.KerasLayer(..., name= 'full_text' )
...
model.add(tf.keras.layers.Dense(1, name= 'positive_review_logits' ))
So können wir die Serving-Funktion erhalten und für die Inferenz verwenden:

serving_fn = tf.keras.models.load_model(export_path). \
signatures[ 'serving_default' ]
outputs = serving_fn( full_text_input =
tf.constant([review1, review2, review3]))
logit = outputs['positive_rezension_logits']
Beachten Sie, dass wir die Eingabe- und Ausgabenamen aus der Serving-Funktion im
Code verwenden.

Web-Endpunkt erstellen

Der obige Code kann in eine Webanwendung oder ein serverloses Framework eingefügt werden, z. B.
Google App Engine, Heroku, AWS Lambda, Azure Functions, Google Cloud Func-
tions, Cloud Run und so weiter. Alle diese Frameworks haben gemeinsam, dass sie
dass sie es dem Entwickler ermöglichen, eine Funktion zu spezifizieren, die ausgeführt werden soll. Die Frameworks
Die Frameworks kümmern sich um die automatische Skalierung der Infrastruktur, um eine große Anzahl von Vorhersage
Vorhersageanfragen pro Sekunde bei geringer Latenz zu bewältigen.

Zum Beispiel können wir die Serving-Funktion von Cloud Functions aus wie folgt aufrufen
wie folgt aufrufen:

serving_fn = Keine
def handler(request):
global serving_fn
wenn serving_fn None ist:
serving_fn = (tf.keras.models.load_model(export_path)
.signatures['serving_default'])
request_json = request.get_json(silent=True)
206 | Kapitel 5: Entwurfsmuster für resilientes Serving

if request_json und 'review' in request_json:
review = request_json['review']
outputs = serving_fn( full_text_input =tf.constant([review]))
return outputs[ 'positive_review_logits' ]
Beachten Sie, dass wir darauf achten sollten, die Serving-Funktion als globale Variable zu definieren (oder als
Singleton-Klasse) zu definieren, damit sie nicht bei jeder Anfrage neu geladen wird. In der Praxis wird die
wird die Serving-Funktion nur bei Kaltstarts aus dem Exportpfad (auf Google Cloud Storage) neu geladen
nur im Falle von Kaltstarts.

Warum es funktioniert
Der Ansatz, ein Modell in eine zustandslose Funktion zu exportieren und die zustandslose
Funktion in einem Webanwendungs-Framework funktioniert, weil Webanwendungs-Frameworks
eine automatische Skalierung bieten, vollständig verwaltet werden können und sprachneutral sind. Außerdem sind sie
Software- und Geschäftsentwicklungsteams vertraut, die möglicherweise keine Erfahrung
mit maschinellem Lernen haben. Dies hat auch Vorteile für die agile Entwicklung - ein ML-Ingenieur
Ein ML-Ingenieur oder Datenwissenschaftler kann das Modell selbstständig ändern, und der Anwendungsentwickler
Der Anwendungsentwickler muss lediglich den Endpunkt ändern, auf den er zugreift.

Automatische Skalierung

Die Skalierung von Web-Endpunkten auf Millionen von Anfragen pro Sekunde ist ein wohlbekanntes technisches Problem.
technisches Problem. Anstatt spezielle Dienste für das maschinelle Lernen zu entwickeln, können wir
können wir uns auf die jahrzehntelange technische Arbeit verlassen, die in den Aufbau von stabilen
Webanwendungen und Webserver. Cloud-Anbieter wissen, wie man Web-Endpunkte
mit minimalen Aufwärmzeiten.

Wir brauchen das Serving-System nicht einmal selbst zu schreiben. Die meisten modernen Enterprise
Frameworks für maschinelles Lernen verfügen über ein Serving-Subsystem. Zum Beispiel, Tensor-
Flow bietet TensorFlow Serving und PyTorch bietet TorchServe. Wenn wir diese
können wir einfach die exportierte Datei bereitstellen und die Software kümmert sich
um die Erstellung eines Web-Endpunkts.

Vollständig verwaltet

Cloud-Plattformen abstrahieren die Verwaltung und Installation von Komponenten wie
TensorFlow Serving ebenfalls ab. Daher ist auf Google Cloud die Bereitstellung der Serving-Funktion
als REST API so einfach wie die Ausführung dieses Befehlszeilenprogramms, das den
Speicherort der SavedModel-Ausgabe:

gcloud ai-platform versions create ${MODEL_VERSION} \
--model ${MODEL_NAME} --origin ${MODEL_LOCATION} \
--laufzeit-version $TFVERSION
In Amazons SageMaker ist der Einsatz eines TensorFlow SavedModel ähnlich einfach
ähnlich simpel und wird mit:

Entwurfsmuster 16: Zustandslose Serving-Funktion | 207
model = Model(model_data=MODEL_LOCATION, role='SomeRole')
predictor = model.deploy(initial_instance_count=1,
instance_type='ml.c5.xlarge')
Wenn ein REST-Endpunkt vorhanden ist, können wir eine Vorhersageanforderung als JSON mit der
Form:

{ "Instanzen" :
[
{ "reviews" : "Der Film basiert auf einem preisgekrönten Roman."},
{ "reviews" : "Der Film ist rasant und hat einige tolle Actionszenen."},
{ "reviews" : "Der Film war sehr langweilig. Ich bin auf halbem Wege ausgestiegen."}
]
}
Wir erhalten die vorhergesagten Werte zurück, die ebenfalls in eine JSON-Struktur verpackt sind:

{ "predictions" : [{ "positive_review_logits" : [0.6965846419334412]},
{ "positive_review_logits" : [1.6177300214767456]},
{ "positive_review_logits" : [-0.754359781742096]}]}
Durch die Möglichkeit für Clients, JSON-Anfragen mit mehreren Instanzen in der Anfrage zu senden
Instanzen in der Anfrage zu senden, was als Batching bezeichnet wird, ermöglichen wir den Clients
den höheren Durchsatz, der mit weniger Netzwerkaufrufen verbunden ist, gegen
Parallelisierung abwägen, wenn sie mehr Anfragen mit weniger
Instanzen pro Anfrage senden.
Neben der Stapelverarbeitung gibt es noch andere Knöpfe und Hebel, um die Leistung zu
Leistung zu verbessern oder die Kosten zu senken. Die Verwendung einer Maschine mit leistungsfähigeren
mit leistungsfähigeren Grafikprozessoren hilft beispielsweise, die Leistung von
Modellen für tiefes Lernen. Die Wahl eines Rechners mit mehreren Beschleunigern
toren und/oder Threads hilft, die Anzahl der Anfragen pro Sekunde zu
ond. Die Verwendung eines Clusters mit automatischer Skalierung von Maschinen kann die Kosten
bei spitzen Arbeitslasten. Diese Art von Optimierungen werden oft vom
ML/DevOps-Team vorgenommen; einige sind ML-spezifisch, andere nicht.
Sprachneutral

Jede moderne Programmiersprache beherrscht REST, und es wird ein Erkennungsdienst
zur Verfügung gestellt, um die notwendigen HTTP-Stubs automatisch zu generieren. Somit können Python-Clients die
die REST-API wie folgt aufrufen. Beachten Sie, dass der folgende Code nichts Framework-spezifisches enthält
unten. Da der Cloud-Dienst die Besonderheiten unseres ML-Modells abstrahiert, brauchen wir keine
müssen wir keine Verweise auf Keras oder TensorFlow angeben:

credentials = GoogleCredentials.get_application_default()
api = discovery.build("ml", "v1", credentials = credentials,
discoveryServiceUrl = "https://storage.googleapis.com/cloud-
ml/discovery/ml_v1_discovery.json")
request_data = {"instances":
[
208 | Kapitel 5: Entwurfsmuster für robustes Serving

{"Bewertungen": "Der Film basiert auf einem preisgekrönten Roman."},
{"Kritiken": "Der Film ist rasant und hat einige tolle Actionszenen."},
{"Kritiken": "Der Film war sehr langweilig. Ich bin auf halbem Wege ausgestiegen."}
]
}
parent = "projects/{}/models/imdb".format("PROJECT", "v1")
response = api.projects().predict(body = request_data,
name = parent).execute()
Die Entsprechung des obigen Codes kann in vielen Sprachen geschrieben werden (wir zeigen Python
weil wir davon ausgehen, dass Sie mit dieser Sprache einigermaßen vertraut sind). Zum Zeitpunkt der Erstellung dieses Buches
können Entwickler auf die Discovery-API von Java, PHP, .NET, Java-
Script, Objective-C, Dart, Ruby, Node.js und Go zugreifen.

Leistungsstarkes Ökosystem

Da Webanwendungs-Frameworks so weit verbreitet sind, gibt es eine Vielzahl von Werkzeugen, die
zur Verfügung, um Webanwendungen zu messen, zu überwachen und zu verwalten. Wenn wir das ML-Modell
auf ein Webanwendungs-Framework aufsetzen, kann das Modell mit Tools überwacht
Tools überwacht und gedrosselt werden, die Software Reliability Engineers (SREs), IT-Administratoren und DevOps-Mitarbeiter
sonnel vertraut sind. Sie müssen nichts über maschinelles Lernen wissen.

Ebenso wissen Ihre Kollegen aus der Geschäftsentwicklung, wie man
Webanwendungen mithilfe von API-Gateways zu messen und zu monetarisieren. Sie können dieses Wissen übernehmen und
und auf die Messung und Monetarisierung von maschinellen Lernmodellen anwenden.

Kompromisse und Alternativen
Wie der Witz von David Wheeler sagt, besteht die Lösung für jedes Problem in der Informatik
darin, eine zusätzliche Ebene der Indirektion hinzuzufügen. Die Einführung einer exportierten zustandslosen Funktion
Spezifikation bietet diese zusätzliche Ebene der Indirektion. Das zustandslose Serving Function
Entwurfsmuster ermöglicht es uns, die Serving-Signatur zu ändern, um zusätzliche Funktionalitäten zu bieten.
Funktionalitäten, wie z. B. zusätzliche Vor- und Nachbearbeitung, die über das ML-Modell hinausgehen. In der Tat
ist es möglich, dieses Entwurfsmuster zu verwenden, um mehrere Endpunkte für ein Modell bereitzustellen.
Dieses Entwurfsmuster kann auch bei der Erstellung von Online-Vorhersagen mit niedriger Latenz für Modelle helfen
Modelle, die auf Systemen wie Data Warehouses trainiert werden, die in der Regel mit langen Abfragen verbunden sind.
mit lang laufenden Abfragen verbunden sind.

Benutzerdefinierte Servierfunktion

Die Ausgabeschicht unseres Textklassifizierungsmodells ist eine dichte Schicht, deren Ausgabe im
dem Bereich (-∞,∞) liegt:

model.add(tf.keras.layers.Dense(1, name='positive_review_logits'))
Unsere Verlustfunktion trägt dem Rechnung:

Entwurfsmuster 16: Zustandslose Serving-Funktion | 209
model.compile(optimizer='adam',
loss=tf.keras.losses.BinaryCrossentropy(
from_logits=True),
metrics=['accuracy'])
Wenn wir das Modell für die Vorhersage verwenden, gibt das Modell natürlich das zurück, wofür es trainiert wurde
trainiert wurde, und gibt die Logits aus. Was die Kunden jedoch erwarten, ist die Wahrscheinlichkeit
Wahrscheinlichkeit, dass die Bewertung positiv ausfällt. Um dieses Problem zu lösen, müssen wir die sigmoide Ausgabe
des Modells zurückgeben.

Wir können dies tun, indem wir eine benutzerdefinierte Serving-Funktion schreiben und sie stattdessen exportieren. Hier ist
eine benutzerdefinierte Serving-Funktion in Keras, die eine Wahrscheinlichkeit hinzufügt und ein Wörterbuch zurückgibt
zurückgibt, das sowohl die Logits als auch die Wahrscheinlichkeiten für jede der Rezensionen enthält, die als
Eingabe:

@tf.function(input_signature=[tf.TensorSpec([None],
dtype=tf.String)])
def add_prob( Bewertungen ):
logits = model(reviews, training=False) # Modell aufrufen
probs = tf.sigmoid(logits)
return {
positive_Bewertungen_logits' : logits,
'positive_rezension_wahrscheinlichkeit' : probs
}
Wir können dann die obige Funktion als Standardfunktion exportieren:

model.save(export_path,
signatures={'serving_default': add_prob})
Die Definition der Methode add_prob wird im export_path gespeichert und bei einer
Antwort auf eine Client-Anfrage aufgerufen.

Die Serving-Signatur des exportierten Modells spiegelt den neuen Eingabenamen (beachten Sie den
Name des Eingabeparameters für add_prob) und die Schlüssel und Datentypen des Ausgabewörterbuchs
Typen:

Das angegebene SavedModel SignatureDef enthält die folgende(n) Eingabe(n):
inputs['reviews'] tensor_info:
dtype: DT_STRING
Form: (-1)
name: serving_default_reviews:0
Das angegebene SavedModel SignatureDef enthält die folgende(n) Ausgabe(n):
outputs['positive_review_logits'] tensor_info:
dtype: DT_FLOAT
Form: (-1, 1)
Name: StatefulPartitionedCall_2:0
outputs['positive_review_probability'] tensor_info:
dtype: DT_FLOAT
Form: (-1, 1)
Name: StatefulPartitionedCall_2:1
Methodenname ist: tensorflow/serving/predict
210 | Kapitel 5: Entwurfsmuster für ausfallsicheres Serving

Wenn dieses Modell eingesetzt und zur Inferenz verwendet wird, enthält die JSON-Ausgabe sowohl
die Logits und die Wahrscheinlichkeiten:

{'Vorhersagen': [
{'positive_rezension_wahrscheinlichkeit': [0.6674301028251648],
'positive_review_logits': [0.6965846419334412]},
{ 'positive_review_probability': [0.8344818353652954] ,
'positive_review_logits': [1.6177300214767456]},
{'positive_review_probability': [0.31987208127975464],
'positive_review_logits': [-0.754359781742096]}
]}
Beachten Sie, dass add_prob eine Funktion ist, die wir schreiben. In diesem Fall haben wir die Ausgabe ein wenig nachbearbeitet.
der Ausgabe nachbearbeitet. Wir hätten jedoch so ziemlich alles (zustandslose) tun können, was
was wir innerhalb dieser Funktion tun wollten.

Mehrere Unterschriften

Es ist durchaus üblich, dass Modelle mehrere Ziele oder Kunden mit unterschiedlichen Bedürfnissen unterstützen.
unterschiedliche Bedürfnisse haben. Durch die Ausgabe eines Wörterbuchs können verschiedene Kunden das herausziehen
was sie wollen, ist dies in manchen Fällen nicht ideal. Zum Beispiel die Funktion
die wir aufrufen mussten, um eine Wahrscheinlichkeit aus den Logits zu erhalten, war einfach tf.sigmoid(). Diese
ist ziemlich billig, und es gibt kein Problem, sie zu berechnen, selbst für Kunden, die
verwerfen werden. Wäre die Funktion hingegen teuer gewesen, hätte ihre Berechnung
für Kunden, die den Wert nicht benötigen, einen erheblichen Mehraufwand bedeuten.

Wenn eine kleine Anzahl von Clients eine sehr teure Operation benötigt, ist es hilfreich, mehrere
mehrere Serving-Signaturen bereitzustellen und den Client zu veranlassen, dem Serving-Framework mitzuteilen, welche
Signatur aufgerufen werden soll. Dies geschieht durch Angabe eines anderen Namens als serving_default
wenn das Modell exportiert wird. Zum Beispiel könnten wir zwei Signaturen ausschreiben, indem wir:

model.save(export_path, signatures={
'serving_default': func1,
expensive_result': func2,
})
Dann enthält die JSON-Eingabeanforderung den Signaturnamen, um auszuwählen, welcher Serving
Endpunkt des Modells gewünscht wird:

{
"signature_name" : "expensive_result",
{ "instances" : ...}
}
Online-Vorhersage

Da die exportierte Serving-Funktion letztlich nur ein Dateiformat ist, kann sie verwendet werden, um
Online-Vorhersagefunktionen verwendet werden, wenn der ursprüngliche Trainingsrahmen für maschinelles Lernen
nicht von Haus aus Online-Vorhersagen unterstützt.

Entwurfsmuster 16: Zustandslose dienende Funktion | 211
Wir können zum Beispiel ein Modell trainieren, das ableitet, ob ein Baby Aufmerksamkeit benötigt oder nicht.
ein logistisches Regressionsmodell auf den Geburtsdatensatz trainieren:

MODELL ERSTELLEN ODER ERSETZEN
mlpatterns.neutral_3classes OPTIONS (model_type='logistic_reg',
input_label_cols=['Gesundheit']) AS
SELECT
IF
(apgar_1min = 10,
Gesund',
IF
(apgar_1min >= 8,
'Neutral',
'NeedsAttention')) AS Gesundheit,
Mehrzahl,
mutter_alter,
Schwangerschaft_Wochen,
je_geboren
FROM
`bigquery- public - data .samples.natality`
WHERE
apgar_1min <= 10
Sobald das Modell trainiert ist, können wir die Vorhersage mit SQL durchführen:

SELECT * FROM ML.PREDICT(MODEL mlpatterns.neutral_3classes,
( SELECT
2 AS Mehrzahl,
32 AS mutter_alter,
41 AS gestation_weeks,
1 AS immer_geboren
)
)
BigQuery ist jedoch in erster Linie für die verteilte Datenverarbeitung gedacht. Es war zwar großartig
für das Training des ML-Modells auf Gigabytes von Daten, aber die Verwendung eines solchen Systems zur
einer einzigen Zeile ist nicht die beste Lösung - die Latenzzeit kann bis zu einer oder zwei Sekunden betragen.
zwei. Die ML.PREDICT-Funktionalität ist eher für die Stapelverarbeitung geeignet.

Um eine Online-Vorhersage durchzuführen, können wir BigQuery bitten, das Modell als
TensorFlow SavedModel zu exportieren:

bq extract -m --destination_format= ML_TF_SAVED_MODEL \
mlpatterns.neutral_3classes gs://${BUCKET}/export/baby_health
Jetzt können wir das SavedModel in einem Serving Framework wie Cloud AI Plat-
die SavedModel unterstützt, um die Vorteile der niedrigen Latenz und der automatischen Skalierung von ML
Modells zu nutzen. Den vollständigen Code finden Sie im Notebook auf GitHub.

Selbst wenn es diese Möglichkeit, das Modell als SavedModel zu exportieren, nicht gegeben hätte, hätten wir
die Gewichte extrahiert, ein mathematisches Modell geschrieben, um das lineare Modell auszuführen,
es in einen Container packen und das Container-Image auf einer Serverplattform bereitstellen.

212 | Kapitel 5: Entwurfsmuster für robustes Serving

Vorhersage-Bibliothek

Anstatt die Serving-Funktion als Microservice zu implementieren, der über eine
REST API aufgerufen werden kann, ist es möglich, den Vorhersagecode als Bibliotheksfunktion zu implementieren. Die
Bibliotheksfunktion würde das exportierte Modell beim ersten Aufruf laden, die Funktion
model.predict() mit den bereitgestellten Eingaben aufrufen und das Ergebnis zurückgeben. Anwendungsentwickler
Anwendungsentwickler, die mit der Bibliothek Vorhersagen treffen müssen, können die Bibliothek dann in ihre
Anwendungen einbinden.

Eine Bibliotheksfunktion ist eine bessere Alternative als ein Microservice, wenn das Modell nicht
nicht über ein Netzwerk aufgerufen werden kann, entweder aus physischen Gründen (es gibt keine Netzwerkanbindung)
tivität) oder aufgrund von Leistungseinschränkungen nicht über ein Netzwerk aufgerufen werden kann. Der Ansatz der Bibliotheksfunktion
die Rechenlast auf den Client, was aus Kostengründen vorzuziehen sein könnte.
Standpunkt des Budgets vorzuziehen. Die Verwendung des Bibliotheksansatzes mit TensorFlow.js kann
Cross-Site-Probleme vermeiden, wenn der Wunsch besteht, das Modell in einem Browser laufen zu lassen.

Der größte Nachteil des Bibliotheksansatzes ist, dass die Wartung und Aktualisierung des
Modells schwierig sind - der gesamte Client-Code, der das Modell verwendet, muss aktualisiert werden, um
die neue Version der Bibliothek zu verwenden. Je häufiger ein Modell aktualisiert wird, desto
attraktiver wird ein Microservices-Ansatz. Ein zweiter Nachteil ist, dass der
Bibliotheksansatz auf Programmiersprachen beschränkt ist, für die Bibliotheken geschrieben werden.
Programmiersprachen beschränkt ist, für die Bibliotheken geschrieben werden, während der REST-API-Ansatz das Modell für Anwendungen öffnet, die in
so ziemlich jeder modernen Programmiersprache.

Der Entwickler der Bibliothek sollte darauf achten, einen Threadpool zu verwenden und die Parallelisierung
um den erforderlichen Durchsatz zu erreichen. In der Regel gibt es jedoch eine Grenze für die Skalierbarkeit.
Skalierbarkeit, die mit diesem Ansatz erreicht werden kann.

Entwurfsmuster 17: Batch Serving
Das Entwurfsmuster "Batch Serving" verwendet eine Software-Infrastruktur, die üblicherweise für
verteilte Datenverarbeitung verwendet wird, um Inferenzen auf eine große Anzahl von Instanzen
auf einmal durchzuführen.

Problem
In der Regel werden Vorhersagen einzeln und auf Abruf durchgeführt. Ob eine
ob eine Kreditkartentransaktion betrügerisch ist, wird zum Zeitpunkt der Zahlungsabwicklung
verarbeitet wird. Ob ein Baby intensivmedizinisch betreut werden muss oder nicht, wird bei der Untersuchung des
Baby unmittelbar nach der Geburt untersucht wird. Wenn Sie also ein Modell in einem
ein ML-Serving-Framework einbinden, wird es so eingerichtet, dass es eine Instanz oder höchstens ein paar Tausend Instanzen verarbeitet.
tausende von Instanzen, die in eine einzige Anfrage eingebettet sind.

Das Serving-Framework ist so konzipiert, dass es eine einzelne Anfrage synchron
und so schnell wie möglich zu verarbeiten, wie in "Design Pattern 16: Stateless Serving Func-

Entwurfsmuster 17: Batch Serving | 213
tion" auf Seite 201. Die Serving-Infrastruktur wird normalerweise als Microservice
Microservice konzipiert, der die schweren Berechnungen (z. B. bei tiefen neuronalen Faltungsnetzen)
neuronale Netze) auf Hochleistungshardware wie Tensor Processing Units (TPUs) oder
Grafikverarbeitungseinheiten (GPUs) auslagert und die mit mehreren Softwareschichten verbundene Ineffizienz
mehreren Software-Schichten.

Es gibt jedoch Umstände, unter denen Vorhersagen asynchron über große Datenmengen durchgeführt werden müssen.
asynchron über große Datenmengen durchgeführt werden müssen. Zum Beispiel kann die Entscheidung, ob eine
Lagerhaltungseinheit (SKU) nachbestellt werden soll, kann ein Vorgang sein, der stündlich durchgeführt wird, nicht jedes Mal
nicht jedes Mal, wenn die SKU an der Registrierkasse gekauft wird. Musikdienste könnten personalisierte
tägliche Wiedergabelisten für jeden ihrer Nutzer erstellen und sie an diese Nutzer weitergeben. Die pro-
sonalisierte Wiedergabeliste wird nicht bei Bedarf als Reaktion auf jede Interaktion des
Interaktion des Nutzers mit der Musiksoftware erstellt. Aus diesem Grund muss das ML-Modell
Vorhersagen für Millionen von Instanzen auf einmal treffen, nicht für eine einzelne Instanz.

Der Versuch, einen Software-Endpunkt, der für die Bearbeitung einer einzelnen Anfrage ausgelegt ist
Millionen von Artikeln oder Milliarden von Nutzern zu senden, wird das ML-Modell überfordern.
Modell überfordern.

Lösung
Das Batch Serving Design Pattern verwendet eine verteilte Datenverarbeitungsinfrastruktur
(MapReduce, Apache Spark, BigQuery, Apache Beam usw.) zur Durchführung von ML
Inferenz auf einer großen Anzahl von Instanzen asynchron durchzuführen.

In der Diskussion über das Entwurfsmuster Stateless Serving Function haben wir ein Textklassifizierungsmodell
Klassifizierungsmodell trainiert, das ausgibt, ob eine Bewertung positiv oder negativ ist. Nehmen wir an
wir wollen dieses Modell auf jede Beschwerde anwenden, die jemals bei der
United States Consumer Finance Protection Bureau (CFPB) eingereicht wurden.

Wir können das Keras-Modell wie folgt in BigQuery laden (der vollständige Code ist in
einem Notebook auf GitHub):

CREATE OR REPLACE MODEL mlpatterns.imdb_sentiment
OPTIONS ( model_type='tensorflow' , model_path='gs://.../*')
Wo man normalerweise ein Modell mit Daten in BigQuery trainieren würde, laden wir hier einfach
ein externes trainiertes Modell geladen. Danach ist es jedoch möglich, mit
BigQuery zu verwenden, um ML-Vorhersagen zu machen. Zum Beispiel die SQL-Abfrage.

SELECT * FROM ML.PREDICT (MODEL mlpatterns.imdb_sentiment,
( SELECT 'Das war sehr gut gemacht.' AS reviews)
)
ergibt eine positive_review_probability von 0,82.

Die Verwendung eines verteilten Datenverarbeitungssystems wie BigQuery zur Durchführung von einmaligen Vorhersagen
Vorhersagen durchzuführen, ist nicht sehr effizient. Was aber, wenn wir das maschinelle Lernen anwenden wollen?

214 | Kapitel 5: Entwurfsmuster für robustes Serving

1 Möchten Sie wissen, wie eine "positive" Beschwerde aussieht? Hier ist sie:
"Ich bekomme morgens, abends und nachts Anrufe. Ich habe ihnen gesagt, sie sollen die vielen Anrufe einstellen, aber sie rufen immer noch an, sogar am
Sonntag am Morgen. Ich hatte zwei Anrufe hintereinander an einem Sonntagmorgen von XXXX XXXX. Ich erhielt neun
Anrufe am Samstag. Auch unter der Woche erhalte ich jeden Tag etwa neun Anrufe.
Der einzige Hinweis darauf, dass der Beschwerdeführer unzufrieden ist, ist, dass er die Anrufer gebeten hat, damit aufzuhören. Ansonsten könnte der Rest der
Ansonsten könnte es sich bei den restlichen Aussagen durchaus um jemanden handeln, der damit prahlt, wie beliebt er ist!"
Modell für jede Beschwerde in der CFPB-Datenbank?^1 Wir können die obige Abfrage einfach anpassen
anpassen und dabei sicherstellen, dass die Spalte consumer_complaint_narrative in der inneren
SELECT als die zu bewertenden Bewertungen:

SELECT * FROM ML.PREDICT(MODEL mlpatterns.imdb_sentiment,
( SELECT verbraucher_beschwerde_erzaehlung AS bewertungen
FROM `bigquery- public - data `.cfpb_complaints.complaint_database
WHERE verbraucher_beschwerde_erzaehlung IS NOT NULL
)
)
Die Datenbank enthält mehr als 1,5 Millionen Beschwerden, die jedoch in etwa 30 Sekunden verarbeitet werden.
30 Sekunden verarbeitet, was die Vorteile eines verteilten Datenverarbeitungssystems beweist.

Warum es funktioniert
Das Entwurfsmuster für zustandslose Serving-Funktionen ist für das Serving mit niedriger Latenz zur Unterstützung
Tausende von gleichzeitigen Abfragen zu bedienen. Die Verwendung eines solchen Frameworks für gelegentliche oder
oder periodische Verarbeitung von Millionen von Elementen kann ziemlich teuer werden. Wenn diese Anfragen
sind, ist es kostengünstiger, eine verteilte Datenverarbeitungsarchitektur zu verwenden, um
Architektur zu verwenden, um Modelle des maschinellen Lernens für Millionen von Objekten aufzurufen. Der Grund dafür ist
dass der Aufruf eines ML-Modells für Millionen von Objekten ein peinlich paralleles Prob- lem ist.
Der Grund dafür ist, dass der Aufruf eines ML-Modells für Millionen von Objekten ein peinlich paralleles Problem ist: Es ist möglich, eine Million Objekte zu nehmen, sie in 1.000 Gruppen zu je 1.000 Objekten aufzuteilen
1.000 Elementen aufzuteilen, jede Gruppe von Elementen an eine Maschine zu senden und dann die Ergebnisse zu kombinieren.
Das Ergebnis des maschinellen Lernmodells für den Artikel Nummer 2.000 ist völlig unabhängig vom Ergebnis der
Das Ergebnis des maschinellen Lernmodells für Artikel Nr. 3.000 ist völlig unabhängig vom Ergebnis des maschinellen Lernmodells für Artikel Nr. 3.000, so dass es
ist es möglich, die Arbeit aufzuteilen und sie zu bewältigen.

Nehmen wir zum Beispiel die Abfrage nach den fünf positivsten Beschwerden:

WITH all_complaints AS (
SELECT * FROM ML.PREDICT(MODEL mlpatterns.imdb_sentiment,
( SELECT verbraucher_beschwerde_erzaehlung AS reviews
FROM `bigquery- public - data `.cfpb_complaints.complaint_database
WHERE verbraucher_beschwerde_erzaehlung IS NOT NULL
)
)
)
SELECT * FROM all_complaints
ORDER BY positive_rezension_wahrscheinlichkeit DESC LIMIT 5
Entwurfsmuster 17: Batch-Serving | 215
Wenn wir uns die Ausführungsdetails in der BigQuery-Webkonsole ansehen, sehen wir, dass die gesamte
Abfrage 35 Sekunden gedauert hat (siehe das Feld mit der Nummer 1 in Abbildung 5-1).

Abbildung 5-1. Die ersten beiden Schritte einer Abfrage zur Ermittlung der fünf "positivsten" Beschwerden in
dem Datensatz des Consumer Financial Protection Bureau für Verbraucherbeschwerden.

Der erste Schritt (siehe Kasten #2 in Abbildung 5-1) liest die Spalte consumer_complaint_narrative
Spalte aus dem öffentlichen BigQuery-Datensatz, in der der Beschwerdebericht nicht NULL ist.
Aus der Anzahl der in Feld Nr. 3 hervorgehobenen Zeilen geht hervor, dass dies das Lesen von
1.582.045 Werte. Die Ausgabe dieses Schritts wird in 10 Shards geschrieben (siehe Feld Nr. 4 in
Abbildung 5-1).

Der zweite Schritt liest die Daten aus diesem Shard (beachten Sie das $12:shard in der Abfrage), aber
erhält auch den file_path und file_contents des maschinellen Lernmodells
imdb_sentiment und wendet das Modell auf die Daten in jedem Shard an. Die Art und Weise, wie Map-
Reduce funktioniert so, dass jeder Shard von einem Worker verarbeitet wird, so dass die Tatsache, dass es 10
Shards gibt, bedeutet, dass der zweite Schritt von 10 Arbeitern ausgeführt wird. Die ursprünglichen 1,5
Millionen Zeilen wären über viele Dateien hinweg gespeichert worden, und so wurde der erste Schritt wahrscheinlich
so viele Arbeiter wie die Anzahl der Dateien, aus denen der Datensatz bestand, verarbeitet wurden.
Datensatzes.

Die weiteren Schritte sind in Abbildung 5-2 dargestellt.

216 | Kapitel 5: Entwurfsmuster für robustes Serving

Abbildung 5-2. Dritter und weitere Schritte der Abfrage zur Ermittlung der fünf "positivsten" Beschwerden
Beschwerden.

Der dritte Schritt sortiert den Datensatz in absteigender Reihenfolge und dauert fünf. Dies geschieht für
jeder Arbeiter, so dass jeder der 10 Arbeiter die 5 positivsten Beschwerden in "seinem" Shard findet.
Scherbe. Die verbleibenden Schritte rufen die restlichen Daten ab, formatieren sie und schreiben sie
sie in die Ausgabe.

Im letzten Schritt (nicht abgebildet) werden die 50 Beschwerden sortiert und die 5 ausgewählt, die
das eigentliche Ergebnis bilden. Die Fähigkeit, die Arbeit auf diese Weise auf viele Mitarbeiter zu verteilen, ist
BigQuery in der Lage, den gesamten Vorgang für 1,5 Millionen Beschwerde
Dokumenten in 35 Sekunden durchzuführen.

Kompromisse und Alternativen
Das Batch Serving Design Pattern hängt von der Möglichkeit ab, eine Aufgabe auf mehrere
Arbeiter aufzuteilen. Es ist also nicht auf Data Warehouses oder sogar SQL beschränkt. Jedes MapReduce
Framework wird funktionieren. SQL-Data-Warehouses sind jedoch in der Regel am einfachsten und werden
oft die Standardwahl, insbesondere wenn die Daten strukturiert sind.

Auch wenn das Batch-Serving verwendet wird, wenn die Latenzzeit keine Rolle spielt, ist es möglich
vorberechnete Ergebnisse und eine regelmäßige Auffrischung einzubeziehen, um dies in Szenarien zu nutzen
in denen der Raum der möglichen Vorhersageeingaben begrenzt ist.

Batch- und Stream-Pipelines

Frameworks wie Apache Spark oder Apache Beam sind nützlich, wenn der Input vorverarbeitet werden muss
Verarbeitung benötigt, bevor sie dem Modell zugeführt werden kann, wenn die Ergebnisse des maschinellen
Modellergebnisse eine Nachbearbeitung erfordern oder wenn entweder die Vorbearbeitung oder die Nachbearbeitung schwer
in SQL auszudrücken sind. Handelt es sich bei den Eingaben für das Modell um Bilder, Audio- oder Videodaten, dann ist SQL
keine Option und es muss ein Datenverarbeitungssystem verwendet werden, das mit unstrukturierten
unstrukturierte Daten verarbeiten kann. Diese Frameworks können auch die Vorteile von beschleunigter Hard-
Hardware wie TPUs und GPUs nutzen, um die Vorverarbeitung der Bilder durchzuführen.

Entwurfsmuster 17: Batch Serving | 217
Ein weiterer Grund für den Einsatz eines Frameworks wie Apache Beam ist, wenn der Client-Code
Zustand aufrechterhalten muss. Ein häufiger Grund, warum der Client einen Zustand beibehalten muss, ist, wenn eine der
eine der Eingaben in das ML-Modell ein Zeitfenster-Durchschnitt ist. In diesem Fall muss der Client-Code
gleitende Mittelwerte des eingehenden Datenstroms durchführen und den gleitenden
gleitenden Mittelwert an das ML-Modell liefern.

Stellen Sie sich vor, wir bauen ein System zur Moderation von Kommentaren auf und möchten
die mehr als zwei Mal am Tag über eine bestimmte Person kommentieren. Zum Beispiel
Beispiel: Die ersten beiden Male, die ein Kommentator etwas über Präsident Obama schreibt,
lassen wir es durchgehen, blockieren aber alle Versuche des Kommentators, Präsident Obama zu erwähnen
Obama für den Rest des Tages. Dies ist ein Beispiel für eine Nachbearbeitung, bei der der
Status beibehalten, da wir einen Zähler benötigen, der angibt, wie oft jeder Kommentator
Kommentator einen bestimmten Prominenten erwähnt hat. Außerdem muss dieser Zähler über einen
einen rotierenden Zeitraum von 24 Stunden.

Wir können dies mit einem verteilten Datenverarbeitungssystem erreichen, das den
Zustand. Hier kommt Apache Beam ins Spiel. Aufrufen eines ML-Modells zur Identifizierung von Erwähnungen eines Prominenten
zu identifizieren und sie mit einem kanonischen Wissensgraphen zu verknüpfen (so dass eine Erwähnung von Obama und eine
Erwähnung von Präsident Obama beide mit en.wikipedia.org/wiki/Barack_Obama verknüpft sind) von
Apache Beam kann mit folgendem Vorgehen erreicht werden (siehe dieses Notebook auf GitHub
für vollständigen Code):

| beam.Map( lambda x : nlp.Document(x, type='PLAIN_TEXT'))
| nlp.AnnotateText(features)
| beam.Map(parse_nlp_result)
wobei parse_nlp_result die JSON-Anfrage analysiert, die durch die Annotate
Text-Transformation durchläuft, die im Verborgenen eine NLP-API aufruft.

Zwischengespeicherte Ergebnisse der Stapelverarbeitung

Wir haben das Batch Serving als eine Möglichkeit diskutiert, ein Modell über Millionen von Elementen aufzurufen, wenn
das Modell normalerweise online mit Hilfe der zustandslosen Serving Function Design Pat
tern. Natürlich kann das Batch Serving auch dann funktionieren, wenn das Modell nicht
Online-Serving nicht unterstützt. Entscheidend ist, dass der Rahmen für maschinelles Lernen, der die
Inferenz in der Lage ist, die Vorteile einer peinlich parallelen Verarbeitung zu nutzen.

Empfehlungsmaschinen müssen beispielsweise eine spärliche Matrix ausfüllen, die aus
jedem Nutzer-Element-Paar. Ein typisches Unternehmen hat vielleicht 10 Millionen Nutzer in allen Zeiten und
10.000 Artikel im Produktkatalog. Um eine Empfehlung für einen Benutzer auszusprechen,
muss für jeden der 10.000 Artikel eine Empfehlungsbewertung berechnet und eine Rangfolge erstellt werden,
und die besten 5 dem Nutzer präsentiert werden. Dies ist nicht möglich, in nahezu Echtzeit von einer
Servierfunktion. Die Forderung nach Echtzeit bedeutet jedoch, dass eine einfache Stapelverarbeitung
auch nicht funktionieren wird.

Verwenden Sie in solchen Fällen die Stapelverarbeitung, um Empfehlungen für alle 10 Millionen Nutzer zu berechnen.
Benutzer:

218 | Kapitel 5: Entwurfsmuster für robustes Serving

SELECT
*
FROM
ML.RECOMMEND(MODEL mlpatterns.recommendation_model)
Speichern Sie es in einer relationalen Datenbank wie MySQL, Datastore oder Cloud Spanner (es gibt
(es gibt vorgefertigte Übertragungsdienste und Dataflow-Vorlagen, die dies tun können). Wenn ein
Benutzer besucht, werden die Empfehlungen für diesen Benutzer aus der Datenbank gezogen und
sofort und mit sehr geringer Latenzzeit bereitgestellt.

Im Hintergrund werden die Empfehlungen in regelmäßigen Abständen aufgefrischt. Zum Beispiel könnten wir
das Empfehlungsmodell stündlich auf der Grundlage der letzten Aktionen auf der
Website. Dann können wir die Schlussfolgerungen nur für die Nutzer durchführen, die die Website in der letzten
Stunde besucht haben:

SELECT
*
FROM
ML.RECOMMEND(MODEL mlpatterns.recommendation_model,
(
SELECT DISTINCT
visitorId
FROM
mlpatterns.analytics_session_data
WHERE
visitTime > TIME_DIFF( CURRENT_TIME (), 1 HOUR)
))
Wir können dann die entsprechenden Zeilen in der relationalen Datenbank aktualisieren, die für die
dienen.

Lambda-Architektur

Ein ML-Produktionssystem, das sowohl Online-Serving als auch Batch-Serving unterstützt, wird
wird als Lambda-Architektur bezeichnet - ein solches ML-Produktionssystem ermöglicht ML-Fachleuten
einen Kompromiss zwischen Latenz (über das Stateless Serving Function-Muster) und Durchsatz (über das
Durchsatz (über das Batch Serving-Muster).

AWS Lambda ist, trotz seines Namens, keine Lambda-Architektur. Es
ist ein serverloses Framework zur Skalierung zustandsloser Funktionen, ähnlich wie
Google Cloud Functions oder Azure Functions.
Typischerweise wird eine Lambda-Architektur durch getrennte Systeme für Online
Serving und Batch Serving unterstützt. In Google Cloud beispielsweise wird die Online-Serving-Infra-
die Online-Serving-Infrastruktur von der Cloud AI Platform Predictions und die Batch-Serving
Batch-Serving-Infrastruktur wird von BigQuery und Cloud Dataflow bereitgestellt (Cloud AI Platform
Predictions bietet eine bequeme Schnittstelle, so dass die Benutzer nicht explizit auf die Daten zugreifen müssen.

Entwurfsmuster 17: Batch Serving | 219
Dataflow). Es ist möglich, ein TensorFlow-Modell zu nehmen und es in BigQuery zu importieren
Batch-Serving. Es ist auch möglich, ein trainiertes BigQuery ML Modell zu nehmen und es als
ein TensorFlow SavedModel für die Online-Auslieferung zu exportieren. Diese zweiseitige Kompatibilität ermöglicht
Nutzer von Google Cloud, jeden Punkt im Spektrum der Latenz-Durchsatz-Abwägung
Kompromiss zu treffen.

Entwurfsmuster 18: Fortgesetzte Modellauswertung
Das Entwurfsmuster "Continued Model Evaluation" behandelt das häufige Problem, dass
zu erkennen und Maßnahmen zu ergreifen, wenn ein eingesetztes Modell nicht mehr für
Zweck ist.

Problem
Sie haben also Ihr Modell trainiert. Sie haben die Rohdaten gesammelt, sie bereinigt, Merkmale
Merkmale, erstellten Einbettungsebenen, stimmten Hyperparameter ab, das ganze Drum und Dran.
Sie erreichen eine Genauigkeit von 96 % bei Ihrem Hold-out-Testsatz. Erstaunlich! Sie haben sogar
Sie haben sogar den mühsamen Prozess der Bereitstellung Ihres Modells durchlaufen, indem Sie es von einem
Jupyter-Notizbuch zu einem Modell für maschinelles Lernen in der Produktion und liefern Vorhersagen
Vorhersagen über eine REST-API. Glückwunsch, Sie haben es geschafft. Sie sind fertig!

Nun, nicht ganz. Der Einsatz ist nicht das Ende des Lebenszyklus eines maschinellen Lernmodells.
Woher wissen Sie, dass Ihr Modell in der freien Wildbahn wie erwartet funktioniert? Was ist, wenn es
unerwartete Änderungen in den eingehenden Daten gibt? Oder das Modell nicht mehr
keine genauen oder nützlichen Vorhersagen mehr? Wie werden diese Änderungen erkannt?

Die Welt ist dynamisch, aber bei der Entwicklung eines maschinellen Lernmodells wird in der Regel ein
statisches Modell aus historischen Daten. Das bedeutet, dass das Modell, sobald es in Produktion geht
Sobald das Modell in Produktion geht, kann es sich verschlechtern und seine Vorhersagen können zunehmend unzuverlässiger werden. Zwei
Zwei der Hauptgründe für die Verschlechterung von Modellen im Laufe der Zeit sind Konzept- und Datendrift.

Konzeptdrift tritt immer dann auf, wenn sich die Beziehung zwischen den Modelleingaben und dem Ziel
geändert haben. Dies geschieht oft, weil sich die zugrunde liegenden Annahmen Ihres Modells
geändert haben, z. B. bei Modellen, die für das Erlernen von gegnerischem oder wettbewerbsorientiertem Verhalten trainiert wurden
wie z. B. Betrugserkennung, Spam-Filter, Börsenhandel, Online-Anzeigenschaltung oder Cybersecurity.
kurität. In diesen Szenarien zielt ein prädiktives Modell darauf ab, Muster zu erkennen, die charakteristisch für erwünschtes (oder unerwünschtes) Verhalten sind.
die für erwünschte (oder unerwünschte) Aktivitäten kennzeichnend sind, während der Gegner lernt, sich anzupassen und
und sein Verhalten bei veränderten Umständen ändern kann. Denken Sie zum Beispiel an ein Modell
entwickelt, um Kreditkartenbetrug zu erkennen. Die Art und Weise, wie Menschen Kreditkarten benutzen, hat sich
hat sich im Laufe der Zeit verändert, und damit haben sich auch die gemeinsamen Merkmale von Kreditkartenbetrug
geändert. Als beispielsweise die "Chip und Pin"-Technologie eingeführt wurde, verlagerten sich betrügerische
Transaktionen immer mehr ins Internet verlagert. Als sich das betrügerische Verhalten anpasste, wurde die Perfor-
die Leistung eines Modells, das vor dieser Technologie entwickelt worden war, plötzlich
und die Modellvorhersagen würden ungenauer werden.

220 | Kapitel 5: Entwurfsmuster für robustes Serving

Ein weiterer Grund für die Verschlechterung der Leistung eines Modells im Laufe der Zeit ist die Datendrift. Wir
haben das Problem der Datendrift in "Allgemeine Herausforderungen beim maschinellen Lernen"
auf Seite 11 in Kapitel 1 vorgestellt. Die Datendrift bezieht sich auf jede Änderung, die an den Daten vorgenommen wurde
die dem Modell zur Vorhersage zugeführt werden, im Vergleich zu den Daten, die zum
Training verwendet wurden. Eine Datendrift kann aus verschiedenen Gründen auftreten: Das Schema der Eingabedaten ändert sich
an der Quelle (z. B. Hinzufügen oder Löschen von Feldern im Vorfeld), Merkmalsverteilungen
tionen ändern sich im Laufe der Zeit (z. B. könnte ein Krankenhaus anfangen, mehr jüngere
Erwachsene, weil in der Nähe ein Skigebiet eröffnet wurde), oder die Bedeutung der Daten ändert sich, auch wenn
die Struktur/das Schema sich nicht ändert (z. B. kann sich im Laufe der Zeit ändern, ob ein Patient als "übergewichtig" gilt).
übergewichtig" ist, kann sich im Laufe der Zeit ändern). Software-Updates könnten neue Fehler einführen oder das
Geschäftsszenario ändert sich und erzeugt eine neue Produktbezeichnung, die zuvor nicht in den
den Trainingsdaten vorhanden war. ETL-Pipelines zum Erstellen, Trainieren und Vorhersagen mit ML-Modellen
Modells können spröde und undurchsichtig sein, und jede dieser Änderungen hätte drastische Auswirkungen auf die
die Leistung Ihres Modells haben.

Die Modellentwicklung ist ein kontinuierlicher Prozess, und um die Konzept- oder Datendrift zu
ist es notwendig, den Trainingsdatensatz zu aktualisieren und das Modell mit frischen Daten neu zu trainieren
Daten, um die Vorhersagen zu verbessern. Aber woher wissen Sie, wann ein erneutes Training notwendig ist?
Und wie oft sollten Sie neu trainieren? Die Datenvorverarbeitung und das Modelltraining können
zeit- und kostenintensiv sein, und jeder Schritt des Modellentwicklungszyklus verursacht
zusätzliche Kosten für Entwicklung, Überwachung und Wartung.

Lösung
Der direkteste Weg, eine Verschlechterung des Modells festzustellen, ist die kontinuierliche Überwachung der
Vorhersageleistung des Modells im Laufe der Zeit zu überwachen und diese Leistung mit denselben
Bewertungsmetriken, die Sie während der Entwicklung verwendet haben. Durch diese Art der kontinuierlichen Modell
Bewertung und Überwachung des Modells können wir feststellen, ob das Modell oder alle Änderungen
die wir an dem Modell vorgenommen haben, so funktionieren, wie sie sollten.

Konzept

Eine kontinuierliche Bewertung dieser Art erfordert den Zugriff auf die Rohdaten der Vorhersageanforderung
Vorhersagedaten und den vom Modell generierten Vorhersagen sowie der Grundwahrheit, die alle an einem
Ort. Google Cloud AI Platform bietet die Möglichkeit, die eingesetzte
Modellversion so zu konfigurieren, dass die Online-Vorhersageeingabe und -ausgabe regelmäßig abgetastet
und in einer Tabelle in BigQuery gespeichert werden. Um die Leistung des Dienstes für eine große
Anzahl von Anfragen pro Sekunde zu halten, können wir die Menge der abgerufenen Daten anpassen, indem wir einen
einen Prozentsatz der Anzahl der Eingabeanfragen festlegen. Um die Leistung zu messen
metriken zu messen, ist es notwendig, diese gespeicherte Stichprobe von Vorhersagen mit der
Grundwahrheit.

In den meisten Situationen kann es einige Zeit dauern, bis die Ground-Truth-Labels verfügbar sind.
Bei einem Abwanderungsmodell kann es zum Beispiel sein, dass es erst beim nächsten Abonnement bekannt wird

Entwurfsmuster 18: Fortgesetzte Modellbewertung | 221
Zyklus, welche Kunden ihren Dienst eingestellt haben. Oder, für ein Finanzprognosemodell
Finanzprognosemodell ist der tatsächliche Umsatz erst nach Abschluss des Quartals und dem
Bericht. In beiden Fällen kann die Auswertung erst erfolgen, wenn die wahren Daten
verfügbar sind.

Um zu sehen, wie die kontinuierliche Bewertung funktioniert, werden wir ein Textklassifizierungsmodell
das mit dem HackerNews-Datensatz trainiert wurde, auf der Google Cloud AI Platform. Der vollständige Code für
dieses Beispiels finden Sie im Continuous Evaluation Notebook im Repository
zu diesem Buch.

Einsatz des Modells

Die Eingabe für unseren Trainingsdatensatz ist ein Artikeltitel und das zugehörige Label ist die Nachrichtenquelle
Quelle, aus der der Artikel stammt, entweder nytimes, techcrunch oder github. Da sich Nachrichten
entwickeln sich die Wörter, die mit einer Schlagzeile der New York Times assoziiert sind, im Laufe der Zeit
ändern. In ähnlicher Weise wirken sich die Veröffentlichungen neuer Technologieprodukte auf die Wörter aus, die
in TechCrunch zu finden sind. Die kontinuierliche Auswertung ermöglicht es uns, die Modellvorhersagen zu überwachen.
Vorhersagen des Modells zu überwachen, um zu verfolgen, wie sich diese Trends auf die Leistung des Modells auswirken und
wenn nötig.

Angenommen, das Modell wird mit einer benutzerdefinierten Serving-Eingabefunktion exportiert, wie unter
in "Entwurfsmuster 16: Zustandslose Serving-Funktion" auf Seite 201:

@tf.function(input_signature=[tf.TensorSpec([None], dtype=tf.string)])
def source_name(text):
labels = tf.constant(['github', 'nytimes', 'techcrunch'],dtype=tf.string)
probs = txtcls_model(text, training=False)
indizes = tf.argmax(probs, axis=1)
pred_source = tf.gather(params=labels, indices=indices)
pred_confidence = tf.reduce_max(probs, axis=1)
return {'Quelle': pred_source,
Konfidenz': pred_confidence}
Wenn wir dieses Modell einsetzen und eine Online-Vorhersage machen, wird das Modell
das Modell die vorhergesagte Nachrichtenquelle als Zeichenkette und einen numerischen Wert dieser Vorhersage zurück.
tion zurück, die angibt, wie sicher das Modell ist. Zum Beispiel können wir eine Online
Vorhersage erstellen, indem wir ein JSON-Eingabebeispiel in eine Datei namens input.json schreiben und zur
Vorhersage:

%%Schreibdatei input.json
{ "text" :
"YouTube führt Videokapitel ein, um das Navigieren in längeren Videos zu erleichtern"}
Dies ergibt die folgende Vorhersageausgabe:

VERTRAUEN QUELLE
0,918685 techcrunch
222 | Kapitel 5: Entwurfsmuster für robustes Serving

Speichern von Vorhersagen

Sobald das Modell implementiert ist, können wir einen Auftrag einrichten, um eine Stichprobe der Vorhersageanforderungen zu speichern.
Der Grund für die Speicherung einer Stichprobe und nicht aller Anfragen ist die Vermeidung einer unnötigen
unnötige Verlangsamung des Serving-Systems zu vermeiden. Dies kann im Abschnitt "Kontinuierliche Auswertung
der Google Cloud AI Platform (CAIP) Konsole tun, indem wir den LabelKey
(die Spalte, die die Ausgabe des Modells ist, in unserem Fall die Quelle, da wir die
Quelle, da wir die Quelle des Artikels vorhersagen), einen ScoreKey in den Vorhersageausgaben (ein
numerischer Wert, in unserem Fall die Konfidenz), und eine Tabelle in BigQuery, in der ein
Teil der Online-Vorhersageanfragen gespeichert wird. In unserem Beispielcode heißt die Tabelle
txtcls_eval.swivel genannt. Sobald dies konfiguriert ist, werden bei jeder Online-Vorhersage
CAIP bei jeder Online-Vorhersage den Modellnamen, die Modellversion, den Zeitstempel der
Zeitstempel der Vorhersageanforderung, die Rohdaten der Vorhersage und die Ausgabe des Modells an die
fizierte BigQuery-Tabelle, wie in Tabelle 5-1 dargestellt.

Tabelle 5-1. Ein Teil der Online-Vorhersageanfragen und die Rohvorhersageausgabe werden
in einer Tabelle in BigQuery gespeichert

Zeile modell modell_version zeit rohdaten roh_vorhersage groundtruth
1 txtcls Wirbel 2020-06-10
01:40:32 UTC
{"instances": [{"text":
"Astronauten docken an
Raumstation nach
Historischen SpaceX
Start"}]}
{"Vorhersagen": [{"source":
"github", "confidence":
0.9994275569915771}]}
null
2 txtcls swivel 2020-06-10
01:37:46 UTC
{"instances": [{"text":
"Senat bestätigt erste
Schwarzen Luftwaffen
Chief"}]}
{"predictions": [{"source":
"nytimes", "confidence":
0.9989787340164185}]}
null
3 txtcls swivel 2020-06-09
21:21:47 UTC
{"instances": [{"text":
"Eine native Mac-App
Wrapper für WhatsApp
Web"}]}
{"predictions": [{"source":
"github", "confidence":
0.745254397392273}]}
null
Erfassen der Grundwahrheit

Es ist auch notwendig, die Grundwahrheit für jede der Instanzen zu erfassen, die an das
Modell zur Vorhersage übermittelt werden. Dies kann je nach Anwendungsfall und Datenverfügbarkeit auf verschiedene Weise
Anwendungsfall und Datenverfügbarkeit. Ein Ansatz wäre die Verwendung eines menschlichen Beschriftungsdienstes.
Alle Instanzen, die dem Modell zur Vorhersage übermittelt werden, oder vielleicht nur diejenigen, bei denen das
Modell ein geringes Vertrauen genießt, werden zur Kommentierung durch den Menschen weitergeleitet. Die meisten Cloud-Pro-
Anbieter bieten irgendeine Form eines Human Labeling Service an, um die Instanzen auf
Instanzen auf diese Weise zu ermöglichen.

Ground-Truth-Labels können auch daraus abgeleitet werden, wie Nutzer mit dem Modell und seinen
seinen Vorhersagen. Indem man die Benutzer eine bestimmte Aktion ausführen lässt, kann man implizites
Feedback für die Vorhersage eines Modells zu erhalten oder ein Ground-Truth-Label zu erzeugen. Zum Beispiel,

Entwurfsmuster 18: Fortgesetzte Modellbewertung | 223
Wenn ein Nutzer eine der vorgeschlagenen alternativen Routen in Google Maps auswählt, dient die gewählte Route als implizite
Route als implizite Grundwahrheit. Expliziter ausgedrückt: Wenn ein Nutzer einen empfohlenen Film bewertet, ist dies ein
Wenn ein Nutzer einen empfohlenen Film bewertet, ist dies ein klarer Hinweis auf die Basiswahrheit für ein Modell, das
das die Nutzerbewertungen vorhersagen soll, um Empfehlungen auszusprechen. Ähnlich verhält es sich, wenn das Modell
wenn das Modell dem Nutzer erlaubt, die Vorhersage zu ändern, z. B. in der Medizin, wenn ein
Arzt die Möglichkeit hat, die vom Modell vorgeschlagene Diagnose zu ändern, ist dies ein klares Signal für
die Grundwahrheit.

Es ist wichtig zu bedenken, wie die Rückkopplungsschleife von Modell
Modellvorhersagen und der Erfassung der Grundwahrheit die Trainingsdaten
auswirken. Nehmen wir zum Beispiel an, Sie haben ein Modell entwickelt, das vorhersagt
vorhersagen, wann ein Einkaufswagen verlassen wird. Sie können sogar
den Status des Warenkorbs in regelmäßigen Abständen überprüfen, um die
für die Modellbewertung zu erstellen. Wenn Ihr Modell jedoch nahelegt, dass ein Benutzer
den Einkaufswagen abbricht und Sie bieten ihm kostenlosen Versand
oder einen Rabatt anbieten, um sein Verhalten zu beeinflussen, dann werden Sie nie
wissen, ob die ursprüngliche Modellvorhersage richtig war. Kurz gesagt, Sie haben
gegen die Annahmen des Modellauswertungsdesigns verstoßen und werden
müssen Sie die Ground-Truth-Labels auf andere Weise ermitteln. Diese Aufgabe der
Schätzung eines bestimmten Ergebnisses unter einem anderen Szenario wird
wird als kontrafaktische Argumentation bezeichnet und tritt häufig in Anwendungsfällen auf
wie Betrugserkennung, Medizin und Werbung, wo die Vorhersagen eines Modells
Vorhersagen eines Modells wahrscheinlich zu einem Eingriff führen, der das Lernen
die tatsächliche Basiswahrheit für dieses Beispiel verschleiern kann.
Bewertung der Modellleistung

Zu Beginn ist die Spalte groundtruth der Tabelle txtcls_eval.swivel in BigQuery
leer gelassen. Wir können die Ground-Truth-Labels bereitstellen, sobald sie verfügbar sind, indem wir den Wert direkt mit einem SQL-Befehl aktualisieren.
den Wert direkt mit einem SQL-Befehl aktualisieren. Natürlich sollten wir sicherstellen, dass die
Bodenwahrheit verfügbar ist, bevor wir einen Auswertungsauftrag ausführen. Beachten Sie, dass die Ground Truth
der gleichen JSON-Struktur folgt wie die Vorhersageausgabe des Modells:

UPDATE
txtcls_eval.swivel
SET
groundtruth = '{"predictions": [{"Quelle": "techcrunch"}]}'
WHERE
raw_data = '{"instances":
[{"text": "YouTube führt Videokapitel ein, um die Navigation in längeren
Videos"}]}'
Um mehr Zeilen zu aktualisieren, würden wir eine MERGE-Anweisung anstelle einer UPDATE-Anweisung verwenden. Sobald die
der Tabelle hinzugefügt wurde, ist es möglich, die Texteingabe und die Vorhersage des
und die Vorhersage Ihres Modells zu überprüfen und mit der Basiswahrheit zu vergleichen (siehe Tabelle 5-2):

224 | Kapitel 5: Entwurfsmuster für robustes Serving

SELECT
model,
model_version,
Zeit,
REGEXP_EXTRACT(raw_data, r'.*"text": "(.*)"') AS text,
REGEXP_EXTRACT(raw_prediction, r'.*"Quelle": "(.*?)"') AS Vorhersage,
REGEXP_EXTRACT(raw_prediction, r'.*"confidence": (0.\d{2}).*') AS confidence,
REGEXP_EXTRACT(groundtruth, r'.*"source": "(.*?)"') AS Grundwahrheit,
FROM
txtcls_eval.swivel
Tabelle 5-2. Sobald die Grundwahrheit verfügbar ist, kann sie zur ursprünglichen BigQuery-Tabelle hinzugefügt werden
und die Leistung des Modells kann bewertet werden

Zeile modell modell_version zeit text vorhersage zuversicht groundtruth
1 txtcls Wirbel 2020-06-10
01:38:13 UTC
Eine native Mac-App
Wrapper für WhatsApp
Web
github 0.77 github
2 txtcls schwenken 2020-06-10
01:37:46 UTC
Der Senat bestätigt den ersten
schwarzen Luftwaffenchef
nytimes 0.99 nytimes
3 txtcls schwenken 2020-06-10
01:40:32 UTC
Astronauten docken an
Raumstation nach
historischem SpaceX-Start
github 0.99 nytimes
4 txtcls schwenken 2020-06-09
21:21:44 UTC
YouTube führt ein
Video-Kapitel ein, um die
leichter zu navigieren
längere Videos
techcrunch 0.77 techcrunch
Mit diesen Informationen, die in BigQuery verfügbar sind, können wir die Bewertungstabelle in einen
Datenframe df_evals laden und direkt die Bewertungsmetriken für diese Modellversion berechnen.
Da es sich um eine Mehrklassen-Klassifizierung handelt, können wir die Präzision, die Wiedererkennung und die F1-
Wert für jede Klasse berechnen. Wir können auch eine Konfusionsmatrix erstellen, mit der wir analysieren können
wo die Vorhersagen des Modells innerhalb bestimmter kategorischer Bezeichnungen beeinträchtigt werden können. Abbildung 5-3
zeigt die Konfusionsmatrix, in der die Vorhersagen des Modells mit der Basis
Wahrheit.

Entwurfsmuster 18: Fortgesetzte Modellbewertung | 225
Abbildung 5-3. Eine Konfusionsmatrix zeigt alle Paare von Grundwahrheitsbezeichnungen und Vorhersagen.
Sie können die Leistung Ihres Modells in verschiedenen Klassen untersuchen.

Kontinuierliche Bewertung

Wir sollten sicherstellen, dass die Ausgabetabelle auch die Modellversion und den Zeitstempel der
Zeitstempel der Vorhersageanfragen, damit wir dieselbe Tabelle für die kontinuierliche Auswertung
Auswertung von zwei verschiedenen Modellversionen verwenden können, um die Metriken zwischen den Modellen zu vergleichen. Für
Wenn wir zum Beispiel eine neuere Version unseres Modells mit dem Namen swivel_v2 einsetzen, die auf
die auf aktuelleren Daten trainiert wurde oder andere Hyperparameter hat, können wir ihre Leistung vergleichen, indem wir die
Leistung vergleichen, indem wir den Evaluierungsdatenrahmen entsprechend der Modellversion aufteilen:

df_v1 = df_evals[df_evals.version == "swivel"]
df_v2 = df_evals[df_evals.version == "swivel_v2"]
Auf ähnliche Weise können wir Auswertungsabschnitte in der Zeit erstellen, die sich nur auf Modellvorhersagen
innerhalb des letzten Monats oder der letzten Woche:

heute = pd.Zeitstempel.jetzt(tz='UTC')
one_month_ago = heute - pd.DateOffset(months=1)
one_week_ago = heute - pd.DateOffset(Wochen=1)
df_prev_month = df_evals[df_evals.time >= one_month_ago]
df_prev_week = df_evals[df_evals.time >= one_week_ago]
Um die obigen Auswertungen kontinuierlich auszuführen, kann das Notebook (oder ein containerisiertes
Formular) zeitgesteuert werden. Wir können es so einrichten, dass eine Modellumschulung ausgelöst wird, wenn die Auswertungsmetrik
tionsmetrik unter einen bestimmten Schwellenwert fällt.

226 | Kapitel 5: Entwurfsmuster für robustes Serving

Warum es funktioniert
Bei der Entwicklung von Modellen für maschinelles Lernen wird implizit davon ausgegangen, dass die
Trainings-, Validierungs- und Testdaten aus der gleichen Verteilung stammen, wie in
Abbildung 5-4. Wenn wir Modelle in der Produktion einsetzen, impliziert diese Annahme, dass
zukünftige Daten den vergangenen Daten ähnlich sind. Sobald sich das Modell jedoch in der Produktion "in
ist, ist diese statische Annahme für die Daten möglicherweise nicht mehr gültig. In der Tat sind viele
ML-Systeme in der Produktion treffen auf sich schnell ändernde, nicht-stationäre Daten, und
werden mit der Zeit veraltet, was sich negativ auf die Qualität der Vorhersagen auswirkt.

Abbildung 5-4. Bei der Entwicklung eines maschinellen Lernmodells stammen die Trainings-, Validierungs- und Testdaten
Daten aus der gleichen Datenverteilung stammen. Sobald das Modell jedoch zum Einsatz kommt, kann sich diese
kann sich diese Verteilung jedoch ändern, was die Leistung des Modells erheblich beeinträchtigt.

Die kontinuierliche Modellbewertung bietet einen Rahmen, um die Leistung eines eingesetzten Modells
Leistung eines Modells ausschließlich anhand neuer Daten zu bewerten. Dies ermöglicht es uns, die Unbeständigkeit eines Modells so früh wie
wie möglich zu erkennen. Anhand dieser Informationen lässt sich bestimmen, wie oft ein Modell neu trainiert werden muss oder
wann es durch eine völlig neue Version ersetzt werden sollte.

Durch die Erfassung von Vorhersage-Eingaben und -Ausgaben und den Vergleich mit der Basiswahrheit ist es
ist es möglich, die Modellleistung quantitativ zu verfolgen oder zu messen, wie verschiedene Modellversionen
Modellversionen mit A/B-Tests in der aktuellen Umgebung, ohne Rücksicht darauf, wie
wie die Versionen in der Vergangenheit abgeschnitten haben.

Kompromisse und Alternativen
Das Ziel der kontinuierlichen Evaluierung ist es, ein Mittel zur Überwachung der Modellleistung zu schaffen
Leistung zu überwachen und die Modelle in der Produktion auf dem neuesten Stand zu halten. Auf diese Weise liefert die kontinuierliche Auswertung
einen Auslöser dafür, wann das Modell neu trainiert werden muss. In diesem Fall ist es wichtig, folgende Punkte zu berücksichtigen
Toleranzschwellen für die Modellleistung, die damit verbundenen Abwägungen und die Rolle der
geplante Umschulung. Es gibt auch Techniken und Werkzeuge, wie TFX, die helfen
Daten- und Konzeptdrift präventiv zu erkennen, indem die Verteilungen der Eingabedaten direkt überwacht werden.

Entwurfsmuster 18: Fortgesetzte Modellbewertung | 227
Auslöser für die Umschulung

Die Leistung eines Modells lässt in der Regel mit der Zeit nach. Die kontinuierliche Bewertung ermöglicht es Ihnen
strukturiert zu messen und bietet einen Auslöser, um das Modell neu zu trainieren.
des Modells. Heißt das also, dass Sie Ihr Modell neu trainieren sollten, sobald die Leistung
einbricht? Das kommt darauf an. Die Antwort auf diese Frage hängt stark vom Geschäftszweck ab und
Anwendungsfall und sollte zusammen mit den Bewertungsmetriken und der Modellbewertung diskutiert werden.
Je nach Komplexität des Modells und der ETL-Pipelines können die Kosten für eine Umschulung
teuer sein. Es gilt abzuwägen, welches Ausmaß an Leistungsverschlechterung
Leistung im Verhältnis zu diesen Kosten akzeptabel ist.

Serverlose Auslöser
Cloud Functions, AWS Lambda und Azure Functions bieten serverlose Möglichkeiten zur
Umschulung über Auslöser zu automatisieren. Der Auslösertyp bestimmt, wie und wann Ihre
Funktion ausgeführt wird. Diese Auslöser können Nachrichten sein, die in einer Nachrichtenwarteschlange veröffentlicht werden, eine
Änderungsbenachrichtigung von einem Cloud-Speicher-Bucket, die anzeigt, dass eine neue Datei hinzugefügt wurde,
Änderungen an Daten in einer Datenbank oder sogar eine HTTPS-Anfrage. Sobald das Ereignis ausgelöst wurde,
wird der Funktionscode ausgeführt.
Im Kontext der Umschulung wäre der Auslöser für das Cloud-Ereignis eine signifikante Änderung oder
Verschlechterung der Modellgenauigkeit. Die Funktion bzw. Aktion würde darin bestehen, die Schulungspipeline aufzurufen
Pipeline aufzurufen, um das Modell neu zu trainieren und die neue Version einzusetzen. "Entwurfsmuster 25: Work-
flow Pipeline" auf Seite 282 beschreibt, wie dies erreicht werden kann. Workflow-Pipelines
containerisieren und orchestrieren den End-to-End-Workflow für maschinelles Lernen von der Daten
Datenerfassung und -validierung bis hin zur Modellerstellung, dem Training und der Bereitstellung. Sobald die neue
Modellversion bereitgestellt wurde, kann sie mit der aktuellen Version verglichen werden, um
um festzustellen, ob sie ersetzt werden sollte.
Der Schwellenwert selbst kann als absoluter Wert festgelegt werden, z. B. wird das Modell neu trainiert
erfolgt, sobald die Modellgenauigkeit unter 95 % fällt. Oder der Schwellenwert kann als Rate der
Veränderung der Leistung festgelegt werden, z. B. wenn die Leistung beginnt, einen Abwärtstrend
Abwärtstrend beginnt. Unabhängig davon, welcher Ansatz gewählt wird, ist die Philosophie für die Wahl des Schwellenwerts
ist die Philosophie für die Wahl des Schwellenwerts ähnlich wie die für das Checkpointing von Modellen während des Trainings. Mit einem höheren, sensibleren
Mit einem höheren, sensibleren Schwellenwert bleiben die Modelle in der Produktion frisch, aber es entstehen höhere Kosten für frei-
häufiges Neutrainieren sowie der technische Aufwand für die Pflege und den Wechsel zwischen
verschiedenen Modellversionen. Bei einem niedrigeren Schwellenwert sinken die Schulungskosten, aber die Modelle
aber die Modelle in der Produktion sind veralteter. Abbildung 5-5 zeigt diesen Zielkonflikt zwischen der Perfor-
Leistungsschwelle und wie sie sich auf die Anzahl der Modellumschulungen auswirkt.

Wenn die Pipeline für die Modellumschulung automatisch durch einen solchen Schwellenwert ausgelöst wird, ist es
ist es wichtig, auch die Auslöser zu verfolgen und zu validieren. Wenn Sie nicht wissen, wann Ihr Modell
neu trainiert wurde, führt unweigerlich zu Problemen. Selbst wenn der Prozess automatisiert ist, müssen Sie

228 | Kapitel 5: Entwurfsmuster für robustes Serving

Sie sollten immer die Kontrolle über die Umschulung Ihres Modells haben, um es besser zu verstehen und
das Modell in der Produktion zu verbessern.

Abbildung 5-5. Ein höherer Schwellenwert für die Modellleistung sorgt für ein qualitativ hochwertigeres
Modell in der Produktion, erfordert aber häufigere Nachschulungen, was kostspielig sein kann.

Geplante Umschulung

Die kontinuierliche Bewertung liefert ein wichtiges Signal, um zu wissen, wann es notwendig ist
Ihr Modell neu zu trainieren. Dieser Prozess der Umschulung wird häufig durch eine Feinabstimmung des
vorherigen Modells unter Verwendung neu gesammelter Trainingsdaten. Wo eine kontinuierliche Auswertung
täglich erfolgen kann, können geplante Umschulungen nur jede Woche oder jeden Monat erfolgen
Monat erfolgen (Abbildung 5-6).

Sobald eine neue Version des Modells trainiert ist, wird ihre Leistung mit der
aktuellen Modellversion verglichen. Das aktualisierte Modell wird nur dann als Ersatz eingesetzt, wenn es
das vorherige Modell in Bezug auf einen Testsatz mit aktuellen Daten übertrifft.

Abbildung 5-6. Bei der kontinuierlichen Bewertung wird das Modell jeden Tag bewertet, wenn neue Daten
gesammelt werden. Regelmäßige Nachschulung und Modellvergleiche ermöglichen eine Bewertung zu diskreten
Zeitpunkten.

Wie oft sollten Sie also eine Umschulung planen? Der Zeitrahmen für die Umschulung hängt ab
dem Geschäftszweck, dem Vorhandensein neuer Daten und den Kosten (in Form von Zeit und Geld) für die
Ausführung der Umschulungspipeline ab. Manchmal bestimmt natürlich der Zeithorizont des Modells
der Zeithorizont des Modells den Zeitpunkt für die Planung von Umschulungsaufträgen. Wenn das Ziel des Modells zum Beispiel darin besteht
die Gewinne des nächsten Quartals vorherzusagen, da Sie neue Ground-Truth-Labels nur

Entwurfsmuster 18: Fortgesetzte Modellbewertung | 229
einmal pro Quartal zu trainieren, ist es nicht sinnvoll, häufiger zu trainieren als das. Allerdings,
wenn die Menge und das Auftreten neuer Daten hoch ist, wäre es von Vorteil, häufiger
häufiger neu zu trainieren. Die extremste Version dieses Problems ist das maschinelle Online-Lernen.
Einige Anwendungen des maschinellen Lernens, wie z. B. die Anzeigenschaltung oder die Empfehlung von Newsfeeds, erfordern
oder die Empfehlung von Newsfeeds, erfordern Online-Entscheidungen in Echtzeit und können ihre Leistung kontinuierlich verbessern.
Leistung verbessern, indem sie mit jedem neuen Trainingsbeispiel neu trainieren und die Parametergewichte
Beispiel.

Im Allgemeinen ist der optimale Zeitrahmen etwas, das Sie als Praktiker
durch Erfahrung und Experimentieren. Wenn Sie versuchen, eine sich schnell bewegende Aufgabe zu modellieren
Aufgabe zu modellieren, wie z. B. gegnerisches oder konkurrierendes Verhalten, dann ist es sinnvoll, einen freieren
häufigen Umschulungszeitplan festzulegen. Wenn das Problem relativ statisch ist, wie die Vorhersage des Geburtsgewichts eines Babys
Gewicht eines Babys, dann sollten weniger häufige Retrainings ausreichen.

In jedem Fall ist es hilfreich, eine automatisierte Pipeline einzurichten, die den
den gesamten Umschulungsprozess mit einem einzigen API-Aufruf durchführen kann. Tools wie Cloud Composer/Apache Air-
flow und AI Platform Pipelines sind nützlich, um ML-Workflows zu erstellen, zu planen und zu überwachen.
Arbeitsabläufe zu planen und zu überwachen - von der Vorverarbeitung der Rohdaten und dem Training bis hin zur Abstimmung der Hyperparameter und
Einsatz. Wir diskutieren dies im nächsten Kapitel unter "Design Pattern 25: Work-
Fluss-Pipeline".

Datenvalidierung mit TFX

Datenverteilungen können sich im Laufe der Zeit ändern, wie in Abbildung 5-7 dargestellt. Betrachten Sie zum Beispiel
man den Datensatz zum Geburtsgewicht. Da sich die medizinischen und gesellschaftlichen Standards
ändern sich im Laufe der Zeit die Beziehungen zwischen Modellmerkmalen wie dem Alter der Mutter oder der
Anzahl der Schwangerschaftswochen, in Bezug auf die Modellbezeichnung, das Gewicht des
Babys. Diese Datendrift wirkt sich negativ auf die Fähigkeit des Modells aus, auf neue Daten zu verallgemeinern.
Kurz gesagt, Ihr Modell ist veraltet und muss anhand neuer Daten neu trainiert werden.

Abbildung 5-7. Datenverteilungen können sich im Laufe der Zeit ändern. Die Datendrift bezieht sich auf jede Änderung
die an den Daten, die Ihrem Modell zur Vorhersage zugeführt werden, im Vergleich zu den
Daten, die für das Training verwendet wurden.

230 | Kapitel 5: Entwurfsmuster für robustes Serving

Während die kontinuierliche Evaluierung eine Post-hoc-Methode zur Überwachung eines eingesetzten
Modells bietet, ist es auch wertvoll, die neuen Daten zu überwachen, die während der Bereitstellung eingehen, und
Änderungen in den Datenverteilungen präventiv zu erkennen.

Die Datenvalidierung von TFX ist ein nützliches Werkzeug, um dies zu erreichen. TFX ist eine End-to-End-Plat
Plattform für den Einsatz von Machine-Learning-Modellen, die von Google zur Verfügung gestellt wird. Die Data Val-
idation-Bibliothek kann verwendet werden, um die beim Training verwendeten Datenbeispiele mit denen zu vergleichen, die
die während der Bereitstellung gesammelt wurden. Validitätsprüfungen erkennen Anomalien in den Daten, Trainings- und
Trainingsdaten oder Datendrift. TensorFlow Data Validation erstellt Datenvisualisierungen
mit Facets, einem Open-Source-Visualisierungswerkzeug für maschinelles Lernen. Die Facets
Übersicht gibt einen Überblick über die Verteilungen von Werten über verschiedene Merkmale
und kann verschiedene häufige und ungewöhnliche Probleme aufdecken, wie unerwartete Feature-Werte
ues, fehlende Merkmalswerte und trainingsbedingte Verzerrungen.

Schätzung des Umschulungsintervalls

Eine nützliche und relativ kostengünstige Taktik, um zu verstehen, wie sich Daten- und Konzeptdrift
zu verstehen, wie sich Daten und Konzeptdrift auf Ihr Modell auswirken, ist es, ein Modell nur mit veralteten Daten zu trainieren und die Leistung des Modells
dieses Modells mit aktuelleren Daten zu bewerten (Abbildung 5-8). Dies imitiert den fortlaufenden Modell
Evaluierungsprozess in einer Offline-Umgebung. Das heißt, man sammelt Daten von vor sechs Monaten oder
Daten von vor sechs Monaten oder einem Jahr sammeln und den üblichen Modellentwicklungsprozess durchlaufen, indem Merkmale generiert
turen, die Optimierung von Hyperparametern und die Erfassung relevanter Bewertungsmetriken. Dann,
Sie diese Bewertungskennzahlen mit den Modellvorhersagen für aktuellere Daten
die nur einen Monat zuvor gesammelt wurden. Wie viel schlechter schneidet Ihr veraltetes Modell
bei den aktuellen Daten? So lässt sich gut abschätzen, wie schnell die Leistung eines Modells
Leistung eines Modells im Laufe der Zeit abnimmt und wie oft ein neues Training erforderlich sein könnte.

Entwurfsmuster 18: Fortgesetzte Modellbewertung | 231
Abbildung 5-8. Das Trainieren eines Modells auf veralteten Daten und die Auswertung auf aktuellen Daten imitiert den
fortgesetzte Modellevaluierung in einer Offline-Umgebung.

Entwurfsmuster 19: Zwei-Phasen-Vorhersagen
Das Two-Phase-Predictions-Entwurfsmuster bietet eine Möglichkeit, das Problem der
große, komplexe Modelle performant zu halten, wenn sie auf verteilten Geräten eingesetzt werden
verteilten Geräten eingesetzt werden, indem die Anwendungsfälle in zwei Phasen aufgeteilt werden, wobei nur die einfachere
Phase auf dem Edge ausgeführt wird.

Problem
Beim Einsatz von Modellen des maschinellen Lernens können wir uns nicht immer darauf verlassen, dass die Endnutzer über eine
zuverlässige Internetverbindungen haben. In solchen Situationen werden Modelle am Rande des Systems bereitgestellt.
-d.h. sie werden auf das Gerät des Benutzers geladen und benötigen keine Internetverbindung, um
Internetverbindung benötigen, um Vorhersagen zu erstellen. Angesichts der Gerätebeschränkungen müssen Modelle, die am Rande
in der Regel kleiner sein als Modelle, die in der Cloud eingesetzt werden, und erfordern daher
und erfordern daher einen Kompromiss zwischen Modellkomplexität und -größe, Aktualisierungshäufigkeit, Genauigkeit
Genauigkeit und geringer Latenz.

Es gibt verschiedene Szenarien, in denen wir unser Modell auf einem Edge-Gerät einsetzen möchten.
Ein Beispiel ist ein Fitness-Tracking-Gerät, bei dem ein Modell Empfehlungen für
ein Modell Empfehlungen für die Benutzer auf der Grundlage ihrer Aktivität, die durch Beschleunigungsmesser und Gyroskopbewegungen
Bewegung. Es ist wahrscheinlich, dass ein Benutzer in einem abgelegenen Außenbereich ohne Kon- taktivität trainiert.
aktivität. In diesen Fällen soll unsere Anwendung trotzdem funktionieren. Ein weiteres Beispiel ist
eine Umweltanwendung, die Temperatur- und andere Umweltdaten nutzt, um
um Vorhersagen über zukünftige Trends zu treffen. In diesen beiden Beispielen müssen wir, auch wenn wir inter-

232 | Kapitel 5: Entwurfsmuster für robustes Serving

Netzkonnektivität kann es langsam und teuer sein, kontinuierlich Vorhersagen
aus einem in der Cloud bereitgestellten Modell zu erstellen.

Um ein trainiertes Modell in ein Format zu konvertieren, das auf Edge-Geräten funktioniert, durchlaufen die Modelle häufig einen
einen Prozess durchlaufen, der als Quantisierung bekannt ist und bei dem die gelernten Modellgewichte mit weniger Bytes dargestellt werden.
mit weniger Bytes dargestellt werden. TensorFlow, zum Beispiel, verwendet ein Format namens TensorFlow
Lite, um gespeicherte Modelle in ein kleineres Format zu konvertieren, das für den Einsatz am Rand optimiert ist.
Zusätzlich zur Quantisierung können Modelle, die für Edge-Geräte bestimmt sind, auch
kleiner anfangen, um in die strengen Speicher- und Prozessoreinschränkungen zu passen.

Die Quantisierung und andere von TF Lite eingesetzte Techniken reduzieren die Größe
und die Vorhersage-Latenzzeit der resultierenden ML-Modelle, was jedoch zu einer geringeren
Modellgenauigkeit. Da wir uns außerdem nicht immer darauf verlassen können, dass Edge-Geräte über
Konnektivität verlassen können, stellt die rechtzeitige Bereitstellung neuer Modellversionen für diese Geräte
eine Herausforderung dar.

Wie sich diese Abwägungen in der Praxis auswirken, zeigt ein Blick auf die Optionen für
Training von Kantenmodellen in Cloud AutoML Vision in Abbildung 5-9.

Abbildung 5-9. Abwägen zwischen Genauigkeit, Modellgröße und Latenz für Modelle
die in der Cloud AutoML Vision am Rande eingesetzt werden.

Um diesen Kompromissen Rechnung zu tragen, brauchen wir eine Lösung, die ein Gleichgewicht zwischen der geringeren Größe und
Latenzzeit von Edge-Modellen gegen die zusätzliche Raffinesse und Genauigkeit von Cloud
Modelle ausgleicht.

Entwurfsmuster 19: Zwei-Phasen-Vorhersagen | 233
Lösung
Mit dem Two-Phase Predictions Design Pattern teilen wir unser Problem in zwei Teile auf.
Wir beginnen mit einem kleineren, kostengünstigeren Modell, das auf dem Gerät bereitgestellt werden kann. Da dieses
Modell typischerweise eine einfachere Aufgabe hat, kann es diese Aufgabe auf dem Gerät mit relativ hoher Genauigkeit erfüllen.
mit relativ hoher Genauigkeit erfüllen. Darauf folgt ein zweites, komplexeres Modell, das in der
der Cloud bereitgestellt und nur bei Bedarf ausgelöst wird. Natürlich erfordert dieses Entwurfsmuster
Natürlich erfordert dieses Entwurfsmuster ein Problem, das in zwei Teile mit unterschiedlichem Komplexitätsgrad aufgeteilt werden kann.
Komplexität. Ein Beispiel für ein solches Problem sind intelligente Geräte wie Google Home, die
durch ein Weckwort aktiviert werden und dann Fragen beantworten und auf Befehle reagieren können
Wecker stellen, Nachrichten lesen und mit integrierten Geräten wie Licht und Thermostat interagieren.
wie Lampen und Thermostaten. Google Home wird zum Beispiel aktiviert, indem man "OK
Google" oder "Hey Google". Sobald das Gerät ein Weckwort erkannt hat, können Nutzer
komplexere Fragen stellen wie "Kannst du einen Termin mit Sara um 10 Uhr vereinbaren?".

Dieses Problem lässt sich in zwei verschiedene Teile aufteilen: ein erstes Modell, das auf ein
und ein komplexeres Modell, das jede andere Benutzeranfrage verstehen und beantworten kann.
andere Benutzeranfragen. Beide Modelle werden eine Audioerkennung durchführen. Das erste Modell,
muss jedoch nur eine binäre Klassifizierung durchführen: Stimmt der Ton, den es gerade gehört hat
mit dem Signalwort überein oder nicht? Obwohl dieses Modell von der Komplexität her einfacher ist, muss es
muss es ständig laufen, was teuer wird, wenn es in der Cloud eingesetzt wird. Das
zweite Modell erfordert Audioerkennung und natürliches Sprachverständnis, um die
um die Anfrage des Nutzers zu analysieren. Dieses Modell muss nur laufen, wenn ein Nutzer eine Frage stellt
Frage stellt, legt aber mehr Wert auf eine hohe Genauigkeit. Das Two-Phase Predictions
Muster kann dieses Problem lösen, indem das Modell für den Wortschatz auf dem Gerät und das komplexere
komplexeren Modells in der Cloud.

Neben diesem Anwendungsfall für intelligente Geräte gibt es viele andere Situationen, in denen das
Zwei-Phasen-Vorhersagemuster verwendet werden kann. Nehmen wir an, Sie arbeiten in einer Fabrik
in der viele verschiedene Maschinen gleichzeitig laufen. Wenn eine Maschine
Wenn eine Maschine nicht mehr richtig funktioniert, macht sie normalerweise ein Geräusch, das mit einer Fehlfunktion in Verbindung gebracht werden kann.
Funktion zugeordnet werden kann. Es gibt verschiedene Geräusche, die mit jeder einzelnen Maschine und
den verschiedenen Arten, wie eine Maschine defekt sein kann. Im Idealfall können Sie ein Modell erstellen, das
problematische Geräusche zu erkennen und zu identifizieren, was sie bedeuten. Mit Two-Phase Predictions können Sie
können Sie ein Offline-Modell erstellen, um anomale Geräusche zu erkennen. Ein zweites Cloud-Modell
kann dann feststellen, ob das übliche Geräusch auf eine Fehlfunktion hinweist.
Zustand hinweist.

Sie können das Zwei-Phasen-Vorhersagemuster auch für ein bildbasiertes Szenario verwenden.
Nehmen wir an, Sie haben Kameras in der freien Wildbahn aufgestellt, um gefährdete Arten zu identifizieren und zu verfolgen.
Arten zu verfolgen. Sie können ein Modell auf dem Gerät haben, das erkennt, ob das zuletzt aufgenommene Bild
Bild ein bedrohtes Tier enthält. Wenn dies der Fall ist, kann dieses Bild an ein
Cloud-Modell gesendet werden, das die spezifische Tierart auf dem Bild bestimmt.

234 | Kapitel 5: Entwurfsmuster für robustes Serving

Zur Veranschaulichung des Zwei-Phasen-Vorhersagemusters verwenden wir einen allgemeinen
Erkennungsdatensatz von Kaggle. Der Datensatz enthält etwa 9.000 Audiobeispiele von
Geräuschen mit insgesamt 41 Kategorien, darunter "Cello", "Klopfen", "Telefon
Telefon", "Trompete" und mehr. Die erste Phase unserer Lösung wird ein Modell sein, das
vorhersagt, ob es sich bei dem gegebenen Geräusch um ein Musikinstrument handelt oder nicht. Dann, für Klänge
Klänge, die nach dem ersten Modell ein Instrument sind, erhalten wir eine Vorhersage von einem Modell
das in der Cloud bereitgestellt wird, um das spezifische Instrument aus insgesamt 18 möglichen
Optionen. Abbildung 5-10 zeigt den zweiphasigen Ablauf für dieses Beispiel.

Abbildung 5-10. Verwendung des Zwei-Phasen-Vorhersagemusters zur Identifizierung von Instrumentenklängen.

Um jedes dieser Modelle zu erstellen, werden wir die Audiodaten in Spektrogramme umwandeln, die
visuelle Darstellungen des Klangs sind. Dadurch können wir gängige Bildmodellarchitekturen
Architekturen zusammen mit dem Transfer Learning Design Pattern zu verwenden, um dieses Problem zu lösen.
Siehe Abbildung 5-11 für ein Spektrogramm eines Saxophon-Audioclips aus unserem Datensatz.

Entwurfsmuster 19: Zwei-Phasen-Vorhersagen | 235
Abbildung 5-11. Die Bilddarstellung (Spektrogramm) eines Saxophon-Audioclips aus
unserem Trainingsdatensatz. Der Code zum Konvertieren von .wav-Dateien in Spektrogramme ist im
GitHub-Repository.

Phase 1: Aufbau des Offline-Modells

Das erste Modell unserer Zwei-Phasen-Vorhersagelösung sollte so klein sein, dass es
dass es auf ein mobiles Gerät geladen werden kann, um schnelle Schlüsse zu ziehen, ohne dass eine Internetverbindung erforderlich ist.
Aktivität angewiesen ist. Aufbauend auf dem oben vorgestellten Instrumentenbeispiel werden wir ein
Beispiel für die erste Vorhersagephase, indem wir ein binäres Klassifikationsmodell erstellen, das
das für die Inferenz auf dem Gerät optimiert ist.

Der ursprüngliche Sound-Datensatz hat 41 Labels für verschiedene Arten von Audioclips. Unser erstes
Modell wird nur zwei Bezeichnungen haben: "Instrument" oder "kein Instrument". Wir erstellen unser
Modell mit Hilfe der MobileNetV2-Modellarchitektur, die auf dem ImageNet-Datensatz trainiert wurde.
MobileNetV2 ist direkt in Keras verfügbar und ist eine Architektur, die für Modelle optimiert ist, die auf dem Gerät bedient werden.
els, die auf dem Gerät bedient werden sollen. Für unser Modell frieren wir die MobileNetV2
Gewichte einfrieren und ohne den oberen Teil laden, damit wir unsere eigene binäre Klassifizierung hinzufügen können
Ausgabeschicht hinzufügen können:

mobileet = tf.keras.applications.MobileNetV2(
input_shape=((128,128,3)),
include_top=False,
weights='imagenet'
)
mobileet.trainable = False
Wenn wir unsere Spektrogrammbilder in Verzeichnissen mit dem entsprechenden Label
Namen organisieren, können wir die ImageDataGenerator-Klasse von Keras verwenden, um unsere Trainings- und
und Bewertungsdatensätze erstellen:

236 | Kapitel 5: Entwurfsmuster für robustes Serving

train_data_gen = image_generator.flow_from_directory(
directory=data_dir,
batch_size=32,
shuffle=True,
target_size=(128,128),
classes = ['not_instrument','instrument'],
class_mode='binary')
Mit unseren Trainings- und Validierungsdatensätzen können wir das Modell wie gewohnt trainieren.
wie wir es normalerweise tun würden. Die typische Herangehensweise für den Export von trainierten Modellen zur Weitergabe ist die Verwendung von
TensorFlow's model.save() Methode. Denken Sie jedoch daran, dass dieses Modell
serviert wird, und deshalb wollen wir es so klein wie möglich halten. Um ein
Modell zu erstellen, das diese Anforderungen erfüllt, werden wir TensorFlow Lite verwenden, eine Bibliothek, die für die
Modelle direkt auf mobilen und eingebetteten Geräten zu erstellen und auszuliefern, die möglicherweise keine
zuverlässige Internetverbindung haben. TF Lite hat einige eingebaute Dienstprogramme zur Quantisierung
Modelle sowohl während als auch nach dem Training.

Um das trainierte Modell für das Edge-Serving vorzubereiten, verwenden wir TF Lite, um es in einem optimierten Format zu exportieren
optimierten Format zu exportieren:

Konverter = tf.lite.TFLiteConverter.from_keras_model(model)
konverter.optimierungen = [tf.lite.Optimize.DEFAULT]
tflite_model = converter.convert()
open('converted_model.tflite', 'wb').write(tflite_model)
Dies ist der schnellste Weg, ein Modell nach dem Training zu quantisieren. Bei Verwendung der TF-Lite-Optimierungs
tionsvorgaben werden die Gewichte unseres Modells auf ihre 8-Bit-Darstellung reduziert. Es wird
Eingaben zur Inferenzzeit quantisiert, wenn wir Vorhersagen für unser Modell treffen. Unter
Ausführen des obigen Codes ist das exportierte TF-Lite-Modell nur noch ein Viertel so groß
wäre, wenn wir es ohne Quantisierung exportiert hätten.

Um Ihr Modell für die Offline-Inferenz weiter zu optimieren, können Sie auch
die Gewichte Ihres Modells während des Trainings quantifizieren oder alle
mathematischen Operationen Ihres Modells zusätzlich zu den Gewichtungen quantifizieren. Zum Zeitpunkt der
Zeitpunkt des Schreibens ist quantisierungsoptimiertes Training für TensorFlow 2-Modelle
els auf der Roadmap.
Um eine Vorhersage für ein TF Lite Modell zu generieren, verwenden Sie den TF Lite Interpreter, der
der für niedrige Latenzzeiten optimiert ist. Wahrscheinlich werden Sie Ihr Modell auf ein Android- oder
iOS-Gerät laden und Vorhersagen direkt aus Ihrem Anwendungscode generieren. Es gibt
APIs für beide Plattformen, aber wir zeigen hier den Python-Code für die Erzeugung von Vorhersagen
zeigen wir hier, damit Sie ihn aus demselben Notebook ausführen können, in dem Sie Ihr Modell erstellt haben.
Zunächst erstellen wir eine Instanz des Interpreters von TF Lite und erfahren Details über das erwartete Eingabe- und
Ausgabeformat, das er erwartet:

interpreter = tf.lite.Interpreter(model_path="converted_model.tflite")
interpreter.allocate_tensors()
Entwurfsmuster 19: Zwei-Phasen-Vorhersagen | 237
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()
Für das MobileNetV2-Binärklassifikationsmodell, das wir oben trainiert haben, sieht input_details
wie folgt aus:

[{'dtype': numpy.float32,
'index': 0,
name': 'mobiletv2_1.00_128_input',
'quantization': (0.0, 0),
'quantization_parameters': {'quantized_dimension': 0,
'Skalen': array([], dtype=float32),
'zero_points': array([], dtype=int32)},
'shape': array([ 1, 128, 128, 3], dtype=int32),
'shape_signature': array([ 1, 128, 128, 3], dtype=int32),
'sparsity_parameters': {}}]
Anschließend übergeben wir das erste Bild aus unserem Validierungsstapel an das geladene TF-Lite-Modell
zur Vorhersage, rufen den Interpreter auf und erhalten die Ausgabe:

input_data = np.array([image_batch[21]], dtype=np.float32)
interpreter.set_tensor(eingabe_daten[0]['index'], eingabe_daten)
interpreter.invoke()
output_data = interpreter.get_tensor(output_details[0]['index'])
print (output_data)
Die resultierende Ausgabe ist ein sigmoides Array mit einem einzelnen Wert im Bereich [0,1], der angibt, ob die
der angibt, ob der eingegebene Klang ein Instrument ist oder nicht.

Je nachdem, wie kostspielig es ist, Ihr Cloud-Modell aufzurufen, können Sie
können Sie die Metrik ändern, für die Sie beim Training des On-Device-Modells optimieren
Gerät-Modell trainieren. Sie können zum Beispiel die Vorhersagegenauigkeit
cision statt Recall zu optimieren, wenn Ihnen die Vermeidung von Fehlalarmen wichtiger ist.
Da unser Modell nun auf dem Gerät arbeitet, können wir schnelle Vorhersagen erhalten, ohne
ohne auf eine Internetverbindung angewiesen zu sein. Wenn das Modell sicher ist, dass es sich bei einem bestimmten Geräusch nicht um ein
Instrument ist, können wir hier aufhören. Wenn das Modell "Instrument" vorhersagt, ist es an der Zeit, weiterzumachen
indem es den Audioclip an ein komplexeres, in der Cloud gehostetes Modell sendet.

Welche Modelle sind für die Kante geeignet?
Wie sollten Sie feststellen, ob ein Modell für die Kante geeignet ist? Es gibt einige
Es gibt einige Überlegungen, die sich auf die Modellgröße, die Komplexität und die verfügbare Hardware beziehen. Als
Faustregel gilt, dass kleinere, weniger komplexe Modelle besser für die Ausführung auf dem Gerät optimiert sind.
auf dem Gerät. Dies liegt daran, dass Edge-Modelle durch den verfügbaren Gerätespeicher begrenzt sind.
Alter. Wenn Modelle durch Quantisierung oder andere Techniken verkleinert werden, geht dies häufig
-geht dies oft auf Kosten der Genauigkeit. Daher sind Modelle mit einer einfacheren Vorhersage
238 | Kapitel 5: Entwurfsmuster für robustes Serving

Die Aufgaben- und Modellarchitektur eignet sich am besten für Edge-Geräte. Mit "einfacher" meinen wir
Kompromisse, wie z. B. die Bevorzugung der Binärklassifizierung gegenüber der Multiklassenklassifizierung oder die Wahl einer weniger komplexen
eine weniger komplexe Modellarchitektur (wie einen Entscheidungsbaum oder ein lineares Regressionsmodell), wenn
möglich ist.
Wenn Sie Modelle am Rande einsetzen und dabei bestimmte Einschränkungen in Bezug auf Modellgröße und Komplexität einhalten müssen
Modellgröße und -komplexität einhalten müssen, lohnt es sich, Edge-Hardware in Betracht zu ziehen, die speziell für
speziell für ML-Inferenz entwickelt wurde. Das Coral Edge TPU-Board zum Beispiel bietet einen
ASIC, der für leistungsstarke Offline-ML-Inferenz auf TensorFlow
Lite-Modelle. In ähnlicher Weise bietet NVIDIA den Jetson Nano für Edge-optimierte, stromsparende
stromsparende ML-Inferenz. Die Hardwareunterstützung für ML-Inferenz entwickelt sich rasch weiter, da
eingebettete, geräteinterne ML wird immer üblicher.
Phase 2: Aufbau des Cloud-Modells

Da unser in der Cloud gehostetes Modell nicht für Inferenzen ohne Netzverbindung optimiert werden muss, können wir
Netzwerkverbindung optimiert werden muss, können wir einen traditionelleren Ansatz für Training, Export
exportieren und dieses Modell bereitstellen. Abhängig von Ihrem Anwendungsfall der Zwei-Phasen-Vorhersage
kann dieses zweite Modell viele verschiedene Formen annehmen. Im Beispiel von Google Home,
könnte Phase 2 mehrere Modelle umfassen: eines, das die Audioeingabe eines Sprechers in Text umwandelt
Text umwandelt, und ein zweites, das NLP durchführt, um den Text zu verstehen und die Anfrage des Nutzers
Anfrage weiterleitet. Wenn der Benutzer nach etwas Komplexem fragt, könnte es sogar ein drittes Modell geben
Modell eine Empfehlung aussprechen, die auf den Vorlieben des Nutzers oder früheren Aktivitäten beruht.

In unserem Instrumentenbeispiel wird die zweite Phase unserer Lösung ein Multiklassenmodell sein
Modell sein, das die Klänge in eine von 18 möglichen Instrumentenkategorien einordnet. Da dieses
Modell nicht auf dem Gerät implementiert werden muss, können wir eine größere Modellarchitektur
wie VGG als Ausgangspunkt verwenden und dann das in Kapitel 4 beschriebene Transfer-Learning-Designmuster
das in Kapitel 4 beschrieben wird.

Wir laden den VGG, der auf dem ImageNet-Datensatz trainiert wurde, geben die Größe unserer Spektrogramme
Spektrogrammbilder im input_shape-Parameter an, und frieren die Gewichte des Modells ein, bevor wir
Softmax-Klassifikations-Ausgabeschicht hinzufügen:

vgg_model = tf.keras.applications.VGG19(
include_top=False,
weights='imagenet',
input_shape=((128,128,3))
)
vgg_model.trainable = False
Unsere Ausgabe ist ein 18-Elemente-Array von Softmax-Wahrscheinlichkeiten:

prediction_layer = tf.keras.layers.Dense(18, activation='softmax')
Wir beschränken unseren Datensatz auf die Audioclips von Instrumenten und transformieren dann die
Instrumentenbezeichnungen in 18-elementige One-Hot-Vektoren. Wir können das gleiche verwenden

Entwurfsmuster 19: Zwei-Phasen-Vorhersagen | 239
image_generator-Ansatz, um unser Modell mit Bildern zu trainieren.
Anstatt dies als TF-Lite-Modell zu exportieren, können wir model.save() verwenden, um unser
Modell für die Auslieferung zu exportieren.

Um die Bereitstellung des Phase-2-Modells in der Cloud zu demonstrieren, verwenden wir die Cloud AI Plat-
form Vorhersage. Wir müssen unsere gespeicherten Modell-Assets in einen Cloud-Speicher hochladen
Bucket hochladen und dann das Modell bereitstellen, indem wir das Framework angeben und AI Platform
Prediction auf unseren Speicher-Bucket verweist.

Für die zweite Phase des Zwei-Phasen-Vorhersagemusters können Sie ein beliebiges Cloud-basiertes
die zweite Phase des Entwurfsmusters für zweiphasige Vorhersagen verwenden. Unter
Neben der Google Cloud AI Platform Prediction bieten auch AWS Sage-
Maker und Azure Machine Learning bieten beide Dienste für die Bereitstellung
benutzerdefinierte Modelle an.
Wenn wir unser Modell als TensorFlow SavedModel exportieren, können wir eine Cloud Stor-
age bucket URL direkt an die save model Methode übergeben:

model.save('gs://Ihr_Storage_bucket/path')
Dies exportiert unser Modell im TF SavedModel-Format und lädt es in unseren Cloud
Speicher-Bucket.

In AI Platform enthält eine Modellressource verschiedene Versionen Ihres Modells. Jedes
Modell kann Hunderte von Versionen haben. Wir erstellen die Modellressource zunächst mit
gcloud, der Google Cloud CLI:

gcloud ai-platform models create instrument_classification
Es gibt mehrere Möglichkeiten, Ihr Modell bereitzustellen. Wir verwenden gcloud und verweisen auf AI Platform
auf das Speicherunterverzeichnis, das unsere gespeicherten Modell-Assets enthält:

gcloud ai-platform versions create v1 \
--model instrument_classification \
--origin 'gs://dein_storage_bucket/path/model_timestamp' \
--runtime-version=2.1 \
--framework='tensorflow' \
--python-version=3.7
Wir können nun Vorhersageanfragen an unser Modell über die AI Platform Prediction
API stellen, die Online- und Batch-Vorhersagen unterstützt. Die Online-Vorhersage ermöglicht es uns, Vorhersagen
Vorhersagen in nahezu Echtzeit für einige wenige Beispiele auf einmal. Wenn wir Hunderte oder Tausende
tausende von Beispielen zur Vorhersage senden möchten, können wir eine Batch-Vorhersage erstellen
Batch-Vorhersageauftrag erstellen, der asynchron im Hintergrund läuft und die Vorhersageergebnisse
in eine Datei ausgibt, wenn er fertig ist.

Für den Fall, dass das Gerät, das unser Modell aufruft, nicht immer mit dem Internet verbunden ist
mit dem Internet verbunden ist, könnten wir Audioclips für die Instrumentenvorhersage auf dem Gerät speichern, während

240 | Kapitel 5: Entwurfsmuster für robustes Serving

er offline ist. Wenn die Verbindung wiederhergestellt ist, können wir diese Clips an das in der Cloud
gehostetes Modell zur Vorhersage senden.

Kompromisse und Alternativen
Während das Zwei-Phasen-Vorhersagemuster in vielen Fällen funktioniert, gibt es Situationen
Situationen, in denen Ihre Endnutzer möglicherweise nur über eine sehr geringe Internetverbindung verfügen und Sie sich daher
nicht darauf verlassen können, ein in der Cloud gehostetes Modell aufrufen zu können. In diesem Abschnitt werden wir Folgendes besprechen
zwei Offline-Alternativen, ein Szenario, in dem ein Kunde viele Vorhersageanfragen gleichzeitig stellen muss
Vorhersageanfragen stellen muss, und Vorschläge, wie eine kontinuierliche Auswertung für Offline-Modelle durchgeführt werden kann.
offline-Modelle.

Einphasiges Standalone-Modell

Manchmal haben die Endnutzer Ihres Modells nur eine geringe oder gar keine Internetverbindung.
Auch wenn die Geräte dieser Nutzer nicht in der Lage sind, zuverlässig auf ein Cloud-Modell zuzugreifen, ist es
ist es dennoch wichtig, ihnen eine Möglichkeit zu geben, auf Ihre Anwendung zuzugreifen. In diesem Fall ist es besser
Sie sich nicht auf einen zweistufigen Vorhersagefluss verlassen, sondern Ihr erstes Modell so robust
so robust machen, dass es autark sein kann.

Zu diesem Zweck können wir eine kleinere Version unseres komplexen Modells erstellen und den Benutzern die Möglichkeit geben
die Möglichkeit geben, dieses einfachere, kleinere Modell herunterzuladen, wenn sie offline sind. Diese
Offline-Modelle sind vielleicht nicht ganz so genau wie ihre größeren Online-Pendants, aber
diese Lösung ist unendlich viel besser als gar keine Offline-Unterstützung. Um komplexere
komplexere Modelle zu erstellen, die für Offline-Inferenz ausgelegt sind, verwenden Sie am besten ein Tool, das es Ihnen ermöglicht
Gewichte Ihres Modells und andere mathematische Operationen sowohl während als auch nach dem
Training. Dies wird als quantisierungsbewusstes Training bezeichnet.

Ein Beispiel für eine Anwendung, die ein einfacheres Offline-Modell bietet, ist Google Trans-
late. Google Translate ist ein robuster Online-Übersetzungsdienst, der in Hunderten von
Sprachen. Es gibt jedoch viele Szenarien, in denen Sie einen Übersetzungsdienst
Übersetzungsdienst ohne Internetzugang nutzen muss. In diesem Fall können Sie mit Google Translate
Offline-Übersetzungen in über 50 verschiedenen Sprachen herunterladen. Diese Offline-Modelle sind klein,
etwa 40 bis 50 Megabyte, und kommen in ihrer Genauigkeit den komplexeren Online-Versionen
Versionen. Abbildung 5-12 zeigt einen Vergleich der Qualität von On-Device- und Online-Übersetzungsmodellen
Modellen.

Entwurfsmuster 19: Zwei-Phasen-Vorhersagen | 241
Abbildung 5-12. Ein Vergleich von phrasenbasierten und (neueren) neuronalen maschinellen
Übersetzungsmodellen und neuronaler maschineller Online-Übersetzung (Quelle: The Keyword).

Ein weiteres Beispiel für ein eigenständiges Ein-Phasen-Modell ist Google Bolo, eine sprachbasierte
Sprachlern-App für Kinder. Die App funktioniert vollständig offline und wurde entwickelt
entwickelt, um Bevölkerungsgruppen zu helfen, die nicht immer einen zuverlässigen Internetzugang
verfügbar ist.

Offline-Unterstützung für bestimmte Anwendungsfälle

Eine andere Lösung, um Ihre Anwendung auch für Benutzer mit minimaler Internetverbindung nutzbar zu machen
zu ermöglichen, besteht darin, nur bestimmte Teile Ihrer Anwendung offline verfügbar zu machen. Dies könnte
einige allgemeine Funktionen offline zu aktivieren oder die Ergebnisse eines ML
Modells für die spätere Offline-Nutzung zwischenzuspeichern. Bei dieser Alternative verwenden wir immer noch
zwei Vorhersagephasen, aber wir schränken die von unserem Offline-Modell abgedeckten Anwendungsfälle ein.
Bei diesem Ansatz arbeitet die App offline ausreichend, bietet aber die volle Funktionalität
wenn die Verbindung wiederhergestellt ist.

Mit Google Maps können Sie zum Beispiel Karten und Wegbeschreibungen im Voraus herunterladen. Unter
zu vermeiden, dass die Wegbeschreibung zu viel Platz auf einem mobilen Gerät einnimmt, können nur
Wegbeschreibungen offline verfügbar gemacht werden (nicht zu Fuß oder mit dem Fahrrad). Ein weiteres Beispiel
könnte eine Fitnessanwendung sein, die Ihre Schritte verfolgt und Empfehlungen für
zukünftige Aktivitäten gibt. Nehmen wir an, die häufigste Verwendung dieser Anwendung ist die Überprüfung, wie viele
Schritte Sie am aktuellen Tag gegangen sind. Um diesen Anwendungsfall offline zu unterstützen, könnten wir
die Daten des Fitness-Trackers über Bluetooth mit dem Gerät des Nutzers synchronisieren, um die Überprüfung des
Fitness-Status des aktuellen Tages offline zu überprüfen. Um die Leistung unserer App zu optimieren, könnten wir
entscheiden, den Fitnessverlauf und die Empfehlungen nur online verfügbar zu machen.

Wir könnten darauf aufbauen, indem wir die Anfragen des Nutzers speichern, während sein Gerät offline ist
offline ist, und sie an ein Cloud-Modell senden, sobald die Verbindung wiederhergestellt ist, um
detailliertere Ergebnisse zu liefern. Zusätzlich könnten wir sogar ein grundlegendes Empfehlungsmodell bereitstellen
Empfehlungsmodell bereitstellen, das offline verfügbar ist, um es durch verbesserte Empfehlungen zu ergänzen.

242 | Kapitel 5: Entwurfsmuster für robustes Serving

Ergebnisse, wenn die App in der Lage ist, die Abfragen des Nutzers an ein in der Cloud gehostetes Modell zu senden. Mit
dieser Lösung erhält der Benutzer immer noch einige Funktionen, wenn er nicht verbunden ist. Wenn
er wieder online ist, kann er von einer voll funktionsfähigen App und einem robusten ML
Modell.

Bearbeitung vieler Vorhersagen in nahezu Echtzeit

In anderen Fällen haben die Endnutzer Ihres ML-Modells zwar eine zuverlässige Verbindung, müssen aber möglicherweise
Hunderte oder sogar Tausende von Vorhersagen für Ihr Modell auf einmal machen müssen. Wenn
Sie nur ein in der Cloud gehostetes Modell haben und jede Vorhersage einen API-Aufruf an einen
gehosteten Dienst erfordert, wird das Abrufen von Vorhersageantworten für Tausende von Beispielen auf einmal
zu viel Zeit in Anspruch nehmen.

Um dies zu verstehen, nehmen wir an, wir haben eingebettete Geräte, die in verschiedenen Bereichen
im gesamten Haus eines Benutzers. Diese Geräte erfassen Daten zu Temperatur, Luftdruck und
Luftdruck und Luftqualität. Wir haben ein Modell in der Cloud zur Erkennung von Anomalien
aus diesen Sensordaten. Da die Sensoren ständig neue Daten erfassen, wäre es
wäre es ineffizient und teuer, jeden eingehenden Datenpunkt an unser Cloud-Modell
Modell zu senden. Stattdessen können wir ein Modell direkt auf den Sensoren einsetzen, um mögliche
Anomaliekandidaten aus den eingehenden Daten zu identifizieren. Wir können dann nur die potenziellen
Anomalien zur konsolidierten Überprüfung an unser Cloud-Modell senden, wobei die Sensormessungen
von allen Standorten berücksichtigt. Dies ist eine Abwandlung der zuvor beschriebenen Zwei-Phasen-Vorhersage
Der Hauptunterschied besteht darin, dass sowohl das Offline- als auch das Cloud-Modell
Modelle dieselbe Vorhersageaufgabe erfüllen, jedoch mit unterschiedlichen Eingaben. In diesem Fall drosseln die Modu-
auch die Anzahl der an das Cloud-Modell gesendeten Vorhersageanfragen auf einmal drosseln.
zu einem bestimmten Zeitpunkt.

Kontinuierliche Auswertung für Offline-Modelle

Wie können wir sicherstellen, dass unsere Gerätemodelle immer auf dem neuesten Stand sind und nicht unter Datenverlusten leiden?
driften? Es gibt einige Optionen für die kontinuierliche Auswertung von Modellen, die
die keine Netzwerkanbindung haben. Erstens könnten wir eine Teilmenge der Vorhersagen speichern, die
auf dem Gerät empfangen werden. Dann könnten wir die Leistung unseres Modells anhand dieser Beispiele regelmäßig bewerten
Beispielen bewerten und feststellen, ob das Modell neu trainiert werden muss. Im Fall unseres Zwei-Phasen
Phasenmodells ist es wichtig, diese Bewertung regelmäßig vorzunehmen, da es wahrscheinlich ist, dass viele
Anrufe an unser geräteinternes Modell nicht an das Cloud-Modell der zweiten Phase weitergeleitet werden. Eine weitere
Option besteht darin, eine Replik unseres geräteinternen Modells zu erstellen, die online nur für kontinuierliche
Evaluierungszwecken. Diese Lösung ist vorzuziehen, wenn unsere Offline- und Cloud-Modelle
ähnliche Vorhersageaufgaben ausführen, wie in dem oben erwähnten Fall der Übersetzung.

Entwurfsmuster 20: Verschlüsselte Vorhersagen
Normalerweise trainieren Sie Ihr Modell auf demselben Satz von Eingangsmerkmalen, den das Modell
das Modell in Echtzeit geliefert wird, wenn es eingesetzt wird. In vielen Situationen kann es jedoch sinnvoll sein

Entwurfsmuster 20: Vorhersagen mit Schlüssel | 243
vorteilhaft, wenn Ihr Modell auch einen vom Kunden bereitgestellten Schlüssel durchläuft. Dies wird als
Dies wird als Keyed Predictions Design Pattern bezeichnet und ist eine Notwendigkeit für die skalierbare Implementierung von sieben
mehrere der in diesem Kapitel besprochenen Entwurfsmuster skalierbar zu implementieren.

Problem
Wenn Ihr Modell als Webdienst bereitgestellt wird und eine einzige Eingabe akzeptiert, dann ist es ziemlich
klar, welche Ausgabe welcher Eingabe entspricht. Was aber, wenn Ihr Modell eine Datei
mit einer Million Eingaben akzeptiert und eine Datei mit einer Million Ausgabevorhersagen zurücksendet?

Man könnte meinen, dass es offensichtlich sein sollte, dass die erste Ausgabeinstanz der
der ersten Eingabeinstanz entspricht, die zweite Ausgabeinstanz der zweiten Eingabeinstanz,
usw. Bei einer 1:1-Beziehung ist es jedoch erforderlich, dass jeder Serverknoten
den gesamten Satz von Eingaben seriell verarbeiten. Es wäre viel vorteilhafter, wenn Sie ein verteiltes
verteiltes Datenverarbeitungssystem verwenden und Instanzen auf mehrere Rechner verteilen, alle
Instanzen auf mehrere Rechner verteilen, alle resultierenden Ausgaben sammeln und zurücksenden. Das Problem bei diesem Ansatz ist, dass
dass die Ergebnisse durcheinander gewürfelt sein werden. Die Anforderung, dass die Ausgaben in der gleichen Reihenfolge
stellt die Skalierbarkeit in Frage, und die Bereitstellung der Ausgaben in ungeordneter Form
müssen die Clients irgendwie wissen, welche Ausgabe welcher Eingabe entspricht.

Das gleiche Problem tritt auf, wenn Ihr Online-Serving-System ein Array von Instanzen akzeptiert
akzeptiert, wie im Muster der zustandslosen Serving-Funktion beschrieben. Das Problem ist, dass die Verarbeitung
einer großen Anzahl von Instanzen lokal zu Hot Spots führen wird. Serverknoten, die nur
Serverknoten, die nur wenige Anfragen erhalten, werden mithalten können, aber jeder Serverknoten, der ein besonders
eine besonders große Anzahl von Anfragen erhält, gerät ins Hintertreffen. Diese Hot Spots zwingen Sie dazu, Ihre
Server-Maschinen leistungsfähiger zu machen, als sie sein müssten. Daher sehen viele Online-Serving
Systeme eine Begrenzung der Anzahl der Instanzen, die in einer Anfrage gesendet werden können.
Anfrage. Gibt es keine solche Grenze, oder ist das Modell so rechenintensiv, dass
dass Anfragen mit weniger Instanzen als dieser Grenze den Server überlasten können, tritt das
das Problem der Hot Spots. Daher wird jede Lösung für das Batch-Serving-Problem
auch das Problem der Hot Spots bei der Online-Auslieferung angehen.

Lösung
Die Lösung ist die Verwendung von Pass-Through-Schlüsseln. Der Kunde muss für jede Eingabe einen Schlüssel
jeder Eingabe. Angenommen, Ihr Modell wird mit drei Eingaben trainiert (siehe Abbildung 5-13)
Eingaben (a, b, c) trainiert, um die Ausgabe d zu erzeugen, die rechts dargestellt ist. Machen Sie
Ihre Kunden liefern (k, a, b, c) an Ihr Modell, wobei k ein Schlüssel mit einer eindeutigen Kennung ist.
Der Schlüssel könnte so einfach sein wie die Nummerierung der Eingabeinstanzen 1, 2, 3, ..., usw. Ihr
Modell gibt dann (k, d) zurück, und so kann der Client herausfinden, welche Ausgabe
Instanz welcher Eingabe-Instanz entspricht.

244 | Kapitel 5: Entwurfsmuster für robustes Serving

Abbildung 5-13. Der Client liefert mit jeder Eingabeinstanz einen eindeutigen Schlüssel. Das Serving-System
tem ordnet diese Schlüssel der entsprechenden Vorhersage zu. Dadurch kann der Kunde
die richtige Vorhersage für jede Eingabe abrufen, selbst wenn die Ausgaben nicht in der richtigen Reihenfolge sind.

Wie man in Keras Schlüssel durchreicht

Damit Ihr Keras-Modell Schlüssel durchlässt, geben Sie beim Export eine Serving-Signatur
beim Exportieren des Modells.

Dies ist zum Beispiel der Code für ein Modell, das sonst vier Eingaben benötigt
(is_male, mother_age, plurality, and gestation_weeks) und zusätzlich einen Schlüssel
den es zusammen mit der ursprünglichen Ausgabe des Modells (dem Babygewicht) an die Ausgabe weitergibt
(das Babygewicht):

# Dienende Funktion, die Schlüssel durchläuft
@tf.function(input_signature=[{
'is_male': tf.TensorSpec([None,], dtype=tf.string, name='is_male'),
'mother_age': tf.TensorSpec([None,], dtype=tf.float32,
name='mutter_alter'),
'Mehrzahl': tf.TensorSpec([None,], dtype=tf.string, name='Mehrzahl'),
Schwangerschaftswochen': tf.TensorSpec([None,], dtype=tf.float32,
name='gestation_weeks'),
'Schlüssel': tf.TensorSpec([None,], dtype=tf.string, name='Schlüssel')
}])
def keyed_prediction(inputs):
feats = inputs.copy()
key = feats.pop('key') # den Schlüssel aus input holen
output = model(feats) # ruft das Modell auf
Entwurfsmuster 20: Vorhersagen mit Schlüssel | 245
return {'Schlüssel': key, 'Babygewicht': output}
Dieses Modell wird dann wie im Design Pattern Stateless Serving Function beschrieben gespeichert
Muster:

model.save(EXPORT_PATH,
signatures={'serving_default': keyed_prediction})
Hinzufügen der Fähigkeit zur verschlüsselten Vorhersage zu einem bestehenden Modell

Beachten Sie, dass der obige Code auch dann funktioniert, wenn das ursprüngliche Modell nicht mit einer Servicefunktion gespeichert wurde.
Funktion gespeichert wurde. Laden Sie einfach das Modell mit tf.saved_model.load(), fügen Sie eine Serving
Funktion an und verwenden Sie den obigen Codeschnipsel, wie in Abbildung 5-14 gezeigt.

Abbildung 5-14. Laden Sie ein SavedModel, fügen Sie eine nicht standardmäßige Serving-Funktion hinzu, und speichern Sie es.

In diesem Fall ist es vorzuziehen, eine Serving-Funktion bereitzustellen, die die ältere Funktion nachbildet,
Verhalten ohne Schlüssel:

# Serving function that does not require a key
@tf.function(input_signature=[{
'is_male': tf.TensorSpec([None,], dtype=tf.string, name='is_male'),
'mother_age': tf.TensorSpec([None,], dtype=tf.float32,
name='mutter_alter'),
'Mehrzahl': tf.TensorSpec([None,], dtype=tf.string, name='Mehrzahl'),
Schwangerschaftswochen': tf.TensorSpec([None,], dtype=tf.float32,
name='gestation_weeks')
}])
def nokey_prediction(inputs):
output = model(inputs) # Modell aufrufen
return { 'babyweight': output}
Verwenden Sie das vorherige Verhalten als Standard und fügen Sie die keyed_prediction als eine neue
dienende Funktion hinzu:

246 | Kapitel 5: Entwurfsmuster für robustes Serving

model.save(EXPORT_PATH,
signatures={'serving_default': nokey_prediction,
'keyed_prediction': keyed_prediction
})
Kompromisse und Alternativen
Warum kann der Server den Eingaben, die er erhält, nicht einfach Schlüssel zuweisen? Für die Online-Vorhersage,
ist es möglich, dass Server eindeutige Anforderungs-IDs zuweisen, denen jegliche semantische Information fehlt.
tion fehlen. Bei der Batch-Vorhersage besteht das Problem darin, dass die Eingaben mit den Ausgaben verknüpft werden müssen.
Der Server, der eine eindeutige ID zuweist, reicht also nicht aus, da er sie nicht mit den Eingaben verknüpfen kann.
mit den Eingaben verknüpft werden kann. Der Server muss den Eingaben, die er erhält, Schlüssel zuweisen
den Eingaben, die er empfängt, Schlüssel zuzuweisen, bevor er das Modell aufruft, die Schlüssel zu verwenden, um die Ausgaben zu ordnen, und dann die Schlüssel zu entfernen
Schlüssel entfernen, bevor er die Ausgaben weiterleitet. Das Problem ist, dass das Ordnen in der verteilten Datenverarbeitung sehr rechen
in der verteilten Datenverarbeitung sehr rechenintensiv ist.

Darüber hinaus gibt es einige weitere Situationen, in denen vom Kunden bereitgestellte Schlüssel verwendet werden
asynchrone Zustellung und Auswertung. Angesichts dieser beiden Situationen ist es besser, wenn
dass das, was einen Schlüssel ausmacht, anwendungsfallspezifisch wird und identifizierbar sein muss.
Daher wird die Lösung einfacher, wenn die Kunden aufgefordert werden, einen Schlüssel zu liefern.

Asynchrone Zustellung

Viele Modelle des maschinellen Lernens in der Produktion sind heutzutage neuronale Netze, und neu-
rale Netze beinhalten Matrixmultiplikationen. Die Matrixmultiplikation auf Hardware wie
GPUs und TPUs ist effizienter, wenn Sie sicherstellen können, dass die Matrizen innerhalb bestimmter
bestimmten Größenbereichen und/oder Vielfachen einer bestimmten Zahl liegen. Es kann daher hilfreich sein, Anfragen
Anfragen zu akkumulieren (natürlich bis zu einer maximalen Latenzzeit) und die eingehenden
Anfragen in Blöcken zu bearbeiten. Da die Pakete aus verschachtelten Anfragen von mehreren Kunden bestehen
ple Clients bestehen, muss der Schlüssel in diesem Fall auch eine Art Client-Kennung haben.

Kontinuierliche Bewertung

Wenn Sie eine kontinuierliche Auswertung vornehmen, kann es hilfreich sein, Metadaten über die
Vorhersageanforderungen zu protokollieren, so dass Sie überwachen können, ob die Leistung generell
oder nur in bestimmten Situationen. Ein solches Slicing wird wesentlich erleichtert, wenn der Schlüssel die betreffende Situation identifiziert.
die betreffende Situation identifiziert. Nehmen wir zum Beispiel an, dass wir eine Fairness-Linse anwenden müssen
Objektiv (siehe Kapitel 7), um sicherzustellen, dass die Leistung unseres Modells in verschiedenen
Kundensegmenten (z. B. Alter des Kunden und/oder Rasse des Kunden). Das
Modell wird das Kundensegment nicht als Eingabe verwenden, aber wir müssen die
Leistung des Modells nach dem Kundensegment bewerten. In solchen Fällen ist es sinnvoll, das
Kundensegment(e) in den Schlüssel eingebettet werden (ein Beispiel für einen Schlüssel wäre 35-Schwarz-
Male-34324323) erleichtert das Slicing.

Eine alternative Lösung besteht darin, dass das Modell nicht erkannte Eingaben ignoriert und
nicht nur die Vorhersageausgaben, sondern auch alle Eingaben, einschließlich der nicht erkannten, zurück.

Entwurfsmuster 20: Verschlüsselte Vorhersagen | 247
Dies ermöglicht dem Client die Zuordnung von Eingaben zu Ausgaben, ist aber teurer in Bezug auf die
Bandbreite und clientseitige Berechnungen.

Da hochleistungsfähige Server mehrere Clients unterstützen, von einem Clus- ter unterstützt werden und
unterstützen und Anfragen stapelweise verarbeiten, um Leistungsvorteile zu erzielen, ist es besser, dies im Voraus zu planen.
vorauszuplanen, d.h. die Clients aufzufordern, bei jeder Vorhersage Schlüssel zu liefern und Schlüssel anzugeben
die keine Kollisionen mit anderen Clients verursachen.

Zusammenfassung
In diesem Kapitel haben wir uns mit Techniken zur Operationalisierung von maschinellen Lernmodellen
um sicherzustellen, dass sie belastbar sind und die Produktionslast bewältigen können. Jedes besprochene Resilienz
Resilienzmuster, das wir besprochen haben, bezieht sich auf die Bereitstellungs- und Serviceschritte in einem typischen
ML-Workflow.

Zu Beginn dieses Kapitels haben wir uns angesehen, wie Sie Ihr trainiertes Machine-Learning-Modell als zustandslose Funktion kapseln können.
Modell als zustandslose Funktion kapselt, indem es das Design Pattern Stateless Serving Function verwendet.
Eine Serving Function entkoppelt die Trainings- und Einsatzumgebung Ihres Modells
indem sie eine Funktion definiert, die Inferenzen auf einer exportierten Version Ihres Modells durchführt,
und an einem REST-Endpunkt bereitgestellt wird. Nicht alle Produktionsmodelle erfordern sofortige
Vorhersageergebnisse, denn es gibt Situationen, in denen Sie einen großen Stapel von Daten
an Ihr Modell zur Vorhersage senden müssen, aber nicht sofort Ergebnisse benötigen. Wir haben gesehen, wie das
Batch Serving Design Pattern dieses Problem löst, indem es eine verteilte Datenverarbeitungsinfra-
verteilte Datenverarbeitungsinfrastruktur verwendet, um viele Modellvorhersageanforderungen asynchron als
Hintergrundjob ausgeführt werden, wobei die Ausgabe an einen bestimmten Ort geschrieben wird.

Als nächstes haben wir uns mit dem Entwurfsmuster Continued Model Evaluation einen Ansatz angesehen
um zu überprüfen, ob Ihr eingesetztes Modell auch bei neuen Daten noch gut funktioniert. Dieses Pat-
Dieses Muster geht das Problem der Daten- und Konzeptabweichung an, indem es Ihr Modell regelmäßig evaluiert
Modells und verwendet diese Ergebnisse, um festzustellen, ob ein erneutes Training erforderlich ist. Im
Two-Phase Predictions Design Pattern haben wir eine Lösung für spezielle Anwendungsfälle gefunden, bei denen Modelle
am Rande eingesetzt werden müssen. Wenn Sie ein Problem in zwei logische Teile zerlegen können
Teile zerlegt werden kann, erstellt dieses Muster zunächst ein einfacheres Modell, das auf dem Gerät bereitgestellt werden kann. Dieses
Randmodell ist mit einem komplexeren Modell verbunden, das in der Cloud gehostet wird. Schließlich wird mit dem
Entwurfsmuster "Keyed Prediction" erörtert, warum es vorteilhaft sein kann, jedem Beispiel einen
Vorhersageanfragen mit jedem Beispiel einen eindeutigen Schlüssel mitzuliefern. Dadurch wird sichergestellt, dass
Ihr Client jede Vorhersageausgabe mit dem richtigen Eingabebeispiel verknüpft.

Im nächsten Kapitel werden wir uns mit den Reproduzierbarkeitsmustern befassen. Diese Muster behandeln Herausforderungen
Herausforderungen, die mit der inhärenten Zufälligkeit in vielen Aspekten des maschinellen
maschinellen Lernens verbunden sind, und konzentrieren sich darauf, bei jedem Durchlauf eines maschinellen Lernprozesses
Prozesses.

248 | Kapitel 5: Entwurfsmuster für robustes Serving

KAPITEL 6

Reproduzierbarkeit Design Patterns
Bewährte Software-Praktiken wie Unit-Tests gehen davon aus, dass, wenn wir ein Stück Code ausführen, es
eine deterministische Ausgabe erzeugt:

def sigmoid(x):
return 1.0 / (1 + np.exp(-x))
class TestSigmoid (unittest.TestCase):
def test_zero(self):
self.assertAlmostEqual(sigmoid(0), 0.5)
def test_neginf(self):
self.assertAlmostEqual(sigmoid(float("-inf")), 0)
def test_inf(self):
self.assertAlmostEqual(sigmoid(float("inf")), 1)
Diese Art der Reproduzierbarkeit ist beim maschinellen Lernen schwierig. Während des Trainings werden maschinelle
Lernmodelle mit Zufallswerten initialisiert und dann anhand der Trainingsdaten angepasst.
Trainingsdaten angepasst. Ein einfacher, von scikit-learn implementierter k-means-Algorithmus erfordert das Setzen
random_state gesetzt werden, um sicherzustellen, dass der Algorithmus jedes Mal die gleichen Ergebnisse liefert
jedes Mal:

def cluster_kmeans(X):
from sklearn import cluster
k_means = cluster.KMeans(n_clusters=10, random_state=10 )
labels = k_means.fit(X).labels_[::]
return labels
Neben dem zufälligen Seed gibt es viele weitere Artefakte, die behoben werden müssen, um
um die Reproduzierbarkeit beim Training zu gewährleisten. Darüber hinaus besteht das maschinelle Lernen aus
verschiedenen Phasen, wie z. B. Training, Einsatz und erneutes Training. Oft ist es wichtig
dass einige Dinge auch über diese Phasen hinweg reproduzierbar sind.

249
In diesem Kapitel werden wir Entwurfsmuster betrachten, die verschiedene Aspekte der Reproduzierbarkeit ansprechen.
Reproduzierbarkeit behandeln. Das Transform-Entwurfsmuster erfasst Datenvorbereitungsabhängigkeiten aus der
der Modelltrainings-Pipeline, um sie während des Servings zu reproduzieren. Wiederholbare Aufteilung
erfasst die Art und Weise, wie die Daten auf Trainings-, Validierungs- und Testdatensätze aufgeteilt werden, um sicherzustellen
dass ein Trainingsbeispiel, das für das Training verwendet wird, niemals für die Auswertung oder den Test verwendet wird
auch wenn der Datensatz wächst. Das Entwurfsmuster Bridged Schema befasst sich mit der Frage, wie man die
Reproduzierbarkeit zu gewährleisten, wenn der Trainingsdatensatz eine Mischung aus Daten ist, die mit verschiedenen
Schema entsprechen. Das Workflow-Pipeline-Entwurfsmuster erfasst alle Schritte des maschinellen
Prozesses des maschinellen Lernens, um sicherzustellen, dass bei einem erneuten Training des Modells Teile der Pipeline
wiederverwendet werden können. Das Feature Store Design Pattern befasst sich mit der Reproduzierbarkeit und Wiederverwendbarkeit von
Features über verschiedene maschinelle Lernaufgaben hinweg. Das Entwurfsmuster Windowed Inference
gewährleistet, dass Merkmale, die dynamisch und zeitabhängig berechnet werden, zwischen Training und
zwischen Training und Bedienung korrekt wiederholt werden können. Die Versionierung von Daten und Modellen ist
ist eine Voraussetzung für die Handhabung vieler der Entwurfsmuster in diesem Kapitel.

Entwurfsmuster 21: Transformieren
Das Transform-Entwurfsmuster erleichtert die Überführung eines ML-Modells in die Produktion.
durch die sorgfältige Trennung von Eingaben, Funktionen und Transformationen.

Problem
Das Problem besteht darin, dass die Eingaben für ein maschinelles Lernmodell nicht die Merkmale sind, die das
das maschinelle Lernmodell bei seinen Berechnungen verwendet. In einem Modell zur Textklassifizierung,
beispielsweise sind die Eingaben die rohen Textdokumente und die Merkmale sind die numerischen
Einbettungsrepräsentationen dieses Textes. Wenn wir ein Modell für maschinelles Lernen trainieren, verwenden wir
trainieren wir es mit Merkmalen, die aus den rohen Eingaben extrahiert werden. Nehmen wir dieses Modell, das trainiert wird
trainiert wurde, um die Dauer von Fahrradtouren in London mithilfe von BigQuery ML vorherzusagen:

CREATE OR REPLACE MODELL ch09eu.bicycle_model
OPTIONS (input_label_cols=['Dauer'],
model_type='linear_reg')
AS
SELECT
Dauer
, start_station_name
, CAST ( EXTRACT (Wochentag aus start_date) AS STRING)
as Wochentag
, CAST ( EXTRACT (Stunde von start_date) AS STRING)
as hourofday
FROM
`bigquery- public - data .london_bicycles.cycle_hire`
Dieses Modell hat drei Merkmale (start_station_name, dayofweek und hourofday)
die aus zwei Eingaben, start_station_name und start_date, berechnet werden, wie in
Abbildung 6-1.

250 | Kapitel 6: Reproduzierbarkeit - Entwurfsmuster

Abbildung 6-1. Das Modell hat drei Merkmale, die aus zwei Eingaben berechnet werden.

Aber der obige SQL-Code vermischt die Eingaben und Funktionen und behält nicht den Überblick über
die durchgeführten Transformationen. Das wird sich rächen, wenn wir versuchen, mit diesem
Vorhersage mit diesem Modell. Da das Modell mit drei Merkmalen trainiert wurde, sieht die
muss die Vorhersagesignatur aussehen:

SELECT * FROM ML.PREDICT(MODELL ch09eu.bicycle_model,(
'Kings Cross' AS start_station_name
, '3' als Wochentag
, '18' als Tageszeit
))
Beachten Sie, dass wir zum Zeitpunkt der Inferenz wissen müssen, auf welche Merkmale das Modell trainiert wurde
trainiert wurde, wie sie zu interpretieren sind, und die Details der angewandten Transformationen
angewendet wurden. Wir müssen wissen, dass wir '3' für dayofweek senden müssen. Diese '3' ... ist
ist das Dienstag oder Mittwoch? Das hängt davon ab, welche Bibliothek vom Modell verwendet wurde, oder
was wir als Wochenbeginn betrachten!

Die Verzerrung zwischen Training und Service, die durch Unterschiede in einem dieser Faktoren zwischen der Trainings- und der
Trainings- und Betriebsumgebung verursacht werden, sind einer der Hauptgründe, warum die Produktion von ML
Modellen so schwierig ist.

Lösung
Die Lösung besteht in der expliziten Erfassung der Transformationen, die zur Umwandlung der Modelleingaben
Eingaben in Merkmale zu konvertieren. In BigQuery ML wird dies mithilfe der TRANSFORM-Klausel durchgeführt. Die Verwendung von
TRANSFORM wird sichergestellt, dass diese Transformationen automatisch während
ML.PREDICT.

Angesichts der Unterstützung für TRANSFORM sollte das obige Modell wie folgt umgeschrieben werden:

CREATE OR REPLACE MODELL ch09eu.bicycle_model
OPTIONS (input_label_cols=['Dauer'],
model_type='linear_reg')
TRANSFORM (
SELECT * EXCEPT (start_date)
, CAST ( EXTRACT (Wochentag aus start_date) AS STRING)
as dayofweek -- Merkmal1
, CAST ( EXTRACT (Stunde aus start_date) AS STRING)
Entwurfsmuster 21: Transformieren | 251
as hourofday -- Merkmal2
)
AS
SELECT
dauer, start_station_name, start_datum -- eingänge
FROM
`bigquery- public - data .london_bicycles.cycle_hire`
Beachten Sie, dass wir die Eingaben (in der SELECT-Klausel) klar von den
Merkmalen (in der TRANSFORM-Klausel). Jetzt ist die Vorhersage viel einfacher. Wir können einfach
den Stationsnamen und einen Zeitstempel (die Eingaben) an das Modell senden:

SELECT * FROM ML.PREDICT(MODELL ch09eu.bicycle_model,(
'Kings Cross' AS start_station_name
, CURRENT_TIMESTAMP () as start_date
))
Das Modell führt dann die entsprechenden Transformationen durch, um die erforderlichen Merkmale zu erstellen.
um die erforderlichen Merkmale zu erzeugen. Dazu erfasst es sowohl die Transformationslogik als auch die
Artefakte (wie Skalierungskonstanten, Einbettungskoeffizienten, Nachschlagetabellen usw.)
zur Durchführung der Transformation.

Solange wir sorgfältig nur die Roheingaben in der SELECT-Anweisung verwenden und alle
Verarbeitung der Eingaben in die TRANSFORM-Klausel aufnehmen, wird BigQuery ML diese
diese Transformationen während der Vorhersage automatisch anwenden.

Kompromisse und Alternativen
Die oben beschriebene Lösung funktioniert, weil BigQuery ML die Trans- formationslogik und Artefakte
Logik und Artefakte für uns verfolgt, sie im Modellgraphen speichert und die
die Transformationen während der Vorhersage anwendet.

Wenn wir ein Framework verwenden, das keine Unterstützung für das Transform-Entwurfsmuster
ist, sollten wir unsere Modellarchitektur so gestalten, dass die während des Trainings durchgeführten Transfor- mationen
tionen, die während des Trainings durchgeführt werden, während des Servings leicht reproduzierbar sind. Dies können wir
Wir können dies erreichen, indem wir sicherstellen, dass die Transformationen im Modellgraphen gespeichert werden oder indem wir ein
Repository der transformierten Features ("Entwurfsmuster 26: Feature Store" auf Seite 295).

Transformationen in TensorFlow und Keras

Angenommen, wir trainieren ein ML-Modell zur Schätzung von Taxitarifen in New York und haben
sechs Eingaben (Breitengrad des Abholers, Längengrad des Abholers, Breitengrad des Abholers, Längengrad des Abholers, Passagierzahl
senger count, and pickup time). TensorFlow unterstützt das Konzept der Feature
Spalten, die im Modellgraphen gespeichert werden. Die API ist jedoch unter der Annahme entworfen
dass die rohen Eingaben die gleichen sind wie die Merkmale.

Nehmen wir an, wir wollen die Breiten- und Längengrade skalieren (siehe "Einfache Datenrepräsentationen
sentations" auf Seite 22 in Kapitel 2), erstellen wir ein transformiertes Feature, das

252 | Kapitel 6: Reproduzierbarkeit - Entwurfsmuster

den euklidischen Abstand, und extrahieren Sie die Uhrzeit aus dem Zeitstempel. Wir müssen
sorgfältig den Modellgraphen entwerfen (siehe Abbildung 6-2) und dabei das Konzept der Transformation
fest im Kopf behalten. Beachten Sie beim Durchgehen des nachstehenden Codes, wie wir die Dinge so einrichten
dass wir in unserem Keras-Modell drei separate Schichten entwerfen - die Eingabeschicht, die
Transform-Schicht und eine DenseFeatures-Schicht.

Abbildung 6-2. Der Modellgraph für das Problem der Taxitarifschätzung in Keras.

Machen Sie zunächst jede Eingabe in das Keras-Modell zu einer Eingabeschicht (der vollständige Code ist in einem Notizbuch
Buch auf GitHub):

inputs = {
colname : tf.keras.layers.Input(
name=colname, shape=(), dtype='float32')
for colname in ['pickup_longitude', 'pickup_latitude',
'dropoff_longitude', 'dropoff_latitude']
}
Entwurfsmuster 21: Transformieren | 253
In Abbildung 6-2 sind dies die Felder mit der Bezeichnung dropoff_latitude, dropoff_longitude,
und so weiter.

Zweitens: Führen Sie ein Wörterbuch der transformierten Merkmale und machen Sie jede Transformation entweder zu einer
tion entweder eine Keras-Vorverarbeitungsschicht oder eine Lambda-Schicht. Hier skalieren wir die Eingaben
mit Lambda-Schichten:

transformiert = {}
for lon_col in ['pickup_longitude', 'dropoff_longitude']:
transformed[lon_col] = tf.keras.layers.Lambda(
lambda x: (x+78)/8.0,
name='scale_{}'.format(lon_col)
)(inputs[lon_col])
for lat_col in ['pickup_latitude', 'dropoff_latitude']:
transformed[lat_col] = tf.keras.layers.Lambda(
lambda x: (x-37)/8.0,
name='scale_{}'.format(lat_col)
)(inputs[lat_col])
In Abbildung 6-2 sind dies die Felder scale_dropoff_latitude, scale_drop
off_longitude und so weiter.

Wir haben auch eine Lambda-Schicht für den euklidischen Abstand, der aus vier Eingabeschichten berechnet wird
aus vier der Eingabeschichten berechnet wird (siehe Abbildung 6-2):

def euclidean(params):
lon1, lat1, lon2, lat2 = params
londiff = lon2 - lon1
latdiff = lat2 - lat1
return tf.sqrt(londiff*londiff + latdiff*latdiff)
transformed['euclidean'] = tf.keras.layers.Lambda(euclidean, name='euclidean')([
inputs['pickup_longitude'],
inputs['pickup_latitude'],
inputs['dropoff_longitude'],
inputs['dropoff_latitude']
])
In ähnlicher Weise ist die Spalte zur Erstellung der Tageszeit aus dem Zeitstempel eine Lambda-Ebene:

transformed['hourofday'] = tf.keras.layers.Lambda(
lambda x: tf.strings.to_number(tf.strings.substr(x, 11, 2),
out_type=tf.dtypes.int32),
name='Wochentag'
)(inputs['pickup_datetime'])
Drittens werden alle diese transformierten Ebenen zu einer DenseFeatures-Ebene verkettet:

dnn_inputs = tf.keras.layers.DenseFeatures(feature_columns.values())(transformed)
Da der Konstruktor für DenseFeatures einen Satz von Merkmalsspalten benötigt, müssen wir
müssen wir angeben, wie wir jeden der transformierten Werte in einen

254 | Kapitel 6: Reproduzierbarkeit - Entwurfsmuster

Input für das neuronale Netz. Wir können sie so verwenden, wie sie sind, sie einmalig kodieren oder
die Zahlen in Schalen einteilen. Der Einfachheit halber verwenden wir sie einfach alle so, wie sie sind:

feature_columns = {
colname: tf.feature_column.numeric_column(colname)
for colname in ['pickup_longitude', 'pickup_latitude',
dropoff_longitude', 'dropoff_latitude']
}
feature_columns['euclidean'] = \
tf.feature_column.numeric_column('euklidisch')
Sobald wir eine DenseFeatures-Eingabeschicht haben, können wir den Rest unseres Keras-Modells aufbauen
wie gewohnt aufbauen:

h1 = tf.keras.layers.Dense(32, activation='relu', name='h1')(dnn_inputs)
h2 = tf.keras.layers.Dense(8, activation='relu', name='h2')(h1)
output = tf.keras.layers.Dense(1, name='fare')(h2)
model = tf.keras.models.Model(inputs, output)
model.compile(optimizer='adam', loss='mse', metrics=['mse'])
Das vollständige Beispiel ist auf GitHub zu finden.

Beachten Sie, dass wir die erste Schicht des Keras-Modells auf Inputs eingestellt haben.
Die zweite Schicht war die Transform-Schicht. Die dritte Schicht war die DenseFeatures
Schicht, die diese kombiniert. Nach dieser Abfolge von Schichten beginnt die übliche Modellarchitektur
beginnt. Da die Transform-Schicht Teil des Modellgraphen ist, funktionieren die üblichen Serving
Function und Batch Serving Lösungen (siehe Kapitel 5) wie gewohnt funktionieren.

Effiziente Transformationen mit tf.transform

Ein Nachteil des obigen Ansatzes ist, dass die Transformationen
während jeder Iteration des Trainings durchgeführt werden. Das ist nicht so schlimm, wenn wir nur mit bekannten Konstanten skalieren.
mit bekannten Konstanten skalieren. Was aber, wenn unsere Transformationen rechenintensiver sind?
teuer sind? Was ist, wenn wir mit dem Mittelwert und der Varianz skalieren wollen, in diesem Fall müssen wir
In diesem Fall müssen wir zuerst alle Daten durchgehen, um diese Variablen zu berechnen.

Es ist hilfreich, zwischen Transformationen auf Instanzebene zu unterscheiden
die direkt Teil des Modells sein können (wobei der einzige Nachteil darin besteht
Nachteil ist, dass man sie bei jeder Trainingsiteration anwenden muss) und Transformationen auf
Transformationen auf Datensatzebene, bei denen wir einen vollständigen Durchlauf zur Berechnung der Gesamtstatistik
oder das Vokabular einer kategorialen Variable zu berechnen. Solche Transformationen auf Datensatzebene
können nicht Teil des Modells sein und müssen als skalierbarer
skalierbarer Vorverarbeitungsschritt durchgeführt werden, der die Transformation erzeugt und
der die Logik und die Artefakte (Mittelwert, Varianz, Vokabular usw.)
usw.), die dem Modell beigefügt werden. Für Transformationen auf Datensatzebene
tionen verwenden Sie tf.transform.
Entwurfsmuster 21: Transformieren | 255
Die tf.transform Bibliothek (die Teil von TensorFlow Extended ist) bietet einen effi-
einen effizienten Weg, um Transformationen über einen Vorverarbeitungsdurchlauf durch die Daten auszuführen
und die resultierenden Features und Transformationsartefakte zu speichern, so dass die Transfor- mationen
tionen von TensorFlow Serving während der Vorhersagezeit angewendet werden können.

Der erste Schritt besteht darin, die Transformationsfunktion zu definieren. Um zum Beispiel alle Eingaben zu skalieren
Eingaben so zu skalieren, dass sie einen Mittelwert von Null und eine Einheitsvarianz haben, und sie in Bereiche einzuteilen, würden wir diese
Vorverarbeitungsfunktion erstellen (siehe den vollständigen Code auf GitHub):

def preprocessing_fn(inputs):
outputs = {}
for key in ...:
outputs[key + '_z'] = tft.scale_to_z_score(inputs[key])
outputs[key + '_bkt'] = tft.bucketize(inputs[key], 5)
Ausgänge zurückgeben
Vor dem Training werden die Rohdaten gelesen und mit der prior-Funktion in
Apache Beam:

transform_dataset, transform_fn = (raw_dataset |
beam_impl.AnalyzeAndTransformDataset(preprocessing_fn))
transformed_data, transformed_metadata = transformed_dataset
Die transformierten Daten werden dann in einem Format ausgegeben, das für das Lesen durch die Trainings-Pipeline
Trainings-Pipeline geeignet ist:

transformed_data | tfrecordio.WriteToTFRecord(
PATH_TO_TFT_ARTIFACTS,
coder=example_proto_coder.ExampleProtoCoder(
transformed_metadata.schema))
Die Beam-Pipeline speichert auch die Vorverarbeitungsfunktion, die ausgeführt werden muss, zusammen mit
mit allen Artefakten, die die Funktion benötigt, in einem Artefakt im TensorFlow-Graphenformat. In
obigen Fall würde dieses Artefakt zum Beispiel den Mittelwert und die Varianz für die
Skalierung der Zahlen und die Bucket-Grenzen für die Bucketisierung der Zahlen. Die
Trainingsfunktion liest transformierte Daten und daher müssen die Transformationen nicht
innerhalb der Trainingsschleife nicht wiederholt werden müssen.

Die Serving-Funktion muss diese Artefakte laden und eine Transformationsschicht erstellen:

tf_transform_output = tft.TFTransformOutput(PATH_TO_TFT_ARTIFACTS)
tf_transform_layer = tf_transform_output.transform_features_layer()
Anschließend kann die Serving-Funktion die Transform-Schicht auf die geparsten Eingabe-Features anwenden
anwenden und das Modell mit den transformierten Daten aufrufen, um die Modellausgabe zu berechnen:

@tf.function
def serve_tf_examples_fn(serialized_tf_examples):
feature_spec = tf_transform_output.raw_feature_spec()
merkmal_spec.pop(_LABEL_KEY)
parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)
256 | Kapitel 6: Reproduzierbarkeit - Entwurfsmuster

transform_features = tf_transform_layer(geparst_features)
return model(transformierte_Merkmale)
Auf diese Weise stellen wir sicher, dass die Transformationen in den Modellgraphen eingefügt werden
einzufügen. Da das Training des Modells auf den transformierten Daten erfolgt
transformierten Daten erfolgt, muss unsere Trainingsschleife diese Transformationen nicht in jeder Epoche durchführen.
jeder Epoche durchführen.

Text- und Bildtransformationen

Bei Textmodellen ist es üblich, den Eingabetext vorzubehandeln (z. B. um Punktierungen, Stoppwörter
Interpunktion, Stoppwörter, Großschreibung, Stemming usw.), bevor der bereinigte
Text als Merkmal für das Modell bereitstellt. Andere gängige Verfahren zur Bearbeitung von Texteingaben
umfasst die Tokenisierung und den Abgleich mit regulären Ausdrücken. Es ist wichtig, dass die gleichen
Bereinigungs- oder Extraktionsschritte zur Inferenzzeit durchgeführt werden.

Die Notwendigkeit, Transformationen zu erfassen, ist auch dann wichtig, wenn es keine explizite
wie bei der Verwendung von Deep Learning mit Bildern. Bildmodelle haben normalerweise eine
Eingabeschicht, die Bilder einer bestimmten Größe annimmt. Bildeingaben mit einer anderen Größe
müssen beschnitten, aufgefüllt oder auf diese feste Größe umgerechnet werden, bevor sie in das
Modell eingespeist werden. Andere häufige Transformationen in Bildmodellen sind Farbmanipulationen
(Gammakorrektur, Graustufenkonvertierung usw.) und die Korrektur der Ausrichtung. Es ist
Es ist wichtig, dass solche Transformationen identisch sind mit denen, die für den
Trainingsdatensatz und den Transformationen, die während der Inferenz durchgeführt werden, identisch sind. Das Transformationsmuster
hilft, diese Reproduzierbarkeit sicherzustellen.

Bei Bildmodellen gibt es einige Transformationen (z. B. Datenerweiterung durch
zufälliges Beschneiden und Zoomen), die nur während des Trainings angewendet werden. Diese Trans-
Diese Trans- formationen müssen während der Inferenz nicht erfasst werden. Solche Transformationen werden
nicht Teil des Transformationsmusters sein.

Alternative Musteransätze

Ein alternativer Ansatz zur Lösung des Problems der Trainingsverzerrung ist die Verwendung von
das Muster des Merkmalsspeichers. Der Merkmalsspeicher umfasst eine koordinierte Berechnungsmaschine
und einen Speicher für transformierte Merkmalsdaten. Die Berechnungsmaschine unterstützt
den Zugriff mit niedriger Latenz für die Inferenz und die Stapelerstellung von transformierten Merkmalen, während das
Datenspeicher einen schnellen Zugriff auf umgewandelte Merkmale für das Modelltraining bietet. Der
Vorteil eines Merkmalsspeichers ist, dass die Transformationsoperationen nicht in den
Operationen in den Modellgraphen passen müssen. Solange der Merkmalsspeicher zum Beispiel Java unterstützt
Java unterstützt, könnten die Vorverarbeitungsoperationen in Java durchgeführt werden, während das Modell selbst
in PyTorch geschrieben werden könnte. Der Nachteil eines Merkmalsspeichers ist, dass er das
Modell vom Merkmalspeicher abhängig macht und die Infrastruktur für die Bereitstellung viel
komplexer macht.

Entwurfsmuster 21: Transformieren | 257
Eine weitere Möglichkeit, die Programmiersprache und das Framework, die für die
Transformation der Merkmale von der Sprache zu trennen, in der das Modell geschrieben wird, ist die
Vorverarbeitung in Containern durchzuführen und diese benutzerdefinierten Container als Teil sowohl des
als auch für das Training und das Serving. Dies wird in "Entwurfsmuster 25: Workflow-Pipeline"
auf Seite 282 beschrieben und wird in der Praxis von Kubeflow Serving übernommen.

Entwurfsmuster 22: Wiederholbare Aufteilung
Um sicherzustellen, dass die Probenahme wiederholbar und reproduzierbar ist, ist es notwendig, eine gut
verteilte Spalte und eine deterministische Hash-Funktion zu verwenden, um die verfügbaren Daten in
Trainings-, Validierungs- und Testdatensätze aufzuteilen.

Problem
In vielen Tutorials zum maschinellen Lernen wird vorgeschlagen, die Daten zufällig in Trainings-, Validierungs- und Testdatensätze aufzuteilen,
Validierungs- und Testdatensätze aufzuteilen, wobei ein Code ähnlich dem folgenden verwendet wird:

df = pd.DataFrame(...)
rnd = np.random.rand(len(df))
train = df[ rnd < 0.8 ]
valid = df[ rnd >= 0.8 & rnd < 0.9 ]
test = df[ rnd >= 0.9 ]
Leider versagt dieser Ansatz in vielen realen Situationen. Der Grund dafür ist, dass es
dass die Zeilen selten unabhängig sind. Wenn wir zum Beispiel ein Modell zur Vorhersage von
Flugverspätungen zu trainieren, werden die Ankunftsverspätungen von Flügen am selben Tag hoch korreliert
miteinander korreliert. Dies führt zu einem Informationsverlust zwischen dem Trainings- und dem Testdatensatz.
und Testdatensatz, wenn einige der Flüge an einem bestimmten Tag im Trainingsdatensatz enthalten sind
und einige andere Flüge desselben Tages im Testdatensatz enthalten sind. Dieser Leckverlust durch
korrelierter Zeilen ist ein häufig auftretendes Problem, das wir beim maschinellen Lernen vermeiden
wenn wir maschinelles Lernen betreiben.

Außerdem ordnet die Funktion rand die Daten bei jeder Ausführung anders an, so dass wir bei einer erneuten Ausführung
Wenn wir das Programm erneut ausführen, erhalten wir 80 % andere Zeilen. Dies kann zu Problemen führen, wenn wir
mit verschiedenen Modellen für maschinelles Lernen experimentieren, um das beste Modell zu
Wir müssen die Leistung der Modelle mit demselben Testdatensatz vergleichen. Unter
müssen wir den zufälligen Seed im Voraus festlegen oder die Daten nach der Aufteilung speichern.
nachdem sie aufgeteilt wurden. Die Aufteilung der Daten hart zu kodieren, ist keine gute Idee, denn,
bei der Durchführung von Techniken wie Jackknifing, Bootstrapping, Kreuzvalidierung und
Hyperparameterabstimmung müssen wir diese Datenaufteilung ändern, und zwar so, dass
die es uns ermöglicht, einzelne Versuche durchzuführen.

Für das maschinelle Lernen wollen wir eine leichte, wiederholbare Aufteilung der Daten, die
unabhängig von der Programmiersprache oder den zufälligen Seeds. Wir wollen auch sicherstellen, dass
korrelierte Zeilen in dieselbe Aufteilung fallen. Wir wollen zum Beispiel keine Flüge am 2. Januar
2. Januar 2019 im Testdatensatz enthalten sein, wenn Flüge an diesem Tag im Trainingsdatensatz enthalten sind.

258 | Kapitel 6: Reproduzierbarkeit - Entwurfsmuster

Lösung
Zunächst ermitteln wir eine Spalte, die die Korrelationsbeziehung zwischen den Zeilen erfasst.
In unserem Datensatz über die Verspätungen von Fluggesellschaften ist dies die Spalte "Datum". Dann verwenden wir die letzten Ziffern einer
einer Hash-Funktion für diese Spalte, um die Daten aufzuteilen. Für das Problem der Verspätungen von Fluggesellschaften können wir
können wir den Farm Fingerprint Hash-Algorithmus auf die Datumsspalte anwenden, um die
verfügbaren Daten in Trainings-, Validierungs- und Testdatensätze aufzuteilen.

Mehr über den Farm Fingerprint-Algorithmus, die Unterstützung für andere
Frameworks und Sprachen, und die Beziehung zwischen Hashing
und Kryptografie finden Sie unter "Entwurfsmuster 1: Hash-Funktionen"
auf Seite 32 in Kapitel 2. Insbesondere sind Open-Source-Wrapper für den
Farm Hash-Algorithmus sind in einer Reihe von Sprachen verfügbar
(einschließlich Python), so dass dieses Muster auch dann angewendet werden kann, wenn die Daten
Daten nicht in einem Data Warehouse sind, das einen wiederholbaren Hash von vornherein unterstützt
unterstützt.
So wird der Datensatz auf der Grundlage des Hashwerts der Datumsspalte aufgeteilt:

SELECT
Fluggesellschaft,
departure_airport,
abflug_zeitplan,
ankunft_flughafen,
ankunft_verspätung
FROM
`bigquery-samples`.airline_ontime_data.flights
WHERE
ABS ( MOD (FARM_FINGERPRINT(date), 10)) < 8 -- 80% für TRAIN
Um die Datumsspalte aufzuteilen, berechnen wir ihren Hash mit der Funktion FARM_FINGERPRINT
und verwenden dann die Modulo-Funktion, um eine beliebige 80%ige Teilmenge der Zeilen zu finden.
Dies ist nun wiederholbar, da die Funktion FARM_FINGERPRINT jedes Mal denselben Wert zurückgibt
den gleichen Wert zurückgibt, können wir sicher sein, dass wir jedes Mal die gleichen
80% der Daten jedes Mal erhalten. Infolgedessen gehören alle Flüge an einem bestimmten Datum zu demselben
gleichen Split-Train, Validierung oder Test gehören. Dies ist unabhängig von der Zufallsauswahl wiederholbar.

Wenn wir unsere Daten nach Ankunft_Flughafen aufteilen wollen (so dass 80 % der Flughäfen im
Trainingsdatensatz sind, vielleicht weil wir versuchen, etwas über die Ausstattung von
vorhersagen wollen), würden wir den Hash nach Ankunft_Flughafen statt nach Datum berechnen.

Es ist auch einfach, die Validierungsdaten zu erhalten: Ändern Sie < 8 in der obigen Abfrage
in =8, und für die Testdaten ändern Sie es in =9. Auf diese Weise erhalten wir 10 % der Stichproben in der Validie
tion und 10 % in der Testphase.

Welche Überlegungen gibt es bei der Auswahl der Spalte, nach der geteilt werden soll? Die Datumsspalte
muss mehrere Eigenschaften aufweisen, damit wir sie als Aufteilungsspalte verwenden können:

Entwurfsmuster 22: Wiederholbare Aufteilung | 259
Zeilen mit demselben Datum neigen dazu, korreliert zu sein - dies ist wiederum der Hauptgrund, warum wir sicherstellen wollen
warum wir sicherstellen wollen, dass alle Zeilen mit demselben Datum im selben Split sind.
Das Datum ist keine Eingabe für das Modell, auch wenn es als Kriterium für die Aufteilung verwendet wird.
Aus dem Datum extrahierte Merkmale wie der Wochentag oder die Stunde des Tages können Eingaben sein,
aber wir können keine tatsächliche Eingabe als Feld für die Aufteilung verwenden, weil das
weil das trainierte Modell 20 % der möglichen Eingabewerte für die Datumsspalte nicht gesehen hat
Spalte gesehen hat, wenn wir 80 % der Daten für das Training verwenden.
Es müssen genügend Datumswerte vorhanden sein. Da wir den Hash berechnen und das Modulo
modulo in Bezug auf 10 zu finden, benötigen wir mindestens 10 eindeutige Hash-Werte. Je
mehr eindeutige Werte wir haben, desto besser. Um sicher zu gehen, empfiehlt sich als Faustregel
3-5× des Nenners für den Modulo anzustreben, in diesem Fall brauchen wir also etwa 40 eindeutige
Daten.
Das Etikett muss gut auf die Daten verteilt sein. Wenn sich herausstellt, dass alle
Verspätungen am 1. Januar auftraten und es den Rest des Jahres keine Verspätungen gab, würde dies
würde dies nicht funktionieren, da die aufgeteilten Datensätze verzerrt wären. Um sicherzugehen, sollten Sie sich ein Diagramm ansehen
und vergewissern Sie sich, dass alle drei Aufteilungen eine ähnliche Verteilung der Bezeichnungen aufweisen. Zur
besonders sicher zu sein, stellen Sie sicher, dass die Verteilung der Kennzeichnung nach Abflugverspätung und anderen
Eingabewerte in den drei Datensätzen ähnlich sind.
Wir können die Überprüfung, ob die Verteilungen der Labels in den drei Datensätzen ähnlich sind, automatisieren.
der drei Datensätze mit Hilfe des Kolomogorov-Smirnov-Tests
Kolomogorov-Smirnow-Test: Stellen Sie einfach die kumulativen Verteilungsfunktionen der
den drei Datensätzen und finden Sie den maximalen Abstand zwischen jedem
Paar. Je kleiner der maximale Abstand ist, desto besser ist die Aufteilung.
Kompromisse und Alternativen
Schauen wir uns einige Varianten an, wie wir das wiederholbare Splitting durchführen können, und diskutieren wir
die Vor- und Nachteile der einzelnen Varianten. Wir wollen auch untersuchen, wie man diese Idee erweitern kann, um wiederholbare
Stichproben, nicht nur Splitting.

Einzelne Abfrage

Wir brauchen nicht drei separate Abfragen, um Trainings-, Validierungs- und Test-Splits zu erzeugen.
Wir können dies mit einer einzigen Abfrage wie folgt tun:

CREATE OR REPLACE TABLE mydataset.mytable AS
SELECT
fluggesellschaft,
abflug_flughafen,
abflug_zeitplan,
ankunft_flughafen,
ankunft_verspätung,
260 | Kapitel 6: Reproduzierbarkeit - Entwurfsmuster

CASE ( ABS ( MOD (FARM_FINGERPRINT(date), 10)))
WHEN 9 THEN 'Test'
WHEN 8 THEN 'Validierung'
ELSE 'Ausbildung' END AS split_col
FROM
`bigquery-samples`.airline_ontime_data.flights
Wir können dann die Spalte split_col verwenden, um zu entscheiden, zu welchem der drei Datensätze eine bestimmte Zeile gehört.
eine bestimmte Zeile fällt. Die Verwendung einer einzigen Abfrage verringert die Berechnungszeit, erfordert aber die Erstellung
eine neue Tabelle zu erstellen oder die Quelltabelle zu ändern, um die zusätzliche Spalte split_col hinzuzufügen.

Zufällige Aufteilung

Was ist, wenn die Zeilen nicht korreliert sind? In diesem Fall wollen wir eine zufällige, wiederholbare Aufteilung
aber wir haben keine natürliche Spalte, nach der wir aufteilen können. Wir können die gesamte Datenzeile hashen, indem wir
in eine Zeichenkette konvertieren und diese Zeichenkette hashen:

SELECT
Fluggesellschaft,
departure_airport,
abflug_zeitplan,
ankunft_flughafen,
ankunft_verspätung
FROM
`bigquery-samples`.airline_ontime_data.flights f
WHERE
ABS ( MOD (FARM_FINGERPRINT( TO_JSON_STRING(f) , 10)) < 8
Beachten Sie, dass doppelte Zeilen immer in der gleichen Aufteilung landen. Diese
könnte genau das sein, was wir wollen. Wenn nicht, dann müssen wir eine eindeutige ID-Spalte
in die SELECT-Abfrage aufnehmen.

Aufteilung auf mehrere Spalten

Wir haben über eine einzelne Spalte gesprochen, die die Korrelation zwischen Zeilen erfasst.
Was ist, wenn es sich um eine Kombination von Spalten handelt, die erfassen, wenn zwei Zeilen korreliert sind? In
solchen Fällen werden die Felder einfach verkettet (dies ist ein Merkmalskreuz), bevor der
Hashes. Nehmen wir zum Beispiel an, wir wollen nur sicherstellen, dass Flüge vom gleichen Flughafen
am selben Tag nicht in verschiedenen Splits auftauchen. In diesem Fall würden wir Folgendes tun
Folgendes tun:

SELECT
Fluggesellschaft,
departure_airport,
abflug_zeitplan,
ankunft_flughafen,
ankunft_verspätung
FROM
`bigquery-samples`.airline_ontime_data.flights
Entwurfsmuster 22: Wiederholbare Aufteilung | 261
WHERE
ABS ( MOD (FARM_FINGERPRINT( CONCAT(date, arrival_airport)) , 10)) < 8
Wenn wir auf ein Feature Cross aus mehreren Spalten splitten, können wir arrival_airport als
eine der Eingaben für das Modell verwenden, da es sowohl in der Trainings- als auch in der Testmenge Beispiele für einen bestimmten Flughafen
sowohl in der Trainings- als auch in der Testmenge. Hätten wir hingegen nur nach
arrival_airport gesplittet hätten, dann hätten die Trainings- und Testmengen eine sich gegenseitig ausschließende Menge von
Ankunftsflughäfen und daher kann arrival_airport keine Eingabe für das Modell sein.

Wiederholbare Probenahme

Die Basislösung ist gut, wenn wir 80 % des gesamten Datensatzes als Training benötigen, aber was ist, wenn
wir mit einem kleineren Datensatz als dem in BigQuery vorhandenen spielen wollen? Diese
ist für die lokale Entwicklung üblich. Der Flugdatensatz umfasst 70 Millionen Zeilen, und vielleicht
wollen wir einen kleineren Datensatz mit einer Million Flügen. Wie würden wir 1 von 70
Flügen und dann 80 % davon als Training auswählen?

Was wir nicht tun können, ist etwas in der Art von:

SELECT
Datum,
airline,
abflug_flughafen,
abflug_zeitplan,
ankunft_flughafen,
ankunft_verspätung
FROM
`bigquery-samples`.airline_ontime_data.flights
WHERE
ABS ( MOD (FARM_FINGERPRINT(Datum), 70)) = 0
AND ABS ( MOD (FARM_FINGERPRINT(Datum), 10)) < 8
Wir können nicht 1 in 70 Zeilen auswählen und dann 8 in 10 Zeilen auswählen. Wenn wir Zahlen auswählen, die
durch 70 teilbar sind, müssen sie natürlich auch durch 10 teilbar sein! Diese zweite Mod-
ulo-Operation ist nutzlos.

Hier ist eine bessere Lösung:

SELECT
Datum,
airline,
abflug_flughafen,
abflug_zeitplan,
ankunft_flughafen,
ankunft_verspätung
FROM
`bigquery-samples`.airline_ontime_data.flights
WHERE
ABS ( MOD (FARM_FINGERPRINT(Datum), 70)) = 0
AND ABS ( MOD (FARM_FINGERPRINT(Datum), 700)) < 560
262 | Kapitel 6: Reproduzierbarkeit - Entwurfsmuster

In dieser Abfrage ist die 700 gleich 7010 und die 560 gleich 708. Die erste Modulo-Operation wählt 1 in
70 Zeilen und die zweite Modulo-Operation wählt in 10 dieser Zeilen 8 aus.

Bei Validierungsdaten ersetzen Sie < 560 durch den entsprechenden Bereich:

ABS ( MOD (FARM_FINGERPRINT(date), 70)) = 0
AND ABS ( MOD (FARM_FINGERPRINT(date), 700)) BETWEEN 560 AND 629
Im vorangegangenen Code stammen unsere eine Million Flüge von nur 1/70 der Tage im
dem Datensatz. Dies kann genau das sein, was wir wollen - zum Beispiel können wir modellieren
das gesamte Spektrum der Flüge an einem bestimmten Tag, wenn wir mit dem kleineren
Datensatz. Wenn wir jedoch nur 1/70 der Flüge an einem bestimmten Tag haben wollen, müssen wir
RAND() verwenden und das Ergebnis aus Gründen der Wiederholbarkeit in einer neuen Tabelle speichern. Aus dieser
kleineren Tabelle können wir mit FARM_FINGERPRINT() eine Stichprobe von 80 % der Daten ziehen. Da diese
neue Tabelle nur eine Million Zeilen umfasst und nur zum Experimentieren dient, kann die Duplizierung
akzeptabel sein.

Sequentielle Aufteilung

Bei Zeitreihenmodellen ist es üblich, sequenzielle Splits von Daten zu verwenden.
Daten. Um zum Beispiel ein Modell zur Nachfragevorhersage zu trainieren, bei dem wir ein Modell auf
Daten der letzten 45 Tage trainiert wird, um die Nachfrage für die nächsten 14 Tage vorherzusagen, würde man das
Modell (vollständiger Code) trainieren, indem wir die erforderlichen Daten abrufen:

CREATE OR REPLACE MODEL ch09eu.numrentals_forecast
OPTIONS (model_type='ARIMA',
time_series_data_col='numrentals',
time_series_timestamp_col='date') AS
SELECT
CAST ( EXTRACT (date from start_date) AS TIMESTAMP ) AS datum
, COUNT (*) AS numrentals
FROM
`bigquery- public - data `.london_bicycles.cycle_hire
GROUP BY datum
HAVING date BETWEEN
DATE_SUB( AKTUELLES_DATUM (), INTERVALL 45 TAG ) UND AKTUELLES_DATUM ()
Eine solche sequenzielle Aufteilung von Daten ist auch in schnelllebigen Umgebungen notwendig, selbst wenn
das Ziel nicht darin besteht, den zukünftigen Wert einer Zeitreihe vorherzusagen. In einem Modell zur Erkennung von Betrug zum Beispiel
Betrugserkennungsmodells passen sich bösartige Akteure schnell an den Betrugsalgorithmus an, und das Modell muss
Daher muss das Modell ständig anhand der neuesten Daten neu trainiert werden, um künftige Betrugsfälle vorherzusagen. Es ist
Es reicht nicht aus, die Bewertungsdaten aus einer zufälligen Aufteilung des historischen
historischen Datensatzes zu generieren, da das Ziel darin besteht, das Verhalten der bösen Akteure
Zukunft zeigen werden. Das indirekte Ziel ist das gleiche wie das eines Zeitreihenmodells, da ein gutes
Modell in der Lage sein wird, auf historischen Daten zu trainieren und zukünftige Betrugsfälle vorherzusagen. Die Daten müssen
zeitlich nacheinander aufgeteilt werden, um dies korrekt zu bewerten. Zum Beispiel (vollständiger
Code):

Entwurfsmuster 22: Wiederholbare Aufteilung | 263
def read_dataset(client, row_restriction , batch_size=2048):
...
bqsession = client.read_session(
...
Zeile_Einschränkung=Zeile_Einschränkung)
dataset = bqsession.parallel_read_rows()
return (dataset.prefetch(1).map(features_and_labels)
.shuffle(batch_size*10).batch(batch_size))
client = BigQueryClient()
train_df = read_dataset(client, 'Zeit <= 144803' , 2048)
eval_df = read_dataset(client, 'Zeit > 144803' , 2048)
Ein weiterer Fall, in dem eine sequenzielle Aufteilung der Daten erforderlich ist, ist, wenn es hohe
Korrelationen zwischen aufeinanderfolgenden Zeitpunkten bestehen. Zum Beispiel bei der Wettervorhersage ist das
Wetter an aufeinanderfolgenden Tagen stark korreliert. Daher ist es nicht sinnvoll, den
den 12. Oktober in den Trainingsdatensatz und den 13. Oktober in den Testdatensatz aufzunehmen, weil
da es zu beträchtlichen Streuungen kommen würde (man stelle sich zum Beispiel vor, dass es am 12. Oktober einen Hurrikan
12. Oktober). Außerdem ist das Wetter in hohem Maße saisonabhängig, so dass es notwendig ist, Tage
aus allen Jahreszeiten in allen drei Splits enthalten. Eine Möglichkeit zur angemessenen Bewertung der Leistung eines
Vorhersagemodells ist die Verwendung eines sequenziellen Splits, wobei die Saisonalität berücksichtigt wird, indem
die ersten 20 Tage eines jeden Monats im Trainingsdatensatz, die nächsten 5 Tage im
Validierungsdatensatz und die letzten 5 Tage im Testdatensatz.

In all diesen Fällen erfordert die wiederholbare Aufteilung nur, dass wir die Logik zur Erstellung der
Logik zur Erstellung des Splits in die Versionskontrolle aufzunehmen und sicherzustellen, dass die Modellversion
aktualisiert wird, wenn diese Logik geändert wird.

Geschichtete Aufteilung

Das obige Beispiel der unterschiedlichen Wettermuster in den verschiedenen Jahreszeiten ist
ein Beispiel für eine Situation, in der die Aufteilung nach der Schichtung des Datensatzes
stratifiziert wurde. Wir mussten sicherstellen, dass es in jeder Aufteilung Beispiele für alle Jahreszeiten gibt,
Deshalb haben wir den Datensatz nach Monaten geschichtet, bevor wir die Aufteilung vorgenommen haben. Wir
die ersten 20 Tage eines jeden Monats im Trainingsdatensatz, die nächsten 5 Tage im
Validierungsdatensatz und die letzten 5 Tage für den Testdatensatz. Hätten wir uns nicht
die Korrelation zwischen aufeinanderfolgenden Tagen keine Rolle gespielt, hätten wir die
die Daten innerhalb jedes Monats zufällig aufgeteilt.

Je größer der Datensatz ist, desto weniger müssen wir uns mit der Schichtung befassen. Bei sehr
großen Datensätzen sind die Chancen sehr hoch, dass die Merkmalswerte gut auf alle Splits verteilt sind.
über alle Splits verteilt sind. Daher besteht beim maschinellen Lernen in großem Maßstab die Notwendigkeit einer Stratifizierung
nur im Fall von schiefen Datensätzen häufig erforderlich. Zum Beispiel in dem
Flüge weniger als 1 % der Flüge vor 6 Uhr morgens ab, so dass die Anzahl der Flüge, die dieses Kriterium erfüllen
Flüge, die dieses Kriterium erfüllen, recht klein sein. Wenn es für unseren geschäftlichen Anwendungsfall entscheidend ist
das Verhalten dieser Flüge korrekt zu erfassen, sollten wir den Datensatz nach der Abflugzeit schichten
der Abflugzeit schichten und jede Schichtung gleichmäßig aufteilen.

264 | Kapitel 6: Reproduzierbarkeit - Entwurfsmuster

Die Abfahrtszeit war ein Beispiel für ein schiefes Merkmal. Bei einem unausgewogenen Klassifizie- rungsproblem
Klassifizierungsproblem (z. B. bei der Erkennung von Betrug, wo die Zahl der Betrugsbeispiele recht
klein ist), könnte es sinnvoll sein, den Datensatz nach dem Label zu schichten und jede Schichtung
gleichmäßig aufteilen. Dies ist auch wichtig, wenn wir ein Multilabel-Problem haben und einige der Labels
seltener sind als andere. Dies wird in "Entwurfsmuster 10: Rebalancing" auf
Seite 122 in Kapitel 3.

Unstrukturierte Daten

Obwohl wir uns in diesem Abschnitt auf strukturierte Daten konzentriert haben, gelten die gleichen Prinzipien
auch für unstrukturierte Daten wie Bilder, Video, Audio oder Freiformtexte gelten. Einfach
die Metadaten verwenden, um die Aufteilung vorzunehmen. Wenn zum Beispiel Videos, die am selben Tag aufgenommen wurden
korreliert sind, verwenden Sie das Aufnahmedatum eines Videos in den Metadaten, um die Videos in
unabhängigen Datensätzen aufzuteilen. Ähnlich verhält es sich, wenn Textbewertungen von ein und derselben Person tendenziell korreliert sind.
zusammenhängen, wird der Farm Fingerprint der user_id des Rezensenten verwendet, um die
Bewertungen auf die Datensätze aufzuteilen. Wenn die Metadaten nicht verfügbar sind oder es keine Korrelation
zwischen den Instanzen, kodieren Sie das Bild oder Video mit Base64 und berechnen
den Fingerabdruck der Kodierung.

Ein natürlicher Weg zur Aufteilung von Textdatensätzen könnte darin bestehen, den Hash des Textes selbst zur Aufteilung zu verwenden.
aufzuteilen. Dies kommt jedoch einer zufälligen Aufteilung gleich und geht nicht auf das Problem der Kor-
Beziehungen zwischen den Bewertungen. Wenn zum Beispiel eine Person das Wort "umwerfend" häufig in
negativen Rezensionen verwendet oder eine Person alle Star Wars-Filme als schlecht bewertet, sind ihre Rezensionen
korreliert. Eine natürliche Methode zur Aufteilung von Bild- oder Audiodatensätzen könnte auch darin bestehen
den Hash des Dateinamens für die Aufteilung zu verwenden, aber das Problem der
Korrelationen zwischen Bildern oder Videos. Es lohnt sich, sorgfältig darüber nachzudenken, wie man
Aufteilung eines Datensatzes zu überlegen. Unserer Erfahrung nach können viele Probleme mit schlechter Leistung von
ML angegangen werden, indem die Aufteilung der Daten (und die Datenerfassung) unter Berücksichtigung potenzieller
Korrelationen berücksichtigt.

Bei der Berechnung von Einbettungen oder dem Vortraining von Auto-Encodern sollten wir darauf achten, dass wir
die Daten zunächst aufzuteilen und diese Vorberechnungen nur für den Trainingsdatensatz durchzuführen.
Aus diesem Grund sollten die Einbettungen der Bilder, Videos oder Texte nicht aufgeteilt werden, es sei denn, diese Einbettungen wurden
eos oder Text durchgeführt werden, es sei denn, diese Einbettungen wurden in einem völlig separaten Datensatz erstellt.

Entwurfsmuster 22: Wiederholbare Aufteilung | 265
Entwurfsmuster 23: Überbrücktes Schema
Das Entwurfsmuster "Bridged Schema" bietet Möglichkeiten, die Daten, die zum Trainieren eines
Modells verwendeten Daten von ihrem älteren, ursprünglichen Datenschema an neuere, bessere Daten anzupassen. Dieses Muster ist nützlich
denn wenn ein Datenlieferant Verbesserungen an seinem Datenfeed vornimmt, dauert es oft
Daten des verbesserten Schemas gesammelt werden, damit wir ein Ersatzmodell angemessen trainieren können.
adäquat ein Ersatzmodell zu trainieren. Mit dem Bridged-Schema-Muster können wir so viele
wie möglich die neueren Daten zu verwenden, sie aber mit einigen älteren Daten zu ergänzen
Modellgenauigkeit zu verbessern.

Problem
Stellen Sie sich eine Verkaufsstellenanwendung vor, die vorschlägt, wie viel Trinkgeld man einem Zusteller geben sollte.
Die Anwendung könnte ein maschinelles Lernmodell verwenden, das den Trinkgeldbetrag vorhersagt,
unter Berücksichtigung der Bestellmenge, der Lieferzeit, der Lieferentfernung usw.
Ein solches Modell würde anhand der tatsächlichen Trinkgelder der Kunden trainiert werden.

Angenommen, eine der Eingaben in das Modell ist die Zahlungsart. In den historischen
Daten wurde diese als "bar" oder "Karte" erfasst. Nehmen wir jedoch an, das Zahlungssystem
Das Zahlungssystem wurde jedoch aktualisiert und liefert nun genauere Angaben zur Art der Karte (Geschenkkarte,
Debitkarte, Kreditkarte), die verwendet wurde. Dies ist eine äußerst nützliche Information, denn
das Trinkgeldverhalten zwischen den drei Kartentypen variiert.

Zum Zeitpunkt der Vorhersage werden die neueren Informationen immer verfügbar sein, da wir
Trinkgeldbeträge für Transaktionen vorhersagen, die nach dem Upgrade des Zahlungssystems
aktualisieren. Da die neuen Informationen äußerst wertvoll sind und dem Vorhersagesystem bereits
dem Vorhersagesystem bereits zur Verfügung stehen, möchten wir sie so schnell wie möglich in das Modell
so schnell wie möglich in das Modell aufnehmen.

Wir können ein neues Modell nicht ausschließlich auf den neueren Daten trainieren, weil die Menge der
neuen Daten recht klein sein wird, da sie sich auf Transaktionen nach dem Upgrade des Zahlungssystems
Upgrade. Da die Qualität eines ML-Modells in hohem Maße von der Menge der
Datenmenge abhängt, ist es wahrscheinlich, dass ein Modell, das nur mit den neuen Daten
schlecht abschneiden wird.

Lösung
Die Lösung besteht darin, das Schema der alten Daten an die neuen Daten anzupassen. Dann werden wir
trainieren wir ein ML-Modell mit so vielen neuen Daten wie möglich und ergänzen es mit
den älteren Daten. Es gibt zwei Fragen zu beantworten. Erstens: Wie werden wir die Tatsache ausgleichen
dass die älteren Daten nur zwei Kategorien für die Zahlungsart enthalten, während die neuen Daten
vier Kategorien haben? Zweitens: Wie wird die Erweiterung durchgeführt, um Datensätze für
Training, Validierung und Test zu erstellen?

266 | Kapitel 6: Reproduzierbarkeit - Entwurfsmuster

Überbrücktes Schema

Nehmen wir den Fall an, dass die älteren Daten zwei Kategorien haben (Bargeld und Karte). In dem neuen
Schema ist die Kartenkategorie nun viel granularer (Geschenkkarte, Debitkarte, Kreditkarte
karte). Wir wissen jedoch, dass eine Transaktion, die in den alten Daten als "Karte" kodiert war
einer dieser Typen gewesen wäre, aber der tatsächliche Typ wurde nicht aufgezeichnet. Es ist möglich
das Schema probabilistisch oder statisch zu überbrücken. Wir empfehlen die statische Methode.
Wir empfehlen die statische Methode, aber es ist einfacher zu verstehen, wenn wir zuerst die probabilistische Methode durchgehen.

Probabilistische Methode. Nehmen wir an, dass wir anhand der neueren Trainingsdaten schätzen, dass von den
der Kartentransaktionen 10 % Geschenkkarten, 30 % Debitkarten und 60 % Kreditkarten sind.
Karten sind. Jedes Mal, wenn ein älteres Trainingsbeispiel in das Trainerprogramm geladen wird, könnten wir
den Kartentyp wählen, indem wir eine gleichmäßig verteilte Zufallszahl im Bereich
im Bereich [0, 100) generieren und eine Geschenkkarte wählen, wenn die Zufallszahl kleiner als 10 ist, eine
Debitkarte, wenn sie im Bereich [10, 40) liegt, und ansonsten eine Kreditkarte. Vorausgesetzt, wir trainieren für
genügend Epochen trainiert, wird jedes Trainingsbeispiel als alle drei Kategorien dargestellt, aber
proportional zur tatsächlichen Häufigkeit des Auftretens. Die neueren Trainingsbeispiele
Natürlich würden die neueren Trainingsbeispiele immer die tatsächlich aufgezeichnete Kategorie aufweisen.

Die Rechtfertigung für den probabilistischen Ansatz besteht darin, dass wir jedes ältere Beispiel so behandeln
Hunderte von Malen geschehen ist. Während der Trainer die Daten durchgeht, wird in jeder
Epoche eine dieser Instanzen simuliert. In der Simulation erwarten wir, dass 10 % der
der Fälle, in denen eine Karte verwendet wurde, die Transaktion mit einer Geschenkkarte stattgefunden hätte.
Deshalb wählen wir für den Wert der kategorialen Eingabe in 10 % der Fälle "Geschenkkarte".
Dies ist natürlich eine Vereinfachung - nur weil Geschenkkarten insgesamt 10 % der Zeit verwendet werden,
ist es nicht der Fall, dass Geschenkkarten in 10 % der Fälle für eine bestimmte Transaktion verwendet werden.
Transaktion verwendet werden. Ein extremes Beispiel wäre, dass Taxi-Unternehmen die Verwendung von Geschenkkarten auf Flugreisen verbieten.
und so ist eine Geschenkkarte für einige historische Beispiele nicht einmal ein legaler Wert.
In Ermangelung zusätzlicher Informationen gehen wir jedoch davon aus, dass die Häufigkeitsverteilung
Verteilung für alle historischen Beispiele gleich ist.

Statische Methode. Kategoriale Variablen werden in der Regel mit einem Punkt kodiert. Wenn wir den
probabilistischen Ansatz folgen und lange genug trainieren, wird der durchschnittliche One-Hot-codierte
Wert, der dem Trainingsprogramm für eine "Karte" in den älteren Daten präsentiert wird, bei [0, 0.1,
0.3, 0.6]. Die erste 0 entspricht der Kategorie Bargeld. Die zweite Zahl ist 0,1
denn in 10 % der Fälle ist diese Zahl bei Kartentransaktionen 1 und in allen anderen Fällen ist sie
in allen anderen Fällen Null ist. In ähnlicher Weise haben wir 0,3 für Debitkarten und 0,6 für Kreditkarten.

Um die älteren Daten in das neuere Schema zu überführen, können wir die älteren kategori
Daten in diese Darstellung transformieren, in die wir die aus den Trainingsdaten geschätzte A-priori-Wahrscheinlichkeit der neuen
Klassen einfügen, die aus den Trainingsdaten geschätzt wurden. Die neueren Daten hingegen
0, 0, 1, 0] für eine Transaktion, von der bekannt ist, dass sie mit einer Debitkarte bezahlt wurde.

Wir empfehlen die statische Methode gegenüber der probabilistischen Methode, weil sie effektiv das ist, was
was passiert, wenn die probabilistische Methode lange genug läuft. Sie ist auch viel

Entwurfsmuster 23: Überbrücktes Schema | 267
einfacher zu implementieren, da jede Kartenzahlung aus den alten Daten den exakt gleichen
gleichen Wert hat (das 4-Elemente-Array [0, 0.1, 0.3, 0.6]). Wir können die älteren Daten in einer
einer Codezeile aktualisieren, anstatt ein Skript zu schreiben, das Zufallszahlen wie bei der
probabilistischen Methode. Außerdem ist diese Methode rechnerisch viel weniger aufwendig.

Erhöhte Daten

Um die Nutzung der neueren Daten zu maximieren, stellen Sie sicher, dass Sie nur zwei Splits der Daten verwenden.
Daten zu verwenden, was in "Entwurfsmuster 12: Prüfpunkte" auf Seite 149 in Kap. 4 besprochen wird.
ter 4 besprochen wird. Angenommen, wir haben 1 Million Beispiele mit dem alten Schema, aber
nur 5.000 Beispiele mit dem neuen Schema zur Verfügung. Wie sollen wir die Trainings- und
Trainings- und Auswertungsdatensätze erstellen?

Betrachten wir zunächst den Bewertungsdatensatz. Es ist wichtig zu erkennen, dass der Zweck des
Ziel des Trainings eines ML-Modells ist es, Vorhersagen für ungesehene Daten zu treffen. Die ungesehenen Daten in unserem
Fall ausschließlich Daten sein, die dem neuen Schema entsprechen. Daher müssen wir
eine ausreichende Anzahl von Beispielen aus den neuen Daten beiseite legen, um die
eralisierungsleistung zu bewerten. Vielleicht brauchen wir 2.000 Beispiele in unserem Bewertungsdatensatz, um
um sicher zu sein, dass das Modell in der Produktion gut funktioniert. Der Evaluierungs
Datenbestand wird keine älteren Beispiele enthalten, die mit dem neuen Schema überbrückt
neueren Schema anzupassen.

Woher wissen wir, ob wir 1.000 oder 2.000 Beispiele im Evaluierungsdatensatz benötigen?
Um diese Zahl zu schätzen, berechnen Sie die Bewertungsmetrik des aktuellen Produktionsmodells
Modells (das auf dem alten Schema trainiert wurde) auf Teilmengen seines Bewertungsdatensatzes und
bestimmen, wie groß die Teilmenge sein muss, damit die Bewertungsmetrik konsistent ist.

Die Berechnung der Bewertungsmetrik für verschiedene Teilmengen könnte wie folgt erfolgen (wie
wie üblich ist der vollständige Code auf GitHub im Code-Repository für dieses Buch zu finden):

for subset_size in range(100, 5000, 100):
sizes.append(subset_size)
# Berechnung der Variabilität der eval-Metrik
# bei dieser Teilmengengröße über 25 Versuche
scores = []
for x in range(1, 25):
indices = np.random.choice(N_eval,
size=subset_size, replace=False)
scores.append(
model.score(df_eval[indices],
df_old.loc[N_train+indices, 'tip'])
)
score_mean.append(np.mean(scores))
score_stddev.append(np.std(scores))
Im obigen Code probieren wir Auswertungsgrößen von 100, 200, ..., 5.000 aus. Bei jeder
Teilmengengröße wird das Modell 25 Mal ausgewertet, und zwar jedes Mal auf einer anderen, zufällig
zufällig ausgewählten Teilmenge der vollständigen Auswertungsmenge. Da es sich hierbei um die Auswertungsmenge des aktuellen...

268 | Kapitel 6: Reproduzierbarkeit - Entwurfsmuster

Produktionsmodell (das wir mit einer Million Beispielen trainieren konnten), könnte der
Evaluierungsdatensatz könnte hier Hunderttausende von Beispielen enthalten. Wir können dann
die Standardabweichung der Bewertungsmetrik über die 25 Teilmengen berechnen, dies
wiederholen und diese Standardabweichung gegen die Auswertungsgröße auftragen.
Größe der Auswertung. Das resultierende Diagramm sieht in etwa so aus wie in Abbildung 6-3.

Abbildung 6-3. Bestimmen Sie die Anzahl der benötigten Evaluationsbeispiele, indem Sie das
Produktionsmodells auf Teilmengen unterschiedlicher Größe und die Verfolgung der Variabilität der Evaluationsmetrik
tionsmetrik in Abhängigkeit von der Größe der Teilmenge. Hier beginnt sich die Standardabweichung bei
bei etwa 2.000 Beispielen.

Aus Abbildung 6-3 geht hervor, dass die Anzahl der Bewertungsbeispiele mindestens
2.000 und idealerweise 3.000 oder mehr betragen muss. Für den Rest dieser Diskussion nehmen wir an, dass wir
dass wir uns für die Auswertung von 2.500 Beispielen entscheiden.

Der Trainingssatz würde die verbleibenden 2.500 neuen Beispiele enthalten (die Menge der
(die Menge der neuen Daten, die nach der Zurückhaltung von 2.500 für die Auswertung zur Verfügung steht), ergänzt durch eine gewisse Anzahl
älteren Beispielen, die an das neue Schema angepasst wurden. Woher wissen wir
wissen wir, wie viele ältere Beispiele wir brauchen? Wir wissen es nicht. Dies ist ein Hyperparameter, den
wir abstimmen müssen. Beim Tipp-Problem zum Beispiel sehen wir bei der Gittersuche von

Entwurfsmuster 23: Überbrücktes Schema | 269
Abbildung 6-4 (das Notebook auf GitHub enthält die vollständigen Details) zeigt, dass die Bewertungsmetrik
bis zu 20.000 Beispielen steil abfällt und dann ein Plateau erreicht.

Abbildung 6-4. Bestimmen Sie die Anzahl der älteren Beispiele, die überbrückt werden sollen, indem Sie eine Hyper-
Parameterabstimmung. In diesem Fall ist es offensichtlich, dass der Nutzen abnimmt, wenn
20.000 überbrückten Beispielen.

Um die besten Ergebnisse zu erzielen, sollten wir die kleinste Anzahl älterer Beispiele wählen, die uns möglich ist.
können - im Idealfall werden wir uns mit der Zeit, wenn die Zahl der neuen Beispiele wächst
immer weniger auf überbrückte Beispiele. Irgendwann können wir die älteren Beispiele ganz abschaffen.
Beispiele ganz loswerden.

Es ist erwähnenswert, dass die Überbrückung bei diesem Problem Vorteile bringt, denn wenn
wir keine überbrückten Beispiele verwenden, ist die Bewertungsmetrik schlechter. Wenn dies nicht der Fall ist,
dann muss die Imputationsmethode (die Methode zur Auswahl des statischen Wertes, der für die Überbrückung
(die Methode zur Auswahl des statischen Wertes für die Überbrückung) überdacht werden. Wir schlagen eine alternative Imputationsmethode (Kaskade)
im nächsten Abschnitt vor.

270 | Kapitel 6: Reproduzierbarkeit - Entwurfsmuster

Es ist äußerst wichtig, die Leistung des neueren
Modells, das auf überbrückten Beispielen trainiert wurde, mit dem älteren, unveränderten
Modells im Evaluierungsdatensatz. Es könnte der Fall sein, dass die neuen
Informationen noch keinen ausreichenden Wert haben.
Da wir den Evaluierungsdatensatz verwenden werden, um zu testen, ob
ob das überbrückte Modell einen Wert hat oder nicht, ist es wichtig, dass der
Datensatz nicht während des Trainings oder der Abstimmung der Hyperparameter verwendet wird. Also,
Techniken wie vorzeitiges Beenden oder Checkpoint-Auswahl müssen
vermieden werden. Verwenden Sie stattdessen die Regularisierung, um die Überanpassung zu kontrollieren. Der
Trainingsverlust muss als Maßstab für die Abstimmung der Hyperparameter dienen.
ric. Weitere Einzelheiten zur Datensparsamkeit durch die Verwendung von nur zwei Hyperparametern finden Sie in der Diskussion des Entwurfsmusters Checkpoints in Kap.
Kapitel 4 für weitere Details zur Datensparsamkeit durch Verwendung von nur zwei
Splits.
Kompromisse und Alternativen
Schauen wir uns einen häufig vorgeschlagenen Ansatz an, der nicht funktioniert, eine komplexe Alternative
zur Überbrückung und eine Erweiterung der Lösung für ein ähnliches Problem.

Schema der Vereinigung

Es kann verlockend sein, einfach eine Vereinigung der älteren und neueren Schemata zu erstellen. Für
könnten wir beispielsweise das Schema für die Zahlungsart mit fünf möglichen Werten definieren
Werte: Bargeld, Karte, Geschenkkarte, Debitkarte und Kreditkarte. Dadurch werden sowohl die historischen
Daten als auch die neueren Daten gültig und ist der Ansatz, den wir im Data Warehousing
Data Warehousing mit solchen Änderungen umzugehen. Auf diese Weise sind die alten Daten und die neuen Daten
so wie sie sind und ohne Änderungen gültig.

Der rückwärtskompatible Union-of-Schemas-Ansatz funktioniert jedoch nicht für maschinelles
Lernen jedoch nicht.

Zum Zeitpunkt der Vorhersage werden wir niemals den Wert "Karte" für die Zahlungsart erhalten, da
die Eingabeanbieter alle hochgestuft worden sind. All diese Trainingsinstanzen
umsonst gewesen sein. Aus Gründen der Reproduzierbarkeit (dies ist der Grund, warum dieses Muster
als Reproduzierbarkeitsmuster klassifiziert), müssen wir das ältere Schema in das
in das neuere Schema überführen und können keine Vereinigung der beiden Schemata vornehmen.

Kaskadenmethode

Unter Imputation versteht man in der Statistik eine Reihe von Techniken, mit denen fehlende Daten
durch einen gültigen Wert zu ersetzen. Ein gängiges Imputationsverfahren ist das Ersetzen eines NULL-Wertes durch
den Mittelwert dieser Spalte in den Trainingsdaten zu ersetzen. Warum wählen wir den Mittelwert?
Weil in Ermangelung weiterer Informationen und unter der Annahme, dass die Werte
normalverteilt sind, der wahrscheinlichste Wert der Mittelwert ist.

Entwurfsmuster 23: Überbrücktes Schema | 271
Die in der Hauptlösung besprochene statische Methode der Zuweisung von A-priori-Häufigkeiten ist
auch eine Imputationsmethode. Wir gehen davon aus, dass die kategoriale Variable
einem Häufigkeitsdiagramm verteilt ist (das wir aus den Trainingsdaten schätzen) und unterstellen
und schreiben der "fehlenden" kategorialen Variable den mittleren, einhändig kodierten Wert (gemäß dieser Häufigkeitsverteilung) zu.
"fehlende" kategoriale Variable.

Kennen wir eine andere Möglichkeit, unbekannte Werte anhand einiger Beispiele zu schätzen? Unter
natürlich! Maschinelles Lernen. Was wir tun können, ist, eine Kaskade von Modellen zu trainieren (siehe
"Entwurfsmuster 8: Kaskade" auf Seite 108 in Kapitel 3). Das erste Modell verwendet die
neue Beispiele, um ein maschinelles Lernmodell zu trainieren, das den Kartentyp vorhersagt.
Typs. Wenn das ursprüngliche Tipp-Modell fünf Eingaben hatte, hat dieses Modell vier Eingaben. Die
Die fünfte Eingabe (die Zahlungsart) wird das Label für dieses Modell sein. Dann wird die Ausgabe des
ersten Modells zum Trainieren des zweiten Modells verwendet.

In der Praxis fügt das Kaskadenmuster zu viel Komplexität für etwas hinzu, das
als vorübergehende Lösung gedacht ist, bis Sie genügend neue Daten haben. Die statische
Methode ist praktisch das einfachste Modell für maschinelles Lernen - es ist das Modell, das wir
das wir erhalten würden, wenn wir uninformative Eingaben hätten. Wir empfehlen den statischen Ansatz und die Verwendung von
Kaskade nur dann zu verwenden, wenn die statische Methode nicht gut genug ist.

Umgang mit neuen Funktionen

Eine weitere Situation, in der eine Überbrückung erforderlich sein kann, ist, wenn der Eingabeanbieter
zusätzliche Informationen zum Input-Feed hinzufügt. In unserem Beispiel mit dem Taxitarif könnten wir zum Beispiel
Daten darüber erhalten, ob die Scheibenwischer des Taxis eingeschaltet sind oder ob sich das Fahrzeug bewegt.
fährt. Aus diesen Daten können wir ein Merkmal darüber erstellen, ob es zu Beginn der Taxifahrt geregnet hat
der Taxifahrt regnete, wie viel Zeit das Taxi im Leerlauf verbringt usw.

Wenn wir neue Eingabemerkmale haben, die wir sofort verwenden wollen, sollten wir die
die älteren Daten (in denen das neue Merkmal fehlt) überbrücken, indem wir einen Wert für das
neues Merkmal. Empfohlene Auswahlmöglichkeiten für den Imputationswert sind:

Der Mittelwert des Merkmals, wenn das Merkmal numerisch und normal verteilt ist
Der Medianwert des Merkmals, wenn das Merkmal numerisch und schief ist oder viele
Ausreißer aufweist
Der Medianwert des Merkmals, wenn das Merkmal kategorisch und sortierbar ist
Der Modus des Merkmals, wenn das Merkmal kategorisch und nicht sortierbar ist
Die Häufigkeit, mit der das Merkmal wahr ist, wenn es sich um ein boolesches Merkmal handelt
Wenn es sich bei dem Merkmal um die Frage handelt, ob es geregnet hat oder nicht, ist es boolesch, so dass der unterstellte Wert
etwa 0,02 sein, wenn es im Trainingsdatensatz 2 % der Zeit regnet. Wenn das
Merkmal der Anteil der untätigen Minuten ist, könnte man den Medianwert verwenden. Der Kaskaden
Kaskadenmuster-Ansatz ist für all diese Fälle praktikabel, aber eine statische Imputation ist einfacher
und oft ausreichend.

272 | Kapitel 6: Reproduzierbarkeit - Entwurfsmuster

1 Man beachte, dass die gesamte Wahrscheinlichkeitsverteilungsfunktion nicht gleichförmig sein muss - wir brauchen nur, dass
die ursprünglichen Bins eng genug sind, um die Wahrscheinlichkeitsverteilungsfunktion durch eine
Treppenfunktion zu approximieren. Diese Annahme scheitert, wenn wir eine stark schiefe Verteilung haben, die in den älteren Daten unzureichend abgetastet wurde.
die in den älteren Daten unzureichend abgetastet wurde. In solchen Fällen ist es möglich, dass 3,46 wahrscheinlicher ist als 3,54, und dies müsste in der
Dies müsste im überbrückten Datensatz berücksichtigt werden.
Handhabung von Präzisionssteigerungen

Wenn der Eingabeanbieter die Genauigkeit seines Datenstroms erhöht, folgen Sie dem
Bridging-Ansatz, um einen Trainingsdatensatz zu erstellen, der aus den höher aufgelösten Daten besteht
Daten besteht, die mit einigen der älteren Daten ergänzt werden.

Bei Fließkommawerten ist es nicht notwendig, die älteren Daten explizit zu überbrücken, um sie
die Genauigkeit der neueren Daten anzupassen. Um zu verstehen, warum, betrachten Sie den Fall, dass einige Daten
ursprünglich mit einer Dezimalstelle angegeben wurden (z. B. 3,5 oder 4,2), jetzt aber mit
zwei Dezimalstellen angegeben werden (z. B. 3,48 oder 4,23). Wenn wir annehmen, dass 3,5 in den älteren Daten aus Werten besteht
aus Werten besteht, die in den neueren Daten gleichmäßig^1 in [3,45, 3,55] verteilt wären, wäre der
statisch unterstellte Wert 3,5, was genau dem Wert entspricht, der in den älteren Daten gespeichert
älteren Daten gespeichert ist.

Für kategorische Werte - zum Beispiel, wenn die älteren Daten den Ort als Staats- oder
Provinzcode gespeichert, während die neueren Daten den Code des Bezirks oder Distrikts enthalten, verwenden Sie die
die Häufigkeitsverteilung der Bezirke innerhalb der Staaten, wie in der Hauptlösung beschrieben, um
eine statische Imputation durchzuführen.

Entwurfsmuster 24: Gefensterte Inferenz
Das Entwurfsmuster "Windowed Inference" behandelt Modelle, die eine fortlaufende
Sequenz von Instanzen benötigen, um die Inferenz durchzuführen. Dieses Muster funktioniert durch Externalisierung
des Modellzustands und den Aufruf des Modells aus einer Stream-Analytics-Pipeline. Dieses Pat-
Diese Vorgehensweise ist auch nützlich, wenn ein Modell für maschinelles Lernen Merkmale benötigt, die
die aus Aggregaten über Zeitfenster berechnet werden müssen. Durch die Externalisierung des Zustands in eine Stream
Stream-Pipeline ausgelagert wird, stellt das Windowed Inference Design Pattern sicher, dass
dynamischen, zeitabhängigen Weise berechneten Merkmale korrekt zwischen Training und
dienen. Auf diese Weise wird eine Verzerrung zwischen Training und Serving im Falle von zeitlichen Aggregaten vermieden.
gate-Merkmalen.

Problem
Werfen Sie einen Blick auf die Verspätungen bei der Ankunft am Flughafen Dallas Fort Worth (DFW), die für ein paar Tage im Mai 2010
Tagen im Mai 2010 in Abbildung 6-5 dargestellt sind (das vollständige Notebook befindet sich auf GitHub).

Entwurfsmuster 24: Gefensterte Inferenz | 273
Abbildung 6-5. Ankunftsverspätungen am Flughafen Dallas Fort Worth (DFW) am 10. und 11. Mai 2010.
Abnormale Ankunftsverspätungen sind mit einem Punkt markiert.

Die Ankunftsverspätungen weisen eine beträchtliche Variabilität auf, aber es ist dennoch möglich
ungewöhnlich große Ankunftsverspätungen (gekennzeichnet durch einen Punkt). Beachten Sie, dass die Definition von "ungewöhnlich"
je nach Kontext unterschiedlich ist. Früh am Morgen (linke Ecke der Grafik) sind die meisten Flüge pünktlich.
pünktlich, so dass selbst die kleine Spitze anomal ist. In der Mitte des Tages (nach 12 Uhr mittags am
10. Mai) nimmt die Variabilität zu, und Verspätungen von 25 Minuten sind keine Seltenheit, aber eine 75-
Minute Verspätung ist immer noch ungewöhnlich.

Ob eine bestimmte Verspätung anomal ist oder nicht, hängt zum Beispiel vom zeitlichen Kontext ab,
von den in den letzten zwei Stunden beobachteten Ankunftsverspätungen. Um festzustellen, dass eine Verspätung
anomal ist, müssen wir den Datenrahmen zunächst nach der Zeit sortieren (wie im Diagramm
in Abbildung 6-5 und unten in Pandas gezeigt):

df = df.sort_values(by='geplante_Zeit').set_index('geplante_Zeit')
Dann müssen wir eine Funktion zur Erkennung von Anomalien auf gleitende Fenster von zwei Stunden anwenden
Stunden anwenden:

df['delay'].rolling('2h').apply(is_anomaly, raw=False)
Die Funktion zur Erkennung von Anomalien, is_anomaly, kann recht ausgefeilt sein, aber lassen Sie uns
nehmen wir den einfachen Fall, dass Extremwerte verworfen werden und ein Datenwert als Anomalie bezeichnet wird, wenn er
mehr als vier Standardabweichungen vom Mittelwert in einem Zwei-Stunden-Fenster aufweist:

def is_anomaly(d):
outcome = d[-1] # the last item
# Minimal- & Maximalwert & aktuelles (letztes) Element verwerfen
xarr = d.drop(index=[d.idxmin(), d.idxmax(), d.index[-1]])
Vorhersage = xarr.mean()
akzeptable_Abweichung = 4 * xarr.std()
return np.abs(Ergebnis - Vorhersage) > akzeptable_Abweichung
Dies funktioniert bei historischen (Trainings-)Daten, da der gesamte Datenrahmen zur Verfügung steht. Unter
natürlich nicht der gesamte Datenrahmen zur Verfügung, wenn wir die Inferenz für unser Produktionsmodell
Datenrahmen. In der Produktion werden wir die Informationen über die Ankunft der Flüge einzeln erhalten,
wenn jeder Flug ankommt. Wir werden also nur einen einzigen Verspätungswert mit einem Zeitstempel haben:

274 | Kapitel 6: Reproduzierbarkeit - Entwurfsmuster

2 Mit anderen Worten, wir berechnen den Durchschnitt.
2010-02-03 08:45:00,19.0
Ist die Verspätung des obigen Flugs (um 08:45 Uhr am 3. Februar) von 19 Minuten ungewöhnlich
oder nicht? Um eine ML-Inferenz für einen Flug durchzuführen, benötigen wir in der Regel nur die Merkmale
dieses Fluges. In diesem Fall benötigt das Modell jedoch Informationen über alle Flüge
zum Flughafen DFW zwischen 06:45 und 08:45:

2010-02-03 06:45:00,?
2010-02-03 06:?:00,?
2010-02-03 08:45:00,19.0
Es ist nicht möglich, einen Flug nach dem anderen abzuleiten. Wir müssen dem Modell irgendwie
die Modellinformationen über alle vorangegangenen Flüge bereitstellen.

Wie führen wir die Inferenz durch, wenn das Modell nicht nur eine Instanz, sondern eine
Folge von Instanzen erfordert?

Lösung
Die Lösung besteht darin, eine zustandsbehaftete Stromverarbeitung durchzuführen, d. h. eine Stromverarbeitung
die den Zustand des Modells über die Zeit verfolgt:

Ein gleitendes Fenster wird auf die Ankunftsdaten von Flügen angewendet. Das gleitende Fenster wird über
2 Stunden, aber das Fenster kann auch häufiger geschlossen werden, z. B. alle 10 Minuten. Unter
einem solchen Fall werden die Gesamtwerte alle 10 Minuten über die letzten 2 Stunden berechnet.
den vorangegangenen 2 Stunden berechnet.
Der interne Modellzustand (dies könnte die Liste der Flüge sein) wird jedes Mal mit Fluginformationen
Fluginformationen aktualisiert, wenn ein neuer Flug eintrifft, wodurch eine 2-stündige historische
Aufzeichnung von Flugdaten.
Jedes Mal, wenn das Fenster geschlossen wird (in unserem Beispiel alle 10 Minuten), wird ein Zeitserien
ML-Modell auf die 2-Stunden-Liste der Flüge trainiert. Dieses Modell wird dann verwendet, um
Vorhersage zukünftiger Flugverspätungen und der Vertrauensbereiche solcher Vorhersagen verwendet.
Die Parameter des Zeitreihenmodells werden in eine Zustandsvariable externalisiert. Wir könnten
ein Zeitreihenmodell wie den autoregressiven integrierten gleitenden Durchschnitt
(ARIMA) oder das Langzeitgedächtnis (LSTM) verwenden, in diesem Fall wären die Modellparameter
Parameter die Koeffizienten des ARIMA-Modells oder die Gewichte des LSTM-Modells sein.
Um den Code verständlich zu halten, werden wir ein Regressionsmodell nullter Ordnung verwenden,^2
und daher sind unsere Modellparameter die durchschnittliche Flugverspätung und die Varianz der
der Flugverspätungen über das zweistündige Fenster.
Entwurfsmuster 24: Gefensterte Inferenz | 275
Wenn ein Flug ankommt, kann seine Ankunftsverspätung anhand des externalisierten Modellzustands als anomal oder nicht klassifiziert werden.
dem externalisierten Modellzustand klassifiziert werden - es ist nicht notwendig, die vollständige Liste der Flüge über
der letzten 2 Stunden zu haben.
Wir können Apache Beam für Streaming-Pipelines verwenden, weil dann derselbe Code
sowohl mit den historischen Daten als auch mit neu ankommenden Daten arbeiten. In Apache Beam wird das
Schiebefenster wie folgt eingerichtet (der vollständige Code ist auf GitHub):

windowed = (Daten
| 'Fenster' >> beam.WindowInto(
beam.window.SlidingWindows(2 * 60 * 60, 10*60))
Das Modell wird aktualisiert, indem alle Flugdaten, die in den letzten zwei Stunden gesammelt wurden, kombiniert
Stunden gesammelt wurden und an eine Funktion namens ModelFn übergeben werden:

model_state = (windowed
| 'model' >> beam.transforms.CombineGlobally(ModelFn()))
ModelFn aktualisiert den internen Modellstatus mit Fluginformationen. Hier besteht der interne
Modellstatus aus einem Pandas-Datenframe bestehen, der mit den Flügen im
Fenster:

class ModelFn (beam.CombineFn):
def create_accumulator(self):
return pd.DataFrame()
def add_input(self, df, window):
return df.append(window, ignore_index=True)
Jedes Mal, wenn das Fenster geschlossen wird, wird die Ausgabe extrahiert. Die Ausgabe hier (wir nennen sie
als externalisierter Modellzustand) besteht aus den Modellparametern:

def extract_output(self, df):
if len(df) < 1:
return {}
orig = df['delay'].values
xarr = np.delete(orig, [np.argmin(orig), np.argmax(orig)])
return {
'Vorhersage': np.mean(xarr),
'akzeptable_Abweichung': 4 * np.std(xarr)
}
276 | Kapitel 6: Reproduzierbarkeit - Entwurfsmuster

Der externalisierte Modellstatus wird alle 10 Minuten auf der Grundlage eines 2-Stunden-Fensters aktualisiert
Fenster:

Vorhersage der Fensterschlusszeit akzeptable_Abweichung
2010-05-10T06:35:00 -2.8421052631578947 10.48412597725367
2010-05-10T06:45:00 -2.6818181818181817 12.083729926046008
2010-05-10T06:55:00 -2.9615384615384617 11.765962341537781
Der oben gezeigte Code zur Extraktion der Modellparameter ähnelt dem des pan-
den Fall, aber er wird innerhalb einer Beam-Pipeline ausgeführt. Dadurch kann der Code in Streaming arbeiten
Streaming arbeiten, aber der Modellstatus ist nur im Kontext des gleitenden Prozesses verfügbar.
dow. Um die Inferenz für jeden ankommenden Flug durchführen zu können, müssen wir den
externalisieren (ähnlich wie wir die Modellgewichte in eine Datei im State
less Serving Function-Muster in eine Datei exportieren, um sie vom Kontext des Trainingsprogramms zu entkoppeln
wo diese Gewichte berechnet werden):

model_external = beam.pvalue.AsSingleton(model_state)
Dieser externalisierte Zustand kann verwendet werden, um festzustellen, ob es sich bei einem bestimmten Flug um eine
Anomalie ist:

def is_anomaly(Flug, model_external_state):
result = flight.copy()
error = Flug['Verspätung'] - modell_externer_Zustand['Vorhersage']
tolerance = model_external_state['acceptable_deviation']
result['is_anomaly'] = np.abs(error) > tolerance
Ergebnis zurückgeben
Die Funktion is_anomaly wird dann auf jedes Element im letzten Bereich des Schiebefensters angewendet
Fensters angewendet:

anomalies = (windowed
| 'latest_slice' >> beam.FlatMap(is_latest_slice)
| 'find_anomaly' >> beam.Map(is_anomaly, model_external))
Kompromisse und Alternativen
Die oben vorgeschlagene Lösung ist im Falle von Datenströmen mit hohem
Datenströmen mit hohem Durchsatz, kann aber weiter verbessert werden, wenn die ML-Modellparameter
online aktualisiert werden können. Dieses Muster ist auch auf zustandsabhängige ML-Modelle anwendbar, wie z. B.
rekurrente neuronale Netze und wenn ein zustandsloses Modell zustandsabhängige Eingangsmerkmale erfordert.

Reduzierung des Rechenaufwands

Im Abschnitt Problem haben wir den folgenden Pandas-Code verwendet:

dfw['delay'].rolling('2h').apply(is_anomaly, raw=False);
Im Abschnitt Lösung lautete der Beam-Code hingegen wie folgt:

Entwurfsmuster 24: Gefensterte Inferenz | 277
windowed = (Daten
| 'window' >> beam.WindowInto(
beam.window.SlidingWindows(2 * 60 * 60, 10*60))
model_state = (windowed
| 'model' >> beam.transforms.CombineGlobally(ModelFn()))
Es gibt bedeutsame Unterschiede zwischen dem rollenden Fenster in Pandas und dem gleitenden
gleitenden Fenster in Apache Beam gibt es bedeutsame Unterschiede, weil die Funktion is_anomaly so oft aufgerufen wird
und wie oft die Modellparameter (Mittelwert und Standardabweichung) abgefragt werden müssen.
werden müssen. Diese werden im Folgenden diskutiert.

Pro Element versus über ein Zeitintervall. Im Pandas-Code wird die Funktion is_anomaly
für jede Instanz im Datensatz aufgerufen. Der Code zur Erkennung von Anomalien berechnet die
die Modellparameter und wendet sie sofort auf das letzte Element im Fenster an. Unter
der Beam-Pipeline wird der Modellstatus ebenfalls in jedem gleitenden Fenster erstellt, aber das
gleitende Fenster basiert in diesem Fall auf der Zeit. Daher werden die Modellparameter
nur einmal alle 10 Minuten berechnet.

Die Erkennung von Anomalien selbst wird für jede Instanz durchgeführt:

anomalies = (windowed
| 'latest_slice' >> beam.FlatMap(is_latest_slice)
| 'find_anomaly' >> beam.Map(is_anomaly, model_external))
Beachten Sie, dass dies eine sorgfältige Trennung von rechenintensivem Training und
kostengünstigen Inferenz trennt. Der rechenintensive Teil wird nur einmal alle
alle 10 Minuten durchgeführt, wobei jede Instanz als Anomalie klassifiziert werden
Anomalie oder nicht.

Datenströme mit hohem Durchsatz. Die Datenmengen nehmen ständig zu, und ein Großteil dieser
Anstieg des Datenvolumens ist auf Echtzeitdaten zurückzuführen. Daher muss dieses Muster
auf Datenströme mit hohem Durchsatz angewandt werden - Ströme, bei denen die Anzahl der Elemente
über Tausende von Elementen pro Sekunde betragen kann. Denken Sie zum Beispiel an Klick-
Ströme von Websites oder Ströme von Maschinenaktivitäten von Computern, tragbaren Geräten oder Autos.
geräten oder Autos.

Die vorgeschlagene Lösung mit einer Streaming-Pipeline hat den Vorteil, dass sie das
dass das Modell nicht bei jeder Instanz neu trainiert werden muss, was der Pandas-Code in der Prob-
lem-Anweisung tut. Allerdings gibt die vorgeschlagene Lösung diese Vorteile wieder zurück, indem sie
einen speicherinternen Datenrahmen mit allen empfangenen Datensätzen erstellt. Wenn wir 5.000 Elemente pro
Sekunde empfangen, dann enthält der speicherinterne Datenrahmen über 10 Minuten 3 Millionen
Datensätze. Da es 12 gleitende Fenster gibt, die zu jedem Zeitpunkt gepflegt werden müssen
(10-Minuten-Fenster, jedes über 2 Stunden), können die Speicheranforderungen
beträchtlich werden.

Die Speicherung aller empfangenen Datensätze für die Berechnung der Modellparameter am Ende des
des Fensters zu berechnen, kann problematisch werden. Wenn der Datenstrom einen hohen Durchsatz hat, ist es

278 | Kapitel 6: Reproduzierbarkeit - Entwurfsmuster

wird es wichtig, die Modellparameter mit jedem Element aktualisieren zu können.
Dies kann durch Ändern der ModelFn wie folgt geschehen (der vollständige Code ist auf GitHub):

Klasse OnlineModelFn (beam.CombineFn):
...
def add_input(self, inmem_state, input_dict):
(sum, sumsq, count) = inmem_state
input = input_dict['delay']
return (sum + input, sumsq + input*input, count + 1)
def extract_output(self, inmem_state):
(sum, sumsq, count) = inmem_state
...
Mittelwert = Summe / Anzahl
variance = (sumsq / count) - mean*mean
stddev = np.sqrt(variance) if variance > 0 else 0
Rückgabe {
'Vorhersage': mean,
'akzeptable_Abweichung': 4 * stddev
}
...
Der Hauptunterschied besteht darin, dass nur drei Gleitkommazahlen (sum, sum^2, count) im Speicher gehalten werden
Fließkommazahlen (sum, sum^2 , count) im Speicher gehalten werden, die für die Extraktion des Ausgangsmodellzustands erforderlich sind, und nicht der gesamte
Datenrahmen der empfangenen Instanzen. Das Aktualisieren der Modellparameter um jeweils eine Instanz
Instanz zu aktualisieren, wird als Online-Aktualisierung bezeichnet und kann nur durchgeführt werden, wenn das Modell
Training keine Iteration über den gesamten Datensatz erfordert. Daher wird in der obigen
Implementierung die Varianz berechnet, indem eine Summe von x^2 beibehalten wird, so dass wir
damit nach der Berechnung des Mittelwerts kein zweiter Durchgang durch die Daten erforderlich ist.

Streaming-SQL

Wenn unsere Infrastruktur aus einer leistungsstarken SQL-Datenbank besteht, die in der Lage ist
Streaming-Daten verarbeiten kann, ist es möglich, den Windowed Inference
auf alternative Weise zu implementieren, indem ein Aggregationsfenster verwendet wird (der vollständige Code ist auf GitHub zu finden).

Wir ziehen die Flugdaten aus BigQuery:

WITH data AS (
SELECT
PARSE_DATETIME('%Y-%m-%d-%H%M',
CONCAT( CAST (date AS STRING),
'-', FORMAT('%04d', ankunft_fahrplan))
) AS geplante_Ankunftszeit,
ankunft_verzoegerung
FROM `bigquery-samples.airline_ontime_data.flights`
WHERE ankunft_flughafen = 'DFW' AND SUBSTR(date, 0, 7) = '2010-05'
),
Anschließend wird der "model_state" erstellt, indem die Modellparameter über ein Zeitfenster berechnet werden
Zeitfenster von zwei Stunden vor bis zu einer Sekunde vor berechnet:

Entwurfsmuster 24: Gefensterte Inferenz | 279
model_state AS (
SELECT
geplante_ankunft_zeit,
Ankunft_Verzögerung,
AVG (arrival_delay) OVER (time_window) AS prediction,
4*STDDEV(arrival_delay) OVER (time_window) AS acceptable_deviation
FROM daten
WINDOW time_window AS
( ORDER BY UNIX_SECONDS( TIMESTAMP (geplante_Ankunftszeit))
BEREICH ZWISCHEN 7200 VOR UND 1 VOR)
)
Schließlich wenden wir den Algorithmus zur Erkennung von Anomalien auf jede Instanz an:

SELECT
*,
( ABS (Ankunftsverspätung - Vorhersage) > akzeptable_Abweichung) AS is_anomaly
FROM model_state
Das Ergebnis sieht wie in Tabelle 6-1 aus, wobei die Ankunftsverspätung von 54 Minuten als
Anomalie gekennzeichnet ist, da alle vorherigen Flüge zu früh ankamen.

Tabelle 6-1. Das Ergebnis einer BigQuery-Abfrage zur Bestimmung, ob eingehende Flugdaten
eine Anomalie sind

geplante_Ankunftszeit Ankunft_Verspätung Vorhersage akzeptable_Abweichung is_anomaly
2010-05-01T05:45:00 -18.0 -8.25 62.51399843235114 falsch
2010-05-01T06:00:00 -13.0 -10.2 56.878818553131005 false
2010-05-01T06:35:00 -1.0 -10.666 51.0790237442599 false
2010-05-01T06:45:00 -9.0 -9.28576 48.86521793473886 false
2010-05-01T07:00:00 54.0 -9.25 45.24220532707422 true
Im Gegensatz zur Apache Beam-Lösung erlaubt uns die Effizienz von verteiltem SQL
2-Stunden-Zeitfenster zu berechnen, das auf jede Instanz zentriert ist (statt mit einer Auflösung
Auflösung von 10-Minuten-Fenstern). Der Nachteil ist jedoch, dass BigQuery zu einer relativ
relativ hohe Latenzzeiten hat (in der Größenordnung von Sekunden) und daher nicht für Echtzeit
Echtzeit-Kontrollanwendungen verwendet werden.

Sequenzielle Modelle

Das Muster der "Windowed Inference", bei dem ein gleitendes Fenster von früheren Instanzen
Inferenzfunktion ein gleitendes Fenster früherer Instanzen zu übergeben, ist nicht nur bei der Erkennung von Anomalien oder sogar bei Zeitserienmodi
els. Insbesondere ist es in jeder Klasse von Modellen, wie z. B. Sequenzmodellen, nützlich, die
einen historischen Zustand benötigen. Ein Übersetzungsmodell muss zum Beispiel mehrere aufeinanderfolgende
mehrere aufeinanderfolgende Wörter sehen, bevor es die Übersetzung durchführen kann, damit die Übersetzung den
den Kontext des Wortes berücksichtigt. Schließlich ist die Übersetzung der Wörter "links", "Chi-
cago" und "road" variieren zwischen den Sätzen "I left Chicago by road" und "Turn left
auf der Chicago Road".

280 | Kapitel 6: Reproduzierbarkeit - Entwurfsmuster

Aus Leistungsgründen wird das Übersetzungsmodell zustandslos sein und
dass der Benutzer den Kontext bereitstellen muss. Wenn das Modell beispielsweise zustandslos ist, können Instan-
Wenn das Modell zustandslos ist, können beispielsweise Instanzen des Modells als Reaktion auf ein erhöhtes
parallel aufgerufen werden, um schnellere Übersetzungen zu erhalten. So kann die Übersetzung des berühmten
Monologs aus Shakespeares Hamlet ins Deutsche in diesen Schritten erfolgen, wobei die
in der Mitte, wo das fettgedruckte Wort zu übersetzen ist:

Input (9 Wörter, 4 auf jeder Seite) Output
Das unentdeckte Land, aus dessen Gebiet kein Reisender zurückkehrt
das unentdeckte Land, aus dessen Gebiet kein Reisender zurückkehrt, gibt Rätsel auf
Land, aus dessen Gebiet kein Reisender zurückkehrt, rätselt das Kein
aus dessen Gebiet kein Reisender zurückkehrt, rätselt der Wille, Reisender
Der Client benötigt also eine Streaming-Pipeline. Die Pipeline könnte den
englischen Text nehmen, ihn in Token umwandeln, jeweils neun Token mitschicken, die Ausgaben sammeln,
und sie zu deutschen Sätzen und Absätzen konkatenieren.

Die meisten Sequenzmodelle, wie z. B. rekurrente neuronale Netze und LSTMs, erfordern
Streaming-Pipelines für eine leistungsstarke Inferenz.

Zustandsbezogene Merkmale

Das Muster "Windowed Inference" kann nützlich sein, wenn ein Eingangsmerkmal des Modells
einen Zustand erfordert, auch wenn das Modell selbst zustandslos ist. Nehmen wir zum Beispiel an, wir trainieren
Angenommen, wir trainieren ein Modell zur Vorhersage von Ankunftsverspätungen, und eine der Eingaben für das Modell ist die Abfahrtsverspätung.
Abflugverspätung. Wir könnten als Eingabe für das Modell die durchschnittliche Abflugverspätung
Abflugverspätung der Flüge von diesem Flughafen in den letzten zwei Stunden.

Beim Training können wir den Datensatz mit einer SQL-Fensterfunktion erstellen:

WITH data AS (
SELECT
SAFE.PARSE_DATETIME('%Y-%m-%d-%H%M',
CONCAT( CAST (date AS STRING), '-',
FORMAT('%04d', abflug_zeitplan))
) AS geplante_Abflugzeit,
Ankunft_Verspätung,
abflug_verspätung,
abflug_flughafen
FROM `bigquery-samples.airline_ontime_data.flights`
WHERE ankunft_flughafen = 'DFW'
),
SELECT
* EXCEPT (planmäßige_Abflugzeit),
EXTRACT (hour from scheduled_depart_time) AS hour_of_day,
AVG (abflug_verspätung) OVER (abflug_zeit_fenster) AS avg_abflug_verspätung
FROM Daten
Entwurfsmuster 24: Gefensterte Inferenz | 281
WINDOW Abflugzeit_Fenster AS
(PARTITION BY abflug_flughafen ORDER BY
UNIX_SECONDS( TIMESTAMP (geplante_Abflugzeit))
BEREICH ZWISCHEN 7200 VOR UND 1 VOR)
Der Trainingsdatensatz enthält nun die durchschnittliche Verspätung als ein weiteres Merkmal:

Zeile Ankunft_Verspätung Abfahrt_Verspätung Abfahrt_Flughafen Stunde_des_Tages avg_depart_delay
1 -3.0 -7.0 LFT 8 -4.0
2 56,0 50,0 LFT 8 41,0
3 -14,0 -9,0 LFT 8 5,0
4 -3,0 0,0 LFT 8 -2,0
Während der Inferenz benötigen wir jedoch eine Streaming-Pipeline zur Berechnung dieser durchschnittlichen
Abfahrtsverzögerung zu berechnen, damit wir sie dem Modell zur Verfügung stellen können. Um die Verzerrung durch das Training zu begrenzen,
ist es vorzuziehen, dieselbe SQL in einer Tumbling-Window-Funktion in einer Streaming-Pipeline zu verwenden
Pipeline zu verwenden, anstatt zu versuchen, die SQL in Scala, Python oder Java zu übersetzen.

Stapelung von Vorhersageanforderungen

Ein weiteres Szenario, in dem wir Windowed Inference verwenden möchten, auch wenn das Modell
zustandslos ist, wenn das Modell in der Cloud bereitgestellt wird, der Client aber
in ein Gerät oder vor Ort eingebettet ist. In solchen Fällen ist die Netzwerklatenz beim Senden von Inferenz
Anfragen einzeln an ein in der Cloud bereitgestelltes Modell zu senden, überwältigend sein. In dieser Situation
Situation kann "Entwurfsmuster 19: Zweiphasige Vorhersagen" auf Seite 232 aus Kapitel 5
verwendet werden, wobei die erste Phase eine Pipeline verwendet, um eine Anzahl von Anfragen zu sammeln, und die zweite Phase diese an den Dienst sendet.
Phase diese in einem Stapel an den Dienst sendet.

Dies ist nur für latenztolerante Anwendungsfälle geeignet. Wenn wir Input-Instanzen über fünf Minuten sammeln
über fünf Minuten sammeln, muss der Client eine Verzögerung von bis zu fünf Minuten
bei der Rückgabe der Vorhersagen tolerieren.

Entwurfsmuster 25: Workflow-Pipeline
Mit dem Workflow-Pipeline-Entwurfsmuster gehen wir das Problem der Erstellung einer
reproduzierbare End-to-End-Pipeline zu erstellen, indem wir die Schritte in unserem
maschinellen Lernprozesses. Die Containerisierung kann explizit erfolgen oder mit Hilfe eines
Framework, das den Prozess vereinfacht.

Problem
Ein einzelner Datenwissenschaftler kann die Datenvorverarbeitung, das Training und die
Modellbereitstellung von einem Ende zum anderen (wie in Abbildung 6-6 dargestellt) in einem einzigen
Skript oder Notizbuch ausführen. Da jedoch jeder Schritt in einem ML-Prozess komplexer wird,

282 | Kapitel 6: Reproduzierbarkeit - Entwurfsmuster

und immer mehr Personen innerhalb einer Organisation zu dieser Codebasis beitragen wollen, ist die Ausführung
Die Ausführung dieser Schritte von einem einzigen Notebook aus ist nicht skalierbar.

Abbildung 6-6. Die Schritte in einem typischen End-to-End-ML-Workflow. Dies soll nicht allumfassend sein
umfassend sein, sondern erfasst die häufigsten Schritte im ML-Entwicklungsprozess.

In der traditionellen Programmierung werden monolithische Anwendungen als solche beschrieben, bei denen die gesamte
die gesamte Logik der Anwendung von einem einzigen Programm verarbeitet wird. Zum Testen einer kleinen Funktion in einer
monolithischen Anwendung zu testen, müssen wir das gesamte Programm ausführen. Das Gleiche gilt für das Deployment oder
Fehlersuche in monolithischen Anwendungen. Die Bereitstellung einer kleinen Fehlerbehebung für einen Teil des
Programmteils erfordert die Bereitstellung der gesamten Anwendung, was schnell
unhandlich werden kann. Wenn die gesamte Codebasis untrennbar miteinander verbunden ist, wird es für einzelne Entwickler schwierig
einzelnen Entwickler schwierig, Fehler zu beheben und unabhängig voneinander an verschiedenen Teilen der
der Anwendung zu arbeiten. In den letzten Jahren wurden monolithische Anwendungen zugunsten einer
Microservices-Architektur ersetzt, bei der einzelne Teile der Geschäftslogik
als isolierte (Mikro-)Codepakete erstellt und bereitgestellt werden. Bei Microservices wird eine große Anwendung
Anwendung in kleinere, besser verwaltbare Teile aufgeteilt, so dass Entwickler Teile einer Anwendung unabhängig voneinander erstellen, debuggen und bereitstellen können,
und Teile einer Anwendung unabhängig voneinander bereitstellen können.

Diese Diskussion zwischen Monolith und Microservice bietet eine gute Analogie für die Skalierung
ML-Workflows, die Zusammenarbeit ermöglichen und sicherstellen, dass ML-Schritte reproduzierbar und
über verschiedene Arbeitsabläufe hinweg wiederverwendbar sind. Wenn jemand ein ML-Modell allein erstellt
kann ein "monolithischer" Ansatz schneller zu wiederholen sein. Außerdem funktioniert er oft
weil eine Person aktiv an der Entwicklung und Pflege jedes Teils beteiligt ist:
Datenerfassung und Vorverarbeitung, Modellentwicklung, Training und Bereitstellung.
Bei der Skalierung dieses Arbeitsablaufs können jedoch verschiedene Personen oder Gruppen in einer Organisation
für verschiedene Schritte verantwortlich sein. Um den ML-Workflow zu skalieren, benötigen wir eine Möglichkeit für
das Team, das das Modell entwickelt, Versuche unabhängig von der Datenvorverarbeitung durchführen
Schritt. Außerdem müssen wir die Leistung für jeden Schritt der Pipeline verfolgen und die
die von jedem Teil des Prozesses erzeugten Ausgabedateien verwalten.

Wenn die anfängliche Entwicklung für jeden Schritt abgeschlossen ist, werden wir außerdem
oder ereignisgesteuerte Pipelineläufe erstellen, die als Reaktion auf Änderungen in der Umgebung aufgerufen werden
die als Reaktion auf Änderungen in Ihrer Umgebung aufgerufen werden, z. B. wenn neue Trainingsdaten zu einem
Bucket. In solchen Fällen muss die Lösung es uns ermöglichen, den gesamten
den gesamten Workflow in einem Aufruf auszuführen und gleichzeitig die Ausgabe zu verfolgen und
Fehler aus einzelnen Schritten zu verfolgen.

Entwurfsmuster 25: Workflow-Pipeline | 283
Lösung
Um die Probleme zu bewältigen, die mit der Skalierung von maschinellen Lernprozessen einhergehen, können wir
können wir jeden Schritt in unserem ML-Workflow zu einem separaten, containerisierten Dienst machen. Container
garantieren, dass wir denselben Code in verschiedenen Umgebungen ausführen können und dass
konsistentes Verhalten zwischen den Läufen. Diese einzelnen containerisierten Schritte
werden dann zu einer Pipeline verkettet, die mit einem REST-API-Aufruf ausgeführt werden
API-Aufruf ausgeführt werden kann. Da die Pipelineschritte in Containern ausgeführt werden, können wir sie auf einem Entwicklungs
Entwicklungslaptop, mit lokaler Infrastruktur oder mit einem gehosteten Cloud-Dienst ausgeführt werden. Dieser Pipeline
Workflow ermöglicht es Teammitgliedern, unabhängig voneinander Pipelineschritte zu entwickeln. Container
bieten auch eine reproduzierbare Möglichkeit, eine gesamte Pipeline von Ende zu Ende auszuführen, da sie die Konsistenz
Konsistenz zwischen den Versionen der Bibliotheksabhängigkeiten und den Laufzeitumgebungen gewährleisten.
Da die Containerisierung von Pipelineschritten außerdem eine Trennung von Kon- trollen ermöglicht, können einzelne
Da die Containerisierung von Pipelineschritten eine Trennung von Kon- ten zulässt, können einzelne Schritte unterschiedliche Laufzeiten und Sprachversionen verwenden.

Es gibt viele Tools zur Erstellung von Pipelines, die sowohl vor Ort als auch in der Cloud
verfügbar, darunter Cloud AI Platform Pipelines, TensorFlow Extended (TFX),
Kubeflow Pipelines (KFP), MLflow und Apache Airflow. Zur Demonstration des Work-
flow Pipeline Design Pattern zu demonstrieren, werden wir unsere Pipeline mit TFX definieren und sie auf
Cloud AI Platform Pipelines, einem gehosteten Dienst zur Ausführung von ML-Pipelines in der Google
Cloud unter Verwendung von Google Kubernetes Engine (GKE) als zugrundeliegende Container-Infra-
struktur.

Schritte in TFX-Pipelines werden als Komponenten bezeichnet, und es gibt sowohl vorgefertigte als auch anpassbare Komponenten.
Komponenten sind verfügbar. Normalerweise ist die erste Komponente in einer TFX-Pipeline eine
die Daten aus einer externen Quelle aufnimmt. Dies wird als ExampleGen-Komponente bezeichnet.
Beispielkomponente bezeichnet, wobei sich Beispiel auf die Terminologie des maschinellen Lernens für eine gelabelte
Instanz, die für das Training verwendet wird. Mit ExampleGen-Komponenten können Sie Daten aus folgenden Quellen beziehen
CSV-Dateien, TFRecords, BigQuery oder einer benutzerdefinierten Quelle. Die BigQueryExampleGen-Kom- ponente
Die Komponente BigQueryExampleGen ermöglicht es uns zum Beispiel, in BigQuery gespeicherte Daten mit unserer Pipeline zu verbinden, indem wir eine
eine Abfrage angeben, die die Daten abruft. Anschließend werden diese Daten als TFRecords in einem
GCS-Bucket, so dass sie von der nächsten Komponente verwendet werden können. Diese Komponente wird
anpassen, indem wir ihr eine Abfrage übergeben. Diese ExampleGen-Komponenten befassen sich mit der Datensammlungs
lektionsphase eines ML-Arbeitsablaufs, wie in Abbildung 6-6 skizziert.

Der nächste Schritt in diesem Arbeitsablauf ist die Datenvalidierung. Sobald wir die Daten aufgenommen haben, können wir
können wir sie an andere Komponenten zur Transformation oder Analyse weitergeben, bevor wir ein Modell trainieren.
Die StatisticsGen-Komponente nimmt die von einem ExampleGen-Schritt aufgenommenen Daten und
generiert zusammenfassende Statistiken über die bereitgestellten Daten. Die Komponente SchemaGen gibt das infer
rotes Schema aus den eingelesenen Daten. Unter Verwendung der Ausgabe von SchemaGen führt der
ExampleValidator die Erkennung von Anomalien in unserem Datensatz und prüft auf Anzeichen

284 | Kapitel 6: Reproduzierbarkeit - Entwurfsmuster

3 Weitere Informationen zur Datenvalidierung finden Sie unter "Design Pattern 30: Fairness Lens" auf Seite 343 in Kapitel 7, Verantwortungsvolle KI.
4 SSHS steht für Saffir-Simpson Hurricane Scale (Saffir-Simpson-Hurrikan-Skala) und ist eine Skala von 1 bis 5, die zur Messung der Stärke und
Schwere eines Hurrikans. Beachten Sie, dass das ML-Modell die Schwere des Hurrikans nicht zu einem späteren Zeitpunkt vorhersagt.
Stattdessen lernt es einfach die in der Saffir-Simpson-Skala verwendeten Schwellenwerte für die Windgeschwindigkeit.
5 Während die Bereitstellung der letzte Schritt in unserer Beispielpipeline ist, umfassen Produktionspipelines oft mehr Schritte,
wie die Speicherung des Modells in einem gemeinsamen Repository oder die Ausführung einer separaten Serving-Pipeline, die CI/CD und
Testen.
der Datenabweichung oder einer möglichen Verzerrung der Trainingsdaten.^3 Die Komponente Transform
nimmt auch die Ausgabe von SchemaGen entgegen und führt das Feature Engineering durch, um die
um unsere Dateneingabe in das richtige Format für unser Modell zu bringen. Dies könnte die Konvertierung
Konvertierung von Freiform-Texteingaben in Einbettungen, Normalisierung numerischer Eingaben und mehr.

Sobald unsere Daten bereit sind, in ein Modell eingespeist zu werden, können wir sie an die Trainer-Komponente übergeben.
Komponente übergeben. Wenn wir unsere Trainer-Komponente einrichten, verweisen wir auf eine Funktion, die unseren Modellcode definiert.
unseren Modellcode definiert, und wir können angeben, wo wir das Modell trainieren möchten. Hier werden wir
zeigen wir, wie man Cloud AI Platform Training von dieser Komponente aus nutzt. Schließlich übernimmt die
Pusher-Komponente für die Bereitstellung des Modells. Es gibt noch viele andere vorgefertigte Kom- ponenten
Komponenten, die von TFX bereitgestellt werden - wir haben hier nur ein paar aufgeführt, die wir in unserer
Pipeline verwenden.

In diesem Beispiel verwenden wir den NOAA-Hurrikandatensatz in BigQuery, um ein Modell zu erstellen
das den SSHS-Code^4 für einen Hurrikan ableitet. Wir halten die Funktionen, Komponenten und den
Modellcode relativ kurz halten, um sich auf die Pipeline-Werkzeuge zu konzentrieren. Die Schritte unserer
Pipeline werden im Folgenden skizziert und folgen grob dem in Abbildung 6-6 skizzierten Arbeitsablauf:

Datenerfassung: Führen Sie eine Abfrage aus, um die Hurrikan-Daten von BigQuery zu erhalten.
Datenvalidierung: Verwenden Sie die Komponente ExampleValidator, um Anomalien zu erkennen und
auf Datenabweichungen zu prüfen.
Datenanalyse und Vorverarbeitung: Erstellen Sie einige Statistiken über die Daten und definieren Sie
das Schema.
Modelltraining: Trainieren Sie ein tf.keras-Modell auf der AI Platform.
Modellbereitstellung: Bereitstellung des trainierten Modells auf der KI-Plattform Prediction.^5
Wenn unsere Pipeline vollständig ist, können wir den gesamten oben beschriebenen Prozess
mit einem einzigen API-Aufruf aufrufen. Lassen Sie uns zunächst das Gerüst einer typischen TFX
Pipeline und den Prozess zu deren Ausführung auf der AI Platform.

Aufbau der TFX-Pipeline

Wir werden die tfx-Befehlszeilentools verwenden, um unsere Pipeline zu erstellen und aufzurufen. Neue Aufrufe
einer Pipeline werden als Läufe bezeichnet, die sich von den Aktualisierungen unterscheiden, die wir an der

Entwurfsmuster 25: Workflow-Pipeline | 285
Pipeline selbst, wie das Hinzufügen einer neuen Komponente. Beides können wir mit der TFX-CLI machen. Wir
können das Gerüst für unsere Pipeline in einem einzigen Python-Skript definieren, das aus zwei
Hauptteile hat:

Eine Instanz von tfx.orchestration.pipeline, in der wir unsere Pipeline und die
Komponenten, die sie enthält.
Eine Instanz von kubeflow_dag_runner aus der tfx-Bibliothek. Damit erstellen wir
und führen unsere Pipeline aus. Neben dem Kubeflow-Runner gibt es auch eine API für
TFX-Pipelines mit Apache Beam, die wir verwenden können, um unsere Pipeline
Linie lokal auszuführen.
Unsere Pipeline (siehe vollständiger Code in GitHub) besteht aus den fünf oben definierten Schritten oder Komponenten
oben definiert sind, und wir können unsere Pipeline mit dem Folgenden definieren:

pipeline.Pipeline(
pipeline_name='huricane_prediction',
pipeline_root='path/to/pipeline/code',
components=[
bigquery_gen, statistics_gen, schema_gen, train, model_pusher
]
)
Um die von TFX bereitgestellte Komponente BigQueryExampleGen zu verwenden, geben wir die Abfrage
die unsere Daten abrufen wird. Wir können diese Komponente in einer einzigen Codezeile definieren, wobei
query unsere BigQuery-SQL-Abfrage als String ist:

bigquery_gen = BigQueryExampleGen(query=query)
Ein weiterer Vorteil der Verwendung von Pipelines ist die Bereitstellung von Werkzeugen zur Verfolgung der
Eingaben, Ausgabeartefakte und Protokolle für jede Komponente. Die Ausgabe der statis
tics_gen-Komponente ist zum Beispiel eine Zusammenfassung unseres Datensatzes, die in Abbildung 6-7 zu sehen ist.
Abbildung 6-7. statistics_gen ist eine vorgefertigte Komponente in TFX, die TF
Data Validation verwendet, um zusammenfassende Statistiken über unseren Datensatz zu erstellen.

Abbildung 6-7. Das Ausgabe-Artefakt der Komponente statistics_gen in einer TFX-Pipeline.

286 | Kapitel 6: Reproduzierbarkeit - Entwurfsmuster

6 Beachten Sie, dass Sie für die Ausführung von TFX-Pipelines auf der AI Platform Ihren Code derzeit auf GCR hosten müssen und
keinen anderen Container-Registry-Dienst wie DockerHub verwenden können.
Ausführen der Pipeline auf der Cloud AI Platform

Wir können die TFX-Pipeline auf der Cloud AI Platform Pipelines ausführen, die die
Details der Infrastruktur für uns verwaltet. Um eine Pipeline auf AI Platform bereitzustellen, müssen wir
verpacken wir unseren Pipeline-Code als Docker-Container und hosten ihn auf Google Container
Registry (GCR) gehostet.^6 Sobald unser containerisierter Pipeline-Code auf GCR,
erstellen wir die Pipeline mit der TFX-CLI:

tfx-Pipeline erstellen \
--pipeline-path=kubeflow_dag_runner.py \
--endpoint='ihre-pipelines-dashboard-url' \
--build-target-image='gcr.io/ihre-pipeline-container-url'
Im obigen Befehl entspricht der Endpunkt der URL unseres AI Platform Pipe-
Linien Dashboard. Wenn der Vorgang abgeschlossen ist, sehen wir die soeben erstellte Pipeline in unserem
Pipelines-Dashboard. Der Befehl create erstellt eine Pipeline-Ressource, die wir
durch Erstellen eines Laufs aufrufen können:

tfx run create --pipeline-name='your-pipeline-name' --endpoint='pipeline-url'
Nach der Ausführung dieses Befehls wird ein Diagramm angezeigt, das in Echtzeit aktualisiert wird, wenn
unsere Pipeline jeden Schritt durchläuft. Über das Pipelines-Dashboard können wir
einzelne Schritte untersuchen, um die von ihnen erzeugten Artefakte, Metadaten und mehr zu sehen. Wir
In Abbildung 6-8 sehen Sie ein Beispiel für die Ausgabe eines einzelnen Schritts.

Wir könnten unser Modell direkt in unserer containerisierten Pipeline auf GKE trainieren, aber TFX
bietet ein Dienstprogramm für die Verwendung von Cloud AI Platform Training als Teil unseres Prozesses. TFX
verfügt auch über eine Erweiterung für die Bereitstellung unseres trainierten Modells auf der AI Platform Prediction.
Wir werden diese beiden Integrationen in unserer Pipeline verwenden. Mit AI Platform Training können wir
die Vorteile spezialisierter Hardware für das Training unserer Modelle nutzen, wie GPUs oder
TPUs, auf kosteneffiziente Weise nutzen. Es bietet auch die Möglichkeit, verteiltes Training zu verwenden,
was die Trainingszeit beschleunigen und die Trainingskosten minimieren kann. Wir können die einzelnen
Trainingsaufträge und ihre Ergebnisse in der AI Platform-Konsole verfolgen.

Entwurfsmuster 25: Workflow-Pipeline | 287
Abbildung 6-8. Ausgabe der Komponente schema_gen für eine ML-Pipeline. Die obere Menü
Leiste zeigt die für jeden einzelnen Pipelineschritt verfügbaren Daten an.

Ein Vorteil des Aufbaus einer Pipeline mit TFX oder Kubeflow Pipe-
Linien ist, dass wir nicht an Google Cloud gebunden sind. Wir können den
Code, den wir hier mit Googles AI Platform demonstrieren, kann
Pipelines auf Azure ML Pipelines, Amazon SageMaker oder vor Ort
Räumlichkeiten.
Um einen Trainingsschritt in TFX zu implementieren, verwenden wir die Komponente Trainer und übergeben ihr
Informationen über die Trainingsdaten, die als Modelleingabe verwendet werden sollen, zusammen mit unserem Modell-Trainingscode.
Trainingscode. TFX bietet eine Erweiterung für die Ausführung des Trainingsschritts auf der AI Platform
die wir verwenden können, indem wir tfx.extensions.google_cloud_ai_platform.trainer importieren
importieren und Details zu unserer AI Platform Trainingskonfiguration angeben. Dazu gehören unser
Projektname, Region und GCR-Speicherort des Containers mit Trainingscode.

In ähnlicher Weise verfügt TFX auch über eine AI Platform Pusher-Komponente für den Einsatz von trainierten
Modelle auf der AI Platform Prediction. Um die Pusher-Komponente mit AI
Platform zu verwenden, müssen wir Details zum Namen und zur Version unseres Modells sowie eine
Serving-Funktion, die AI Platform das Format der Eingabedaten mitteilt, die sie für unser Modell
unser Modell erwarten soll. Damit haben wir eine vollständige Pipeline, die Daten aufnimmt, sie analysiert, eine
Datenumwandlung durchführt und schließlich das Modell mit AI Platform trainiert und einsetzt.

Warum es funktioniert
Ohne unseren ML-Code als Pipeline laufen zu lassen, wäre es für andere schwierig, unsere Arbeit zuverlässig
unsere Arbeit zu reproduzieren. Sie müssten unseren Vorverarbeitungs-, Modellentwicklungs-, Trainings- und
den Code für die Vorverarbeitung, die Modellentwicklung, das Training und die Bereitstellung nehmen und versuchen, die gleiche Umgebung zu replizieren, in der wir ihn ausgeführt haben.
und dabei die Abhängigkeiten von Bibliotheken, die Authentifizierung und vieles mehr berücksichtigen. Wenn es eine
Logik vorhanden ist, die die Auswahl nachgelagerter Komponenten auf der Grundlage der Ausgabe von
Upstream-Komponenten steuert, muss diese Logik ebenfalls zuverlässig repliziert werden. Die Arbeit.

288 | Kapitel 6: Reproduzierbarkeit - Entwurfsmuster

Flow Pipeline-Designmuster können andere unseren gesamten ML-Workflow ausführen und überwachen
von Ende zu Ende sowohl in lokalen als auch in Cloud-Umgebungen ausführen und überwachen, während sie
die Ausgabe der einzelnen Schritte zu debuggen. Die Containerisierung jedes Schritts der Pipeline
stellt sicher, dass andere sowohl die von uns zur Erstellung verwendete Umgebung
und den gesamten in der Pipeline erfassten Workflow reproduzieren können. Dies ermöglicht uns auch, die Umgebung möglicherweise
Monate später zu reproduzieren, um regulatorische Anforderungen zu erfüllen. Mit TFX und
AI Platform Pipelines bietet das Dashboard auch eine Benutzeroberfläche zur Verfolgung der
Fakten, die bei jeder Pipeline-Ausführung erzeugt werden. Dies wird im Abschnitt "Trade-
Kompromisse und Alternativen" auf Seite 315.

Da jede Pipeline-Komponente in einem eigenen Container untergebracht ist, können verschiedene Teammitglieder
können verschiedene Teammitglieder separate Teile einer Pipeline parallel erstellen und testen. Dies ermöglicht eine
schnellere Entwicklung und minimiert die Risiken, die mit einem eher monolithischen ML
Prozesses verbunden sind, bei dem die Schritte untrennbar miteinander verbunden sind. Die Paketabhängigkeiten
Die Paketabhängigkeiten und der Code, die für die Erstellung der Datenvorverarbeitung erforderlich sind, können sich zum Beispiel
signifikant von denen für die Modellbereitstellung unterscheiden. Durch die Erstellung dieser Schritte als Teil
einer Pipeline kann jeder Teil in einem separaten Container mit eigenen Abhängigkeiten erstellt
Abhängigkeiten erstellt und nach Fertigstellung in eine größere Pipeline integriert werden.

Zusammenfassend lässt sich sagen, dass das Workflow-Pipeline-Muster uns die Vorteile bietet, die ein
gerichteten azyklischen Graphen (DAG), zusammen mit den vorgefertigten Komponenten, die mit
Pipeline-Frameworks wie TFX. Da die Pipeline ein DAG ist, haben wir die Möglichkeit
einzelne Schritte auszuführen oder eine gesamte Pipeline von Ende zu Ende zu durchlaufen. Dies ermöglicht auch
Dies ermöglicht uns auch die Protokollierung und Überwachung für jeden Schritt der Pipeline über verschiedene Läufe hinweg,
und ermöglicht die Nachverfolgung von Artefakten aus jedem Schritt und der Pipeline-Ausführung an einem zentralen Ort.
zentraler Stelle. Vorgefertigte Komponenten bieten eigenständige, gebrauchsfertige Schritte für gängige
Komponenten von ML-Workflows, einschließlich Training, Auswertung und Inferenz. Diese
Komponenten werden als einzelne Container ausgeführt, wo auch immer wir unsere Pipeline ausführen wollen.

Kompromisse und Alternativen
Die wichtigste Alternative zur Verwendung eines Pipeline-Frameworks ist die Ausführung der Schritte unseres ML
Arbeitsablaufs mit einem behelfsmäßigen Ansatz zur Verfolgung der Notizbücher und
die mit jedem Schritt verbunden sind. Natürlich entsteht ein gewisser Aufwand bei der Konvertierung
der verschiedenen Teile unseres ML-Workflows in eine organisierte Pipeline. In diesem Abschnitt,
werden wir uns einige Variationen und Erweiterungen des Workflow-Pipeline-Designmusters ansehen:
die manuelle Erstellung von Containern, die Automatisierung einer Pipeline mit Tools für kontinuierliche Integration und
Integration und kontinuierliche Bereitstellung (CI/CD), Prozesse für den Übergang von einer Entwicklungs
von einer Entwicklungs- zur Produktions-Workflow-Pipeline und alternative Tools zum Aufbau und
Pipelines. Wir werden auch untersuchen, wie Pipelines für die Verfolgung von Metadaten verwendet werden können.

Entwurfsmuster 25: Workflow-Pipeline | 289
Benutzerdefinierte Komponenten erstellen

Anstatt vorgefertigte oder anpassbare TFX-Komponenten zu verwenden, um unsere Pipe zu konstruieren
Komponenten zu verwenden, können wir unsere eigenen Container definieren, die wir als Komponenten verwenden, oder eine Python
Funktion in eine Komponente umwandeln.

Um die von TFX bereitgestellten containerbasierten Komponenten zu nutzen, verwenden wir die Methode create_con
tainer_component-Methode und übergeben ihr die Ein- und Ausgänge für unsere Komponente und
ein Docker-Basis-Image zusammen mit beliebigen Einstiegsbefehlen für den Container. Für
Beispiel: Die folgende container-basierte Komponente ruft das Kommandozeilen-Tool
bq auf, um einen BigQuery-Datensatz herunterzuladen:

Komponente = create_container_component(
name='DownloadBQData',
parameter={
datensatz_name': string,
'Speicherort': string
},
image='google/cloud-sdk:278.0.0',
,
command=[
'bq', 'extract', '--compression=csv', '--field_delimiter=,',
InputValuePlaceholder('dataset_name'),
InputValuePlaceholder('speicher_ort'),
]
)
Es ist am besten, ein Basis-Image zu verwenden, das bereits die meisten der benötigten Abhängigkeiten enthält. Wir verwenden
verwenden das Google Cloud SDK-Image, das uns das bq-Befehlszeilenwerkzeug zur Verfügung stellt.

Es ist auch möglich, eine benutzerdefinierte Python-Funktion in eine TFX-Komponente umzuwandeln, indem man
den @component-Dekorator. Zur Veranschaulichung: Nehmen wir an, wir haben einen Schritt zur Vorbereitung von
Ressourcen, die in unserer Pipeline verwendet werden, wo wir einen Cloud Storage Bucket erstellen. Wir
können diesen benutzerdefinierten Schritt mit folgendem Code definieren:

from google.cloud import storage
client = storage.Client(project="ihr-cloud-projekt")
@Komponente
def CreateBucketComponent(
bucket_name: Parameter[string] = 'your-bucket-name',
) -> OutputDict(bucket_info=string):
client.create_bucket('gs://' + bucket_name)
bucket_info = storage_client.get_bucket('gs://' + bucket_name)
return {
'bucket_info': bucket_info
}
Anschließend können wir diese Komponente zu unserer Pipeline-Definition hinzufügen:

290 | Kapitel 6: Reproduzierbarkeit - Entwurfsmuster

create_bucket = CreateBucketComponent(
bucket_name='mein-bucket')
Integration von CI/CD mit Pipelines

Zusätzlich zum Aufrufen von Pipelines über das Dashboard oder programmatisch über die CLI
oder API aufzurufen, ist es wahrscheinlich, dass wir die Ausführung unserer Pipeline bei der Produktion des
des Modells automatisieren. Zum Beispiel können wir unsere Pipeline immer dann aufrufen, wenn eine bestimmte
Menge an neuen Trainingsdaten verfügbar ist. Oder wir möchten einen Pipelinelauf auslösen
wenn sich der Quellcode für die Pipeline ändert. Das Hinzufügen von CI/CD zu unserer Workflow Pipe-
Linie kann helfen, Auslöserereignisse mit Pipelineläufen zu verbinden.

Es gibt viele verwaltete Dienste für die Einrichtung von Auslösern zur Ausführung einer Pipeline
wenn wir ein Modell anhand neuer Daten neu trainieren wollen. Wir könnten einen verwalteten Planungsservice verwenden
Dienst verwenden, um unsere Pipeline nach einem Zeitplan aufzurufen. Alternativ können wir auch einen serverlosen
Dienst wie Cloud Functions verwenden, um unsere Pipeline aufzurufen, wenn neue Daten
zu einem Speicherort hinzugefügt werden. In unserer Funktion könnten wir Bedingungen festlegen, wie z. B. einen
Schwellenwert für die Menge der neu hinzugefügten Daten, die ein erneutes Training erfordern, für die Erstellung eines
neuen Pipeline-Lauf. Sobald genügend neue Trainingsdaten verfügbar sind, können wir einen
Pipelinelauf zum erneuten Training und zur erneuten Bereitstellung des Modells instanziieren, wie in Abbildung 6-9 gezeigt.

Abbildung 6-9. Ein CI/CD-Workflow mit Cloud-Funktionen zum Aufrufen einer Pipeline, wenn genügend
neue Daten zu einem Speicherort hinzugefügt werden.

Entwurfsmuster 25: Workflow-Pipeline | 291
Wenn wir unsere Pipeline auf der Grundlage von Änderungen am Quellcode auslösen wollen, kann ein verwalteter CI/CD
Dienst wie Cloud Build helfen. Wenn Cloud Build unseren Code ausführt, wird er als eine
Reihe von containerisierten Schritten ausgeführt. Dieser Ansatz passt gut in den Kontext von Pipelines.
Wir können Cloud Build mit GitHub Actions oder GitLab Triggers in dem Repository verbinden
wo sich unser Pipeline-Code befindet. Wenn der Code übertragen wird, erstellt Cloud Build
die mit unserer Pipeline verbundenen Container auf der Grundlage des neuen Codes und erstellt einen
einen Lauf erstellen.

Apache Airflow und Kubeflow-Pipelines

Neben TFX sind auch Apache Airflow und Kubeflow Pipelines Alternativen für die
Implementierung des Workflow-Pipeline-Musters. Wie TFX, behandeln sowohl Airflow als auch KFP
Pipelines als DAG, wobei der Workflow für jeden Schritt in einem Python-Skript definiert wird.
Sie verwenden dieses Skript und bieten APIs für die Planung und Orchestrierung
des Graphen auf der angegebenen Infrastruktur. Sowohl Airflow als auch KFP sind Open Source und
können daher vor Ort oder in der Cloud ausgeführt werden.

Es ist üblich, Airflow für Data Engineering zu verwenden, daher ist es eine Überlegung wert für
Daten-ETL-Aufgaben eines Unternehmens in Betracht. Airflow bietet zwar robuste Werkzeuge für die
für die Ausführung von Aufträgen bietet, wurde es als Allzwecklösung entwickelt und war nicht auf ML
Workloads konzipiert. KFP hingegen wurde speziell für ML entwickelt und
arbeitet auf einer niedrigeren Ebene als TFX und bietet mehr Flexibilität bei der Definition von Pipelineschritten
definiert werden. Während TFX seinen eigenen Orchestrierungsansatz implementiert, können wir mit KFP
wählen, wie wir unsere Pipelines über seine API orchestrieren wollen. Die Beziehung zwischen
TFX, KFP und Kubeflow ist in Abbildung 6-10 zusammengefasst.

292 | Kapitel 6: Reproduzierbarkeit - Entwurfsmuster

Abbildung 6-10. Die Beziehung zwischen TFX, Kubeflow Pipelines, Kubeflow und der darunter liegenden
liegenden Infrastruktur. TFX arbeitet auf der höchsten Ebene auf Kubeflow Pipelines,
mit vorgefertigten Komponenten, die spezifische Ansätze für allgemeine Workflow-Schritte bieten.
Kubeflow Pipelines bietet eine API für die Definition und Orchestrierung einer ML-Pipeline.
mehr Flexibilität bei der Implementierung der einzelnen Schritte bietet. Sowohl TFX als auch KFP laufen auf
Kubeflow, einer Plattform für die Ausführung containerbasierter ML-Workloads auf Kubernetes. Alle
Tools in diesem Diagramm sind quelloffen, so dass die zugrundeliegende Infrastruktur, in der die Pipe-Lines
Leitungen ausgeführt werden, liegt im Ermessen des Benutzers - einige Optionen sind GKE, Anthos, Azure, AWS oder
vor Ort.

Entwicklungs- versus Produktionspipelines

Die Art und Weise, wie eine Pipeline aufgerufen wird, ändert sich oft, wenn wir von der Entwicklung zur Produktion übergehen.
produktion. Wahrscheinlich werden wir unsere Pipeline in einem Notebook erstellen und prototypisieren wollen,
wo wir unsere Pipeline erneut aufrufen können, indem wir eine Notebook-Zelle ausführen, Fehler debuggen und
Code in derselben Umgebung aktualisieren. Sobald wir für die Produktion bereit sind, können wir
können wir unseren Komponentencode und die Pipelinedefinition in ein einziges Skript verschieben. Mit unserer
Pipeline, die in einem Skript definiert ist, können wir Läufe planen und es für andere
anderen in unserem Unternehmen, die Pipeline auf reproduzierbare Weise aufzurufen. Ein verfügbares Tool
für die Produktion von Pipelines ist Kale, das Jupyter-Notebook-Code in ein Skript
Kale wandelt den Code des Jupyter-Notizbuchs mithilfe der Kubeflow-Pipelines-API in ein Skript um.

Eine Produktionspipeline ermöglicht auch die Orchestrierung eines ML-Workflows. Mit Orchestrierung
Mit Orchestrierung meinen wir das Hinzufügen von Logik zu unserer Pipeline, um zu bestimmen, welche Schritte ausgeführt werden,
und was das Ergebnis dieser Schritte sein wird. Wir könnten zum Beispiel entscheiden, dass wir nur

Entwurfsmuster 25: Workflow-Pipeline | 293
Modelle mit einer Genauigkeit von mindestens 95 % in der Produktion einsetzen möchten. Wenn neue
Daten einen Pipelinelauf auslösen und ein aktualisiertes Modell trainieren, können wir eine Logik
Logik hinzufügen, um die Ausgabe unserer Bewertungskomponente zu prüfen und die Bereitstellungskomponente auszuführen.
Komponente auszuführen, wenn die Genauigkeit über unserem Schwellenwert liegt, oder den Pipelinelauf zu beenden, wenn dies nicht der Fall ist. Sowohl Air-
flow und Kubeflow Pipelines, die bereits in diesem Abschnitt besprochen wurden, bieten APIs für
Pipeline-Orchestrierung.

Abstammungsverfolgung in ML-Pipelines

Eine weitere Funktion von Pipelines ist die Verfolgung von Modellmetadaten und
Artefakte, auch bekannt als Lineage Tracking. Jedes Mal, wenn wir eine Pipeline aufrufen, wird eine Reihe von
Artefakte erzeugt. Diese Artefakte können Zusammenfassungen von Datensätzen, exportierte
exportierte Modelle, Ergebnisse der Modellevaluierung, Metadaten über bestimmte Pipeline-Aufrufe und
mehr. Mithilfe der Verfolgung der Abstammung können wir den Verlauf unserer Modellversionen zusammen mit
anderen zugehörigen Modell-Artefakten. In AI Platform Pipelines können wir zum Beispiel mit
Pipelines Dashboard sehen, mit welchen Daten eine Modellversion trainiert wurde, aufgeschlüsselt
aufgeschlüsselt nach Datenschema und Datum. Abbildung 6-11 zeigt das Lineage Explorer Dash-
Board für eine TFX-Pipeline, die auf AI Platform läuft. Damit können wir die Eingabe
und Output-Artefakte, die mit einem bestimmten Modell verbunden sind.

Abbildung 6-11. Der Abschnitt Lineage Explorer des AI Platform Pipelines Dashboards für eine
TFX-Pipeline.

Ein Vorteil der Verwendung von Lineage Tracking zur Verwaltung von Artefakten, die während unseres Pipe-
Line-Run generierten Artefakte zu verwalten, ist, dass es sowohl Cloud-basierte als auch lokale Umgebungen unterstützt. Diese
Flexibilität, wo Modelle trainiert und eingesetzt werden und wo Modell-Metadaten
Daten gespeichert werden. Die Verfolgung der Abstammung ist ebenfalls ein wichtiger Aspekt, um ML-Pipelines
reproduzierbar zu machen, da sie Vergleiche zwischen Metadaten und Artefakten aus
verschiedenen Pipelineläufen ermöglicht.

294 | Kapitel 6: Reproduzierbarkeit - Entwurfsmuster

Entwurfsmuster 26: Merkmalsspeicher
Das Feature Store Design Pattern vereinfacht die Verwaltung und Wiederverwendung von Features
Wiederverwendung von Features über Projekte hinweg, indem es den Feature-Erstellungsprozess von der Entwicklung der Modelle, die diese Features verwenden, entkoppelt.
Modellen entkoppelt wird, die diese Features verwenden.

Problem
Ein gutes Feature-Engineering ist entscheidend für den Erfolg vieler Lösungen für maschinelles Lernen.
tionen. Sie ist jedoch auch einer der zeitaufwändigsten Teile der Modellentwicklung.
Entwicklung. Einige Funktionen erfordern umfangreiches Fachwissen, um korrekt berechnet zu werden, und
Änderungen in der Geschäftsstrategie können sich darauf auswirken, wie ein Merkmal berechnet werden sollte. Um
um sicherzustellen, dass solche Merkmale auf konsistente Weise berechnet werden, ist es besser, wenn diese Merkmale
der Kontrolle von Fachexperten und nicht von ML-Ingenieuren unterliegen. Einige Eingabefelder
können verschiedene Datenrepräsentationen zulassen (siehe Kapitel 2), um sie
um sie für maschinelles Lernen zugänglicher zu machen. Ein ML-Ingenieur oder Datenwissenschaftler wird
typischerweise mit mehreren verschiedenen Transformationen experimentieren, um festzustellen, welche
welche hilfreich sind und welche nicht, bevor er entscheidet, welche Merkmale im endgültigen
Modell verwendet werden. Oftmals stammen die für das ML-Modell verwendeten Daten nicht aus einer einzigen Quelle.
Einige Daten können aus einem Data Warehouse stammen, einige Daten können in einem Speicherbereich als
unstrukturierte Daten, und andere Daten können in Echtzeit durch Streaming gesammelt werden.
Auch die Struktur der Daten kann zwischen den einzelnen Quellen variieren, so dass für jeden
Input eigene Feature-Engineering-Schritte erfordert, bevor er in ein Modell eingespeist werden kann. Diese
Entwicklung erfolgt häufig auf einer VM oder einem persönlichen Rechner, wodurch die Merkmalserstellung an die Software gebunden ist.
an die Softwareumgebung gebunden ist, in der das Modell erstellt wird, und je
je komplexer das Modell wird, desto komplizierter werden diese Datenpipelines.

Ein Ad-hoc-Ansatz, bei dem Funktionen nach Bedarf für ML-Projekte erstellt werden, mag für die
für die einmalige Modellentwicklung und das Training funktionieren, aber wenn Organisationen skalieren, wird diese Methode
wird diese Methode des Feature-Engineerings jedoch unpraktisch und führt zu erheblichen Problemen:

Ad-hoc-Funktionen sind nicht leicht wiederzuverwenden. Features werden immer wieder neu erstellt,
entweder von einzelnen Benutzern oder in Teams, oder sie verlassen nie die Pipelines (oder Notizbücher)
bücher), in denen sie erstellt wurden. Dies ist besonders problematisch für höherwertige
Features, die komplex zu berechnen sind. Der Grund dafür könnte sein, dass sie
teuren Prozessen abgeleitet werden, wie z. B. vortrainierte Benutzer- oder Katalogelementeinbettungen.
dings. In anderen Fällen kann es daran liegen, dass die Merkmale aus vorgelagerten
vorgelagerten Prozessen gewonnen werden, wie z. B. von Fachleuten, der Verfügbarkeit von Verträgen oder Marktsegmenten.
Marktsegmente. Eine weitere Quelle der Komplexität ist, wenn Merkmale auf höherer Ebene, wie z.B.
die Anzahl der Bestellungen eines Kunden im letzten Monat, Aggregationen über die
Zeit. Es ist mühsam und zeitaufwendig, dieselben Merkmale für jedes neue Projekt von Grund auf neu zu erstellen.
neuen Projekt.
Entwurfsmuster 26: Merkmalsspeicher | 295
Die Datenverwaltung wird erschwert, wenn jedes ML-Projekt die Merkmale aus
sensiblen Daten anders berechnet.
Ad-hoc-Features lassen sich nicht so einfach zwischen Teams oder Projekten austauschen. In vielen
Unternehmen werden dieselben Rohdaten von mehreren Teams verwendet, aber die einzelnen Teams
aber die einzelnen Teams können die Merkmale unterschiedlich definieren und es gibt keinen einfachen Zugriff auf die Merkmalsdokumentation.
tion. Dies behindert auch die effektive Zusammenarbeit zwischen den Teams und führt zu isolierter
Siloarbeit und unnötigerweise doppelter Aufwand.
Ad-hoc-Merkmale, die für Training und Bedienung verwendet werden, sind inkonsistent, d. h. Training und Bedienung
schief. Das Training erfolgt in der Regel anhand historischer Daten mit Batch-Features
die offline erstellt werden. Das Serving wird jedoch in der Regel online durchgeführt. Wenn die
Merkmalspipeline für das Training von der in der Produktion verwendeten Pipeline für die
(z. B. unterschiedliche Bibliotheken, Vorverarbeitungscodes oder Sprachen), dann
besteht die Gefahr einer Verzerrung zwischen Training und Produktion.
Die Produktion von Features ist schwierig. Beim Übergang zur Produktion gibt es kein
standardisiertes Framework für die Bereitstellung von Merkmalen für Online-ML-Modelle und für
Batch-Features für die Offline-Modellschulung. Modelle werden offline mit Merkmalen trainiert.
in Batch-Prozessen erstellten Features trainiert, aber wenn diese Features in der Produktion
werden diese Merkmale oft mit dem Schwerpunkt auf niedriger Latenz und weniger auf hohem Durchsatz erstellt.
Der Rahmen für die Erzeugung und Speicherung von Merkmalen ist nicht flexibel genug, um beide
dieser Szenarien.
Kurz gesagt, der Ad-hoc-Ansatz für das Feature Engineering verlangsamt die Modellentwicklung und
führt zu doppeltem Aufwand und Ineffizienz des Arbeitsablaufs. Außerdem ist die Merkmalserstellung
inkonsistent zwischen Training und Inferenz und birgt das Risiko, dass das Training
oder Datenlecks durch versehentliches Einbringen von Label-Informationen in die Modell
Eingabe-Pipeline.

Lösung
Die Lösung besteht darin, einen gemeinsamen Merkmalspeicher zu schaffen, einen zentralen Ort zum Speichern und Dokumentieren von
zu speichern und zu dokumentieren, die für die Erstellung von Modellen für maschinelles Lernen verwendet
über Projekte und Teams hinweg gemeinsam genutzt werden können. Der Merkmalspeicher dient als Schnittstelle
zwischen den Pipelines des Dateningenieurs zur Erstellung von Merkmalen und dem
Arbeitsablauf des Datenwissenschaftlers, der Modelle unter Verwendung dieser Merkmale erstellt (Abbildung 6-12). Auf diese Weise gibt es ein
zentrales Repository, in dem vorberechnete Features gespeichert werden, was die Entwicklungszeit
und hilft bei der Entdeckung von Merkmalen. Dies ermöglicht auch die Anwendung der grundlegenden Software-Engineering-Prinzipien
Software-Engineering-Prinzipien der Versionierung, Dokumentation und Zugriffskontrolle auf die erstellten Features angewendet werden.
die erstellt werden.

Ein typischer Merkmalsspeicher wird mit zwei Hauptmerkmalen entwickelt: Werkzeuge zur schnellen Verarbeitung
große Merkmalsdatensätze schnell zu verarbeiten, und eine Methode zur Speicherung von Merkmalen, die sowohl den Zugriff mit
Latenzzeit (für Inferenzen) und den Zugriff auf große Stapel (für das Modelltraining) unterstützt. Es gibt

296 | Kapitel 6: Reproduzierbarkeit - Entwurfsmuster

auch eine Metadatenschicht, die die Dokumentation und Versionierung verschiedener Merkmalsätze
und eine API, die das Laden und Abrufen von Merkmalsdaten verwaltet.

Abbildung 6-12. Ein Feature Store bildet eine Brücke zwischen Rohdatenquellen und Modell
Training und Bereitstellung.

Der typische Arbeitsablauf eines Daten- oder ML-Ingenieurs besteht darin, Rohdaten (strukturiert oder
Streaming) aus einer Datenquelle zu lesen, verschiedene Transformationen auf die Daten anzuwenden
bevorzugten Verarbeitungsframeworks auf die Daten anzuwenden und die umgewandelten Merkmale im
speichern. Anstatt Feature-Pipelines zur Unterstützung eines einzelnen ML-Modells zu erstellen, entkoppelt das Fea-
ture Store-Muster die Entwicklung von Merkmalen von der Modellentwicklung entkoppelt. Insbesondere
Tools wie Apache Beam, Flink oder Spark werden häufig für den Aufbau eines Feature
Featurespeicher verwendet, da sie Daten sowohl im Batch- als auch im Streaming-Verfahren verarbeiten können. Auch dies
Dies reduziert auch das Auftreten von Trainingsverzerrungen, da die Merkmalsdaten von
die gleichen Pipelines zur Merkmalserstellung befüllt werden.

Nachdem die Merkmale erstellt wurden, werden sie in einem Datenspeicher abgelegt, um sie für das Training
und zu bedienen. Für den Abruf von Merkmalen wird die Geschwindigkeit optimiert. Ein Modell in der Produktion
Online-Anwendung unterstützt, muss möglicherweise Echtzeit-Vorhersagen innerhalb von
Millisekunden erstellen, was eine niedrige Latenzzeit erforderlich macht. Für das Training ist eine höhere Latenz jedoch
kein Problem. Stattdessen liegt der Schwerpunkt auf einem hohen Durchsatz, da historische Merkmale
in großen Stapeln zum Training herangezogen werden. Ein Merkmalsspeicher erfüllt diese beiden Anwendungsfälle
indem er verschiedene Datenspeicher für den Online- und Offline-Zugriff auf Merkmale verwendet. Zum Beispiel kann ein Merkmalspeicher
Merkmalsspeicher beispielsweise Cassandra oder Redis als Datenspeicher für den Online-Zugriff auf Merkmale und
Hive oder BigQuery für den Abruf historischer, großer Batch-Featuresätze.

Ein typischer Merkmalsspeicher enthält schließlich viele verschiedene Merkmalssätze, die
Features, die aus unzähligen Rohdatenquellen erstellt wurden. Die Metadatenschicht wird zur Dokumentation der
Feature-Sets zu dokumentieren und ein Register für die einfache Auffindung von Features und die Zusammenarbeit zwischen Teams bereitzustellen.
ration zwischen den Teams.

Entwurfsmuster 26: Merkmalsspeicher | 297
7 Die Daten sind in der BigQuery-Tabelle verfügbar: bigquery-public-data.new_york_taxi_trips.tlc_yellow_trips_2016.
Feste

Ein Beispiel für dieses Muster in der Praxis ist Feast, ein Open-Source-Fea-
quelloffener Funktionsspeicher für maschinelles Lernen, der von Google Cloud und Gojek entwickelt wurde. Er wurde
Google Cloud-Dienste auf, die Big Query für das Offline-Modelltraining und Redis
für die Online-Bereitstellung mit niedriger Latenz (Abbildung 6-13). Apache Beam wird für die Feature-Erstellung verwendet.
Dies ermöglicht konsistente Datenpipelines sowohl für Batch- als auch für Streaming-Daten.

Abbildung 6-13. High-Level-Architektur des Feast-Featurespeichers. Feast ist aufgebaut auf
Google BigQuery, Redis und Apache Beam.

Um zu sehen, wie dies in der Praxis funktioniert, verwenden wir einen öffentlichen BigQuery-Datensatz mit
Informationen über Taxifahrten in New York City enthält.^7 Jede Zeile der Tabelle enthält einen
Zeitstempel der Abholung, den Breiten- und Längengrad der Abholung, den Breiten- und Längengrad der Rückgabe
und Längengrad, die Anzahl der Fahrgäste und die Kosten der Taxifahrt. Das Ziel des ML
Modells ist es, die Kosten der Taxifahrt, bezeichnet als fare_amount, anhand dieser
Merkmale.

Dieses Modell profitiert von der Entwicklung zusätzlicher Merkmale aus den Rohdaten. Für
Da Taxifahrten beispielsweise auf der Entfernung und der Dauer der Fahrt basieren, können

298 | Kapitel 6: Reproduzierbarkeit - Entwurfsmuster

Die Berechnung der Entfernung zwischen Abholung und Abgabe ist eine nützliche Funktion. Sobald dieses
Merkmal im Datensatz berechnet ist, können wir es in einem Merkmalssatz zur späteren Verwendung speichern.

Hinzufügen von Feature-Daten zu Feast. Daten werden in Feast unter Verwendung von FeatureSets gespeichert. Ein FeatureSet
enthält das Datenschema und die Datenquelleninformationen, unabhängig davon, ob die Daten aus einem
Pandas-Datenframe oder einem Kafka-Streaming-Thema stammen. Anhand von FeatureSets weiß Feast
woher Feast die Daten, die es für ein Feature benötigt, beziehen kann, wie es sie einlesen kann und einige grundlegende
Charakteristiken über die Datentypen. Gruppen von Features können zusammen aufgenommen und gespeichert werden
gespeichert werden, und Feature-Sets sorgen für eine effiziente Speicherung und logische Trennung der Daten
innerhalb dieser Speicher.

Sobald unser Feature Set registriert ist, startet Feast einen Apache Beam Job, um den
Feature Store mit Daten aus der Quelle zu füllen. Ein Feature Set wird verwendet, um sowohl offline
Offline- und Online-Feature-Stores zu generieren, was sicherstellt, dass die Entwickler ihr Modell mit denselben Daten trainieren und bedienen.
denselben Daten trainieren und bedienen. Feast stellt sicher, dass die Quelldaten mit dem erwarteten Schema
des Merkmalsatzes entsprechen.

Die Aufnahme von Feature-Daten in Feast erfolgt in vier Schritten, wie in Abbildung 6-14 dargestellt.

Abbildung 6-14. Das Einlesen von Feature-Daten in Feast erfolgt in vier Schritten: Erstellen eines Feature-
Set, Hinzufügen von Entitäten und Features, Registrieren des FeatureSets und Einlesen von Feature-Daten in das
FeatureSet.

Entwurfsmuster 26: Merkmalsspeicher | 299
Die vier Schritte sind wie folgt:

Erstellen Sie ein FeatureSet. Das FeatureSet spezifiziert die Entitäten, Features und die Quelle.
Hinzufügen von Entitäten und Features zum FeatureSet.
Registrieren Sie das FeatureSet. Dadurch wird ein benanntes FeatureSet in Feast erstellt. Das Fea-
ture set enthält keine Feature-Daten.
Laden Sie die Feature-Daten in das FeatureSet.
Ein Notizbuch mit dem vollständigen Code für dieses Beispiel ist im Repository zu diesem Buch zu finden.
Repository zu diesem Buch.

Erstellen eines FeatureSets. Wir stellen eine Verbindung zu einem Feast-Einsatz her, indem wir einen Client mit
dem Python-SDK:

from feast import Client, FeatureSet, Entity, ValueType
# Verbindung zu einer bestehenden Feast-Bereitstellung
client = Client(core_url='localhost:6565')
Wir können überprüfen, ob der Client verbunden ist, indem wir die vorhandenen FeatureSets mit
dem Befehl client.list_feature_sets(). Wenn dies ein neuer Einsatz ist, wird
eine leere Liste zurück. Um ein neues Feature-Set zu erstellen, rufen Sie die Klasse FeatureSet auf und geben
den Namen des Feature-Sets:

# Erstellen eines FeatureSets
taxi_fs = FeatureSet("taxi_rides")
Hinzufügen von Entitäten und Features zum FeatureSet. Im Kontext von Feast bestehen FeatureSets
aus Entitäten und Merkmalen bestehen. Entitäten werden als Schlüssel zum Nachschlagen von Feature-Werten verwendet und
werden verwendet, um Features zwischen verschiedenen FeatureSets zu verbinden, wenn Datensätze für
Training oder Bedienung. Die Entität dient als Bezeichner für jedes relevante Merkmal.
Merkmal, das Sie im Datensatz haben. Sie ist ein Objekt, das modelliert werden kann und Informationen speichert.
tion. Im Kontext eines Fahrdienstes oder eines Essenslieferdienstes könnte eine relevante Entität
customer_id, order_id, driver_id oder restaurant_id sein. Im Zusammenhang mit einem Churn
Modells könnte eine Entität eine customer_id oder segment_id sein. Hier ist die Entität die
taxi_id, ein eindeutiger Bezeichner für den Taxiverkäufer jeder Fahrt.

In diesem Stadium enthält die von uns erstellte Merkmalsgruppe taxi_rides keine Entitäten oder Merkmale.
turen. Wir können den Feast-Core-Client verwenden, um diese aus einem Pandas-Dataframe anzugeben, der
der die Rohdateneingaben und Entitäten enthält, wie in Tabelle 6-2 dargestellt.

300 | Kapitel 6: Reproduzierbarkeit - Entwurfsmuster

Tabelle 6-2. Der Datensatz für Taxifahrten enthält Informationen über Taxifahrten in New York. Die
Entität ist die taxi_id, eine eindeutige Kennung für den Taxiverkäufer jeder Fahrt

Zeile pickup_datetime pickup_lat pickup_lon dropoff_lat dropoff_lon num_pass taxi_id fare_amt
1 2020-05-31 11:29:48
UTC
40.787403 -73.955848 40.723042 -73.993106 2 0 15.3
2 2011-04-06 14:30:00
UTC
40.645343 -73.776698 40.71489 -73.987242 2 0 45.0
3 2020-04-24 13:11:06
UTC
40.650105 -73.785373 40.638858 -73.9678 2 2 32.1
4 2020-02-20 09:07:00
UTC
40.762365 -73.925733 40.740118 -73.986487 2 1 21.3
Definieren von Streaming-Datenquellen bei der Erstellung eines Feature-Sets
Benutzer können beim Erstellen eines Feature-Sets Streaming-Datenquellen definieren. Sobald ein Feature
Quelle registriert ist, beginnt Feast automatisch, seine Speicher mit Daten aus dieser
Daten aus dieser Quelle. Dies ist ein Beispiel für ein Feature Set mit einer vom Benutzer bereitgestellten Quelle
das Streaming-Daten aus einem Kafka-Thema abruft:
feature_set = FeatureSet(
name="stream_feature",
entities=[
Entität("taxi_id", ValueType.INT64)
],
features=[
Merkmal("Verkehr_letzte_5min", ValueType.INT64)
],
source=KafkaSource(
brokers="mybroker:9092",
topic="mein_feature_topic"
)
)
Der Zeitstempel pickup_datetime ist hier wichtig, da er zum Abrufen von
Batch-Features abzurufen und wird verwendet, um zeitlich korrekte Joins für Batch-Features zu gewährleisten. Zum Erstellen eines
zusätzliches Merkmal zu erstellen, z. B. den euklidischen Abstand, laden Sie den Datensatz in einen Pandas-Datensatz
Frame und berechnen Sie das Merkmal:

# Datenrahmen laden
taxi_df = pd.read_csv("taxi-train.csv")
# Merkmale berechnen, Euklidischer Abstand
taxi_df['euclid_dist'] = taxi_df.apply(compute_dist, axis=1)
Wir können Entitäten und Merkmale mit .add(...) zum Merkmalssatz hinzufügen. Alternativ dazu kann die
Methode .infer_fields_from_df(...) die Entitäten und Merkmale für unsere

Entwurfsmuster 26: Merkmalsspeicher | 301
FeatureSet direkt aus dem Pandas-Datenframe. Wir geben einfach den Spaltennamen
der die Entität repräsentiert. Das Schema und die Datentypen für die Features des Feature
Sets werden dann aus dem Datenrahmen abgeleitet:

# Inferieren Sie die Merkmale des Merkmalssatzes aus dem Pandas DataFrame
taxi_fs.infer_fields_from_df(taxi_df,
entities=[Entity(name='taxi_id', dtype=ValueType.INT64)],
replace_existing_features= True )
Registrierung des FeatureSet. Sobald das FeatureSet erstellt ist, können wir es bei Feast registrieren
mit client.apply(taxi_fs) registrieren. Um zu bestätigen, dass das FeatureSet korrekt registriert wurde
registriert wurde oder um den Inhalt eines anderen FeatureSets zu untersuchen, können wir es abrufen
mit .get_feature_set(...) abrufen:

print (client .get_feature_set ("taxi_rides"))
Dies gibt ein JSON-Objekt zurück, das das Datenschema für das Merkmal "taxi_rides" enthält
enthält:

{
"spec" : {
"name" : "taxi_fahrten",
"entities" : [
{
"name" : "schlüssel",
"valueType" : "INT64"
}
],
"features" : [
{
"name" : "dropoff_lon",
"valueType" : "DOUBLE"
},
{
"name" : "pickup_lon",
"valueType" : "DOUBLE"
},
...
...
],
}
}
Einlesen von Feature-Daten in das FeatureSet. Sobald wir mit unserem Schema zufrieden sind, können wir
die Datenrahmen-Feature-Daten mit .ingest(...) in Feast einlesen. Wir spezifizieren das
FeatureSet, genannt taxi_fs, und den Datenrahmen, aus dem die Feature-Daten aufgefüllt werden sollen
Daten zu füllen, genannt taxi_df.

# Laden von Merkmalsdaten in Feast für diesen spezifischen Merkmalsatz
client. ingest(taxi_fs, taxi_df)
302 | Kapitel 6: Reproduzierbarkeit - Entwurfsmuster

Der Fortschritt während dieses Erfassungsschritts wird auf dem Bildschirm ausgedruckt und zeigt, dass wir
28.247 Zeilen in das Merkmal "taxi_rides" in Feast aufgenommen haben:

100%|██████████|28247/28247 [00:02<00:00, 2771.19rows/s]
Ingestion abgeschlossen!
Verschluckungsstatistiken:
Erfolg: 28247/28247 Zeilen eingelesen
In diesem Stadium listet der Aufruf von client.list_feature_sets() nun das Feature Set
taxi_rides auf, das wir gerade erstellt haben, und gibt [default/taxi_rides] zurück. Hier bezieht sich default
auf den Projektbereich des Featuresets innerhalb von Feast. Dieser kann bei der Instan-
Bei der Instan- dierung des Feature-Sets kann dies geändert werden, um bestimmte Feature-Sets im Projektbereich zu halten.

Datensätze können sich im Laufe der Zeit ändern, so dass sich auch die Feature-Sets
auch ändern. Sobald ein Feature-Set in Feast erstellt wurde, gibt es nur wenige
Änderungen vorgenommen werden können. Die folgenden Änderungen sind zum Beispiel
erlaubt:
Hinzufügen neuer Features.
Entfernen vorhandener Merkmale. (Beachten Sie, dass die Merkmale
werden und in der Aufzeichnung verbleiben, sie werden also nicht vollständig
vollständig entfernt. Dies hat zur Folge, dass neue Merkmale die Namen
Namen von zuvor gelöschten Merkmalen übernehmen können.)
Ändern der Schemata von Features.
Ändern der Quelle des Feature-Sets oder des max_age der Feature
set Beispiele.
Die folgenden Änderungen sind nicht erlaubt:
Änderungen des Feature-Set-Namens.
Änderungen an Entitäten.
Änderungen an den Namen bestehender Features.
Abrufen von Daten aus Feast

Sobald ein Merkmalssatz mit Merkmalen beschafft wurde, können wir historische oder Online
Merkmale abrufen. Benutzer und Produktionssysteme rufen Feature-Daten über eine von Feast bereitgestellte
Datenzugriffsschicht. Da Feast sowohl Offline- als auch Online-Speichertypen unterstützt, ist es kom- plett
mon, Feast für beide zu verwenden, wie in Abbildung 6-15 gezeigt. Die gleichen Feature
Merkmalsdaten sind in den beiden Merkmalspeichern enthalten, wodurch die Konsistenz zwischen Training
und Bereitstellung.

Entwurfsmuster 26: Merkmalsspeicher | 303
Abbildung 6-15. Merkmalsdaten können entweder offline abgerufen werden, indem historische Merkmale für
Modelltraining, oder online, für die Bereitstellung.

Der Zugriff auf diese Bereitstellungen erfolgt über einen separaten Online- und Batch-Client:

_feast_online_client = Client(serving_url='localhost:6566')
_feast_batch_client = Client(serving_url='localhost:6567',
core_url='localhost:6565')
Batch-Serving. Für das Training eines Modells wird die Abfrage historischer Merkmale durch BigQuery unterstützt
unterstützt und über .get_batch_features(...) mit dem Batch-Serving-Client abgerufen. In diesem
Fall stellen wir Feast einen Pandas-Datenrahmen zur Verfügung, der die Entitäten und Zeit
Zeitstempel, mit denen die Feature-Daten verknüpft werden sollen. Dies ermöglicht Feast die Erstellung eines zeitpunkt
Zeitpunkt korrekten Datensatz auf der Grundlage der angeforderten Merkmale zu erstellen:

# Erstellen einer Entität df mit allen Entitäten und Zeitstempeln
entity_df = pd.DataFrame(
{
"datetime": taxi_df.datetime,
"taxi_id": taxi_df.taxi_id,
}
)
Um historische Merkmale abzurufen, werden die Merkmale im Merkmalssatz durch den Merkmalssatznamen und den Merkmalnamen, getrennt durch einen Doppelpunkt, referenziert.
Namen des Merkmalssatzes und den Namen des Merkmals, getrennt durch einen Doppelpunkt, referenziert,
taxi_rides:pickup_lat:

FS_NAME = taxi_rides
model_features = ['pickup_lat',
'pickup_lon',
'dropoff_lat',
'dropoff_lon',
'num_pass',
304 | Kapitel 6: Reproduzierbarkeit - Entwurfsmuster

'euclid_dist']
label = 'fare_amt'
features = model_features + [label]
# Abrufen des Trainingsdatensatzes von Feast
dataset = _feast_batch_client. get_batch_features(
feature_refs=[FS_NAME + ":" + feature for feature in features],
entity_rows=entity_df).to_dataframe()
Der Datenrahmen-Datensatz enthält nun alle Merkmale und das Label für unser Modell, das
direkt aus dem Feature-Speicher.

Online-Ausgabe. Beim Online-Serving speichert Feast nur die neuesten Entitätswerte, im
im Gegensatz zum historischen Serving, bei dem alle historischen Werte gespeichert werden. Online-Serving
mit Feast ist auf eine sehr geringe Latenz ausgelegt, und Feast bietet eine gRPC-API, die von
Redis. Um Online-Features abzurufen, z. B. bei der Erstellung von Online-Vorhersagen mit
Vorhersagen mit dem trainierten Modell zu machen, verwenden wir .get_online_features(...), wobei wir die Features
die wir erfassen wollen und die Entität:

# Online-Merkmale für eine einzelne taxi_id abrufen
online_features = _feast_online_client.get_online_features(
feature_refs=["taxi_rides:pickup_lat",
"taxi_rides:pickup_lon",
"taxi_rides:dropoff_lat", "taxi_rides:dropoff_lat",
"taxi_rides:dropoff_lon",
"taxi_rides:num_pass",
"taxi_rides:euclid_dist"],
entity_rows=[
GetOnlineFeaturesRequest.EntityRow(
fields={
"taxi_id": Value(
int64_val=5)
}
)
]
)
Dies speichert online_features als eine Liste von Karten, wobei jedes Element in der Liste die
letzten Merkmalswerte für die angegebene Entität, hier taxi_id = 5:

field_values {
Felder {
key: "taxi_id"
Wert {
int64_val: 5
}
}
felder {
key: "taxi_rides:dropoff_lat"
value {
Entwurfsmuster 26: Funktionsspeicher | 305
8 Siehe den Gojek-Blog "Feast: Bridging ML Models and Data".
double_val: 40.78923797607422
}
}
Felder {
key: "taxi_rides:dropoff_lon"
value {
double_val: -73.96871948242188
}
Um eine Online-Vorhersage für dieses Beispiel zu erstellen, übergeben wir die Feldwerte aus dem
Objekt, das in online_features zurückgegeben wurde, als Pandas-Datenframe namens predict_df an
model.predict:

predict_df = pd.DataFrame.from_dict(online_features_dict)
model.predict(predict_df)
Warum es funktioniert
Merkmalspeicher funktionieren, weil sie die Entwicklung von Merkmalen von der Verwendung der Merkmale entkoppeln,
Die Entwicklung und Erstellung von Merkmalen kann unabhängig von der Nutzung
von der Verwendung von Merkmalen während der Modellentwicklung. Wenn Features zum Feature Store hinzugefügt werden
werden, stehen sie sofort für das Training und die Nutzung zur Verfügung und werden
an einem einzigen Ort gespeichert. Dadurch wird die Konsistenz zwischen Modellschulung und -bereitstellung gewährleistet.

Ein Modell, das als kundenorientierte Anwendung dient, kann zum Beispiel nur 10
Eingabewerte von einem Kunden erhalten, aber diese 10 Eingaben müssen möglicherweise in viele
weitere Merkmale umgewandelt werden, bevor sie an ein Modell gesendet werden. Diese konstruierten
Merkmale werden im Merkmalsspeicher verwaltet. Es ist entscheidend, dass die Pipeline zum
Abrufen von Merkmalen während der Entwicklung die gleiche ist wie bei der Bereitstellung des Modells. Ein Merkmalspeicher
Merkmalsspeicher stellt diese Konsistenz sicher (Abbildung 6-16).

Feast erreicht dies durch die Verwendung von Beam im Backend für Feature-Ingestion-Pipelines
die Feature-Werte in die Feature-Sets schreiben, und verwendet Redis und BigQuery für die Online- bzw.
und offline (bzw.) Feature-Abruf (Abbildung 6-17).^8 Wie bei jedem Feature-Speicher,
die Ingestion-Pipeline auch partielle Fehler oder Race-Bedingungen behandeln, die dazu führen können, dass
die dazu führen können, dass einige Daten in einem Speicher vorhanden sind, in dem anderen aber nicht.

306 | Kapitel 6: Reproduzierbarkeit - Entwurfsmuster

Abbildung 6-16. Ein Merkmalspeicher stellt sicher, dass die Feature-Engineering-Pipelines konsistent sind
zwischen Modellschulung und Auslieferung. Siehe auch https://docs.feast.dev/.

Abbildung 6-17. Feast verwendet Beam im Backend für die Aufnahme von Merkmalen sowie Redis und Big-
Query für den Online- und Offline-Abruf von Merkmalen.

Verschiedene Systeme können Daten in unterschiedlichen Raten produzieren, und ein Feature Store ist flexibel genug
ist flexibel genug, um mit diesen unterschiedlichen Rhythmen umzugehen, sowohl bei der Aufnahme als auch beim Abruf
(Abbildung 6-18). Sensordaten könnten zum Beispiel in Echtzeit erzeugt werden und jedes Jahr eintreffen

Entwurfsmuster 26: Merkmalsspeicher | 307
oder es könnte eine monatliche Datei geben, die von einem externen System erzeugt wird
die eine Zusammenfassung der Transaktionen des letzten Monats enthält. Jede dieser Dateien muss pro-
erfasst und in den Feature Store aufgenommen werden. Aus demselben Grund kann es unterschiedliche
Zeithorizonte für den Abruf von Daten aus dem Feature Store geben. Zum Beispiel kann eine benutzerorientierte
Online-Anwendung mit einer sehr geringen Latenzzeit arbeiten und sekundengenaue Merkmale verwenden,
während beim Trainieren des Modells die Merkmale in einem größeren Stapel offline abgerufen werden, jedoch mit
höherer Latenz.

Abbildung 6-18. Das Feature-Store-Entwurfsmuster kann sowohl die Anforderungen an die Daten erfüllen
für große Stapel während des Trainings als auch extrem niedrige Latenzzeiten für
Online-Anwendungen erfüllen.

Es gibt keine einzige Datenbank, die sowohl die Skalierung auf potenziell Terabytes an Daten
Daten und eine extrem niedrige Latenzzeit in der Größenordnung von Millisekunden. Der Merkmalsspeicher erreicht
Der Merkmalsspeicher erreicht dies mit getrennten Online- und Offline-Merkmalsspeichern und stellt sicher, dass die Merkmale in beiden
Features in beiden Szenarien einheitlich behandelt werden.

Schließlich fungiert ein Feature Store als versionskontrolliertes Repository für Feature-Datensätze,
und ermöglicht die Anwendung der gleichen CI/CD-Praktiken wie bei der Code- und Modellentwicklung auf den
auf den Feature-Engineering-Prozess übertragen werden. Das bedeutet, dass neue ML-Projekte mit einem Prozess beginnen
Auswahl von Merkmalen aus einem Katalog beginnen, anstatt das Feature Engineering von Grund auf durchzuführen.
Unternehmen einen Skaleneffekt erzielen können, wenn neue Funktionen erstellt und
Da neue Merkmale erstellt und dem Merkmalsspeicher hinzugefügt werden, wird es einfacher und schneller, neue Modelle zu
neue Modelle zu erstellen, die diese Merkmale wiederverwenden.

Kompromisse und Alternativen
Das Feast-Framework, das wir besprochen haben, basiert auf Google BigQuery, Redis und
Apache Beam. Es gibt jedoch auch Feature Stores, die auf anderen Tools und Tech
stapeln. Und obwohl ein Feature Store der empfohlene Weg ist, um Features in großem Umfang zu verwalten
Skalierung ist, bietet tf.transform eine alternative Lösung, die das Problem der

308 | Kapitel 6: Reproduzierbarkeit - Entwurfsmuster

die Schieflage bei der Ausbildung, aber nicht die Wiederverwendbarkeit von Merkmalen. Es gibt auch einige alternative
Verwendungszwecke eines Merkmalsspeichers, auf die wir noch nicht näher eingegangen sind, z. B. wie ein Merkmalsspeicher
Daten aus verschiedenen Quellen und Daten, die in unterschiedlichen Zeitabständen eintreffen, behandelt.

Alternative Implementierungen

Viele große Technologieunternehmen, wie Uber, LinkedIn, Airbnb, Netflix und Com-
cast, hosten ihre eigene Version eines Feature-Stores, obwohl die Architekturen und Tools variieren.
Die Michelangelo Palette von Uber basiert auf Spark/Scala und verwendet Hive für die Offline
Erstellung und Cassandra für Online-Funktionen. Hopsworks bietet eine weitere quelloffene
Feature-Store-Alternative zu Feast und basiert auf Dataframes mit Spark und pan-
das mit Hive für die Offline-Erstellung und MySQL Cluster für den Online-Zugriff auf Funktionen. Airbnb hat
Airbnb hat seinen eigenen Feature-Store als Teil seines ML-Produktionsframeworks namens Zipline entwickelt. Es
Es verwendet Spark und Flink für Feature-Engineering-Aufträge und Hive für die Feature-Speicherung.

Unabhängig davon, welche Technologie verwendet wird, sind die Hauptkomponenten des Funktionsspeichers
gleich:

Ein Werkzeug zur schnellen Verarbeitung großer Feature-Engineering-Aufträge, wie Spark, Flink oder
Beam.
Eine Speicherkomponente zur Unterbringung der erstellten Feature-Sets, wie z. B. Hive,
Cloud-Speicher (Amazon S3, Google Cloud Storage), BigQuery, Redis, BigTable,
und/oder Cassandra. Die von Feast verwendete Kombination (BigQuery und Redis) ist opti-
für den Abruf von Merkmalen offline oder online (mit niedriger Latenz) optimiert.
Eine Metadatenschicht zur Aufzeichnung von Versionsinformationen, Dokumentation und Merkmalen.
Dokumentation und Feature-Register zur Vereinfachung der Suche und gemeinsamen Nutzung von Feature-Sets.
Eine API zum Einlesen und Abrufen von Merkmalen in/aus dem Merkmalsspeicher.
Entwurfsmuster transformieren

Wenn der Feature-Engineering-Code beim Training und bei der Inferenz nicht derselbe ist, besteht die
besteht die Gefahr, dass die beiden Codequellen nicht konsistent sind. Dies führt zu Trainingsverzerrungen
und die Modellvorhersagen können unzuverlässig sein, da die Merkmale möglicherweise nicht dieselben sind.
gleich sind. Merkmalspeicher umgehen dieses Problem, indem sie ihre Merkmalsentwicklungsaufgaben
Merkmalsdaten sowohl in eine Online- als auch in eine Offline-Datenbank schreiben. Und während ein Feature
Feature Store selbst nicht die Feature-Transformationen durchführt, bietet er eine Möglichkeit, die
die vorgelagerten Feature-Engineering-Schritte von der Modellbereitstellung zu trennen und zeitnahe
Korrektheit.

Das in diesem Kapitel besprochene Transform-Entwurfsmuster bietet auch eine Möglichkeit, die
Feature-Transformationen getrennt und reproduzierbar zu halten. Zum Beispiel kann tf.transform
Daten vorverarbeiten, indem genau derselbe Code sowohl für das Training eines Modells als auch für
Vorhersagen in der Produktion zu verwenden, wodurch eine Verzerrung durch Training vermieden wird. Diese
wird sichergestellt, dass die Pipelines für das Training und die Bereitstellung von Merkmalen konsistent sind.

Entwurfsmuster 26: Merkmalsspeicher | 309
Der Merkmalspeicher bietet jedoch den zusätzlichen Vorteil der Wiederverwendbarkeit von Merkmalen, den
tf.transform nicht hat. Obwohl tf.transform-Pipelines die Reproduzierbarkeit
Reproduzierbarkeit gewährleisten, werden die Features nur für dieses Modell erstellt und entwickelt und sind nicht einfach
anderen Modellen und Pipelines geteilt oder wiederverwendet werden.

Andererseits achtet tf.transform besonders darauf, dass die Feature-Erstellung
während des Servings auf beschleunigter Hardware durchgeführt wird, da sie Teil des Serving
Graphen ist. Feature-Stores bieten diese Möglichkeit heute in der Regel nicht.

Entwurfsmuster 27: Modellversionierung
Beim Entwurfsmuster "Model Versioning" wird die Abwärtskompatibilität erreicht, indem
Bereitstellung eines geänderten Modells als Microservice mit einem anderen REST-Endpunkt. Dies ist
eine notwendige Voraussetzung für viele der anderen in diesem Kapitel besprochenen Muster.

Problem
Wie wir bei der Datendrift (eingeführt in Kapitel 1) gesehen haben, können Modelle mit der Zeit veralten
Modelle mit der Zeit veralten und müssen regelmäßig aktualisiert werden, um sicherzustellen, dass sie die
Ziele einer Organisation und die mit ihren Trainingsdaten verbundene Umgebung widerspiegeln.
Die Bereitstellung von Modellaktualisierungen für die Produktion hat unweigerlich Auswirkungen auf das Verhalten der Modelle bei neuen Daten.
auf neue Daten auswirken, was eine Herausforderung darstellt - wir brauchen einen Ansatz, um die
tionsmodelle auf dem neuesten Stand zu halten und gleichzeitig die Abwärtskompatibilität für bestehende
Benutzer.

Die Aktualisierung eines bestehenden Modells kann eine Änderung der Modellarchitektur beinhalten, um
um die Genauigkeit zu verbessern oder ein Modell anhand aktuellerer Daten neu zu trainieren, um eine Abweichung zu korrigieren.
Während diese Arten von Änderungen wahrscheinlich kein anderes Modellausgabeformat erfordern,
aber sie werden sich auf die Vorhersageergebnisse auswirken, die die Benutzer von einem Modell erhalten. Nehmen wir als Beispiel
ein Modell erstellen, das das Genre eines Buches anhand seiner Beschreibung vorhersagt
vorhersagt und die vorhergesagten Genres verwendet, um den Benutzern Empfehlungen zu geben. Wir trainieren unser
Modell zunächst auf einem Datensatz älterer klassischer Bücher trainiert, haben aber nun Zugang zu neuen Daten über
Tausende von neueren Büchern, die wir für das Training verwenden können. Das Training mit diesem aktualisierten Datensatz
verbessert die Gesamtgenauigkeit des Modells, verringert aber die Genauigkeit bei älteren "klassischen" Büchern leicht.
Bücher. Um damit umzugehen, brauchen wir eine Lösung, die es den Benutzern ermöglicht, eine ältere Version
unseres Modells wählen können, wenn sie dies wünschen.

Alternativ dazu könnten die Endnutzer unseres Modells mehr Informationen darüber verlangen, wie
das Modell zu einer bestimmten Vorhersage kommt. In einem medizinischen Anwendungsfall könnte ein Arzt
in einem medizinischen Anwendungsfall die Regionen in einem Röntgenbild sehen, die ein Modell dazu veranlasst haben, das Vorhandensein einer Krankheit vorherzusagen.
zu erkennen, anstatt sich nur auf die vorhergesagte Bezeichnung zu verlassen. In diesem Fall müsste die Antwort eines
Modells aktualisiert werden, um diese hervorgehobenen Bereiche einzubeziehen. Dieser
Prozess ist als Erklärbarkeit bekannt und wird in Kapitel 7 näher erläutert.

310 | Kapitel 6: Reproduzierbarkeit - Entwurfsmuster

Wenn wir Updates für unser Modell bereitstellen, wollen wir wahrscheinlich auch verfolgen, wie das
Modells in der Produktion zu verfolgen und es mit früheren Iterationen zu vergleichen. Wir
vielleicht auch eine Möglichkeit, ein neues Modell nur mit einer Teilmenge unserer Benutzer zu testen. Sowohl Perfor-
Leistungsüberwachung und Split-Tests sind zusammen mit anderen möglichen Modelländerungen
sind schwer zu lösen, wenn bei jeder Aktualisierung ein einzelnes Produktionsmodell ersetzt wird.
Dadurch würden Anwendungen, die darauf angewiesen sind, dass unsere Modellausgabe einem bestimmten Format entspricht
bestimmten Format angewiesen sind. Um dies zu bewerkstelligen, brauchen wir eine Lösung, die es uns ermöglicht
Modell kontinuierlich zu aktualisieren, ohne die bestehenden Benutzer zu beeinträchtigen.

Lösung
Um Aktualisierungen eines Modells zuverlässig zu handhaben, sollten Sie mehrere Modellversionen mit unterschiedlichen
enten REST-Endpunkten. Dies stellt die Abwärtskompatibilität sicher, indem mehrere Ver
mehrere Versionen eines Modells zu einem bestimmten Zeitpunkt bereitstellen, können die
den Dienst weiterhin nutzen können. Die Versionierung ermöglicht auch eine fein abgestufte Leistungs
Leistungsüberwachung und analytische Verfolgung über alle Versionen hinweg. Wir können die Genauigkeit und
Nutzungsstatistiken vergleichen und auf dieser Grundlage entscheiden, wann eine bestimmte Version offline genommen werden sollte. Wenn
eine Modellaktualisierung, die wir nur mit einer kleinen Untergruppe von Benutzern testen wollen, ermöglicht das
Model Versioning Design Pattern die Möglichkeit, A/B-Tests durchzuführen.

Außerdem ist bei der Modellversionierung jede bereitgestellte Version unseres Modells ein Mikrodienst.
Service und entkoppelt so Änderungen an unserem Modell von unserem Anwendungsfrontend. Um
Unterstützung für eine neue Version hinzuzufügen, müssen die Anwendungsentwickler unseres Teams nur den
den Namen des API-Endpunkts ändern, der auf das Modell verweist. Natürlich, wenn eine neue Modellver
Modellversion Änderungen am Antwortformat des Modells mit sich bringt, müssen wir natürlich Änderungen
Änderungen an unserer Anwendung vornehmen, um dies zu berücksichtigen, aber das Modell und der Anwendungscode sind immer noch getrennt.
raten. Datenwissenschaftler oder ML-Ingenieure können daher eine neue Modellversion
testen, ohne sich Sorgen machen zu müssen, dass unsere Produktionsanwendung beschädigt wird.

Arten von Modellbenutzern

Wenn wir von "Endnutzern" unseres Modells sprechen, sind damit zwei verschiedene Gruppen von Menschen gemeint
schen. Wenn wir unseren Modell-API-Endpunkt für Anwendungsentwickler außerhalb unserer Organisation zur Verfügung stellen, können diese
außerhalb unserer Organisation zur Verfügung stellen, können diese Entwickler als eine Art von Modellbenutzer angesehen werden.
Sie erstellen Anwendungen, die sich auf unser Modell stützen, um Vorhersagen für andere zu treffen.
ner. Der Vorteil der Abwärtskompatibilität, der mit der Modellversionierung einhergeht, ist für diese
für diese Benutzer am wichtigsten. Wenn sich das Format der Antwort unseres Modells ändert, möchten Anwendungs
Anwendungsentwickler möglicherweise eine ältere Modellversion verwenden, bis sie ihren Anwendungscode
Anwendungscode aktualisiert haben, um das neueste Antwortformat zu unterstützen.

Die andere Gruppe von Endnutzern bezieht sich auf diejenigen, die eine Anwendung verwenden, die unser
eingesetztes Modell aufruft. Dies könnte ein Arzt sein, der sich auf unser Modell verlässt, um das Vorhandensein einer
Vorhandensein einer Krankheit in einem Bild, jemand, der unsere Buchempfehlungs-App nutzt, die Geschäftseinheit unserer Organisation, die die Ergebnisse eines
Geschäftseinheit unseres Unternehmens, die die Ergebnisse eines von uns entwickelten Modells zur Umsatzvorhersage analysiert, und

Entwurfsmuster 27: Modellversionierung | 311
mehr. Diese Gruppe von Benutzern wird wahrscheinlich weniger Probleme mit der Abwärtskompatibilität haben, aber
möchte aber vielleicht die Möglichkeit haben, selbst zu entscheiden, ab wann sie eine neue Funktion in ihrer Anwendung nutzen möchte. Außerdem, wenn
wenn wir die Nutzer in verschiedene Gruppen einteilen können (z. B. basierend auf ihrer App-Nutzung), können wir jeder Gruppe
jeder Gruppe verschiedene Modellversionen auf der Grundlage ihrer Präferenzen anbieten.

Modellversionierung mit einem verwalteten Dienst

Um die Versionierung zu demonstrieren, erstellen wir ein Modell zur Vorhersage von Flugverspätungen und stellen dieses
dieses Modell auf der Cloud AI Platform Prediction. Da wir uns in den vorherigen Kapiteln TensorFlow's
SavedModel angesehen haben, werden wir hier ein XGBoost-Modell verwenden.

Sobald wir unser Modell trainiert haben, können wir es exportieren, um es für die Ausgabe vorzubereiten:

model.save_model('model.bst')
Um dieses Modell auf AI Platform bereitzustellen, müssen wir eine Modellversion erstellen, die
die auf diese model.bst in einem Cloud Storage Bucket verweist.

In AI Platform können einer Modellressource viele Versionen zugeordnet sein. Zum Erstellen
eine neue Version mit der gcloud CLI zu erstellen, führen wir Folgendes in einem Terminal aus:

gcloud ai-platform versions create 'v1' \
--model 'flight_delay_prediction' \
--origin gs://dein-gcs-bucket \
--runtime-version=1.15 \
--framework 'XGBOOST' \
--python-version=3.7
Nachdem dieses Modell bereitgestellt wurde, ist es nun über den Endpunkt /models/ zugänglich
flight_delay_predictions/versions/v1 in einer an unser Projekt gebundenen HTTPS-URL zugänglich. Da dies
die einzige Version ist, die wir bisher bereitgestellt haben, gilt sie als Standardversion. Das bedeutet
wenn wir in unserer API-Anforderung keine Version angeben, verwendet der Vorhersagedienst v1.
Jetzt können wir Vorhersagen für unser eingesetztes Modell treffen, indem wir ihm Beispiele in dem
Format senden, das unser Modell erwartet - in diesem Fall ein 110-Elemente-Array mit pseudocodierten Flughafen
Codes (den vollständigen Code finden Sie im Notizbuch auf GitHub). Das Modell gibt eine sigmoide
einen Float-Wert zwischen 0 und 1 zurück, der die Wahrscheinlichkeit angibt, dass ein bestimmter Flug
mehr als 30 Minuten verspätet ist.

Um eine Vorhersageanfrage an unser eingesetztes Modell zu stellen, verwenden wir den folgenden gcloud
Befehl, wobei input.json eine Datei mit unseren durch Zeilenumbrüche getrennten Beispielen ist, die wir für
Vorhersage:

gcloud ai-platform predict --model 'flight_delay_prediction'
--version 'v1'
--json-request 'input.json'
Wenn wir fünf Beispiele zur Vorhersage senden, erhalten wir ein Array mit fünf Elementen zurück.
zurück, das der sigmoiden Ausgabe für jedes Testbeispiel entspricht, wie im Folgenden dargestellt:

[0.019, 0.998, 0.213, 0.002, 0.004]
312 | Kapitel 6: Reproduzierbarkeit - Entwurfsmuster

Nun, da wir ein funktionierendes Modell in der Produktion haben, stellen wir uns vor, dass unser Data Science
Team beschließt, das Modell von XGBoost auf TensorFlow umzustellen, da dies zu einer
Genauigkeit führt und ihnen den Zugang zu zusätzlichen Werkzeugen im TensorFlow
Ökosystem. Das Modell hat das gleiche Eingabe- und Ausgabeformat, aber seine Architektur und das
exportierte Asset-Format hat sich geändert. Anstelle einer .bst Datei ist unser Modell nun im
TensorFlow SavedModel Format. Idealerweise können wir unsere zugrunde liegenden Modell-Assets
von unserem Anwendungsfrontend getrennt halten - so können sich die Anwendungsentwickler
sich auf die Funktionalität unserer Anwendung zu konzentrieren, anstatt auf eine Änderung der Modellformatierung
die sich nicht auf die Art und Weise auswirkt, wie Endbenutzer mit dem Modell interagieren. Hier kann die Modellver-
sionierung helfen. Wir werden unser TensorFlow Modell als eine zweite Version unter dem
derselben flight_delay_prediction-Modellressource. Endbenutzer können auf die neue
Version aktualisieren, indem sie einfach den Versionsnamen im API-Endpunkt ändern.
Endpunkt ändern.

Um unsere zweite Version bereitzustellen, exportieren wir das Modell und kopieren es in ein neues Unterverzeichnis
Unterverzeichnis in dem zuvor verwendeten Bucket. Wir können denselben Deploy-Befehl verwenden wie
Wir können denselben Bereitstellungsbefehl wie oben verwenden, wobei wir den Versionsnamen durch v2 ersetzen und auf den Cloud-Speicherort des neuen Modells verweisen.
des neuen Modells. Wie in Abbildung 6-19 dargestellt, können wir nun beide bereitgestellten Versionen in unserer Cloud-Konsole sehen.
Versionen in unserer Cloud-Konsole.

Abbildung 6-19. Das Dashboard zur Verwaltung von Modellen und Versionen in der Cloud AI Plat-
form-Konsole.

Beachten Sie, dass wir auch v2 als neue Standardversion festgelegt haben, so dass Benutzer, die keine
Version angeben, erhalten sie eine Antwort von v2. Da das Eingabe- und Ausgabeformat unseres
Modells identisch sind, können die Kunden ein Upgrade durchführen, ohne sich Gedanken über Änderungen zu machen.

Entwurfsmuster 27: Modellversionierung | 313
Sowohl Azure als auch AWS bieten ähnliche Modellversionierungsdienste an.
zur Verfügung. Auf Azure ist die Modellbereitstellung und -versionierung mit
Azure Machine Learning. In AWS sind diese Dienste in
SageMaker.
Ein ML-Ingenieur, der eine neue Version eines Modells als ML-Modell-Endpunkt einsetzt, kann
einen API-Gateway wie Apigee verwenden, der bestimmt, welche Modellversion er
aufruft. Dafür gibt es verschiedene Gründe, einschließlich Split-Tests einer neuen Version.
Für Split-Tests möchten sie vielleicht eine Modellaktualisierung mit einer zufällig ausgewählten
Gruppe von 10 % der Anwendungsnutzer testen, um festzustellen, wie es sich auf die allgemeine
mit der Anwendung auswirkt. Das API-Gateway bestimmt die aufzurufende Modellversion
anhand der ID oder IP-Adresse eines Benutzers aufzurufen ist.

Wenn mehrere Modellversionen eingesetzt werden, ermöglicht AI Platform die Leistungsüberwachung
Überwachung und Analyse über alle Versionen hinweg. So können wir Fehler auf eine bestimmte Version zurückführen,
den Datenverkehr überwachen und dies mit zusätzlichen Daten kombinieren, die wir in unserer
Anwendung sammeln.

Versionierung für den Umgang mit neu verfügbaren Daten
Neben dem Umgang mit Änderungen an unserem Modell selbst ist ein weiterer Grund für die Verwendung der Versionierung
zu verwenden, wenn neue Trainingsdaten verfügbar werden. Angenommen, diese neuen Daten folgen dem
demselben Schema wie das ursprüngliche Modell trainiert wurden, ist es wichtig, zu verfolgen, wann
die Daten für jede neu trainierte Version erfasst wurden. Ein Ansatz, dies zu verfolgen, ist
ist die Kodierung des Zeitstempelbereichs jedes Trainingsdatensatzes im Namen einer Modellversion.
sion zu kodieren. Wenn zum Beispiel die neueste Version eines Modells auf Daten aus dem Jahr 2019 trainiert wurde, könnte man die Version
könnte man die Version v20190101_20191231 nennen.
Wir können diesen Ansatz in Kombination mit "Entwurfsmuster 18: Fortgesetzte Modell
Evaluation" auf Seite 220 (besprochen in Kapitel 5), um zu bestimmen, wann ältere
Modellversionen offline zu nehmen oder wie weit die Trainingsdaten zurückreichen sollen. Kontinuierliche Evaluierung
tion könnte uns helfen, festzustellen, dass unser Modell am besten funktioniert, wenn es mit Daten
Daten aus den letzten zwei Jahren trainiert wird. Daraus könnten dann die Versionen abgeleitet werden, die wir entfernen wollen,
und wie viele Daten beim Training neuerer Versionen verwendet werden sollen.
314 | Kapitel 6: Reproduzierbarkeit - Entwurfsmuster

Kompromisse und Alternativen
Während wir das Entwurfsmuster der Modellversionierung gegenüber der Pflege einer einzigen
Modellversion empfehlen, gibt es einige Implementierungsalternativen zu der oben skizzierten Lösung
oben skizzierten Lösung. Hier sehen wir uns andere serverlose und Open-Source-Tools für dieses Muster an
und den Ansatz, mehrere Serving-Funktionen zu erstellen. Wir werden auch diskutieren, wann man
eine völlig neue Modellressource anstelle einer Version zu erstellen.

Andere Tools zur serverlosen Versionierung

Wir haben einen verwalteten Dienst verwendet, der speziell für die Versionierung von ML-Modellen entwickelt wurde, aber
aber wir könnten ähnliche Ergebnisse mit anderen serverlosen Angeboten erzielen. Unter der Haube ist jede
Modellversion eine zustandslose Funktion mit einem bestimmten Eingabe- und Ausgabeformat,
die hinter einem REST-Endpunkt bereitgestellt wird. Wir könnten also einen Dienst wie Cloud Run,
verwenden, um jede Version in einem separaten Container zu erstellen und bereitzustellen. Jeder Container
hat eine eindeutige URL und kann durch eine API-Anfrage aufgerufen werden. Dieser Ansatz gibt uns
mehr Flexibilität bei der Konfiguration der bereitgestellten Modellumgebung und ermöglicht das Hinzufügen von
Funktionen wie die serverseitige Vorverarbeitung von Modelleingaben. In unserem Flugbeispiel
möchten wir vielleicht nicht verlangen, dass die Clients kategorische Werte in einem Schritt kodieren.
Stattdessen könnten wir den Clients die kategorischen Werte als Strings übergeben und die Vorverarbeitung in unserem Container durchführen.
Verarbeitung in unserem Container.

Warum sollten wir einen verwalteten ML-Dienst wie AI Platform Prediction verwenden, anstatt ein
einem allgemeineren serverlosen Tool? Da AI Platform speziell für die Bereitstellung von ML
Modellbereitstellung entwickelt wurde, bietet sie integrierte Unterstützung für die Bereitstellung von Modellen mit GPUs, die für ML
miert für ML. Auch die Verwaltung von Abhängigkeiten ist hier möglich. Als wir unser
XGBoost-Modell bereitgestellt haben, mussten wir uns nicht um die Installation der richtigen XGBoost
Version oder anderer Bibliotheksabhängigkeiten kümmern.

TensorFlow Bedienung

Anstatt Cloud AI Platform oder ein anderes Cloud-basiertes serverloses Angebot für
Modellversionierung zu nutzen, könnten wir ein Open-Source-Tool wie TensorFlow Serving verwenden. Der
empfohlene Ansatz für die Implementierung von TensorFlow Serving ist die Verwendung eines Docker
Container über das neueste tensorflow/serving Docker Image zu verwenden. Mit Docker können wir
können wir das Modell mit jeder beliebigen Hardware bedienen, einschließlich GPUs. Die
TensorFlow Serving API hat eingebaute Unterstützung für die Modellversionierung, die einem ähnlichen Ansatz folgt
ähnlichen Ansatz wie der, der im Abschnitt Lösung diskutiert wurde. Zusätzlich zu TensorFlow
Serving gibt es auch andere Open Source Model Serving Optionen, einschließlich Seldon
und MLFlow.

Mehrere Servierfunktionen

Eine weitere Alternative zur Bereitstellung mehrerer Versionen ist die Definition mehrerer Serving-Funktionen
Funktionen für eine einzige Version eines exportierten Modells zu definieren. "Entwurfsmuster 16: Zustandsloses Serving

Entwurfsmuster 27: Modellversionierung | 315
Funktion" auf Seite 201 (eingeführt in Kapitel 5) erklärt, wie man ein trainiertes
Modell als zustandslose Funktion für den Einsatz in der Produktion zu exportieren. Dies ist besonders nützlich, wenn
Modell-Eingaben eine Vorverarbeitung benötigen, um die vom Client gesendeten Daten in die vom Modell erwartete Form umzuwandeln.
mat zu transformieren, die das Modell erwartet.

Um die Anforderungen verschiedener Gruppen von Modellbenutzern zu erfüllen, können wir beim Export unseres Modells mehrere
Serving-Funktionen definieren, wenn wir unser Modell exportieren. Diese Serving-Funktionen sind Teil einer
einer exportierten Modellversion, und dieses Modell wird an einem einzigen REST-Endpunkt bereitgestellt.
In TensorFlow werden Serving-Funktionen durch Modellsignaturen implementiert, die
die das Eingabe- und Ausgabeformat definieren, das ein Modell erwartet. Wir können mehrere
Serving Funktionen mit dem @tf.function Dekorator definieren und jeder Funktion eine Input
Signatur.

In dem Anwendungscode, in dem wir unser eingesetztes Modell aufrufen, würden wir bestimmen
welche Serving-Funktion auf der Grundlage der vom Client gesendeten Daten zu verwenden ist. Zum Beispiel, eine
Anfrage wie zum Beispiel:

{"signature_name": " get_genre " , "instances": ... }
an die exportierte Signatur mit dem Namen get_genre gesendet werden, während eine Anfrage wie:

{"signature_name": " get_genre_with_explanation ", "instances": ... }
an die exportierte Signatur mit dem Namen get_genre_with_explanation gesendet werden.

Der Einsatz mehrerer Signaturen kann daher das Problem der Abwärtskompatibilität lösen.
lem lösen. Es gibt jedoch einen wesentlichen Unterschied - es gibt nur ein Modell, und wenn
Wenn dieses Modell eingesetzt wird, werden alle Signaturen gleichzeitig aktualisiert. In unserem ursprünglichen
Beispiel der Änderung des Modells von nur einem Genre zu mehreren Genres
Genres, änderte sich die Modellarchitektur. Der Ansatz mit mehreren Signaturen würde bei diesem Beispiel nicht
Beispiel nicht funktionieren, da wir zwei verschiedene Modelle haben. Die Mehrfachsignatur
Lösung ist auch nicht geeignet, wenn wir verschiedene Versionen des Modells getrennt halten
getrennt halten und die ältere Version mit der Zeit veralten lassen wollen.

Die Verwendung mehrerer Signaturen ist besser als die Verwendung mehrerer Versionen, wenn Sie beide
beide Modellsignaturen auch in Zukunft beibehalten wollen. In dem Szenario, in dem es einige Kunden gibt
Kunden, die einfach nur die beste Antwort wollen, und andere Kunden, die sowohl die beste
Antwort und eine Erklärung wünschen, bietet es einen zusätzlichen Vorteil, alle Signaturen
mit einem neueren Modell zu aktualisieren, anstatt jedes Mal eine Version nach der anderen zu aktualisieren, wenn das
Modell neu trainiert und neu eingesetzt wird.

Welche Szenarien gibt es, in denen wir beide Versionen des Modells beibehalten wollen?
Modells? Bei einem Textklassifizierungsmodell gibt es möglicherweise einige Kunden, die
Rohtext an das Modell senden müssen, und andere, die in der Lage sind, den Rohtext in Matrizen
bevor sie eine Vorhersage erhalten. Anhand der Anfragedaten des Clients kann das Modell
Rahmenwerk bestimmen, welche Funktion verwendet werden soll. Die Übergabe von Texteinbettungs
Matrizen an ein Modell weiterzugeben, ist weniger kostspielig als die Vorverarbeitung von Rohtext, so dass dies ein Beispiel für eine

316 | Kapitel 6: Reproduzierbarkeit - Entwurfsmuster

plädieren, wo mehrere Serving-Funktionen die serverseitige Verarbeitungszeit reduzieren könnten. Es ist
Es ist auch erwähnenswert, dass wir mehrere Serving-Funktionen mit mehreren Modellversionen haben können.
Versionen haben können, obwohl die Gefahr besteht, dass dies zu viel Komplexität erzeugt.

Neue Modelle versus neue Modellversionen

Manchmal kann es schwierig sein, zu entscheiden, ob eine weitere Modellversion oder eine
oder eine völlig neue Modellressource. Wir empfehlen, ein neues Modell zu erstellen, wenn sich die
Vorhersageaufgabe ändert. Eine neue Vorhersageaufgabe führt normalerweise zu einem anderen Modell
Ausgabeformat, und eine Änderung dieses Formats könnte dazu führen, dass bestehende Clients nicht mehr funktionieren. Wenn wir uns
unsicher sind, ob wir eine neue Version oder ein neues Modell verwenden sollen, können wir überlegen, ob wir
bestehende Kunden ein Upgrade durchführen sollen. Wenn die Antwort ja lautet, haben wir wahrscheinlich das Modell verbessert
das Modell verbessert, ohne die Vorhersageaufgabe zu ändern, und die Erstellung einer neuen Version ist ausreichend.
funktionieren. Wenn wir das Modell so verändert haben, dass die Benutzer entscheiden müssen, ob sie das Update
entscheiden müssen, ob sie die Aktualisierung wünschen, müssen wir wahrscheinlich eine neue Modellressource erstellen.

Um dies in der Praxis zu sehen, lassen Sie uns zu unserem Flugvorhersagemodell zurückkehren, um ein Beispiel zu sehen.
Das aktuelle Modell hat definiert, was es als Verspätung ansieht (30+ Minuten Verspätung), aber unsere
Endnutzer haben dazu möglicherweise unterschiedliche Meinungen. Einige Benutzer sind der Meinung, dass bereits 15 Minuten Verspätung
als verspätet, während andere meinen, ein Flug sei nur dann verspätet, wenn er mehr als eine Stunde
verspätet ist. Nehmen wir an, wir möchten, dass unsere Benutzer ihre eigene Definition von
Definition von "verspätet" übernehmen können, anstatt unsere zu verwenden. In diesem Fall würden wir das "Entwurfsmuster 5:
Reframing" auf Seite 80 (besprochen in Kapitel 3), um dies in ein Regressionsmodell zu ändern.
Das Eingabeformat für dieses Modell ist dasselbe, aber die Ausgabe ist jetzt ein numerischer Wert
der die Verzögerungsvorhersage darstellt.

Die Art und Weise, wie unsere Modellnutzer diese Antwort auswerten, wird sich natürlich von der ersten
Version. Mit unserem neuesten Regressionsmodell könnten App-Entwickler die
die vorausgesagte Verspätung bei der Suche nach Flügen anzeigen und damit etwas wie "Dieser Flug hat
Dieser Flug hat normalerweise mehr als 30 Minuten Verspätung" aus der ersten Version. In diesem Szenario ist die
beste Lösung darin, eine neue Modellressource zu erstellen, vielleicht mit dem Namen flight_model_regres
zu erstellen, um die Änderungen zu berücksichtigen. Auf diese Weise können die App-Entwickler wählen, welche sie verwenden möchten, und
und wir können die Leistung jedes Modells weiterhin durch die Bereitstellung neuer
Versionen.

Zusammenfassung
Dieses Kapitel konzentrierte sich auf Entwurfsmuster, die verschiedene Aspekte der Reproduzierbarkeit behandeln.
Reproduzierbarkeit behandeln. Beginnend mit dem Transformationsdesign haben wir gesehen, wie dieses Muster verwendet wird, um die
Reproduzierbarkeit der Datenvorbereitungsabhängigkeiten zwischen der Modellschulungs
und der Modellierungspipeline zu gewährleisten. Erreicht wird dies durch die explizite Erfassung der
Transformationen, die zur Umwandlung der Modelleingaben in die Modellmerkmale angewendet werden. Das
Repeatable Splitting Design Pattern erfasst die Art und Weise, wie die Daten zwischen Trainings-, Vali-

Zusammenfassung | 317
und Testdatensätzen, um sicherzustellen, dass ein im Training verwendetes Beispiel niemals für
Auswertung oder Prüfung verwendet wird, selbst wenn der Datensatz wächst.

Das Entwurfsmuster "Bridged Schema" befasst sich mit der Frage, wie die Reproduzierbarkeit sichergestellt werden kann, wenn ein
Trainingsdatensatz ein Hybrid aus neueren Daten und älteren Daten mit einem anderen Schema ist. Dieses
ermöglicht die Kombination von zwei Datensätzen mit unterschiedlichen Schemata auf konsistente Weise für
Training. Als Nächstes haben wir das Entwurfsmuster Windowed Inference erörtert, das sicherstellt
das sicherstellt, dass Merkmale, die dynamisch und zeitabhängig berechnet werden, zwischen
zwischen Training und Bedienung korrekt wiederholt werden können. Dieses Entwurfsmuster ist besonders nützlich
wenn Modelle für maschinelles Lernen Merkmale benötigen, die aus
Aggregaten über Zeitfenster berechnet werden.

Das Workflow-Pipeline-Designmuster befasst sich mit dem Problem der Erstellung einer durchgängig
reproduzierbaren Pipeline durch Containerisierung und Orchestrierung der Schritte in unserem
Workflow des maschinellen Lernens. Als nächstes haben wir gesehen, wie das Feature Store Design Pattern
Reproduzierbarkeit und Wiederverwendbarkeit von Merkmalen über verschiedene maschinelle
Lernaufgaben. Schließlich haben wir uns das Entwurfsmuster Model Versioning angesehen, bei dem die Rückwärtskompatibilität
Rückwärtskompatibilität erreicht wird, indem ein geändertes Modell als Microservice mit
einem anderen REST-Endpunkt.

Im nächsten Kapitel befassen wir uns mit Entwurfsmustern, die helfen, KI verantwortungsvoll einzusetzen.

318 | Kapitel 6: Reproduzierbarkeit - Entwurfsmuster

KAPITEL 7

Verantwortungsvolle KI
Bis zu diesem Punkt haben wir uns auf Muster konzentriert, die Daten- und Ingenieurteams dabei helfen sollen
Teams bei der Vorbereitung, Erstellung, Schulung und Skalierung von Modellen für die Produktion helfen. Diese Muster
richteten sich hauptsächlich an Teams, die direkt in den ML-Modellentwicklungsprozess eingebunden sind.
Sobald ein Modell in Produktion ist, geht sein Einfluss weit über die Teams hinaus, die es erstellt haben.
In diesem Kapitel befassen wir uns mit den anderen Beteiligten an einem Modell, sowohl innerhalb als auch
außerhalb einer Organisation. Zu den Stakeholdern können Führungskräfte gehören, deren Geschäftsziele
Geschäftsziele die Ziele eines Modells diktieren, die Endnutzer eines Modells, Wirtschaftsprüfer und Compliance
Aufsichtsbehörden.

In diesem Kapitel werden wir uns auf mehrere Gruppen von Modellakteuren beziehen:

Ersteller von Modellen
Datenwissenschaftler und ML-Forscher, die direkt an der Erstellung von ML-Modellen beteiligt sind.

ML-Ingenieure
Mitglieder von ML Ops-Teams, die direkt an der Bereitstellung von ML-Modellen beteiligt sind.

Entscheidungsträger in Unternehmen
Entscheiden, ob sie das ML-Modell in ihre Geschäftsprozesse oder kundenorientierten Anwendungen einbinden wollen
oder kundenorientierte Anwendungen integrieren wollen, und müssen beurteilen, ob das Modell
für diesen Zweck geeignet ist.

Endnutzer von ML-Systemen
Nutzen Sie die Vorhersagen eines ML-Modells. Es gibt viele verschiedene Arten von
Modell-Endnutzer: Kunden, Mitarbeiter und Mischformen davon. Beispiele sind ein
ein Kunde, der von einem Modell eine Filmempfehlung erhält, ein Angestellter in einer Fabrik
ein Angestellter in einer Fabrikhalle, der anhand eines visuellen Inspektionsmodells feststellt, ob ein Produkt defekt ist
oder ein Mediziner, der ein Modell zur Unterstützung der Patientendiagnose verwendet.

319
Regulierungs- und Compliance-Agenturen
Personen und Organisationen, die eine Zusammenfassung auf Führungsebene darüber benötigen, wie ein Modell
Entscheidungen aus der Perspektive der Einhaltung von Vorschriften trifft. Dies könnte
Finanzprüfer, Regierungsbehörden oder Governance-Teams innerhalb einer Organisation
Organisation sein.

In diesem Kapitel werden wir uns mit Mustern befassen, die die Auswirkungen eines Modells auf Einzelpersonen und Gruppen
viduals and groups outside the team and organization building a model. Das Heuris-
tische Benchmark-Entwurfsmuster bietet eine Möglichkeit, die Leistung des Modells in einen
Kontext zu stellen, den Endbenutzer und Entscheidungsträger verstehen können. Das Explainable Predic- tions
tions-Muster bietet Ansätze zur Verbesserung des Vertrauens in ML-Systeme, indem es ein
Verständnis der Signale fördert, die ein Modell zur Erstellung von Vorhersagen verwendet. Das Fairness Lens
zielt darauf ab, sicherzustellen, dass sich Modelle bei verschiedenen Untergruppen von
Benutzer und Vorhersageszenarien verhalten.

Zusammengenommen fallen die Muster in diesem Kapitel unter die Praxis der verantwortungsvollen KI.
Dies ist ein aktiver Forschungsbereich, der sich mit den besten Möglichkeiten befasst, Fairness
ness, Interpretierbarkeit, Datenschutz und Sicherheit in KI-Systeme zu integrieren. Empfohlene Praktiken
für verantwortungsbewusste KI ist die Anwendung eines menschenzentrierten Designansatzes durch
mit einer Vielzahl von Benutzern und Anwendungsszenarien während der Projektentwicklung
Projektentwicklung einbeziehen, die Grenzen von Datensätzen und Modellen verstehen und
ML-Systeme auch nach der Einführung zu überwachen und zu aktualisieren. Verantwortungsvolle KI-Muster sind nicht
nicht auf die drei, die wir in diesem Kapitel besprechen - viele der Muster aus früheren Kapiteln
Kapiteln (wie Kontinuierliche Auswertung, Wiederholbare Aufteilung und Neutrale Klasse, um
um nur einige zu nennen) bieten Methoden zur Umsetzung dieser empfohlenen Praktiken und zur Erreichung
das Ziel, Fairness, Interpretierbarkeit, Privatsphäre und Sicherheit in KI-Systeme einzubauen.

Entwurfsmuster 28: Heuristischer Benchmark
Das Heuristic Benchmark-Muster vergleicht ein ML-Modell mit einer einfachen, leicht zu
Heuristik, um die Leistung des Modells den Entscheidungsträgern zu
Entscheidungsträgern zu erklären.

Problem
Angenommen, ein Fahrradverleih möchte die voraussichtliche Dauer der Verleihvorgänge nutzen, um
um eine Lösung für eine dynamische Preisgestaltung zu entwickeln. Nach dem Training eines ML-Modells zur Vorhersage der Dauer
der Mietdauer eines Fahrrads trainiert hat, wertet er das Modell anhand eines Testdatensatzes aus und stellt fest
dass der mittlere absolute Fehler (MAE) des trainierten ML-Modells 1.200 Sekunden beträgt. Unter
sie dieses Modell den Entscheidungsträgern des Unternehmens vorstellen, werden sie wahrscheinlich gefragt werden: "Ist
ein MAE von 1.200 Sekunden gut oder schlecht?" Auf diese Frage müssen wir vorbereitet sein
auf diese Frage vorbereitet sein, wenn wir ein Modell entwickeln und es den Interessenvertretern der Wirtschaft präsentieren. Wenn wir
ein Bildklassifizierungsmodell auf Artikel in einem Produktkatalog trainieren und der mittlere

320 | Kapitel 7: Verantwortungsvolle KI

Wenn die durchschnittliche Genauigkeit (MAP) 95 % beträgt, kann man erwarten, dass man gefragt wird: "Ist ein MAP von 95% gut
oder schlecht?"

Es nützt nichts, mit den Händen zu winken und zu sagen, das hängt vom Problem ab. Aber natürlich,
tut es das. Was ist also ein guter MAE-Wert für das Fahrradverleihproblem in New York City?
Wie sieht es in London aus? Was ist ein guter MAP für die Klassifizierung von Produktkatalogbildern?
tion?

Die Leistung von Modellen wird in der Regel in kalten, harten Zahlen angegeben, die für den
die für den Endnutzer nur schwer in einen Kontext zu bringen sind. Die Erläuterung der Formeln für MAP, MAE usw.
usw. zu erklären, vermittelt nicht die Intuition, nach der Entscheidungsträger in Unternehmen fragen.

Lösung
Wenn es sich um das zweite ML-Modell handelt, das für eine Aufgabe entwickelt wird, besteht eine einfache Lösung darin, die
Vergleich der Leistung des Modells mit der aktuell eingesetzten Version. Es ist recht
Es ist einfach zu sagen, dass der MAE-Wert jetzt 30 Sekunden niedriger oder der MAP-Wert 1 % höher ist. Dies
funktioniert auch dann, wenn der aktuelle Produktions-Workflow nicht mit ML arbeitet. Solange diese Aufgabe
bereits in der Produktion durchgeführt wird und Bewertungskennzahlen gesammelt werden, können wir
können wir die Leistung unseres neuen ML-Modells mit der aktuellen Produktionsmethodik vergleichen.
Methodik vergleichen.

Aber was ist, wenn es keine aktuelle Produktionsmethodik gibt und wir ein
das allererste Modell für eine neue Aufgabe bauen? In solchen Fällen besteht die Lösung in der Erstellung eines
einfachen Benchmark zu erstellen, der ausschließlich zum Vergleich mit unserem neu entwickelten
ML-Modell zu vergleichen. Wir nennen dies einen heuristischen Benchmark.

Ein guter heuristischer Benchmark sollte intuitiv leicht zu verstehen und relativ
trivial zu berechnen sein. Wenn wir den von der Benchmark verwendeten Algorithmus verteidigen oder debuggen müssen
zu verteidigen oder zu debuggen, sollten wir nach einem einfacheren, verständlicheren Algorithmus suchen. Gute
Beispiele für einen heuristischen Benchmark sind Konstanten, Faustregeln oder Massenstatistiken
(wie der Mittelwert, Median oder Modus). Vermeiden Sie die Versuchung, auch nur ein einfaches
Modell für maschinelles Lernen, wie z. B. eine lineare Regression, auf einem Datensatz zu trainieren und dieses als
lineare Regression ist wahrscheinlich nicht intuitiv genug, vor allem, wenn wir anfangen
kategoriale Variablen, mehr als eine Handvoll Eingaben oder konstruierte
Merkmale.

Verwenden Sie keinen heuristischen Benchmark, wenn es bereits eine operationelle
Praxis vorhanden ist. Stattdessen sollten wir unser Modell mit
den bestehenden Standard vergleichen. Die bestehende betriebliche Praxis muss nicht
muss nicht unbedingt ML verwenden - es ist einfach die Technik, die derzeit
zur Lösung des Problems verwendet wird.
Entwurfsmuster 28: Heuristisches Benchmarking | 321
Beispiele für gute heuristische Benchmarks und Situationen, in denen wir sie einsetzen könnten
sind in Tabelle 7-1 aufgeführt. Beispielcode für die Implementierungen dieser heuristischen
Benchmarks befindet sich im GitHub-Repository dieses Buches.

Tabelle 7-1. Heuristische Benchmarks für einige ausgewählte Szenarien (siehe Code in GitHub)

Szenario Heuristischer Benchmark Beispielaufgabe Implementierung für Beispielaufgabe
Regressionsproblem mit
Merkmale und Interaktionen
zwischen Merkmalen nicht
nicht gut verstanden werden
Unternehmen.
Mittelwert oder Medianwert der
des Kennzeichnungswertes über die
Trainingsdaten.
Wählen Sie den Median, wenn
es viele Ausreißer gibt.
Zeitintervall vor einer
Frage auf Stack
Overflow beantwortet wird.
Sagen Sie voraus, dass es 2,120 Sekunden dauern wird
immer.
2.120 Sekunden ist der Median der Zeit bis zur
ersten Antwort über den gesamten Trainingsdatensatz
Trainingsdatensatz.
Binäres Klassifikationsproblem
bei dem Merkmale und
Interaktionen zwischen
Merkmalen nicht gut
vom Unternehmen nicht gut verstanden werden.
Gesamtanteil der
Positiven in den Trainingsdaten
Daten.
Ob eine
akzeptierte Antwort in
Stack Overflow wird
bearbeitet wird.
Vorhersage von 0,36 als Ausgabewahrscheinlichkeit
für alle Antworten.
0,36 ist der Anteil der akzeptierten Antworten
insgesamt, die bearbeitet werden.
Mehrstufige Klassifizierung
Problem, bei dem Merkmale und
Interaktionen zwischen
Merkmalen nicht gut
vom Unternehmen nicht gut verstanden werden.
Verteilung des Label
Wertes über die Trainingsdaten
Daten.
Land, aus dem ein
Stack Overflow
Frage beantwortet wird
beantwortet wird.
Sagen Sie 0,03 für Frankreich voraus, 0,08 für Indien,
und so weiter.
Dies sind die Bruchteile der Antworten
die von Personen aus Frankreich, Indien,
und so weiter.
Regressionsproblem, bei dem
es ein einziges, sehr
wichtiges, numerisches Merkmal gibt.
Lineare Regression auf der Grundlage
auf dem, was intuitiv,
das einzige wichtigste
Merkmal.
Vorhersage des Taxitarifs
Höhe des Taxitarifs bei gegebenen Abhol
und Abfahrtsorte.
Die Entfernung zwischen
den beiden Punkten ist,
intuitiv ein Schlüssel
Merkmal.
Fahrpreis = 4,64 $ pro Kilometer.
Die 4,64 $ werden aus den
Trainingsdaten über alle Fahrten berechnet.
Regressionsproblem mit
einem oder zwei wichtigen
Merkmalen. Die Merkmale können
numerisch oder kategorisch sein
sollten aber allgemein
Heuristiken verwendet werden.
Nachschlagetabelle, in der die
Zeilen und Spalten
den Schlüssel
Merkmalen (gegebenenfalls diskretisiert
erforderlich) und die
Vorhersage für jede Zelle ist
das durchschnittliche Label in dieser
Zelle, geschätzt über die
Trainingsdaten.
Vorhersage der Dauer des
Fahrradverleihs.
Hier sind die beiden wichtigsten
Merkmale sind die
Station, an der das Fahrrad
ausgeliehen wird
und ob es
Stoßzeiten für
für Pendler.
Nachschlagetabelle der durchschnittlichen Ausleihdauer
von jeder Station basierend auf der Hauptverkehrszeit
gegenüber der Nicht-Spitzenstunde.
Klassifizierungsproblem mit
ein oder zwei wichtigen
Merkmalen. Die Merkmale können
numerisch oder kategorisch sein.
Wie oben, mit dem Unterschied, dass die
Vorhersage für jede Zelle ist
die Verteilung der Kennzeichnungen
in dieser Zelle ist.
Wenn das Ziel die Vorhersage einer
einzelne Klasse vorherzusagen, berechnen Sie den
Modus der Kennzeichnung in jeder
Zelle.
Vorhersage, ob ein
Stack Overflow
Frage beantwortet wird
Antwort innerhalb eines
Tag beantwortet wird.
Die wichtigste
Merkmal ist hier das
primäre Markierung.
Berechnen Sie für jedes Tag den Anteil der
Fragen, die innerhalb eines Tages beantwortet werden
Tag beantwortet werden.
322 | Kapitel 7: Verantwortungsvolle KI

Szenario Heuristischer Benchmark Beispielaufgabe Implementierung für Beispielaufgabe
Regressionsproblem, das
die Vorhersage des
Vorhersage des zukünftigen Wertes einer Zeitreihe.
Persistenz oder linearer
Trend. Berücksichtigung der Saisonalität
berücksichtigen. Bei jährlichen
Daten, Vergleich mit dem
gleichen Tag/Woche/Quartal
des Vorjahres.
Vorhersage des wöchentlichen Umsatzes
Volumen
Voraussage, dass der Umsatz der nächsten Woche = s 0
wobei s 0 der Umsatz dieser Woche ist.
(oder)
Umsatz der nächsten Woche = s 0 + (s 0 - s-1)
wobei s-1 der Umsatz der letzten Woche ist.
(oder)
Umsatz der nächsten Woche = s-1y, wobei s-1y für
die Verkäufe der entsprechenden Woche des letzten
Jahr ist.
Vermeiden Sie die Versuchung, die
drei Optionen zu kombinieren, da der Wert der
der relativen Gewichte nicht intuitiv ist.
Klassifizierungsproblem
wird derzeit gelöst von
menschlichen Experten gelöst wird.
Dies ist üblich für Bild,
Video- und Textaufgaben und
umfasst Szenarien, in denen es
kostenintensiv ist, um
routinemäßige Lösung des Problems
mit menschlichen Experten zu lösen.
Leistung der menschlichen
Experten.
Erkennung von Augenkrankheiten
anhand von Netzhautscans.
Drei oder mehr Ärzte untersuchen
jedes Bild. Behandeln Sie die Entscheidung einer
Mehrheit der Ärzte als richtig,
und betrachten Sie die Prozentrangfolge des
des ML-Modells unter den menschlichen Experten.
Vorbeugende oder vorausschauende
Wartung.
Durchführung der Wartung nach
einem festen Zeitplan.
Vorbeugende
Wartung eines Autos.
Autos einmal alle drei Monate zur Wartung
alle drei Monate.
Die drei Monate sind die durchschnittliche Zeit bis zum
bis zum Ausfall des Fahrzeugs ab dem letzten Wartungsdatum.
Erkennung von Anomalien. 99. Perzentil-Wert
geschätzt aus dem
Trainingsdatensatz.
Erkennen eines Denial of
Service (DoS)-Angriff
aus dem Netzwerkverkehr.
Finden Sie das 99. Perzentil der Anzahl
von Anfragen pro Minute in den historischen
Daten. Wenn in einem beliebigen einminütigen Zeitraum die
Anzahl der Anfragen diese Zahl überschreitet
überschreitet, kennzeichnen Sie dies als DoS-Angriff.
Empfehlungsmodell. Empfehlen Sie das beliebteste
beliebtesten Artikel in der
Kategorie des
des letzten Kaufs des Kunden.
Empfehlen Sie Filme an
Benutzer.
Wenn ein Nutzer gerade Inception gesehen (und gemocht) hat
(ein Science-Fiction-Film) gesehen hat, empfehlen Sie ihm Icarus
(der beliebteste Science-Fiction-Film, den
den sie noch nicht gesehen haben).
Viele der Szenarien in Tabelle 7-1 beziehen sich auf "wichtige Funktionen". Diese sind wichtig
in dem Sinne, dass sie in der Branche weithin anerkannt sind, da sie einen
wohlverstandenen Einfluss auf das Vorhersageproblem haben. Insbesondere handelt es sich nicht um Merkmale
tures, die mit Hilfe von Feature-Bedeutungsmethoden in Ihrem Trainingsdatensatz ermittelt wurden. Zum Beispiel
Beispiel: In der Taxibranche ist es allgemein anerkannt, dass der wichtigste Bestimmungs
der wichtigste Bestimmungsfaktor für einen Taxitarif die Entfernung ist und dass längere Fahrten mehr kosten. Das macht die
Das macht die Entfernung zu einem wichtigen Merkmal, nicht das Ergebnis einer Merkmalsbedeutungsstudie.

Entwurfsmuster 28: Heuristischer Benchmark | 323
Kompromisse und Alternativen
Wir werden oft feststellen, dass ein heuristischer Benchmark über den primären Zweck hinaus nützlich ist
der Erklärung der Modellleistung. In einigen Fällen kann der heuristische Benchmark
eine spezielle Datenerhebung erforderlich. Schließlich gibt es auch Fälle, in denen ein heuristischer Bench-
Benchmark unzureichend sein kann, weil der Vergleich selbst Kontext benötigt.

Überprüfung der Entwicklung

Oft erweist sich ein heuristischer Benchmark als nützlich, um die Leistung von ML-Modellen zu erklären.
Leistung von ML-Modellen. Während der Entwicklung kann er auch bei der Diagnose von
Probleme mit einem bestimmten Modellansatz.

Nehmen wir an, wir erstellen ein Modell zur Vorhersage der Mietdauer und
unser Benchmark ist eine Nachschlagetabelle der durchschnittlichen Mietdauer unter Angabe des Bahnhofsnamens
und der Angabe, ob es sich um die Hauptverkehrszeit handelt oder nicht:

CREATE TEMPORARY FUNCTION is_peak_hour(start_date TIMESTAMP ) AS
EXTRACT (DAYOFWEEK FROM start_date) BETWEEN 2 AND 6 -- Wochentag
AND (
EXTRACT (HOUR FROM start_date) ZWISCHEN 6 UND 10
ODER
EXTRACT (HOUR FROM start_date) BETWEEN 15 AND 18)
;
SELECT
start_station_name,
is_peak_hour(start_date) AS is_peak,
AVG (Dauer) AS predicted_duration,
FROM `bigquery- public - data .london_bicycles.cycle_hire`
GROUP BY 1, 2
Während wir unser Modell entwickeln, ist es sinnvoll, die Leistung unseres ML
Modells mit diesem Benchmark zu vergleichen. Zu diesem Zweck werden wir die Leistung des Modells
Leistung des Modells auf verschiedenen Schichten des Evaluierungsdatensatzes. Hier wird der Auswertungs
Hier wird der Evaluierungsdatensatz nach start_station_name und is_peak stratifiziert. Auf diese Weise können wir
können wir leicht feststellen, ob unser Modell die stark frequentierten Bahnhöfe überbetont und
und seltene Sender in den Trainingsdaten ignoriert. Wenn dies der Fall ist, können wir
Komplexität des Modells erhöhen oder den Datensatz so ausbalancieren, dass die weniger
beliebten Stationen.

Menschliche Experten

Wir haben empfohlen, dass bei Klassifizierungsproblemen wie der Diagnose von Augenkrankheiten - wo
die Arbeit von menschlichen Experten durchgeführt wird, der Benchmark eine Gruppe
solcher Experten. Indem man jedes Bild von drei oder mehr Ärzten untersuchen lässt, ist es möglich
inwieweit menschliche Ärzte Fehler machen und die Fehlerquote des Modells mit der
Fehlerquote des Modells mit derjenigen der menschlichen Experten vergleichen. Im Falle eines solchen Bildes

324 | Kapitel 7: Verantwortungsvolle KI

Klassifizierungsproblemen ist dies eine natürliche Erweiterung der Beschriftungsphase, da die
Bezeichnungen für Augenkrankheiten durch menschliche Beschriftung erstellt werden.

Manchmal ist es von Vorteil, auf menschliche Experten zurückzugreifen, auch wenn wir die tatsächliche
Wahrheit haben. Wenn wir zum Beispiel ein Modell zur Vorhersage der Kosten für eine Autoreparatur nach einem Unfall erstellen
können wir uns historische Daten ansehen und die tatsächlichen Kosten für die Reparatur ermitteln. Wir werden
in der Regel keine menschlichen Experten für dieses Problem heranziehen, da die Basiswahrheit direkt
aus dem historischen Datensatz verfügbar ist. Für die Zwecke der Kommunikation
des Benchmarks kann es jedoch hilfreich sein, Versicherungsvertreter die Autos für eine Schadensschätzung
und die Schätzungen unseres Modells mit denen der Agenten zu vergleichen.

Der Einsatz menschlicher Experten muss nicht auf unstrukturierte Daten beschränkt sein, wie dies bei Augenkrankheiten oder
Schätzung von Schadenskosten. Wenn wir zum Beispiel ein Modell erstellen, um vorherzusagen, ob
ob ein Kredit innerhalb eines Jahres refinanziert wird oder nicht, werden die Daten tabellarisch sein und die
Wahrheit wird in den historischen Daten verfügbar sein. Aber auch in diesem Fall könnten wir Experten bitten
Experten bitten, Kredite zu identifizieren, die refinanziert werden, um zu erfahren
um zu ermitteln, wie oft die Kreditvermittler im Außendienst richtig liegen würden.

Gebrauchswert

Selbst wenn wir ein operationelles Modell oder eine ausgezeichnete Heuristik zum Vergleich haben, müssen wir
müssen wir immer noch die Auswirkungen der Verbesserung, die unser Modell bietet, erklären. 
dass die MAE um 30 Sekunden niedriger oder der MAP um 1 % höher ist, reicht möglicherweise
nicht genug sein. Die nächste Frage könnte lauten: "Ist eine Verbesserung um 1 % gut? Ist
ist es die Mühe wert, ein ML-Modell in die Produktion zu geben, anstatt eine einfache
heuristischen Regel?"

Wenn Sie können, ist es wichtig, die Verbesserung der Modellleistung in
den Nutzwert des Modells. Dieser Wert kann monetär sein, er kann aber auch
mit anderen Maßstäben des Nutzens übereinstimmen, z. B. bessere Suchergebnisse, frühere Erkennung von Krankheiten oder
weniger Abfall durch verbesserte Fertigungseffizienz. Dieser Nutzwert ist nützlich
bei der Entscheidung, ob dieses Modell eingesetzt werden soll oder nicht, da die Einführung oder Änderung eines
Denn der Einsatz oder die Änderung eines Produktionsmodells ist immer mit gewissen Kosten in Bezug auf Zuverlässigkeit und Fehlerbudgets verbunden.
ets. Wenn das Bildklassifizierungsmodell zum Beispiel zum Vorausfüllen eines Bestellformulars verwendet wird, können wir
können wir berechnen, dass eine Verbesserung um 1 % zu 20 weniger abgebrochenen Bestellungen pro
Tag bedeutet und daher einen bestimmten Geldbetrag wert ist. Wenn dies mehr ist als die Schwellen-.
hold, den unser Site Reliability Engineering Team festgelegt hat, würden wir das Modell einsetzen.

Bei unserem Fahrradverleihproblem könnte es möglich sein, die Auswirkungen auf das Geschäft mit diesem Modell zu messen.
dieses Modell zu messen. Zum Beispiel könnten wir die erhöhte Verfügbarkeit von
Verfügbarkeit von Fahrrädern oder die erhöhten Gewinne bei Verwendung des Modells in einer dynamischen
Lösung der Preisgestaltung.

Entwurfsmuster 28: Heuristischer Benchmark | 325
1 DR ist eine Augenkrankheit, die Millionen von Menschen auf der ganzen Welt betrifft. Sie kann zur Erblindung führen, aber wenn sie
früh erkannt wird, kann sie erfolgreich behandelt werden. Um mehr zu erfahren und den Datensatz zu finden, siehe hier.
2 In dieser Studie wurden Erklärungen verwendet, um Anmerkungen in radiologischen Bildern zu identifizieren und zu korrigieren.
Entwurfsmuster 29: Erklärbare Vorhersagen
Das Entwurfsmuster "Erklärbare Vorhersagen" erhöht das Vertrauen der Nutzer in ML-Systeme, indem
indem es den Nutzern ein Verständnis dafür vermittelt, wie und warum Modelle bestimmte Vorhersagen
tionen treffen. Während Modelle wie Entscheidungsbäume von vornherein interpretierbar sind, macht die Architektur
Architektur von tiefen neuronalen Netzen von Natur aus schwer zu erklären. Für alle
Modellen ist es nützlich, Vorhersagen interpretieren zu können, um die Kombinationen
Kombinationen von Merkmalen zu verstehen, die das Modellverhalten beeinflussen.

Problem
Bei der Bewertung eines Modells für maschinelles Lernen, um festzustellen, ob es pro- duktionsreif ist
Kennzahlen wie Genauigkeit, Präzision, Recall und mittlerer quadratischer Fehler nur einen Teil der
Teil der Geschichte. Sie liefern Daten darüber, wie korrekt die Vorhersagen eines Modells im
Vorhersagen eines Modells im Vergleich zu den Werten der Grundwahrheit im Testsatz sind, aber sie geben keinen
zu diesen Vorhersagen gekommen ist. In vielen ML-Szenarien können Benutzer zögern, die Vorhersage eines
Vorhersage eines Modells für bare Münze zu nehmen.

Um dies zu verstehen, schauen wir uns ein Modell an, das den Schweregrad einer diabetischen Retinop-
athie (DR) anhand eines Netzhautbildes vorhersagt.^1 Das Modell liefert eine Softmax-Ausgabe, die angibt
die Wahrscheinlichkeit, dass ein einzelnes Bild zu einer von 5 Kategorien gehört, die den
Schweregrad der DR auf dem Bild - von 1 (keine DR vorhanden) bis 5 (proliferative DR,
die schlimmste Form). Nehmen wir an, dass das Modell für ein bestimmtes Bild mit 95%iger Sicherheit
dass das Bild eine proliferative DR enthält. Dies mag wie ein sehr
genaues Ergebnis, aber wenn sich ein Arzt bei der Diagnose eines Patienten ausschließlich auf diese
Modellausgabe verlässt, um eine Diagnose für den Patienten zu stellen, hat er immer noch keinen Einblick, wie das
Modell zu dieser Vorhersage gekommen ist. Vielleicht hat das Modell die richtigen Regionen im Bild identifiziert
Bild, die auf einen DR hinweisen, aber es besteht auch die Möglichkeit, dass die Vorhersage des Modells auf
auf Pixeln im Bild basiert, die keinen Hinweis auf die Krankheit zeigen. Ein Beispiel,
Vielleicht enthalten einige Bilder im Datensatz Arztnotizen oder Anmerkungen. Das Modell
könnte fälschlicherweise das Vorhandensein eines Kommentars für seine Vorhersage verwenden,
und nicht die kranken Bereiche im Bild.^2 In der derzeitigen Form des Modells gibt es keine
Vorhersage Regionen in einem Bild zuzuordnen, was es für den Arzt schwierig macht, dem
Arzt, dem Modell zu vertrauen.

Die medizinische Bildgebung ist nur ein Beispiel - es gibt viele Branchen, Szenarien und
Modellarten, bei denen ein mangelnder Einblick in den Entscheidungsprozess eines Modells zu
zu Problemen mit dem Vertrauen der Nutzer führen kann. Wenn ein ML-Modell verwendet wird, um die Kreditwürdigkeit einer Person
Kreditwürdigkeit oder eine andere finanzielle Kennzahl vorherzusagen, werden die Menschen wahrscheinlich wissen wollen, warum sie

326 | Kapitel 7: Verantwortungsvolle KI

eine bestimmte Punktzahl erhalten hat. War es ein Zahlungsverzug? Zu viele Kreditlinien? Kurze
Kreditgeschichte? Vielleicht stützt sich das Modell ausschließlich auf demografische Daten, um seine
demografische Daten und führt damit ohne unser Wissen eine Verzerrung in das Modell ein.
wissen. Da wir nur den Score kennen, können wir nicht wissen, wie das Modell zu seiner Vorhersage kommt.
Vorhersage kommt.

Neben den Endnutzern von Modellen gibt es noch eine weitere Gruppe von Akteuren, die sich
die sich mit Regulierungs- und Compliance-Standards für ML-Modelle befassen, da Modelle in bestimmten
Branchen eine Prüfung oder zusätzliche Transparenz erfordern können. Beteiligte, die mit der
die mit der Prüfung von Modellen befasst sind, benötigen wahrscheinlich eine übergeordnete Zusammenfassung darüber, wie das Modell zu seinen Vorhersagen kommt
zu seinen Vorhersagen kommt, um seine Verwendung und Wirkung zu rechtfertigen. Metriken wie Genauigkeit sind in diesem Fall nicht
ohne einen Einblick in die Gründe, warum ein Modell die Vorhersagen macht, die es macht,
kann sein Einsatz problematisch werden.

Schließlich können wir als Datenwissenschaftler und ML-Ingenieure die Qualität unseres Modells nur
nur bis zu einem gewissen Grad verbessern, wenn wir die Merkmale, auf die es sich stützt, um
Vorhersagen. Wir brauchen eine Möglichkeit, um zu überprüfen, ob die Modelle so funktionieren, wie wir es
erwarten. Nehmen wir zum Beispiel an, wir trainieren ein Modell mit Tabellendaten, um vorherzusagen
ob sich ein Flug verspätet. Das Modell wird mit 20 Merkmalen trainiert. Unter der Haube,
stützt es sich vielleicht nur auf 2 dieser 20 Merkmale, und wenn wir den Rest entfernen würden, könnten wir
die Leistung unseres Systems erheblich verbessern. Oder vielleicht ist jedes dieser 20 Merkmale
notwendig, um den Grad an Genauigkeit zu erreichen, den wir brauchen. Ohne weitere Details darüber
was das Modell verwendet, ist es schwer zu wissen.

Lösung
Um mit den inhärenten Unbekannten in ML umzugehen, brauchen wir einen Weg, um zu verstehen, wie Modelle
unter der Haube arbeiten. Techniken zum Verstehen und Kommunizieren, wie und
warum ein ML-Modell Vorhersagen macht, sind ein aktives Forschungsgebiet. Auch als Inter-
oder Modellverständnis genannt, ist die Erklärbarkeit ein neues und sich schnell entwickelndes Gebiet
innerhalb von ML und kann je nach Modellarchitektur und der Art der Daten, auf denen es trainiert wurde, verschiedene Formen annehmen.
der Art der Daten, für die es trainiert wurde. Die Erklärbarkeit kann auch dazu beitragen, Verzerrungen in ML-Modellen aufzudecken,
was wir bei der Diskussion des Fairness Lens-Musters in diesem Kapitel behandeln. Hier werden wir uns
tiefe neuronale Netze mit Hilfe von Feature-Attributionen zu erklären. Zum Verständnis
Kontext zu verstehen, betrachten wir zunächst die Erklärbarkeit für Modelle mit weniger komplexen
Architekturen.

Einfachere Modelle wie Entscheidungsbäume sind einfacher zu erklären als tiefe
Modelle zu erklären, da sie oft von vornherein interpretierbar sind. Das bedeutet, dass ihre gelernten
Gewichte einen direkten Einblick in die Art und Weise geben, wie das Modell Vorhersagen trifft. Wenn wir ein
lineares Regressionsmodell mit unabhängigen, numerischen Eingangsmerkmalen haben, können die Gewichte
manchmal interpretierbar sein. Nehmen wir zum Beispiel ein lineares Regressionsmodell, das Folgendes vorhersagt

Entwurfsmuster 29: Erklärbare Vorhersagen | 327
3 Das hier diskutierte Modell wurde auf einem öffentlichen UCI-Datensatz trainiert.
Kraftstoffverbrauch eines Autos.^3 In scikit-learn können wir die gelernten Koeffizienten eines linearen
Regressionsmodells mit der folgenden Formel:

model = LinearRegression().fit(x_train, y_train)
Koeffizienten = model.coef_
Die resultierenden Koeffizienten für jedes Merkmal in unserem Modell sind in Abbildung 7-1 dargestellt.

Abbildung 7-1. Die gelernten Koeffizienten unseres linearen Regressionsmodells für den Kraftstoffverbrauch,
das die Kilometerleistung eines Autos vorhersagt. Wir haben get_dummies() von Pandas verwendet, um das
um das Herkunftsmerkmal in eine boolesche Spalte zu konvertieren, da es kategorisch ist.

Die Koeffizienten zeigen uns die Beziehung zwischen den einzelnen Merkmalen und dem Ergebnis des Modells.
Ergebnis, den vorhergesagten Meilen pro Gallone (MPG). Aus diesen Koeffizienten können wir zum Beispiel
schließen, dass für jeden zusätzlichen Zylinder in einem Auto der von unserem Modell vorhergesagte MPG-Wert
sinkt. Unser Modell hat auch gelernt, dass neue Autos, wenn sie auf den Markt kommen (gekennzeichnet durch das Merkmal
"Modelljahr"), oft eine höhere Kraftstoffeffizienz aufweisen. Wir können viel lernen
mehr über die Beziehungen zwischen den Merkmalen und der Leistung unseres Modells
als aus den gelernten Gewichten einer versteckten Schicht in einem tiefen neuronalen Netz.
Netzwerks. Aus diesem Grund werden Modelle wie das oben gezeigte oft als
interpretierbar durch Design.

328 | Kapitel 7: Verantwortungsvolle KI

4 Die scikit-learn-Dokumentation geht näher darauf ein, wie man die gelernten Gewichte in linearen Modellen richtig interpretiert.
linearen Modellen.
Es ist zwar verlockend, den gelernten Gewichten in linearen Regressions- oder
Gewichtungen in linearen Regressions- oder Entscheidungsbaummodellen zuzuweisen, muss man
dabei äußerst vorsichtig sein. Die Schlussfolgerungen, die wir zuvor gezogen haben
sind immer noch richtig (z. B. umgekehrte Beziehung zwischen der Anzahl der Zylinder
Zylinderzahl und Kraftstoffeffizienz), aber wir können nicht von der Größe der Koeffizienten auf die
aus der Größe der Koeffizienten nicht schließen, dass z. B. das kategoriale Herkunftsmerkmal
oder die Anzahl der Zylinder für unser Modell wichtiger sind als
Pferdestärken oder Gewicht. Erstens wird jedes dieser Merkmale in einer
eine andere Einheit. Ein Zylinder ist nicht gleichzusetzen mit einem Pfund.
Die Autos in diesem Datensatz haben maximal 8 Zylinder, wiegen aber über
über 3.000 Pfund. Außerdem ist die Herkunft ein kategoriales Merkmal, das
ein kategorisches Merkmal, das mit Dummy-Werten dargestellt wird, so dass jeder Herkunftswert nur 0 oder 1 sein kann.
Die Koeffizienten sagen uns auch nichts über die Beziehung zwischen
zwischen den Merkmalen in unserem Modell. Mehr Zylinder sind oft korreliert mit
mit mehr Pferdestärken korreliert, aber wir können dies nicht aus den gelernten
Gewichte.^4
Bei komplexeren Modellen verwenden wir Post-hoc-Erklärbarkeitsmethoden, um die
um die Beziehungen zwischen den Merkmalen eines Modells und seiner Ausgabe anzunähern. Normalerweise führen Post-hoc
Methoden diese Analyse, ohne sich auf Modellinterna wie gelernte
Gewichte. Dies ist ein Bereich der laufenden Forschung, und es gibt eine Vielzahl von vorgeschlagenen
Erklärungsmethoden sowie Werkzeuge zum Hinzufügen dieser Methoden zu Ihrem ML-Workflow
Fluss. Die Art von Erklärungsmethoden, die wir besprechen werden, sind als Feature-Attributionen bekannt.
Diese Methoden zielen darauf ab, die Ausgabe eines Modells - sei es ein Bild, eine Klassifizierung oder ein numerischer Wert - einem Merkmal zuzuordnen.
Klassifizierung oder einen numerischen Wert - seinen Merkmalen zuzuordnen, indem jedem Merkmal Attributionswerte zugewiesen werden, die angeben, wie viel
jedem Merkmal Attributionswerte zugewiesen werden, die angeben, wie viel dieses Merkmal zum Ergebnis beigetragen hat. Es gibt zwei Arten
von Feature-Attributionen:

Instanz-Ebene
Feature-Attribute, die den Output eines Modells für eine einzelne Vorhersage erklären.
Bei einem Modell, das vorhersagt, ob jemandem ein Kredit genehmigt werden sollte, würde beispielsweise eine
Kreditlinie bewilligt wird, würde eine Merkmalsattribution auf Instanzebene Aufschluss darüber geben
warum der Antrag einer bestimmten Person abgelehnt wurde. In einem Bildmodell könnte eine Attribution auf Instanz-
Attribution auf Instanzebene die Pixel in einem Bild hervorheben, die zu der Vorhersage
eine Katze enthält.

Global
Globale Merkmalszuschreibungen analysieren das Verhalten des Modells in einem Aggregat, um
Schlussfolgerungen über das Verhalten des Modells als Ganzes zu ziehen. Typischerweise wird dies
Dies geschieht in der Regel durch die Mittelung von Merkmalszuschreibungen auf Instanzebene aus einem Testdatensatz. In einem
Modell, das vorhersagt, ob ein Flug Verspätung hat, könnten globale Attributionen sagen

Entwurfsmuster 29: Erklärbare Vorhersagen | 329
5 Wir konzentrieren uns auf diese beiden Erklärbarkeitsmethoden, da sie weit verbreitet sind und eine Vielzahl von Modelltypen abdecken.
Modellarten abdecken, aber es gibt viele andere Methoden und Frameworks, die in dieser Analyse nicht berücksichtigt wurden, wie z. B. LIME und
ELI5.
Wir stellen fest, dass extreme Wetterbedingungen insgesamt das wichtigste Merkmal bei der Vorhersage von
Verspätungen.
Die beiden untersuchten Methoden zur Merkmalszuweisung^5 sind in Tabelle 7-2 dargestellt und bieten
bieten unterschiedliche Ansätze, die sowohl für Erklärungen auf Instanzebene als auch für globale
Erklärungen verwendet werden können.

Tabelle 7-2. Beschreibungen der verschiedenen Erklärungsmethoden und Links zu den entsprechenden Forschungsarbeiten

Name Beschreibung Papier
Sampled Shapley Basierend auf dem Konzept des Shapley-Wertes, bestimmt dieser Ansatz
bestimmt den marginalen Beitrag eines Merkmals, indem er berechnet, wie sehr das
wie sehr das Hinzufügen und Entfernen dieses Merkmals eine Vorhersage beeinflusst,
analysiert über mehrere Kombinationen von Merkmalswerten.
https://oreil.ly/ubEjW
Integrierte Gradienten (IG) Unter Verwendung einer vordefinierten Modell-Basislinie berechnet IG die Ableitungen
(Gradienten) entlang des Pfades von dieser Basislinie zu einer bestimmten Eingabe.
https://oreil.ly/sy8f8
a Der Shapley-Wert wurde 1951 in einem Aufsatz von Lloyd Shapley eingeführt und basiert auf Konzepten aus der Spieltheorie.
Obwohl wir diese Ansätze von Grund auf neu implementieren könnten, gibt es Werkzeuge, die
um den Prozess der Merkmalszuordnung zu vereinfachen. Die verfügbaren Open-Source- und
Cloud-basierte Erklärungswerkzeuge ermöglichen es uns, uns auf das Debuggen, Verbessern und Zusammenfassen
rizing unserer Modelle konzentrieren.

Modell Grundlinie

Um diese Werkzeuge nutzen zu können, müssen wir zunächst das Konzept einer Baseline verstehen, wie es
für Erklärungsmodelle mit Merkmalszuweisungen. Das Ziel jeder Erklärbarkeitsmethode
Methode ist es, die Frage zu beantworten: "Warum hat das Modell X vorhergesagt?" Merkmalsattribu-
tionen versuchen, dies zu erreichen, indem sie für jedes Merkmal numerische Werte angeben, die angeben
wie viel dieses Merkmal zum endgültigen Ergebnis beigetragen hat. Nehmen wir zum Beispiel ein Modell
das anhand einiger demografischer und gesundheitlicher Daten vorhersagt, ob ein Patient an einer Herzerkrankung
Daten. Stellen wir uns für ein einzelnes Beispiel in unserem Testdatensatz vor, dass der Attributionswert
für das Cholesterinmerkmal eines Patienten 0,4 ist und der Attributionswert für seinen Blutdruck
-0.2. Ohne Kontext bedeuten diese Attributionswerte nicht viel, und unsere erste Frage
wird wahrscheinlich lauten: "0,4 und -0,2 im Verhältnis zu was?" Dieses "was" ist die Grundlinie des Modells.

Wenn wir Werte für die Merkmalszuordnung erhalten, sind sie alle relativ zu einem vordefinierten Basis
Basisvorhersagewert für unser Modell. Basisprognosen können entweder informativ oder
uninformativ sein. Uninformative Basiswerte werden in der Regel mit einem durchschnittlichen Fall
über einen Trainingsdatensatz. Bei einem Bildmodell könnte eine uninformative Basislinie ein

330 | Kapitel 7: Verantwortungsvolle KI

einfarbiges schwarzes oder weißes Bild. Bei einem Textmodell könnte eine uninformative Grundlinie 0 Werte für die
Werte für die Einbettungsmatrizen des Modells oder Stoppwörter wie "der", "ist" oder "und" sein. In einem
Modell mit numerischen Eingaben besteht ein üblicher Ansatz zur Auswahl einer Basislinie darin, eine
eine Vorhersage unter Verwendung des Medianwerts für jedes Merkmal im Modell.

Festlegung von Grundlinien
Je nachdem, ob unser Modell eine Regressions- oder eine Klassifizierungsaufgabe erfüllt, müssen wir über eine Baseline
eine Regressions- oder eine Klassifikationsaufgabe durchführt. Bei einer Regressionsaufgabe hat ein Modell
Modell genau einen numerischen Basisvorhersagewert. Nehmen wir in unserem Beispiel für den Autokilometerstand an
Nehmen wir an, wir entscheiden uns für den Medianansatz zur Berechnung unserer Basislinie. Der
Median für die acht Merkmale in unserem Datensatz ist das folgende Array:
[151.0, 93.5, 2803.5, 15.5, 76.0, 1.0, 0.0, 0.0]
Wenn wir dies an unser Modell senden, beträgt die vorhergesagte MPG 22,9. Folglich wird für jede
Vorhersage, die wir für dieses Modell treffen, 22,9 MPG als Basiswert für den Vergleich
Vorhersagen.
Stellen wir uns nun vor, dass wir dem Reframing-Muster folgen, um dieses Problem von einem Regres- sions- in ein Klassifizierungsproblem umzuwandeln.
Regressionsproblem in ein Klassifikationsproblem zu verwandeln. Dazu definieren wir "niedrig", "mittel" und "hoch" für die
für die Kraftstoffeffizienz, und unser Modell wird daher ein drei- elementiges Softmax-Array ausgeben, das die Wahrscheinlichkeit
max-Array aus, das die Wahrscheinlichkeit angibt, mit der ein bestimmtes Auto der jeweiligen Klasse entspricht. Unter Verwendung von
der gleichen Median-Basiseingabe wie oben, gibt unser Klassifizierungsmodell nun das Folgende als
die folgende Vorhersage als Grundlinie:
[0.1, 0.7, 0.2]
Damit haben wir jetzt für jede Klasse einen anderen Basisvorhersagewert. Nehmen wir an
wir erstellen eine neue Vorhersage für ein Beispiel aus unserem Testsatz, und unser Modell gibt
das folgende Array aus, das mit einer Wahrscheinlichkeit von 90 % vorhersagt, dass dieses Auto einen "niedrigen" Kraftstoffverbrauch hat
Effizienz hat:
[0.9, 0.06, 0.04]
Die daraus resultierenden Merkmalszuordnungswerte sollten erklären, warum das Modell 0,9
im Vergleich zum Basisvorhersagewert von 0,1 für die Klasse "niedrig". Wir können uns auch
auch die Merkmalszuordnungswerte für die anderen Klassen betrachten, um zu verstehen, warum unser Modell zum Beispiel
Modell vorausgesagt hat, dass dasselbe Auto mit einer Wahrscheinlichkeit von 6 % zu unserer Klasse "mittlerer" Kraftstoffverbrauch
Effizienzklasse gehört.
Abbildung 7-2 zeigt die Merkmalsattributionen auf Instanzebene für ein Modell, das die
Dauer einer Fahrradtour vorhersagt. Die uninformative Basislinie für dieses Modell ist eine Fahrtdauer von
13,6 Minuten, die wir erhalten, indem wir eine Vorhersage unter Verwendung des Medianwerts für
jedes Merkmal in unserem Datensatz. Wenn die Vorhersage eines Modells geringer ist als der Basiswert
Wenn die Vorhersage eines Modells unter dem Basiswert liegt, sollten wir erwarten, dass die meisten Attributionswerte negativ sind, und umgekehrt.
In diesem Beispiel erhalten wir eine vorhergesagte Dauer von 10,71, die unter dem Modellwert liegt.

Entwurfsmuster 29: Erklärbare Vorhersagen | 331
Basislinie und erklärt, warum viele der Attributionswerte negativ sind. Wir können die wichtigsten
die wichtigsten Merkmale ermitteln, indem wir den absoluten Wert der Merkmalsattribu-
tionen. In diesem Beispiel war die Entfernung der Reise das wichtigste Merkmal, wodurch die
Vorhersage des Modells um 2,4 Minuten gegenüber der Basislinie verringert. Zusätzlich sollten wir als
Außerdem sollten wir zur Kontrolle sicherstellen, dass die Werte der Merkmalszuordnung in etwa der
Differenz zwischen der aktuellen Vorhersage und der Basisvorhersage.

Abbildung 7-2. Die Merkmalszuordnungswerte für ein einzelnes Beispiel in einem Modell zur Vorhersage der
Dauer der Fahrradfahrt. Die Grundlinie des Modells, die anhand des Medians der einzelnen Merkmalswerte
berechnet wird, beträgt 13,6 Minuten, und die Attributionswerte zeigen, wie stark jedes Merkmal die Vorhersage beeinflusst hat.
die Vorhersage beeinflusst hat.

Informative Baselines hingegen vergleichen die Vorhersage eines Modells mit einem speziellen
bestimmten alternativen Szenario. Bei einem Modell, das betrügerische Transaktionen identifiziert, könnte eine informa- tive
tive Baseline die Frage beantworten: "Warum wurde diese Transaktion als Betrug gekennzeichnet
und nicht als nicht betrügerisch?" Anstatt den Median der Merkmalswerte über den gesamten
gesamten Trainingsdatensatzes zur Berechnung der Basislinie zu verwenden, würden wir nur den Median der
nicht-betrügerischen Werte. Bei einem Bildmodell enthalten die Trainingsbilder vielleicht einen bedeutenden
einen bedeutenden Anteil an schwarzen und weißen Pixeln, und die Verwendung dieser als
ungenaue Vorhersagen ergeben. In diesem Fall müssten wir ein anderes
informatives Basisbild.

332 | Kapitel 7: Verantwortungsvolle KI

Heuristische Benchmarks und Modell-Baselines
Wie hängen Modell-Baselines mit dem Heuristic Benchmark Design Pattern zusammen? Ein heuris-
tischer Benchmark ist als Ausgangspunkt für die Zusammenfassung eines Modells auf globaler
Ebene zusammenzufassen, oft bevor Erklärbarkeit implementiert wird. Bei der Verwendung von Erklärbarkeit ist die Art
Art der Basislinie (informativ oder nicht informativ) und die Art und Weise, wie wir sie berechnen, ist
uns überlassen. Die im heuristischen Benchmark-Muster beschriebenen Techniken können auch verwendet werden
zur Bestimmung der Basislinie eines Modells für die Verwendung mit einer Erklärungsmethode verwendet werden.
Sowohl heuristische Benchmarks als auch Modell-Baselines bieten einen Rahmen für die Beantwortung
die Frage: "Warum hat das Modell X im Vergleich zu Y getan?" Heuristische Benchmarks sind
ein erster Schritt in der Modellanalyse und stellen einen möglichen Ansatz zur Berechnung einer
Grundlinie. Wenn wir in diesem Abschnitt den Begriff "Baseline" verwenden, beziehen wir uns speziell auf
den Wert, der bei Erklärungsmethoden als Bezugspunkt verwendet wird.
SHAP

Die Open-Source-Bibliothek SHAP bietet eine Python-API zum Abrufen von Merkmalszuweisungen
für viele Arten von Modellen und basiert auf dem Konzept des Shapley-Werts, das in
Tabelle 7-2. Um die Werte für die Feature-Attribution zu bestimmen, berechnet SHAP, wie viel
das Hinzufügen oder Entfernen jedes Merkmals zur Vorhersageleistung eines Modells beiträgt. Es durch-
diese Analyse über viele verschiedene Kombinationen von Merkmalswerten und Modell
Ausgabe.

SHAP ist rahmenunabhängig und arbeitet mit Modellen, die auf Bild-, Text- oder Tabellendaten
Daten trainiert wurden. Um zu sehen, wie SHAP in der Praxis funktioniert, verwenden wir den zuvor erwähnten
referenziert. Diesmal erstellen wir ein Deep Model mit der Keras Sequential
API:

model = tf.keras.Sequential([
tf.keras.layers.Dense(16, input_shape=(len(x_train.iloc[0])),
tf.keras.layers.Dense(16, activation='relu'),
tf.keras.layers.Dense(1)
])
Um SHAP zu verwenden, erstellen wir zunächst ein DeepExplainer-Objekt, dem wir unser Modell und eine
Untermenge von Beispielen aus unserem Trainingssatz übergeben. Dann erhalten wir die Attributionswerte für die
ersten 10 Beispiele in unserem Testsatz:

shap importieren
explainer = shap.DeepExplainer(model, x_train[:100])
attribution_values = explainer.shap_values(x_test.values[:10])
Entwurfsmuster 29: Erklärbare Vorhersagen | 333
SHAP verfügt über einige eingebaute Visualisierungsmethoden, die das Verständnis der
resultierenden Attributionswerte zu verstehen. Wir werden SHAPs force_plot()-Methode verwenden, um die Attributionswerte für das erste Beispiel in
Attributionswerte für das erste Beispiel in unserem Testset mit folgendem Code darzustellen:

shap.force_plot(
explainer.expected_value[0],
shap_values[0][0,:],
x_test.iloc[0,:]
)
In dem obigen Code ist explainer.expected_value die Basislinie unseres Modells. SHAP berechnet
die Basislinie als den Mittelwert der Modellausgabe über den Datensatz, den wir bei der
übergeben haben (in diesem Fall x_train[:100]), wir könnten aber auch unseren eigenen
einen eigenen Basislinienwert an force_plot übergeben. Der Grundwahrheitswert für dieses Beispiel
ist 14 Meilen pro Gallone, und unser Modell sagt 13,16 voraus. Unsere Erklärung wird daher
die Vorhersage unseres Modells von 13,16 mit Merkmalszuordnungswerten erklären. In diesem Fall,
beziehen sich die Attributionswerte auf den Basiswert des Modells von 24,16 MPG. Die Attribu-
Attributionswerte sollten sich daher zu etwa 11 addieren, der Differenz zwischen der Basislinie des Modells
der Differenz zwischen der Basislinie des Modells und der Vorhersage für dieses Beispiel. Wir können die wichtigsten Merkmale identifizieren.
die wichtigsten Merkmale, indem wir uns die Merkmale mit dem höchsten absoluten Wert ansehen. Abbildung 7-3 zeigt die
Abbildung 7-3 zeigt die resultierende Darstellung für die Attributionswerte dieses Beispiels.

Abbildung 7-3. Die Merkmalszuordnungswerte für ein Beispiel aus unserem Kraftstoffverbrauchsvorhersagemodell
vorhersagemodells. In diesem Fall ist das Gewicht des Fahrzeugs der wichtigste Indikator für den Kraftstoffverbrauch.
mit einem Merkmalszuweisungswert von etwa 6. Wäre die Vorhersage unseres Modells über
über dem Basiswert von 24,16 gelegen, würden wir stattdessen überwiegend negative Attributionswerte sehen.

In diesem Beispiel ist der wichtigste Indikator für die Kraftstoffeffizienz das Gewicht, wodurch
die Vorhersage unseres Modells um ca. 6 MPG gegenüber dem Basiswert verringert. Danach folgen
Pferdestärken, Hubraum und dann das Modelljahr des Fahrzeugs. Wir können eine Zusammenfassung (oder
(oder eine globale Erklärung) der Merkmalszuordnungswerte für die ersten 10 Beispiele aus
unserem Testsatz mit dem Folgenden:

shap.summary_plot(
shap_values,
feature_names=data.columns.tolist(),
class_names=['MPG']
)
Das Ergebnis ist die in Abbildung 7-4 gezeigte zusammenfassende Darstellung.

334 | Kapitel 7: Verantwortungsvolle KI

In der Praxis würden wir einen größeren Datensatz haben und die Attribute auf globaler Ebene für mehr Beispiele berechnen wollen.
für mehr Beispiele berechnen. Diese Analyse könnten wir dann nutzen, um das Verhalten
Verhalten unseres Modells für andere Interessengruppen innerhalb und außerhalb unserer Organisation zusammenfassen.

Abbildung 7-4. Ein Beispiel für die Zuordnung von Merkmalen auf globaler Ebene für das Modell der Kraftstoffeffizienz,
berechnet anhand der ersten 10 Beispiele aus dem Testdatensatz.

Erklärungen aus eingesetzten Modellen

SHAP bietet eine intuitive API zum Abrufen von Attributen in Python, die typischerweise in einer
Skript- oder Notebook-Umgebung. Dies funktioniert gut während der Modellentwicklung, aber
aber es gibt Szenarien, in denen Sie Erklärungen zu einem eingesetzten Modell
zusätzlich zu den Vorhersageergebnissen des Modells. In diesem Fall sind Cloud-basierte Erklärbarkeitstools
Tools die beste Option. Hier demonstrieren wir, wie man Feature-Attributionen für ein
Modell mit Explainable AI von Google Cloud zu erhalten. Zum Zeitpunkt der Erstellung dieses Artikels,
funktioniert Explainable AI mit benutzerdefinierten TensorFlow-Modellen und tabellarischen Datenmodellen, die
mit AutoML.

Wir werden ein Bildmodell auf AI Platform einsetzen, um Erklärungen zu zeigen, aber wir könnten auch
Explainable AI mit TensorFlow-Modellen verwenden, die auf tabellarischen oder Textdaten trainiert wurden. Um zu beginnen,
werden wir ein TensorFlow Hub-Modell einsetzen, das auf dem ImageNet-Datensatz trainiert wurde. So können wir
wir uns auf die Aufgabe konzentrieren können, Erklärungen zu erhalten, werden wir kein Transfer-Lernen auf
und verwenden die ursprünglichen 1.000 Label-Klassen von ImageNet:

model = tf.keras.Sequential([
hub.KerasLayer(".../mobilenet_v2/classification/2",
input_shape=(224,224,3)),
tf.keras.layers.Softmax()
])
Entwurfsmuster 29: Erklärbare Vorhersagen | 335
Um ein Modell mit Erklärungen auf der AI Platform bereitzustellen, müssen wir zunächst eine Meta-
Datendatei erstellen, die vom Erklärungsdienst zur Berechnung der Merkmalszuweisungen verwendet wird.
Diese Metadaten werden in einer JSON-Datei bereitgestellt und enthalten Informationen über die Basislinie
die wir verwenden möchten, und die Teile des Modells, die wir erklären wollen. Zur Vereinfachung dieses
Prozess zu vereinfachen, stellt Explainable AI ein SDK zur Verfügung, das die Metadaten mit folgendem Code generiert
folgenden Code erzeugt:

from explainable_ai_sdk.metadata.tf.v2 import SavedModelMetadataBuilder
model_dir = 'pfad/zu/savedmodel/dir'
model_builder = SavedModelMetadataBuilder(model_dir)
model_builder.set_image_metadata('input_tensor_name')
model_builder.save_metadata(model_dir)
In diesem Code wurde keine Modell-Basislinie angegeben, was bedeutet, dass die Standardwerte verwendet werden (für Bildmodelle
Modelle ist dies ein Schwarz-Weiß-Bild). Wir können optional einen input_baselines
Parameter zu set_image_metadata hinzufügen, um eine benutzerdefinierte Grundlinie anzugeben. Die Ausführung der
save_metadata-Methode wird eine explanation_metadata.json-Datei in einem
model-Verzeichnis (der vollständige Code befindet sich im GitHub-Repository).

Wenn wir dieses SDK über AI Platform Notebooks verwenden, haben wir auch die Möglichkeit, Erklärungen
Erklärungen lokal in einer Notebook-Instanz zu erstellen, ohne unser Modell in der
Wolke. Dies ist über die Methode load_model_from_local_path möglich.

Mit unserem exportierten Modell und der Datei explanation_metadata.json in einem Storage Bucket,
sind wir bereit, eine neue Modellversion zu erstellen. Dabei geben wir die Erklärungsmethode
Erklärungsmethode, die wir verwenden möchten.

Um unser Modell auf AI Platform bereitzustellen, können wir unser Modellverzeichnis in einen Cloud
Storage Bucket kopieren und die gcloud CLI verwenden, um eine Modellversion zu erstellen. AI Platform hat
drei mögliche Erklärungsmethoden zur Auswahl:

Integrierte Farbverläufe (IG)
Dies implementiert die im IG-Papier vorgestellte Methode und funktioniert mit jedem diffe- renzierbaren
TensorFlow-Modell - Bild, Text oder tabellarisch. Für Bildmodelle
die auf der AI Platform eingesetzt werden, liefert IG ein Bild mit hervorgehobenen Pixeln, die
die Regionen, die die Vorhersage des Modells signalisiert haben.

Abgetasteter Shapley
Basierend auf dem Sampled Shapley-Papier wird hier ein ähnlicher Ansatz wie bei der Open-Source
quelloffenen SHAP-Bibliothek. Auf AI Platform können wir diese Methode mit tabellarischen und
Text-TensorFlow-Modellen verwenden. Denn IG funktioniert nur mit differenzierbaren Modellen,
verwendet AutoML Tables Sampled Shapley zur Berechnung von Merkmalszuweisungen für alle
Modelle.

336 | Kapitel 7: Verantwortungsvolle KI

6 Weitere Einzelheiten zu diesen Erklärungsmethoden und ihrer Umsetzung finden Sie im Whitepaper Explainable AI.
XRAI
Dieser Ansatz baut auf IG auf und wendet Glättung an, um regionsbasierte
Zuschreibungen. XRAI funktioniert nur mit Bildmodellen, die auf der KI-Plattform bereitgestellt werden.
In unserem gcloud-Befehl geben wir die Erklärungsmethode an, die wir verwenden möchten, sowie
die wir verwenden möchten, sowie die Anzahl der Integralschritte oder Pfade, die die Methode bei der Berechnung von Attributionswerten verwenden soll.
6 Der Parameter steps bezieht sich auf die Anzahl der Merkmalskombinationen, die für jede Ausgabe
Nationen, die für jede Ausgabe abgetastet werden. Im Allgemeinen wird eine Erhöhung dieser Zahl die
Erklärungsgenauigkeit:

!gcloud beta ai-platform versions create $VERSION_NAME \
--model $MODEL_NAME \
--origin $GCS_VERSION_LOCATION \
--runtime-version 2.1 \
--Rahmenwerk TENSORFLOW \
--python-version 3.7 \
--machine-type n1-standard-4 \
--explanation-method xrai \
--num-integral-steps 25
Sobald das Modell bereitgestellt ist, können wir mithilfe des Explainable AI SDK Erklärungen erhalten:

model = explainable_ai_sdk.load_model_from_ai_platform(
GCP_PROJECT,
MODEL_NAME,
VERSION_NAME
)
request = model.explain([test_img])
# Bild mit Pixelattributionen ausgeben
request[0].visualize_attributions()
In Abbildung 7-5 sehen wir einen Vergleich der IG- und XRAI-Erklärungen, die von Explainable AI
von Explainable AI für unser ImageNet-Modell. Die hervorgehobenen Pixelregionen zeigen die
Pixel, die am meisten zur Vorhersage von "Husky" durch unser Modell beigetragen haben.

IG wird in der Regel für "nicht natürliche" Bilder empfohlen, z. B. für Aufnahmen in der Medizin,
Fabrik- oder Laborumgebung. XRAI eignet sich in der Regel am besten für Bilder, die in natürlicher
Umgebungen wie bei diesem Husky. Um zu verstehen, warum IG für nicht natürliche Bilder bevorzugt wird
natürliche Bilder bevorzugt wird, sehen Sie sich die IG-Attribute für das Bild der diabetischen Retinopathie in
Abbildung 7-6. In Fällen wie diesem medizinischen Bild ist es hilfreich, die Attributionen auf einer feinkörnigen,
Pixelebene zu sehen. Bei dem Hundebild hingegen ist es hilfreich, die genauen Pixel zu kennen, die
die unser Modell zur Vorhersage von "Husky" veranlasst haben, weniger wichtig, und XRAI liefert uns eine übergeordnete
Zusammenfassung der wichtigen Regionen.

Entwurfsmuster 29: Erklärbare Vorhersagen | 337
Abbildung 7-5. Die von Explainable AI zurückgegebenen Feature-Attribute für ein ImageNet
Modell, das auf der AI Platform bereitgestellt wurde. Auf der linken Seite ist das Originalbild zu sehen. Die IG-Attribute
sind in der Mitte zu sehen und die XRAI-Attribute sind rechts zu sehen. Der Schlüssel
unten zeigt, was die Regionen in XRAI bedeuten - hellere Regionen sind die wichtigsten
die wichtigsten und die dunkleren Bereiche die unwichtigsten Regionen.

Abbildung 7-6. Im Rahmen einer Studie von Rory Sayres und Kollegen im Jahr 2019 wurden verschiedene Gruppen von
Ophthalmologen gebeten, den Grad der DR auf einem Bild in drei Szenarien zu bewerten: das Bild selbst ohne
Szenarien zu bewerten: das Bild selbst ohne Modellvorhersagen, das Bild mit Modellvorhersagen,
und das Bild mit Vorhersagen und Pixelzuweisungen (hier gezeigt). Wir können sehen, wie
Pixelzuordnungen das Vertrauen in die Vorhersage des Modells erhöhen können.

338 | Kapitel 7: Verantwortungsvolle KI

Explainable AI arbeitet auch in AutoML Tables, einem Tool zum Trainieren
und Einsatz von tabellarischen Datenmodellen. AutoML Tables übernimmt die Daten
Datenvorverarbeitung und wählt das beste Modell für unsere Daten aus, was bedeutet
wir müssen keinen Modellcode schreiben. Feature-Attribute
durch Explainable AI sind standardmäßig für Modelle aktiviert, die in
AutoML Tables trainierten Modelle standardmäßig aktiviert, und es werden sowohl globale als auch
werden bereitgestellt.
Kompromisse und Alternativen
Erklärungen bieten zwar einen wichtigen Einblick in die Entscheidungsfindung eines Modells,
sind sie nur so gut wie die Trainingsdaten des Modells, die Qualität Ihres Modells und die
gewählte Basislinie. In diesem Abschnitt werden wir einige Einschränkungen der Erklärbarkeit erörtern, sowie
sowie einige Alternativen zu Merkmalszuweisungen.

Verzerrung der Datenauswahl

Es wird oft gesagt, dass maschinelles Lernen "Garbage in, garbage out" ist. Mit anderen Worten, ein
Modell ist nur so gut wie die Daten, mit denen es trainiert wird. Wenn wir ein Bildmodell trainieren, um
10 verschiedene Katzenrassen zu identifizieren, sind diese 10 Katzenrassen alles, was es kennt. Wenn wir dem Modell
ein Bild eines Hundes zeigt, kann es nur versuchen, den Hund in eine der 10 Katzenkategorien einzuordnen
auf die es trainiert worden ist. Es kann dies sogar mit hoher Sicherheit tun. Das bedeutet, dass Modelle
sind eine direkte Darstellung ihrer Trainingsdaten.

Wenn wir die Unausgewogenheit der Daten vor dem Training eines Modells nicht erkennen, können Erklärungsmethoden wie
Feature-Attributionen helfen, Verzerrungen bei der Datenauswahl aufzudecken. Ein Beispiel: Nehmen wir an
wir erstellen ein Modell zur Vorhersage des Bootstyps auf einem Bild. Nehmen wir an, es
beschriftet ein Bild aus unserem Testsatz korrekt als "Kajak", aber mit Hilfe von Feature-Attributionen,
finden wir heraus, dass sich das Modell auf das Paddel des Bootes verlässt, um "Kajak" vorherzusagen, und nicht
die Form des Bootes. Dies ist ein Zeichen dafür, dass unser Datensatz möglicherweise nicht genügend Variationen in den Trainingsbildern für jede Klasse aufweist.
Trainingsbilder für jede Klasse enthält - wir müssen wahrscheinlich zurückgehen und mehr
Bilder von Kajaks in verschiedenen Winkeln, sowohl mit als auch ohne Paddel.

Kontrafaktische Analyse und Erklärungen anhand von Beispielen

Zusätzlich zu den Merkmalszuweisungen - die im Abschnitt "Lösungen" beschrieben werden - gibt es
gibt es viele andere Ansätze, um die Ergebnisse von ML-Modellen zu erklären. Dieser Abschnitt ist nicht
soll keine erschöpfende Liste aller Erklärungsmethoden bieten, da sich dieser Bereich
sich schnell weiterentwickelt. Hier werden zwei weitere Ansätze kurz beschrieben: die kontrafaktische
Analyse und beispielbasierte Erklärungen.

Die kontrafaktische Analyse ist eine Erklärungsmethode auf Instanzebene, die sich darauf bezieht
Beispiele aus unserem Datensatz mit ähnlichen Merkmalen zu finden, die zu anderen Vorhersagen
Vorhersagen aus unserem Modell ergeben. Eine Möglichkeit, dies zu tun, ist das What-If Tool, ein Open
Source-Tool zur Bewertung und Visualisierung der Ergebnisse von ML-Modellen. Wir bieten eine

Entwurfsmuster 29: Erklärbare Vorhersagen | 339
Ein ausführlicherer Überblick über das WENN-Tool im Fairness-Objektiv-Entwurfsmuster.
konzentrieren wir uns hier speziell auf die Funktion der kontrafaktischen Analyse. Bei der Visualisierung
Datenpunkte aus unserem Testsatz im WENN-Tool visualisiert werden, haben wir die Möglichkeit, die
Option, den kontrafaktischen Datenpunkt anzuzeigen, der dem ausgewählten Punkt am nächsten liegt. Auf diese Weise können wir
Merkmalswerte und Modellvorhersagen für diese beiden Datenpunkte vergleichen, was
Dies kann uns helfen, die Merkmale, auf die sich unser Modell am meisten verlässt, besser zu verstehen. In Abbildung 7-7 sehen wir
sehen wir einen kontrafaktischen Vergleich für zwei Datenpunkte aus einem Hypothekenantrag
Datensatz. In Fettdruck sind die Merkmale dargestellt, in denen sich die beiden Datenpunkte unterscheiden, und
unten sehen wir die Modellausgabe für beide.

Beispielbasierte Erklärungen vergleichen neue Beispiele und die dazugehörigen Vorhersagen
tionen mit ähnlichen Beispielen aus unserem Trainingsdatensatz. Diese Art der Erklärung ist besonders
besonders nützlich, um zu verstehen, wie unser Trainingsdatensatz das Modellverhalten beeinflusst.
Beispielbasierte Erklärungen funktionieren am besten bei Bild- oder Textdaten und können intuitiver sein
als Merkmalszuweisungen oder kontrafaktische Analysen, da sie die Vorhersage eines Modells
Vorhersage eines Modells direkt auf die für das Training verwendeten Daten abbilden.

340 | Kapitel 7: Verantwortungsvolle KI

7 Weitere Einzelheiten zu Quick, Draw! und Erklärungen anhand von Beispielen finden Sie in diesem Dokument.
Abbildung 7-7. Kontrafaktische Analyse im WENN-Tool für zwei Datenpunkte aus einem US
Hypothekenantragsdatensatz. Die Unterschiede zwischen den beiden Datenpunkten sind fett gedruckt.
Weitere Informationen zu diesem Datensatz finden Sie in der Diskussion über das Fairness-Objektiv
in diesem Kapitel.

Um diesen Ansatz besser zu verstehen, schauen wir uns das Spiel Quick, Draw an!^7 Das Spiel
fordert die Spieler auf, einen Gegenstand zu zeichnen, und errät in Echtzeit, was sie zeichnen, indem es ein
tiefes neuronales Netzwerk, das anhand von Tausenden von Zeichnungen anderer Spieler trainiert wurde. Nachdem die Spieler
Nachdem die Spieler eine Zeichnung beendet haben, können sie sehen, wie das neuronale Netz zu seiner Vorhersage gekommen ist, indem sie sich

Entwurfsmuster 29: Erklärbare Vorhersagen | 341
Beispiele aus dem Trainingsdatensatz. In Abbildung 7-8 sehen wir die beispielbasierten
Erklärungen für eine Zeichnung von Pommes frites, die das Modell erfolgreich erkannt hat.

Abbildung 7-8. Beispielbasierte Erklärungen aus dem Spiel Quick, Draw! zeigen, wie das
Modell anhand von Beispielen aus dem Trainingsdatensatz korrekt "Pommes frites" für die gegebene Zeichnung
dem Trainingsdatensatz.

Beschränkungen der Erklärungen

Die Erklärbarkeit stellt eine erhebliche Verbesserung des Verständnisses und der Interpretation von Modellen dar.
Modelle, aber wir sollten vorsichtig sein, wenn wir den Erklärungen unseres Modells zu viel Vertrauen schenken
Erklärungen unseres Modells zu vertrauen oder anzunehmen, dass sie einen perfekten Einblick in ein Modell bieten. Erklärungen in
Erklärungen in jeglicher Form sind ein direktes Spiegelbild unserer Trainingsdaten, des Modells und der ausgewählten Basisdaten.
Das heißt, wir können nicht erwarten, dass unsere Erklärungen von hoher Qualität sind, wenn unsere Trainingsdaten
Trainingsdatensatz eine ungenaue Darstellung der Gruppen ist, die von unserem Modell widergespiegelt werden, oder wenn die
Basislinie, die wir gewählt haben, nicht gut für das Problem funktioniert, das wir lösen wollen.

Außerdem ist die Beziehung, die die Erklärungen zwischen den Merkmalen eines Modells
und dem Ergebnis nur für unsere Daten und unser Modell repräsentativ und nicht unbedingt für die

342 | Kapitel 7: Verantwortungsvolle KI

Umgebung außerhalb dieses Kontexts. Nehmen wir zum Beispiel an, wir trainieren ein Modell zur Erkennung
um betrügerische Kreditkartentransaktionen zu erkennen, und es findet als Merkmal auf globaler Ebene
dass der Betrag einer Transaktion das Merkmal ist, das am ehesten auf Betrug hindeutet. Nach
wäre es falsch, daraus zu schließen, dass der Betrag immer der größte Indikator für
Kreditkartenbetrug ist - dies ist nur im Zusammenhang mit unserem Trainingsdatensatz der Fall,
Modells und des festgelegten Basiswerts.

Wir können Erklärungen als eine wichtige Ergänzung zu Genauigkeit, Fehler und anderen
Metriken, die zur Bewertung von ML-Modellen verwendet werden. Sie bieten nützliche Einblicke in die Qualität eines Modells
Qualität und mögliche Verzerrungen eines Modells, sollten aber nicht das einzige Kriterium für ein hochwertiges Modell sein.
Wir empfehlen, Erklärungen als ein Kriterium der Modellevaluierung zusätzlich zu
Daten- und Modellevaluierung sowie viele der anderen in diesem und den vorangegangenen Kapiteln skizzierten
und vorherigen Kapiteln beschrieben sind.

Entwurfsmuster 30: Fairness-Objektiv
Das Entwurfsmuster Fairness Lens schlägt die Verwendung von Vor- und Nachverarbeitungs
Techniken vor, um sicherzustellen, dass die Modellvorhersagen für verschiedene
Gruppen von Nutzern und Szenarien. Fairness beim maschinellen Lernen ist ein sich ständig weiterentwickelndes
Forschungsgebiet, und es gibt keine allgemeingültige Lösung oder Definition, die ein
Modell "fair" zu machen. Die Bewertung eines gesamten ML-Workflows - von der Datenerfassung bis zur
der Datenerhebung bis zum Einsatz des Modells - ist für die Entwicklung erfolgreicher und
Modelle von hoher Qualität.

Problem
Mit dem Wort "Maschine" im Namen ist es einfach anzunehmen, dass ML-Modelle nicht voreingenommen sein können.
voreingenommen sein können. Schließlich sind Modelle das Ergebnis von Mustern, die von einem Computer gelernt wurden, oder? Das
Problem bei dieser Denkweise ist, dass die Datensätze, aus denen Modelle lernen, von
Menschen erstellt werden, nicht von Maschinen, und Menschen sind voller Voreingenommenheit. Diese inhärente menschliche Voreingenommenheit ist
unvermeidlich, aber nicht unbedingt immer schlecht. Nehmen Sie zum Beispiel einen Datensatz, der zum Trainieren
ein Modell zur Erkennung von Finanzbetrug zu trainieren - diese Daten werden wahrscheinlich sehr unausgewogen sein und
sehr wenige betrügerische Beispiele enthalten, da Betrug in den meisten Fällen relativ selten ist. Dies ist ein
Beispiel für eine natürlich auftretende Verzerrung, da sie die statistischen Eigenschaften des
des Originaldatensatzes widerspiegelt. Voreingenommenheit ist dann schädlich, wenn sie sich auf verschiedene Gruppen von Menschen
unterschiedlich auswirken. Dies wird als problematische Voreingenommenheit bezeichnet, und darauf werden wir uns
in diesem Abschnitt. Wenn diese Art von Verzerrung nicht berücksichtigt wird, kann sie ihren Weg
in die Modelle einfließen und negative Auswirkungen haben, da die Produktionsmodelle die Verzerrungen in den Daten
in den Daten widerspiegeln.

Problematische Voreingenommenheit gibt es auch in Situationen, in denen man sie nicht erwartet. Zum Beispiel
Beispiel: Stellen Sie sich vor, wir bauen ein Modell zur Identifizierung verschiedener Arten von Kleidung und
Accessoires. Wir haben die Aufgabe, alle Schuhbilder für den Trainingsdatensatz zu sammeln.
Datensatz zu sammeln. Wenn wir an Schuhe denken, merken wir uns das erste, was uns einfällt

Entwurfsmuster 30: Fairness-Objektiv | 343
8 Für einen detaillierteren Blick darauf, wie rassistische und geschlechtsspezifische Vorurteile in Bildklassifizierungsmodelle einfließen können, siehe
Joy Buolamwini und Timmit Gebru, "Gender Shades: Intersectional Accuracy Disparities in Commercial
Gender Classification", Proceedings of Machine Learning Research 81 (2018): 1-15.
mind. Ist es ein Tennisschuh? Loafer? Flip Flop? Wie wäre es mit einem Stöckelschuh? Stellen wir uns vor, dass
wir in einem Klima leben, in dem es das ganze Jahr über warm ist, und die meisten Leute, die wir kennen, tragen
tragen ständig Sandalen. Wenn wir an einen Schuh denken, ist eine Sandale das erste, was uns einfällt.
in den Sinn. Infolgedessen sammeln wir eine Vielzahl von Bildern von Sandalen mit unterschiedlichen
Arten von Riemen, Sohlenstärken, Farben und mehr. Wir fügen diese dem größeren
Bekleidungsdatensatz bei, und wenn wir das Modell an einem Testsatz von Bildern der Schuhe unserer Freunde testen
Schuhe unseres Freundes testen, erreicht es eine Genauigkeit von 95 % bei der Bezeichnung "Schuh". Das Modell sieht vielversprechend aus, aber
Probleme treten auf, wenn unsere Kollegen aus verschiedenen Orten das Modell mit Bildern ihrer
ihrer Absätze und Turnschuhe testen. Für ihre Bilder wird die Bezeichnung "Schuh" überhaupt nicht zurückgegeben.

Dieses Beispiel zeigt eine Verzerrung in der Verteilung der Trainingsdaten, und obwohl
Auch wenn es übertrieben vereinfacht erscheinen mag, kommt diese Art von Verzerrung in der Produktion häufig vor.
Eine Verzerrung der Datenverteilung tritt auf, wenn die von uns gesammelten Daten nicht genau die
gesamten Bevölkerung widerspiegeln, die unser Modell verwenden wird. Wenn unser Datensatz menschenzentriert ist, kann diese Art
kann diese Art von Verzerrung besonders deutlich werden, wenn unser Datensatz keine gleichmäßige Vertretung
Altersgruppen, Rassen, Geschlechter, Religionen, sexuellen Orientierungen und anderen Identitätsmerkmalen
tics.^8

Selbst wenn unser Datensatz im Hinblick auf diese Identitätsmerkmale ausgewogen erscheint
ist, kann es dennoch zu Verzerrungen in der Art und Weise kommen, wie diese Gruppen in den Daten dargestellt werden.
Angenommen, wir trainieren ein Stimmungsanalysemodell, um Restaurantbewertungen auf einer
Skala von 1 (extrem negativ) bis 5 (extrem positiv) zu klassifizieren. Wir haben darauf geachtet, dass wir eine ausgewogene
eine ausgewogene Darstellung der verschiedenen Restauranttypen in den Daten zu erhalten. Es zeigt sich jedoch
Es zeigt sich jedoch, dass die Mehrheit der Bewertungen für Fischrestaurants positiv ist, während die meisten
die meisten Bewertungen vegetarischer Restaurants negativ sind. Diese Verzerrung der Datenrepräsentation wird
direkt durch unser Modell dargestellt. Wann immer neue Bewertungen für vegetarische Restaurants hinzugefügt werden
Restaurants hinzugefügt werden, ist die Wahrscheinlichkeit, dass sie als negativ eingestuft werden, sehr viel höher, was
die Wahrscheinlichkeit beeinflussen könnte, dass jemand eines dieser Restaurants in Zukunft
Zukunft zu besuchen. Dies wird auch als Berichtsverzerrung bezeichnet, da der Datensatz (hier die "berichteten" Daten)
Daten) die reale Welt nicht genau widerspiegelt.

Ein weit verbreiteter Irrtum beim Umgang mit Datenverzerrungen ist, dass das Entfernen der verzerrten Bereiche
Verzerrungen aus einem Datensatz das Problem beheben. Nehmen wir an, wir erstellen ein Modell zur Vorhersage
die Wahrscheinlichkeit, dass jemand einen Kredit nicht bedienen kann. Wenn wir feststellen, dass das Modell Menschen
Menschen unterschiedlicher Rassen ungerecht behandelt, könnten wir annehmen, dass dies durch einfaches Entfernen
Rasse als Merkmal aus dem Datensatz entfernt. Das Problem dabei ist, dass aufgrund systemischer Verzerrungen
Merkmale wie Rasse und Geschlecht sich oft implizit in anderen Merkmalen wie
Postleitzahl oder Einkommen. Dies wird als implizite oder stellvertretende Voreingenommenheit bezeichnet. Das Entfernen von offensichtlichen Merkmalen

344 | Kapitel 7: Verantwortungsvolle KI

mit potenzieller Voreingenommenheit, wie Rasse und Geschlecht, kann oft schlimmer sein, als wenn man sie
als wenn man sie weglässt, da es dadurch schwieriger wird, Fälle von Voreingenommenheit im Modell zu erkennen und zu korrigieren.

Bei der Erhebung und Aufbereitung von Daten kann ein weiterer Bereich, in dem Verzerrungen auftreten, die Art und Weise sein
der Art und Weise, wie die Daten beschriftet werden. Teams lagern die Beschriftung großer Datensätze oft aus, aber es ist
Es ist jedoch wichtig, darauf zu achten, wie die Beschrifter einen Datensatz verzerren können,
insbesondere wenn die Beschriftung subjektiv ist. Dies wird als Experimentatorverzerrung bezeichnet. Stellen Sie sich vor:
wir bauen ein Stimmungsanalysemodell auf und haben die Beschriftung an eine Gruppe von
Gruppe von 20 Personen ausgelagert, die jeden Text auf einer Skala von 1 (negativ) bis 5 (positiv) bewerten sollen.
(negativ) bis 5 (positiv) zu bewerten. Diese Art der Analyse ist äußerst subjektiv und kann von der
von der Kultur, der Erziehung und vielen anderen Faktoren beeinflusst werden. Bevor wir diese Daten zum
Daten zum Trainieren unseres Modells verwenden, sollten wir sicherstellen, dass diese Gruppe von 20 Etikettierern eine vielfältige
Bevölkerung widerspiegelt.

Neben den Daten kann auch die gewählte Zielfunktion während des Modelltrainings zu Verzerrungen führen.
gewählte Zielfunktion. Wenn wir zum Beispiel unser Modell für die Gesamtgenauigkeit optimieren,
spiegelt dies möglicherweise nicht genau die Leistung des Modells in allen Datenbereichen wider. In Fällen
unausgewogenen Datensätzen kann die Verwendung der Genauigkeit als einzige Metrik
Fälle übersehen, in denen unser Modell unterdurchschnittlich abschneidet oder unfaire Entscheidungen über
Minderheitsklassen in unseren Daten trifft.

In diesem Buch haben wir gesehen, dass ML die Produktivität verbessern kann,
Geschäftswert zu steigern und Aufgaben zu automatisieren, die zuvor manuell ausgeführt wurden. Als Data-Scien-
als Datenwissenschaftler und ML-Ingenieure haben wir eine gemeinsame Verantwortung dafür, dass die von uns erstellten Modelle
dass die von uns erstellten Modelle keine nachteiligen Auswirkungen auf die Bevölkerung haben, die sie verwendet.

Lösung
Um problematische Verzerrungen beim maschinellen Lernen in den Griff zu bekommen, brauchen wir Lösungen sowohl für die Identifizierung
Bereiche mit schädlichen Verzerrungen in den Daten zu identifizieren, bevor ein Modell trainiert wird, und unser trainiertes
Modells durch eine Fairness-Linse. Das Fairness Lens Design Pattern bietet Ansätze
für die Erstellung von Datensätzen und Modellen, die alle Gruppen von Nutzern gleich behandeln. Wir demonstrieren
Techniken für beide Arten der Analyse anhand des WENN-Tools, eines Open-Source
Open-Source-Tool für Datensatz- und Modellevaluierung, das in vielen Python-Notebook-Umgebungen
Umgebungen ausgeführt werden kann.

Entwurfsmuster 30: Fairness-Objektiv | 345
Bevor Sie mit den in diesem Abschnitt beschriebenen Tools fortfahren, sollten Sie
lohnt es sich, den Datensatz und die Vorhersageaufgabe zu analysieren, um festzustellen
um festzustellen, ob es ein Potenzial für problematische Verzerrungen gibt. Dazu muss man sich
Dazu muss man sich genauer ansehen, wer von einem Modell betroffen ist und wie diese
Gruppen betroffen sein werden. Wenn eine problematische Verzerrung wahrscheinlich erscheint, können die in diesem
nischen Ansätze, die in diesem Abschnitt beschrieben werden, einen guten Ausgangspunkt
einen guten Ausgangspunkt für die Abschwächung dieser Art von Verzerrung. Wenn andererseits die
Verzerrung im Datensatz natürlich vorkommende Verzerrungen enthält, die keine
nachteilige Auswirkungen auf verschiedene Personengruppen hat, kann "Entwurfsmuster
10: Rebalancing " auf Seite 122 in Kapitel 3 Lösungen für den
Umgang mit Daten, die von Natur aus unausgewogen sind.
In diesem Abschnitt beziehen wir uns auf einen öffentlichen Datensatz von US-Hypothekenanträgen.
kationen. Kreditinstitute in den USA sind verpflichtet, Informationen über einen individuellen
wie die Art des Kredits, das Einkommen des Antragstellers, die Agentur, die den
und den Status des Antrags. Wir werden ein Modell zur Kreditantragsgenehmigung
Modell auf diesem Datensatz trainieren, um verschiedene Aspekte der Fairness zu demonstrieren. Nach unserem
wird dieser Datensatz unseres Wissens nach von keiner Kreditagentur zum Trainieren von ML-Modellen verwendet, und
daher sind die von uns aufgeworfenen Fairness-Fehler nur hypothetisch.

Wir haben eine Teilmenge dieses Datensatzes erstellt und einige Vorverarbeitungen vorgenommen, um ihn in ein
ein binäres Klassifizierungsproblem zu machen - ob ein Antrag genehmigt oder abgelehnt wurde. Unter
Abbildung 7-9 ist eine Vorschau des Datensatzes zu sehen.

Abbildung 7-9. Eine Vorschau auf einige Spalten aus dem US-Hypothekenantragsdatensatz, auf den
auf den in diesem Abschnitt Bezug genommen wird.

Vor der Ausbildung

Da ML-Modelle eine direkte Darstellung der Daten sind, mit denen sie trainiert werden, ist es
ist es möglich, vor der Erstellung oder dem Training eines Modells einen erheblichen Teil der Verzerrungen zu verringern
durch eine gründliche Datenanalyse und die Verwendung der Ergebnisse dieser Analyse zur Anpassung
unserer Daten. Konzentrieren Sie sich in dieser Phase auf die Identifizierung von Verzerrungen bei der Datenerfassung oder Datendarstellung
die im Abschnitt "Probleme" beschrieben wurden. Tabelle 7-3 zeigt einige Fragen, die für
für jede Art von Verzerrung, je nach Datentyp.

346 | Kapitel 7: Verantwortungsvolle KI

Tabelle 7-3. Beschreibungen der verschiedenen Arten von Datenverzerrungen

Definition Überlegungen zur Analyse
Verteilung der Daten
Verzerrung
Daten, die nicht eine
gleichmäßige Vertretung aller
möglichen Gruppen, die das Modell
das Modell in der Produktion verwenden
Enthalten die Daten einen ausgewogenen Satz von Beispielen in allen relevanten
demografischen Merkmalen (Geschlecht, Alter, Rasse, Religion usw.)?
Enthält jede Bezeichnung in den Daten eine ausgewogene Aufteilung aller möglichen
Variationen dieser Bezeichnung? (z. B. das Schuhbeispiel im Abschnitt Problem
Abschnitt.)
Daten
Darstellung
Verzerrung
Daten, die gut ausgewogen sind,
aber nicht repräsentieren
verschiedene Teile der Daten
gleichmäßig
Sind bei Klassifizierungsmodellen die Kennzeichnungen für die relevanten Merkmale ausgewogen?
Zum Beispiel in einem Datensatz, der für die Vorhersage der Kreditwürdigkeit bestimmt ist,
enthalten die Daten eine gleichmäßige Vertretung von Geschlecht, Rasse und
andere Identitätsmerkmale von Personen, die als unwahrscheinlich für die Rückzahlung
einen Kredit zurückzahlen?
Gibt es Verzerrungen in der Art und Weise, wie verschiedene demografische Gruppen in den Daten vertreten sind?
in den Daten? Dies ist besonders wichtig für Modelle zur Vorhersage von Stimmungen
oder einen Bewertungswert.
Gibt es subjektive Verzerrungen, die von Datenetikettierern eingeführt wurden?
Sobald wir unsere Daten untersucht und auf Verzerrungen hin korrigiert haben, sollten wir die gleichen
Überlegungen berücksichtigen, wenn wir unsere Daten in Trainings-, Test- und Validierungssätze aufteilen.
Sätze. Das heißt, sobald unser kompletter Datensatz ausgeglichen ist, ist es wichtig, dass unsere Trainings-, Test- und Validierungssets dieselbe
und Validierung dieselbe Ausgewogenheit beibehalten. Um zu unserem Beispiel mit dem Schuhbild zurückzukehren
Nehmen wir an, wir haben unseren Datensatz so verbessert, dass er verschiedene Bilder von 10 Schuhtypen enthält.
Schuhe. Der Trainingsdatensatz sollte einen ähnlichen Prozentsatz jedes Schuhtyps enthalten wie die
Test- und Validierungssätze. So wird sichergestellt, dass unser Modell reale Szenarien widerspiegelt und bewertet wird.
Szenarien der realen Welt bewertet wird.

Um zu sehen, wie diese Datensatzanalyse in der Praxis aussieht, verwenden wir das WENN-Tool für
auf den oben vorgestellten Hypotheken-Datensatz anwenden. Damit können wir den aktuellen Saldo
unserer Daten über verschiedene Slices hinweg visualisieren. Das WENN-Tool funktioniert sowohl mit als auch ohne ein
Modell. Da wir unser Modell noch nicht erstellt haben, können wir das What-If Tool
Widget initialisieren, indem wir ihm nur unsere Daten übergeben:

config_builder = WitConfigBuilder(test_examples, column_names)
WitWidget(config_builder)
In Abbildung 7-10 ist zu sehen, wie das Tool aussieht, wenn es geladen wird und 1.000
Beispiele aus unserem Datensatz lädt. Die erste Registerkarte ist der "Datenpunkt-Editor", der einen
Datenpunkt-Editor", der einen Überblick über unsere Daten bietet und uns ermöglicht, einzelne Beispiele zu untersuchen. In dieser Visualisierung
dieser Darstellung sind unsere Datenpunkte nach der Bezeichnung gefärbt - ob ein Hypothekenantrag
Antrag genehmigt wurde. Ein einzelnes Beispiel ist ebenfalls hervorgehoben, und wir können
die damit verbundenen Merkmalswerte.

Entwurfsmuster 30: Fairness-Objektiv | 347
Abbildung 7-10. Der "Datenpunkt-Editor" des WENN-Tools, in dem wir sehen können, wie unsere Daten
aufgeschlüsselt sind, und die Merkmale für einzelne Beispiele aus unserem Datensatz überprüfen.

Es gibt viele Optionen zur Anpassung der Visualisierung im Datenpunkt-Editor,
Dies kann uns dabei helfen zu verstehen, wie unser Datensatz auf verschiedene Slices aufgeteilt ist.
Wenn wir die gleiche Farbkodierung nach Bezeichnung beibehalten und die Spalte agency_code aus dem Dropdown-Menü
die Spalte agency_code aus dem Dropdown-Menü Binning | Y-Achse aus, zeigt das Tool nun ein Diagramm an, das zeigt, wie ausgewogen unsere
Daten in Bezug auf die Agentur, die den Kredit für jeden Antrag übernommen hat, ausgewogen sind. Dies wird in
in Abbildung 7-11. Unter der Annahme, dass diese 1.000 Datenpunkte den Rest unseres Datensatzes gut repräsentieren
unseres Datensatzes repräsentieren, sind in Abbildung 7-11 einige Fälle von potenzieller Verzerrung zu erkennen:

Verzerrung der Datendarstellung
Der Prozentsatz der nicht genehmigten HUD-Anträge ist höher als bei anderen Agenturen
in unseren Daten vertreten. Ein Modell wird dies wahrscheinlich lernen, was dazu führt, dass es "nicht
nicht genehmigt" für Anträge vorhersagt, die vom HUD stammen.

Verzerrung der Datenerfassung
Wir verfügen möglicherweise nicht über genügend Daten über Kredite, die von FRS, OCC, FDIC oder
NCUA, um agency_code als Merkmal in unserem Modell zu verwenden. Wir sollten sicherstellen
dass der prozentuale Anteil der Anträge für jede Behörde in unserem Datensatz die realen
Trends widerspiegelt. Wenn zum Beispiel eine ähnliche Anzahl von Krediten über FRS und
HUD abgewickelt werden, sollten wir eine gleiche Anzahl von Beispielen für jede dieser Agenturen in
unserem Datensatz haben.

348 | Kapitel 7: Verantwortungsvolle KI

9 Weitere Informationen zum Ändern einer Vorhersageaufgabe finden Sie in Kapitel 3 unter "Entwurfsmuster 5: Reframing" auf Seite 80 und "Entwurfsmuster 9: Neutrale Klasse" auf Seite 117.
Muster 9: Neutrale Klasse" auf Seite 117 in Kapitel 3.
Abbildung 7-11. Eine Teilmenge des US-Hypothekendatensatzes, sortiert nach der Spalte agency_code im
des Datensatzes.

Wir können diese Analyse für andere Spalten in unseren Daten wiederholen und unsere Schlussfolgerungen nutzen
um Beispiele hinzuzufügen und unsere Daten zu verbessern. Es gibt viele weitere Optionen zum Erstellen
benutzerdefinierten Visualisierungen im WENN-Tool - weitere Ideen finden Sie im vollständigen Code auf GitHub
Ideen.

Eine weitere Möglichkeit, unsere Daten mithilfe des WENN-Tools zu verstehen, ist die Registerkarte Merkmale
wie in Abbildung 7-12 gezeigt. Hier wird gezeigt, wie die Daten in den einzelnen Spalten
in unserem Datensatz. Daraus können wir erkennen, wo wir Daten hinzufügen oder entfernen oder unsere Vorhersageaufgabe ändern müssen.
Vorhersageaufgabe ändern müssen.^9 Vielleicht möchten wir zum Beispiel unser Modell darauf beschränken, Vorhersagen
Vorhersagen nur für Refinanzierungs- oder Wohnungsbaudarlehen treffen, da für andere mögliche
Daten für andere mögliche Werte in der Spalte loan_purpose zur Verfügung stehen.

Entwurfsmuster 30: Fairness-Objektiv | 349
Abbildung 7-12. Die Registerkarte Merkmale im WENN-Tool, auf der Histogramme darüber angezeigt werden, wie ein
Datensatz für jede Spalte ausgeglichen ist.

Sobald wir unseren Datensatz und unsere Vorhersageaufgabe verfeinert haben, können wir alles andere berücksichtigen, was wir
die wir während der Modellschulung optimieren möchten. Vielleicht ist uns zum Beispiel am wichtigsten
die Genauigkeit unseres Modells bei Anträgen, die es als "genehmigt" vorhersagt. Während der Modell
Modellschulung sollten wir den AUC (oder eine andere Metrik) für die Klasse "genehmigt" in diesem
in diesem binären Klassifizierungsmodell optimieren.

Wenn wir alles getan haben, um Verzerrungen bei der Datenerfassung auszuschließen, und feststellen
dass für eine bestimmte Klasse nicht genügend Daten verfügbar sind, können wir
"Entwurfsmuster 10: Rebalancing" auf Seite 122 in Kapitel 3 befolgen.
In diesem Muster werden Techniken zur Erstellung von Modellen für den Umgang mit
unausgewogenen Daten.
Verzerrungen in anderen Formen von Daten
Obwohl wir hier einen Tabellendatensatz gezeigt haben, sind Verzerrungen auch bei anderen Datentypen üblich.
von Daten. Der von Jigsaw zur Verfügung gestellte Datensatz für zivile Kommentare ist ein gutes Beispiel für
Bereiche, in denen wir Verzerrungen in Textdaten finden können. Dieser Datensatz kennzeichnet Kommentare nach
nach ihrer Giftigkeit (von 0 bis 1) und wurde verwendet, um Modelle für die Kennzeichnung
giftiger Online-Kommentare. Jeder Kommentar in diesem Datensatz ist mit einem Tag versehen, der angibt, ob eines der
Sammlung von Identitätsattributen, wie die Erwähnung einer Religion, Rasse oder sexuellen
sexuellen Orientierung. Wenn wir diese Daten zum Trainieren eines Modells verwenden wollen, ist es wichtig, dass wir
auf Verzerrungen in der Datendarstellung achten. Das heißt, dass die Identitätsbegriffe in einem Kommentar
die Toxizität eines Kommentars nicht beeinflussen, und eine solche Verzerrung sollte
vor dem Training eines Modells berücksichtigt werden.
350 | Kapitel 7: Verantwortungsvolle KI

10 Es gibt viele weitere Optimierungen vor dem Training, die an diesem Datensatz vorgenommen werden könnten. Wir haben nur eine ausgewählt
um zu zeigen, was möglich ist.

Nehmen Sie als Beispiel den folgenden erfundenen Kommentar: "Mint Chip ist ihr bestes Eis
Geschmacksrichtung, ganz klar." Wenn wir "Mint Chip" durch "Rocky Road" ersetzen würden, sollte der
Kommentar mit demselben Toxizitätswert versehen werden (idealerweise 0). Ähnlich wäre es, wenn der
Kommentar stattdessen lauten würde: "Mint Chip ist das Schlimmste. Wenn du diese Geschmacksrichtung magst, bist du ein Idiot,"
würden wir eine höhere Toxizitätsbewertung erwarten, und diese Bewertung sollte immer gleich sein, wenn wir
Mint Chip" durch einen anderen Geschmacksnamen ersetzen. Wir haben in diesem Beispiel Eiscreme verwendet.
Beispiel Eiscreme verwendet, aber es ist leicht vorstellbar, wie sich dies bei kontroverseren Identitätsbegriffen
umstritteneren Identitätsbegriffen abläuft, insbesondere in einem menschenzentrierten Datensatz - ein Konzept, das als kontrafaktische
Fairness.
Nach dem Training
Selbst bei einer strengen Datenanalyse können sich in einem trainierten Modell Verzerrungen einschleichen. Dies kann
Architektur des Modells, Optimierungsmetriken oder Datenverzerrungen, die vor dem
vor dem Training nicht erkannt wurde. Um dies zu beheben, ist es wichtig, unser
Modell unter dem Gesichtspunkt der Fairness zu bewerten und andere Metriken als die
Modellgenauigkeit. Das Ziel dieser Analyse nach dem Training ist es, die Abwägungen zwischen
zwischen der Modellgenauigkeit und den Auswirkungen der Vorhersagen eines Modells auf verschiedene
Gruppen haben.
Das WENN-Tool ist eine solche Option für die Post-Modell-Analyse. Um zu zeigen, wie man
um zu demonstrieren, wie es auf ein trainiertes Modell angewendet werden kann, bauen wir auf dem Beispiel unseres Hypothekendatensatzes auf. Basierend auf unserer
Analyse haben wir den Datensatz dahingehend verfeinert, dass er nur Darlehen für die
Refinanzierung oder den Erwerb von Wohneigentum enthalten,^10 und ein XGBoost-Modell trainiert, um vorherzusagen, ob
ob ein Antrag genehmigt wird oder nicht. Da wir XGBoost verwenden, haben wir
alle kategorialen Merkmale mit der Pandas-Methode get_dummies() in boolesche Spalten
Methode.
Wir nehmen einige Ergänzungen zu unserem obigen WENN-Tool-Initialisierungscode vor, und zwar dieses Mal
Übergabe einer Funktion, die unser trainiertes Modell aufruft, zusammen mit Configs, die unsere
Label-Spalte und den Namen für jedes Label:
def custom_fn(examples):
df = pd.DataFrame(examples, columns=columns)
preds = bst.predict_proba(df)
return preds
config_builder = (WitConfigBuilder(test_examples, columns)
.set_custom_predict_fn(custom_fn)
.set_target_feature('mortgage_status')
.set_label_vocab(['verweigert', 'genehmigt']))
WitWidget(config_builder, height=800)
Entwurfsmuster 30: Fairness-Objektiv | 351
Nachdem wir nun unser Modell an das Tool übergeben haben, zeigt die resultierende Visualisierung in
Abbildung 7-13 zeigt unsere Testdatenpunkte entsprechend der Vorhersagezuverlässigkeit unseres Modells
auf der y-Achse dargestellt.

Abbildung 7-13. Der Datenpunkt-Editor des WENN-Tools für ein binäres Klassifikationsmodell. Die
y-Achse ist die Vorhersageausgabe des Modells für jeden Datenpunkt, die von 0 (verweigert) bis 1
(genehmigt).

Auf der Registerkarte Leistung und Fairness des WENN-Tools können wir die Fairness unseres Modells
über verschiedene Datenabschnitte hinweg bewerten. Indem wir eines der Merkmale unseres Modells für "Slice by" auswählen, können wir
können wir die Ergebnisse des Modells für verschiedene Werte dieses Merkmals vergleichen. In Abbildung 7-14,
haben wir nach dem Merkmal agency_code_HUD unterteilt - ein boolescher Wert, der angibt, ob ein
Antrag vom HUD gezeichnet wurde (0 für Nicht-HUD-Kredite, 1 für HUD-Kredite).

Abbildung 7-14. Die Registerkarte Leistung & Fairness des WENN-Tools zeigt die Leistung unseres XGBoost
Modellleistung über verschiedene Merkmalswerte hinweg.

352 | Kapitel 7: Verantwortungsvolle KI

11 In diesem Artikel werden die Optionen des WENN-Tools für Fairness-Optimierungsstrategien näher erläutert.

12 Weitere Einzelheiten zur Chancengleichheit als Maßstab für Fairness finden Sie hier.

Aus diesen Diagrammen zu Leistung und Fairness können wir ersehen:
Die Genauigkeit unseres Modells bei den vom HUD überwachten Krediten ist deutlich höher - 94 %
im Vergleich zu 85%.
Laut der Konfusionsmatrix werden Darlehen, die nicht vom HUD überwacht werden, mit einer höheren
72 % im Vergleich zu 55 %. Dies ist wahrscheinlich auf die Verzerrung der Datendarstellung zurückzuführen
(wir haben den Datensatz absichtlich so belassen, um zu zeigen, wie Modelle
wie Modelle Datenverzerrungen verstärken können).
Es gibt einige Möglichkeiten, auf diese Erkenntnisse zu reagieren, wie im Feld "Optimierungsstrategie" in Abbildung 7-14 gezeigt
in Abbildung 7-14 gezeigt werden. Diese Optimierungsmethoden beinhalten die Änderung der Klassifizierungsschwelle unseres Modells
Schwellenwert - der Schwellenwert, bei dem ein Modell eine positive Klassifizierung ausgibt - zu ändern.
tion ausgibt. Im Kontext dieses Modells, welche Vertrauensschwelle ist für uns in Ordnung, um
einen Antrag als "genehmigt" zu kennzeichnen? Wenn unser Modell zu mehr als 60 % sicher ist, dass ein Antrag
zuversichtlich ist, dass ein Antrag genehmigt werden sollte, sollten wir ihn dann genehmigen? Oder ist es nur dann in Ordnung, Anträge zu genehmigen
genehmigen, wenn unser Modell mehr als 98 % Sicherheit bietet? Diese Entscheidung ist weitgehend
hängt weitgehend vom Kontext und der Vorhersageaufgabe des Modells ab. Wenn wir vorhersagen wollen, ob ein Bild
ob ein Bild eine Katze enthält oder nicht, ist es vielleicht in Ordnung, wenn wir das Label "Katze" zurückgeben, auch wenn unser
Modell nur zu 60 % sicher ist. Wenn wir jedoch ein Modell haben, das vorhersagt, ob ein
ob ein medizinisches Bild eine Krankheit enthält oder nicht, muss der Schwellenwert wahrscheinlich viel
höher sein.
Das WENN-Tool hilft uns bei der Auswahl eines Schwellenwerts auf der Grundlage verschiedener Optimierungen. Optimierung
Die Optimierung für "Demografische Parität" würde beispielsweise sicherstellen, dass unser Modell
dass unser Modell denselben Prozentsatz an Anträgen für HUD- und Nicht-HUD-Kredite genehmigt.^11
Alternativ dazu würde die Verwendung einer Fairness-Metrik für Chancengleichheit^12 sicherstellen, dass die Daten
Datenpunkte aus der HUD- und Nicht-HUD-Scheibe mit einem Grundwahrheitswert von
"genehmigt" im Testdatensatz die gleiche Chance haben, vom Modell als "genehmigt" vorhergesagt zu werden.
"genehmigt" durch das Modell vorhergesagt zu werden.
Es ist zu beachten, dass die Änderung des Schwellenwerts für die Vorhersage eines Modells nur eine Möglichkeit ist, auf die Fairness
Bewertungsmetriken. Es gibt viele andere Ansätze, darunter die Neuverteilung der
Trainingsdaten, das Umlernen eines Modells zur Optimierung für eine andere Metrik und vieles mehr.
Entwurfsmuster 30: Fairness-Objektiv | 353
Das WENN-Tool ist modellunabhängig und kann für jede Art von Modell verwendet werden
Modells verwendet werden, unabhängig von der Architektur oder dem Framework. Es funktioniert mit
Modellen, die in einem Notebook oder in TensorBoard geladen sind, Modellen
Modellen, die über TensorFlow Serving bereitgestellt werden, und Modellen, die auf der Cloud AI
Plattform Vorhersage. Das What-If Tool Team hat auch ein Tool für
textbasierte Modelle, das Language Interpretability Tool (LIT).
Eine weitere wichtige Überlegung für die Bewertung nach dem Training ist das Testen unseres Modells an
einem ausgewogenen Satz von Beispielen. Wenn es bestimmte Bereiche in unseren Daten gibt, von denen wir annehmen, dass sie
problematisch für unser Modell sein werden, wie z. B. Eingaben, die durch Datensammlung oder
oder Repräsentationsverzerrungen betroffen sein könnten, sollten wir sicherstellen, dass unser Testsatz genügend dieser Fälle enthält.
Fälle enthält. Nach der Aufteilung unserer Daten werden wir dieselbe Art von Analyse verwenden, die wir im
"Vor dem Training" dieses Abschnitts auf jede Aufteilung unserer Daten angewandt haben: Training, Validierung,
und Test.

Wie aus dieser Analyse hervorgeht, gibt es keine Einheitslösung oder Bewertungsmetrik
für Modellfairness. Es handelt sich um einen kontinuierlichen, iterativen Prozess, der
ML-Workflow eingesetzt werden sollte - von der Datenerfassung bis zum eingesetzten Modell.

Kompromisse und Alternativen
Neben den im Abschnitt "Lösungen" beschriebenen Vor- und Nachschulungstechniken gibt es viele weitere Möglichkeiten, die Fairness des Modells
Trainingstechniken, die im Abschnitt "Lösungen" besprochen wurden. Hier stellen wir Ihnen ein paar
alternative Werkzeuge und Verfahren zur Erzielung fairer Modelle vor. ML-Fairness ist ein sich schnell
sich schnell entwickelndes Forschungsgebiet - die in diesem Abschnitt vorgestellten Tools sind nicht als
eine erschöpfende Liste, sondern vielmehr einige Techniken und Werkzeuge, die derzeit zur
Verbesserung der Modellfairness. Wir werden auch die Unterschiede zwischen der Fairness
Lens und Explainable Predictions erörtert, da sie miteinander verwandt sind und oft zusammen verwendet werden.
zusammen verwendet werden.

Fairness-Indikatoren

Fairness Indicators (FI) sind eine Reihe von Open-Source-Tools, die helfen sollen, die
die Verteilung eines Datensatzes vor dem Training zu verstehen und die Modellleistung anhand
unter Verwendung von Fairness-Metriken. Die in FI enthaltenen Werkzeuge sind TensorFlow Data Validation
(TFDV) und TensorFlow Model Analysis (TFMA). Fairness-Indikatoren werden am häufigsten
als Komponenten in TFX-Pipelines verwendet (siehe "Entwurfsmuster 25: Workflow-Pipeline" auf Seite 282 in Kapitel 6 für
auf Seite 282 in Kapitel 6 für weitere Details) oder über TensorBoard. Mit TFX, gibt es
zwei vorgefertigte Komponenten, die Fairness-Indikator-Tools verwenden:

BeispielValidator für Datenanalyse, Erkennung von Drift und Trainingsverzerrung
mit TFDV.
Evaluator verwendet die TFMA-Bibliothek zur Bewertung eines Modells über Teilmengen eines Datensatzes.
Ein Beispiel für eine interaktive Visualisierung, die mit TFMA erstellt wurde, finden Sie in
354 | Kapitel 7: Verantwortungsvolle KI

Abbildung 7-15. Hier wird ein Merkmal in den Daten (Höhe) betrachtet und die
die Falsch-Negativ-Rate des Modells für jeden möglichen kategorischen Wert dieses Merkmals.
Abbildung 7-15. Vergleich der Falsch-Negativ-Rate eines Modells über verschiedene Teilmengen von Daten.

Aus dem Fairness Indicators Python-Paket kann TFMA auch als eigenständiges
eigenständiges Werkzeug verwendet werden, das sowohl mit TensorFlow- als auch mit Nicht-TensorFlow-Modellen arbeitet.

Automatisierung der Datenauswertung

Die Methoden zur Bewertung der Fairness, die wir im Abschnitt "Lösungen" erörtert haben, konzentrierten sich auf
manuelle, interaktive Daten- und Modellanalyse. Diese Art der Analyse ist vor allem in den
vor allem in den Anfangsphasen der Modellentwicklung. Wenn wir unser Modell operationalisieren und
und uns auf die Pflege und Verbesserung des Modells konzentrieren, wird die Suche nach Möglichkeiten zur
der Fairness zu automatisieren, werden wir die Effizienz steigern und sicherstellen, dass Fairness in unseren
ML-Prozess integriert ist. Dies können wir durch "Entwurfsmuster 18: Fortgesetzte Modellbewertung" auf Seite 220 erreichen.
uation" auf Seite 220 in Kapitel 5 oder mit "Entwurfsmuster 25: Workflow
Pipeline" auf Seite 282 in Kapitel 6 unter Verwendung von Komponenten wie denen von TFX für
Datenanalyse und Modellauswertung.

Entwurfsmuster 30: Fairness-Objektiv | 355
Listen zulassen und verbieten

Wenn wir keine Möglichkeit finden, inhärente Verzerrungen in unseren Daten oder unserem Modell direkt zu beheben, ist es möglich.
ist es möglich, unser Produktionsmodell mit Hilfe von Erlaubnis- und Verbotslisten mit Regeln zu versehen.
Dies gilt vor allem für Klassifikations- oder generative Modelle, wenn es Bezeichnungen oder
Wörter gibt, die unser Modell nicht zurückgeben soll. Zum Beispiel wurden geschlechtsspezifische Wörter wie
"Mann" und "Frau" aus der Funktion zur Erkennung von Bezeichnungen der Google Cloud Vision API
Funktion entfernt. Da das Geschlecht nicht allein anhand des Aussehens bestimmt werden kann, hätte es
würde es unfaire Verzerrungen verstärken, wenn diese Bezeichnungen zurückgegeben würden, wenn die Vorhersage des Modells
ausschließlich auf visuellen Merkmalen basiert. Stattdessen gibt die Vision API "Person" zurück. Ähnlich verhält es sich mit der
Funktion "Smart Compose" in Google Mail die Verwendung von geschlechtsspezifischen Pronomen bei der
Sätzen wie "Ich treffe nächste Woche einen Investor. Willst du dich mit
___?"

Diese Erlaubnis- und Verbotslisten können in einer von zwei Phasen in einem ML
Arbeitsablauf angewendet werden:

Datenerfassung
Wenn wir ein Modell von Grund auf trainieren oder das Transfer Learning
um eine eigene Klassifizierungsschicht hinzuzufügen, können wir den Beschriftungssatz unseres Modells in der
Datenerfassungsphase festlegen, bevor ein Modell trainiert wurde.

Nach dem Training
Wenn wir uns für die Vorhersagen auf ein vorab trainiertes Modell verlassen und die gleichen
Beschriftungen aus diesem Modell verwenden, kann eine Liste mit erlaubten und nicht erlaubten Beschriftungen in der Produktion implementiert werden.
tion implementiert werden - nachdem das Modell eine Vorhersage geliefert hat, aber bevor diese Bezeichnungen den
Endbenutzer angezeigt werden. Dies könnte auch für Textgenerierungsmodelle gelten, bei denen wir nicht die
keine vollständige Kontrolle über alle möglichen Modellausgaben haben.

Datenerweiterung

Zusätzlich zu den bereits erörterten Lösungen für die Datenverteilung und -darstellung,
ein weiterer Ansatz zur Minimierung der Modellverzerrung ist die Datenerweiterung. Mit
werden die Daten vor dem Training mit dem Ziel verändert, potenzielle Verzerrungsquellen
Quellen der Verzerrung zu beseitigen. Eine spezielle Art der Datenerweiterung wird als Ablation bezeichnet und ist
besonders bei Textmodellen anwendbar. In einem Textmodell zur Stimmungsanalyse zum Beispiel
können wir Identitätsbegriffe aus dem Text entfernen, um sicherzustellen, dass sie die Vorhersagen unseres Modells
Vorhersagen beeinflussen. In Anlehnung an das Eiscreme-Beispiel, das wir weiter oben in diesem Abschnitt verwendet haben, könnte der
Satz "Mint Chip ist ihre beste Eissorte" würde nach Anwendung des Modells zu "BLANK ist ihre
beste Eissorte", nachdem wir die Ablation angewendet haben. Wir würden dann alle anderen Wörter ersetzen
im gesamten Datensatz, die die Stimmungsvorhersage des Modells nicht beeinflussen sollen, durch dasselbe Wort
durch dasselbe Wort (wir haben hier BLANK verwendet, aber alles, was im Rest der Textdaten nicht
der Textdaten nicht vorkommt). Beachten Sie, dass diese Ablationstechnik zwar für viele Textmodelle gut
Textmodellen gut funktioniert, muss man jedoch vorsichtig sein, wenn man Bereiche mit Verzerrungen aus Tabellendaten
Tabellendatensätzen vorsichtig sein, wie im Abschnitt Problem erwähnt.

356 | Kapitel 7: Verantwortungsvolle KI

Ein weiterer Ansatz zur Datenerweiterung besteht in der Generierung neuer Daten, und er wurde von
von Google Translate verwendet, um geschlechtsspezifische Verzerrungen bei der Übersetzung von Texten in und aus
geschlechtsneutralen und geschlechtsspezifischen Sprachen. Die Lösung beinhaltete das Umschreiben von Trans-
so umgeschrieben, dass die Übersetzung gegebenenfalls sowohl in der weiblichen als auch in der männlichen Form angeboten wird.
der weiblichen und männlichen Form angeboten wird. Zum Beispiel würde der geschlechtsneutrale englische Satz
"Wir sind Ärzte" würde bei der Übersetzung ins Spanische zwei Ergebnisse liefern, wie in
Abbildung 7-16. Im Spanischen kann das Wort "wir" sowohl in der weiblichen als auch in der männlichen
Form haben.

Abbildung 7-16. Beim Übersetzen eines geschlechtsneutralen Wortes in einer Sprache (hier das Wort
"wir" im Englischen) in eine Sprache, in der dieses Wort geschlechtsspezifisch ist, bietet Google Translate
nun mehrere Übersetzungen an, um geschlechtsspezifische Verzerrungen zu minimieren.

Modell-Karten

Ursprünglich in einem Forschungspapier eingeführt, bieten Model Cards einen Rahmen für
für die Darstellung der Fähigkeiten und Grenzen eines Modells. Das Ziel von Model Cards ist es
die Transparenz von Modellen zu verbessern, indem sie Details zu Szenarien liefern, in denen ein Modell
und nicht verwendet werden sollte, da die Abschwächung problematischer Verzerrungen nur funktioniert, wenn ein Modell
so verwendet wird, wie es beabsichtigt war. Auf diese Weise fördern die Modellkarten die Verantwortlichkeit
für die Verwendung eines Modells im richtigen Kontext.

Die ersten veröffentlichten Model Cards bieten Zusammenfassungen und Fairness-Metriken für die Gesichts
Erkennung und die Objekterkennung in der Google Cloud Vision API. Zum Erzeugen von
Model Cards für unsere eigenen ML-Modelle zu generieren, bietet TensorFlow ein Model Card Toolkit
(MCT), das als eigenständige Python-Bibliothek oder als Teil einer TFX-Pipeline ausgeführt werden kann.
Das Toolkit liest exportierte Modell-Assets und erzeugt eine Reihe von Diagrammen mit verschiedenen
Leistungs- und Fairness-Metriken.

Fairness versus Erklärbarkeit

Die Konzepte der Fairness und der Erklärbarkeit im ML werden manchmal verwechselt, da sie
oft zusammen verwendet werden und beide Teil der größeren Initiative der verantwortungsvollen KI sind.
Fairness bezieht sich speziell auf die Identifizierung und Beseitigung von Verzerrungen in Modellen, und

Entwurfsmuster 30: Fairness-Objektiv | 357
Die Erklärbarkeit ist ein Ansatz, um das Vorhandensein von Verzerrungen zu diagnostizieren. Ein Beispiel,
die Anwendung der Erklärbarkeit auf ein Stimmungsanalysemodell könnte aufzeigen, dass das Modell
Vorhersage auf Identitätsbegriffe stützt, während es stattdessen
Worte wie "schlimm", "erstaunlich" oder "nicht" verwenden sollte.

Die Erklärbarkeit kann auch außerhalb des Kontexts der Fairness genutzt werden, um Dinge aufzudecken wie
warum ein Modell bestimmte betrügerische Transaktionen anzeigt, oder die Pixel, die ein Modell dazu
Modell veranlasst hat, in einem medizinischen Bild "krank" vorauszusagen. Erklärbarkeit ist also eine Methode
zur Verbesserung der Modelltransparenz. Manchmal kann Transparenz Bereiche aufdecken, in denen ein
Modell bestimmte Gruppen ungerecht behandelt, aber sie kann auch einen tieferen Einblick in den
in den Entscheidungsprozess eines Modells geben.

Zusammenfassung
Peter Parker bezog sich vielleicht nicht auf das maschinelle Lernen, als er sagte,
"Aus großer Macht folgt große Verantwortung", aber das Zitat trifft hier sicherlich zu. ML
hat das Potenzial, Branchen zu verändern, die Produktivität zu steigern und neue Erkenntnisse aus Daten zu gewinnen.
aus Daten. Angesichts dieses Potenzials ist es besonders wichtig, dass wir verstehen, wie sich unsere
Modelle auf verschiedene Gruppen von Stakeholdern auswirken werden. Modell-Stakeholder könnten
unterschiedliche demografische Gruppen von Modellnutzern, Regulierungsbehörden, ein Data Science
Team oder Geschäftsteams innerhalb einer Organisation sein.

Die in diesem Kapitel beschriebenen Muster der verantwortlichen KI sind ein wesentlicher Bestandteil jedes ML
Arbeitsablaufs - sie können uns helfen, die von unseren Modellen erzeugten Vorhersagen besser zu verstehen
Sie können uns helfen, die von unseren Modellen erzeugten Vorhersagen besser zu verstehen und potenziell unerwünschtes Verhalten zu erkennen, bevor die Modelle in Produktion gehen. Starten Sie
mit dem heuristischen Benchmark-Muster haben wir uns angeschaut, wie man eine erste Metrik
für die Modellbewertung. Diese Metrik ist nützlich als Vergleichspunkt für das Verständnis
Modellversionen zu verstehen und das Modellverhalten für Entscheidungsträger zusammenzufassen.
Entscheidungsträger. Im Muster "Erklärbare Vorhersagen" haben wir gezeigt, wie man mit Hilfe von Feature
um zu sehen, welche Merkmale für die Vorhersage eines Modells am wichtigsten waren.
tion. Merkmalsattributionen sind eine Art von Erklärungsmethode und können sowohl für
sowohl für die Bewertung der Vorhersage an einem einzelnen Beispiel als auch für eine Gruppe von Testeingaben.
Das Entwurfsmuster Fairness Lens schließlich stellt Werkzeuge und Metriken vor, die sicherstellen, dass die Vorhersagen eines
Vorhersagen eines Modells alle Gruppen von Nutzern auf eine faire, gerechte und
unvoreingenommen.

358 | Kapitel 7: Verantwortungsvolle KI

KAPITEL 8

Verbundene Patterns
Wir haben uns vorgenommen, einen Katalog von Entwurfsmustern für maschinelles Lernen zu erstellen, Lösungen für wiederkehrende
wiederkehrende Probleme beim Entwurf, Training und Einsatz von Modellen und Pipelines für maschinelles Lernen
Pipelines. In diesem Kapitel bieten wir eine Kurzreferenz zu diesem Inventar von Mustern.

Wir haben die Muster in diesem Buch danach geordnet, wo sie in einem typischen ML-Workflow verwendet werden.
ML-Arbeitsablauf verwendet werden. So hatten wir ein Kapitel über Eingabedarstellung und ein weiteres über
Modellauswahl. Dann haben wir Muster diskutiert, die die typische Trainingsschleife verändern und
Inferenz widerstandsfähiger machen. Wir schlossen mit Mustern, die einen verantwortungsvollen
Nutzung von ML-Systemen fördern. Dies ist vergleichbar mit der Organisation eines Rezeptbuchs mit separaten Abschnitten über
Vorspeisen, Suppen, Hauptgerichten und Nachspeisen. Eine solche Organisation kann es jedoch schwierig machen
zu bestimmen, wann man welche Suppe wählt und welche Desserts zu einem bestimmten
Vorspeise passen. Deshalb zeigen wir in diesem Kapitel auch auf, wie die Muster miteinander zusammenhängen.
zueinander stehen. Schließlich stellen wir auch "Speisepläne" zusammen, indem wir erörtern, wie die Muster
terns für gängige Kategorien von ML-Aufgaben zusammenwirken.

Muster-Referenz
Wir haben viele verschiedene Entwurfsmuster besprochen und wie sie verwendet werden können, um
Herausforderungen, die beim maschinellen Lernen auftreten, zu bewältigen. Hier ist eine Zusammenfassung.

359
Kapitel Entwurfsmuster Problem gelöst Lösung
Daten
Darstellung
Hashed Feature Probleme im Zusammenhang mit kategorischen
Merkmalen wie unvollständiges
Vokabular, Modellgröße aufgrund von
Kardinalität, und Kaltstart.
Eimer einen deterministischen und portablen Hash
der String-Darstellung und akzeptieren den
Kollisionen in der Datendarstellung
Repräsentation.
Einbettungen Merkmale mit hoher Kardinalität, bei denen
enge Beziehungen wichtig sind
zu bewahren sind.
Lernen Sie eine Datendarstellung, die
Daten mit hoher Kardinalität in einen niedriger
dimensionalen Raum abbildet, und zwar so, dass die
Informationen, die für das Lernproblem
erhalten bleibt.
Feature-Cross-Modell-Komplexität unzureichend für das Lernen
Beziehungen zwischen Merkmalen zu lernen.
Helfen Sie den Modellen, die Beziehungen zwischen
Eingaben schneller zu erlernen, indem explizit jede
Kombination von Eingabewerten ein separates
Merkmal machen.
Multimodale Eingaben Wie wählt man zwischen mehreren
möglichen Datendarstellungen.
Verkettung aller verfügbaren Daten
Repräsentationen.
Problem
Repräsentation
Reframing Mehrere Probleme, einschließlich Vertrauen
für numerische Vorhersagen, ordinale
Kategorien, Einschränkung der Vorhersage
Bereich und Multitasking-Lernen.
Ändern Sie die Darstellung der Ausgabe eines
eines Problems des maschinellen Lernens, zum Beispiel,
Darstellung eines Regressionsproblems als
Klassifizierung (und umgekehrt).
Multilabel Mehr als ein Label gilt für ein bestimmtes
Trainingsbeispiel.
Kodieren Sie das Label mit einem Multi-Hot-Array,
und k-Sigmoide als Ausgabeschicht verwenden.
Ensembles Bias-Varianz-Ausgleich bei kleinen und
mittelgroßen Problemen.
Kombinieren Sie mehrere Modelle für maschinelles Lernen
Modelle und aggregieren ihre Ergebnisse, um
Vorhersagen zu treffen.
Kaskade Wartbarkeits- oder Driftprobleme, wenn ein
Problem des maschinellen Lernens
in eine Reihe von ML-Problemen aufgeteilt wird.
Behandeln Sie ein ML-System als einen einheitlichen Arbeitsablauf
für die Zwecke des Trainings, der Bewertung
und Vorhersage.
Neutrale Klasse Die Klassenbezeichnung für eine Teilmenge von
Beispielen ist im Wesentlichen willkürlich.
Einführen eines zusätzlichen Labels für ein
Klassifizierungsmodell ein, disjunkt von den
aktuellen Bezeichnungen.
Rebalancing Stark unausgewogene Daten. Downsample, Upsample, oder Verwendung einer gewichteten
gewichtete Verlustfunktion, abhängig von verschiedenen
Überlegungen.
360 | Kapitel 8: Verbundene Patterns

Kapitel Entwurfsmuster Problem gelöst Lösung
Patterns, die
Modell modifizieren
Ausbildung

Nützliche Überanpassung Verwendung von Methoden des maschinellen Lernens zum
Lernen eines physikalischen Modells oder
dynamisches System.
Verzicht auf die üblichen Verallgemeinerungstechniken
um den Trainingsdatensatz absichtlich zu überanpassen
Trainingsdatensatz.
Kontrollpunkte Verlorener Fortschritt bei langlaufenden
Trainingsaufträgen aufgrund von Maschinenausfällen.
Regelmäßiges Speichern des vollständigen Zustands des Modells
regelmäßig zu speichern, so dass teilweise trainierte
Modelle verfügbar sind und verwendet werden können, um
Training von einem Zwischenpunkt aus wieder aufzunehmen
wieder aufgenommen werden können, anstatt bei Null zu beginnen.
Transfer Learning Mangel an großen Datensätzen, die benötigt werden
zum Trainieren komplexer maschineller Lern
Modelle.
Nehmen Sie einen Teil eines zuvor trainierten Modells,
friert die Gewichte ein und verwendet diese
nicht trainierbaren Schichten in einem neuen Modell, das
ein ähnliches Problem löst.
Verteilung
Strategie
Das Training großer neuronaler Netze kann
sehr lange dauern, was das Experimentieren
Experimentieren.
Führen Sie die Trainingsschleife in großem Umfang über
mehrere Worker aus, wobei die Vorteile von
Caching, Hardware-Beschleunigung und
Parallelisierung.
Hyperparameter
Abstimmung
Wie bestimmt man die optimalen
Hyperparameter eines maschinellen
Lernmodells.
Einfügen der Trainingsschleife in eine
Optimierungsmethode ein, um den optimalen
Satz von Modell-Hyperparametern zu finden.
Ausfallsicherheit Stateless Serving
Funktion

Das ML-Produktionssystem muss in der Lage sein
synchron Tausende bis Millionen
Millionen von Vorhersageanfragen pro
Sekunde.
Exportieren Sie das maschinelle Lernmodell als eine
zustandslose Funktion exportieren, damit es von mehreren
von mehreren Clients auf skalierbare Weise gemeinsam genutzt werden kann.
Batch Serving Durchführen von Modellvorhersagen über
großen Datenmengen unter Verwendung eines
Endpunkt, der für die Bearbeitung einzelner
Anfragen auf einmal zu verarbeiten, überfordert
das Modell.
Verwenden Sie eine Software-Infrastruktur, die üblicherweise
für die verteilte Datenverarbeitung verwendet wird, um
Inferenz asynchron auf einer großen Anzahl
Anzahl von Instanzen gleichzeitig durchzuführen.
Fortgesetztes Modell
Bewertung
Die Modellleistung eingesetzter
Modellen verschlechtert sich mit der Zeit entweder durch
Datenabweichung, Konzeptabweichung oder andere
Änderungen an den Pipelines, die das Modell mit
Daten in das Modell einspeisen.
Erkennen Sie, wann ein eingesetztes Modell nicht mehr
nicht mehr zweckdienlich ist, indem Sie kontinuierlich die
Modellvorhersagen und die Bewertung der Modell
Leistung.
Zweiphasig
Vorhersagen
Große, komplexe Modelle müssen
leistungsfähig gehalten werden, wenn sie
am Rande oder auf verteilten Geräten eingesetzt werden.
Teilen Sie den Anwendungsfall in zwei Phasen auf, wobei
wobei nur die einfachere Phase am Rande
dem Rand ausgeführt wird.
Keyed Predictions Wie lassen sich die Modellvorhersagen
die an die entsprechende Modelleingabe zurückgegeben werden
Modelleingabe bei der Übermittlung großer
Vorhersageaufträgen.
Erlauben Sie dem Modell, während der Vorhersage einen Client-
unterstützten Schlüssel während der Vorhersage, der
verwendet werden kann, um Modelleingaben mit Modell
Vorhersagen.
Muster Referenz | 361
Kapitel Entwurfsmuster Problem gelöst Lösung
Reproduzierbarkeit Transformieren Die Eingaben in ein Modell müssen
transformiert werden, um die Merkmale zu erzeugen, die das
Modell erwartet, und dieser Prozess muss
konsistent sein zwischen Training und
dienen.
Erfassen und speichern Sie explizit die
Transformationen, die zur Umwandlung der
Modelleingaben in Merkmale umzuwandeln.
Wiederholbar
Aufteilung
Bei der Erstellung von Datensplits ist es
ist es wichtig, eine Methode zu haben, die
leicht und wiederholbar ist, unabhängig
unabhängig von der Programmiersprache oder
zufälligen Seeds.
Identifizieren Sie eine Spalte, die die
Korrelationsbeziehung zwischen den Zeilen erfasst und
verwenden Sie den Farm Fingerprint Hashing
Algorithmus, um die verfügbaren Daten in
Trainings-, Validierungs- und Testdatensätze aufzuteilen.
Überbrücktes Schema Wenn neue Daten verfügbar werden, können alle
Änderungen am Datenschema die
verhindern, dass sowohl die neuen als auch die alten
Daten für die Umschulung zu verwenden.
Passen Sie die Daten von ihrem älteren, ursprünglichen Daten
Datenschema an das Schema der neueren, besseren
besseren Daten.
Gefenstert
Inferenz
Einige Modelle erfordern eine laufende
Sequenz von Instanzen zur Ausführung
Inferenz, oder die Merkmale müssen
über ein Zeitfenster hinweg so aggregiert
Zeitfenster so aggregiert werden, dass eine Trainings
Trainingsverzerrung vermeidet.
Externalisieren des Modellstatus und Aufrufen des
Modell aus einer Stream-Analytics-Pipeline auf, um
um sicherzustellen, dass dynamisch berechnete
dynamischen, zeitabhängigen Weise berechnet werden
zwischen Training und Auslieferung korrekt
dienen.
Workflow-Pipeline Wenn Sie den ML-Workflow skalieren, führen Sie
Versuche unabhängig voneinander und verfolgen
Leistung für jeden Schritt der
Pipeline.
Machen Sie jeden Schritt des ML-Workflows zu einem
separaten, containerisierten Dienst, der
verkettet werden können, um eine Pipeline zu
mit einem einzigen REST-API-Aufruf ausgeführt werden kann
Merkmalsspeicher Der Ad-hoc-Ansatz bei der Merkmal
Engineering verlangsamt die Modell
Modellentwicklung und führt zu doppeltem
doppelten Aufwand zwischen den Teams sowie
Ineffizienz der Arbeitsabläufe.
Erstellen Sie einen Feature-Speicher, einen zentralisierten
zentraler Ort zum Speichern und Dokumentieren von
Datensätze, die für die Erstellung von
Modelle für maschinelles Lernen verwendet werden und
Projekten und Teams gemeinsam genutzt werden können.
Modellversionierung Es ist schwierig, die Leistung zu überwachen
Überwachung und Split-Test von Modell
Modelländerungen durchzuführen, während ein einziges Modell
in Produktion zu haben oder Modelle zu aktualisieren
zu aktualisieren, ohne bestehende Benutzer zu beeinträchtigen.
Stellen Sie ein geändertes Modell als Microservice
mit einem anderen REST-Endpunkt, um eine
Abwärtskompatibilität für bereitgestellte
Modelle zu erreichen.
362 | Kapitel 8: Verbundene Patterns

Kapitel Entwurfsmuster Problem gelöst Lösung
Verantwortliche AI Heuristik
Benchmarking
Die Erläuterung der Modellleistung durch
komplizierten Bewertungsmaßstäben ist
nicht die Intuition, die Entscheidungsträger in
Entscheidungsträger benötigen.
Vergleichen Sie ein ML-Modell mit einer einfachen,
leicht zu verstehenden Heuristik.
Erklärbar
Vorhersagen
Manchmal ist es notwendig zu wissen
warum ein Modell bestimmte
Vorhersagen macht, entweder zur Fehlersuche oder für
Regulierungs- und Konformitätsstandards.
Wenden Sie Techniken zur Erklärung von Modellen an, um
zu verstehen, wie und warum Modelle
Vorhersagen treffen und das Vertrauen der Nutzer in ML
Systeme zu verbessern.
Fairnessobjektiv Voreingenommenheit kann dazu führen, dass Modelle des maschinellen Lernens
Modelle nicht alle Benutzer gleich behandeln
und können sich nachteilig auf einige
Populationen haben.
Verwenden Sie Tools, um Verzerrungen in Datensätzen zu erkennen, bevor
vor dem Training zu erkennen und die trainierten Modelle
unter dem Aspekt der Fairness, um sicherzustellen
um sicherzustellen, dass die Modellvorhersagen für verschiedene
Nutzergruppen und verschiedene Szenarien gerecht sind.
Interaktionen zwischen Mustern
Entwurfsmuster existieren nicht isoliert. Viele von ihnen stehen in enger Beziehung zueinander
direkt oder indirekt miteinander verbunden und ergänzen sich oft gegenseitig. Das Interaktionsdiagramm
tionsdiagramm in Abbildung 8-1 fasst die gegenseitigen Abhängigkeiten und einige Beziehungen
zwischen verschiedenen Entwurfsmustern. Wenn Sie ein Muster verwenden, können Sie
Wenn Sie ein Muster verwenden, sollten Sie darüber nachdenken, wie Sie andere Muster, die damit in Beziehung stehen, einbeziehen können.

Hier werden wir einige der Möglichkeiten aufzeigen, wie diese Muster zusammenhängen und wie
sie bei der Entwicklung einer vollständigen Lösung zusammen verwendet werden können. Zum Beispiel kann bei der Arbeit
mit kategorialen Merkmalen kann das Entwurfsmuster Hashed Feature mit dem
mit dem Entwurfsmuster Embeddings kombiniert werden. Diese beiden Muster arbeiten zusammen, um
Modellinputs mit hoher Kardinalität, wie z.B. die Arbeit mit Text. In TensorFlow wird dies
demonstriert, indem eine categorical_column_with_hash_bucket feature col-
Spalte mit einer einbettenden Merkmalsspalte, um die spärliche, kategoriale Texteingabe
in eine dichte Darstellung umzuwandeln:

import tensorflow.feature_column as fc
keywords = fc.categorical_column_with_hash_bucket("keywords",
hash_bucket_size=10K)
keywords_embedded = fc.embedding_column(keywords, num_buckets=16)
Bei der Erörterung von Einbettungen haben wir gesehen, dass diese Technik empfohlen wird, wenn
das Feature-Cross-Entwurfsmuster zu verwenden. Hashed Features gehen Hand in Hand mit dem
Repeatable Splitting Design Pattern, da der Farm Fingerprint Hashing Algorithmus
für die Datenaufteilung verwendet werden kann. Und bei Verwendung des Entwurfsmusters Hashed Features oder Embeddings
ist es üblich, auf Konzepte des Hyperparameter-Tunings zurückzugreifen, um
die optimale Anzahl von Hash-Buckets oder die richtige Einbettungsdimension zu bestimmen
verwenden.

Wechselwirkungen zwischen Mustern | 363
Abbildung 8-1. Viele der in diesem Buch besprochenen Muster sind miteinander verwandt oder können
zusammen verwendet werden. Dieses Bild ist im GitHub-Repository für dieses Buch verfügbar.

In der Tat ist das Hyperparameter-Tuning-Muster ein gängiger Teil des Arbeitsablaufs beim maschinellen Lernen
Arbeitsablaufs und wird oft in Verbindung mit anderen Mustern verwendet. Zum Beispiel können wir
Hyperparameter-Tuning verwendet werden, um die Anzahl der zu verwendenden älteren Beispiele zu bestimmen
wenn wir das Muster "Bridged Schema" implementieren. Und bei der Verwendung von Hyperparameter
Tuning ist es wichtig, zu berücksichtigen, wie wir die Modellprüfpunkte mit
virtuellen Epochen und verteiltem Training eingerichtet haben. Das Checkpoints-Entwurfsmuster
natürlich mit dem Transfer Learning verbunden, da frühere Modell-Checkpoints häufig
während der Feinabstimmung verwendet werden.

Einbettungen tauchen überall im maschinellen Lernen auf, daher gibt es viele Möglichkeiten, wie
Einbettungsmuster mit anderen Mustern interagiert. Am auffälligsten ist vielleicht das
ist das Transfer Learning, da die von den Zwischenschichten eines vorgefertigten Modells erzeugten
vortrainierten Modells im Wesentlichen gelernte Merkmalseinbettungen sind. Wir haben auch gesehen, wie
Einbindung des Entwurfsmusters Neutral Class in ein Klassifikationsmodell, entweder natu-
oder durch das Reframing-Muster, diese gelernten Einbettungen verbessern kann. Weiter
Wenn diese Einbettungen als Merkmale für ein Modell verwendet werden, könnte es außerdem
Modell verwendet werden, könnte es von Vorteil sein, sie mit dem Muster Feature Store zu
zugänglich und versionierbar sind. Oder, im Fall von Transfer Learning, könnte die Ausgabe des trainierten Modells
Ausgabe als die anfängliche Ausgabe eines Kaskadenmusters angesehen werden.

364 | Kapitel 8: Verbundene Patterns

Wir haben auch gesehen, wie man das Rebalancing-Muster durch die Kombination von zwei
anderen Entwurfsmustern: Reframing und Kaskade. Reframing würde es uns ermöglichen, den
den unausgewogenen Datensatz als eine Klassifizierung von "normal" oder "Ausreißer" darzustellen. Die
Ausgabe dieses Modells wird dann an ein sekundäres Regressionsmodell weitergeleitet, das
das für die Vorhersage auf beiden Datenverteilungen optimiert ist. Diese Muster werden wahrscheinlich auch
zum Muster "Erklärbare Vorhersagen" führen, da es bei unausgewogenen Daten besonders wichtig ist
ist es besonders wichtig, zu überprüfen, ob das Modell die richtigen Signale für die
Vorhersage aufnimmt. Es wird sogar empfohlen, das Muster der erklärbaren Vorhersagen zu berücksichtigen
zu berücksichtigen, wenn eine Lösung mit einer Kaskade von mehreren Modellen erstellt wird, da dies die
Erklärbarkeit der Modelle einschränken kann. Dieser Kompromiss zwischen der Erklärbarkeit von Modellen zeigt sich auch bei den
Ensemble- und Multimodel-Input-Mustern, da sich diese Techniken ebenfalls nicht
sich nicht gut für einige Erklärungsmethoden eignen.

Das Cascade-Entwurfsmuster könnte auch bei der Verwendung des Bridged Schema hilfreich sein
und könnte als alternatives Muster verwendet werden, indem es ein vorläufiges Modell
das fehlende Werte des sekundären Schemas unterstellt. Diese beiden Muster können dann
kombiniert werden, um den resultierenden Feature-Satz für die spätere Verwendung zu speichern, wie im Feature
speichern. Dies ist ein weiteres Beispiel, das die Vielseitigkeit des Feature
Store-Musters und wie es oft mit anderen Entwurfsmustern kombiniert wird. Zum Beispiel, ein
Feature Store eine bequeme Möglichkeit zur Pflege und Nutzung von Streaming-Model-Merkmalen
turen, die durch das Windowed Inference-Muster entstehen können. Merkmalspeicher arbeiten auch
Feature-Stores arbeiten auch Hand in Hand mit der Verwaltung verschiedener Datensätze, die im Reframing-Pattern entstehen können.
Reframing-Muster entstehen können, und bieten eine wiederverwendbare Version der Techniken, die bei der Verwendung des
Transform-Muster entstehen. Die Fähigkeit zur Versionierung von Features, wie sie im Feature
Store-Pattern besprochene Feature-Versionierung spielt auch beim Model Versioning Design Pattern eine Rolle.

Das Model Versioning Pattern hingegen ist eng verwandt mit dem Stateless
Serving Function und Continued Model Evaluation verwandt. Bei Continued Model
Evaluation können verschiedene Modellversionen verwendet werden, um zu beurteilen, wie sich die Leistung eines Modells
Leistung eines Modells im Laufe der Zeit verschlechtert hat. In ähnlicher Weise bieten die verschiedenen Modellsignaturen der
Serving-Funktion eine einfache Möglichkeit, verschiedene Modellversionen zu erstellen. Dieser
Ansatz der Modellversionierung über das Muster der zustandslosen Serving Function kann
mit dem Reframing-Muster verbunden werden, bei dem zwei verschiedene Modellversionen ihre eigenen REST-API-Endpunkte bereitstellen können.
ihre eigenen REST-API-Endpunkte für die zwei verschiedenen Modellausgaben
Darstellungen.

Wir haben auch besprochen, dass es bei der Verwendung des Musters Continued Model Evaluation
oft vorteilhaft ist, auch die im Workflow-Pipeline-Muster vorgestellten Lösungen
vorgestellt werden, um Auslöser einzurichten, die die Umschulungspipeline initiieren, und um die
Nachverfolgung der Abstammung für verschiedene Modellversionen, die erstellt werden. Fortgesetztes Modell
Evaluation ist auch eng mit dem Muster Keyed Predictions verbunden, da dieses
einen Mechanismus zur einfachen Verknüpfung der Grundwahrheit mit den Modellvorhersageergebnissen bietet.
In gleicher Weise ist das Muster Keyed Predictions auch mit dem Muster Batch
Serving-Muster. Aus demselben Grund wird das Batch Serving-Muster häufig verwendet in

Muster Wechselwirkungen | 365
in Verbindung mit dem Stateless Serving Function-Muster zur Ausführung von Vorhersageaufgaben
die sich wiederum auf das Transform-Muster stützt, um die Konsistenz zwischen Training und
Konsistenz zwischen Training und Serving aufrechtzuerhalten.

Muster innerhalb von ML-Projekten
Systeme für maschinelles Lernen ermöglichen es Teams innerhalb eines Unternehmens, maschinelle
Lösungen für maschinelles Lernen in großem Umfang zu entwickeln und zu pflegen. Sie bieten eine Plattform zur Automatisierung
und Beschleunigung aller Phasen des ML-Lebenszyklus, von der Datenverwaltung über das
Training von Modellen, Bewertung der Leistung, Bereitstellung von Modellen, Bereitstellung von Vorhersagen und Überwachung der Leistung.
Überwachung der Leistung. Die Muster, die wir in diesem Buch besprochen haben, tauchen in jedem
jedem maschinellen Lernprojekt. In diesem Abschnitt werden wir die Phasen des ML-Lebenszyklus beschreiben
Lebenszyklus und wo viele dieser Muster wahrscheinlich auftauchen.

ML-Lebenszyklus
Der Aufbau einer maschinellen Lernlösung ist ein zyklischer Prozess, der mit einem klaren
Verständnis der Geschäftsziele beginnt und schließlich dazu führt, dass ein maschinelles Lernmodell
maschinelles Lernmodell in der Produktion, das diesem Ziel dient. Dieser Überblick über den ML
Lebenszyklus (siehe Abbildung 8-2) bietet einen nützlichen Fahrplan, mit dem ML einen
Wert für Unternehmen zu schaffen. Jede der Phasen ist gleich wichtig, und die Nichterfüllung
Schritte nicht abgeschlossen werden, erhöht sich das Risiko, dass in späteren Phasen irreführende
Erkenntnisse oder Modelle ohne Wert.

Abbildung 8-2. Der ML-Lebenszyklus beginnt mit der Definition des geschäftlichen Anwendungsfalls und führt schließlich
zu einem maschinellen Lernmodell in der Produktion, das diesem Ziel dient.

366 | Kapitel 8: Verbundene Patterns

Der ML-Lebenszyklus besteht aus drei Phasen, wie in Abbildung 8-2 dargestellt: Entdeckung, Entwicklung
Entwicklung und Einsatz. Für die einzelnen Schritte jeder Phase gibt es eine kanonische Reihenfolge
Stufe. Diese Schritte werden jedoch iterativ durchgeführt, und frühere Schritte
Je nach den Ergebnissen und Erkenntnissen der späteren Phasen können frühere Schritte erneut durchlaufen werden.

Entdeckung

Maschinelles Lernen ist ein Werkzeug zur Lösung eines Problems. Die Entdeckungsphase eines ML
Projekts beginnt mit der Definition des geschäftlichen Anwendungsfalls (Schritt 1 in Abbildung 8-2). Dies ist ein entscheidender
Dies ist ein entscheidender Zeitpunkt für Unternehmensleiter und ML-Praktiker, um sich auf die Besonderheiten des
Problem abzustimmen und ein Verständnis dafür zu entwickeln, was ML tun kann und was nicht, um das
Ziel zu erreichen.

Es ist wichtig, den Geschäftswert in jeder Phase des Lebenszyklus im Auge zu behalten.
In den verschiedenen Stadien müssen viele Entscheidungen getroffen werden, und
oft gibt es keine einzige "richtige" Antwort. Vielmehr wird die beste Option dadurch bestimmt, wie
wie das Modell zur Erreichung des Geschäftsziels eingesetzt wird. Während ein machbares Ziel für ein
Forschungsprojekt sein könnte, 0,1 % mehr Genauigkeit bei einem Benchmark-Datensatz zu erreichen, ist dies
ist dies in der Industrie nicht akzeptabel. Bei einem Produktionsmodell, das für ein Unternehmen
Unternehmen erstellt wird, hängt der Erfolg von Faktoren ab, die enger mit dem Geschäft verbunden sind, wie
Kundenbindung, die Optimierung von Geschäftsprozessen, die Erhöhung der Kundenbindung
oder die Senkung der Abwanderungsrate. Es kann auch indirekte Faktoren geben, die mit dem Geschäfts
Anwendungsfall, die die Entwicklungsentscheidungen beeinflussen, wie die Geschwindigkeit der Inferenz, die Modellgröße oder die
Modellinterpretierbarkeit. Jedes Projekt zum maschinellen Lernen sollte mit einem gründlichen
Verständnis der Geschäftsmöglichkeiten und der Frage, wie ein maschinelles Lernmodell
eine spürbare Verbesserung der aktuellen Abläufe bewirken kann.

Eine erfolgreiche Entdeckungsphase erfordert die Zusammenarbeit zwischen den Fachleuten der
und Experten für maschinelles Lernen, um die Durchführbarkeit eines ML-Ansatzes zu bewerten.
Es ist von entscheidender Bedeutung, dass jemand, der das Geschäft und die Daten versteht, mit Teams zusammenarbeitet, die die technischen Herausforderungen und den technischen Aufwand verstehen.
mit Teams zusammenarbeitet, die die technischen Herausforderungen und den damit verbundenen technischen
die damit verbunden wären. Wenn die Gesamtinvestition der Entwicklungsressourcen den Wert für das Unternehmen überwiegt
den Wert für das Unternehmen übersteigt, dann ist es keine lohnende Lösung. Es ist möglich, dass
der technische Aufwand und die Kosten der Ressourcen für die Produktion den Nutzen übersteigen
den Nutzen eines Modells übersteigt, das die Abwanderungsvorhersage letztlich nur um 0,1 % verbessert. Oder
vielleicht auch nicht. Wenn der Kundenstamm eines Unternehmens 1 Milliarde Menschen umfasst, dann sind 0,1 % immer noch 1
Millionen zufriedenere Kunden.

In der Findungsphase ist es wichtig, die Geschäftsziele und den Umfang der Aufgabe zu umreißen.
Umfang der Aufgabe. Dies ist auch der Zeitpunkt, an dem festgelegt wird, welche Messgrößen zur
Erfolg gemessen oder definiert wird. Der Erfolg kann für verschiedene Organisationen oder
sogar innerhalb verschiedener Gruppen derselben Organisation. Siehe z. B. die Erörterung
über mehrere Ziele in "Allgemeine Herausforderungen beim maschinellen Lernen" auf Seite
11 in Kapitel 1. Erstellen von gut definierten Metriken und Leistungskennzahlen

Muster innerhalb von ML-Projekten | 367
(KPIs) zu Beginn eines ML-Projekts kann dazu beitragen, dass sich alle auf ein gemeinsames Ziel ausrichten.
gemeinsamen Ziel ausgerichtet sind. Im Idealfall gibt es bereits ein Verfahren, das eine konve
eine geeignete Grundlage für die Messung künftiger Fortschritte bietet. Dies könnte ein Modell sein
oder auch nur eine regelbasierte Heuristik sein, die derzeit im Einsatz ist.
Maschinelles Lernen ist nicht die Antwort auf alle Probleme, und manchmal ist eine regelbasierte Heu-
ristik schwer zu schlagen. Entwicklung sollte nicht um der Entwicklung willen betrieben werden. Ein Basis-
Basismodell, egal wie einfach es ist, ist hilfreich, um Designentscheidungen zu treffen
und zu verstehen, wie jede Designentscheidung die Nadel auf der vorgegebenen
Bewertungsmaßstab. In Kapitel 7 haben wir die Rolle einer heuristischen Benchmark sowie
sowie andere Themen im Zusammenhang mit verantwortungsvoller KI, die häufig zur Sprache kommen, wenn es um die
die Auswirkungen und den Einfluss des maschinellen Lernens mit den Stakeholdern des Unternehmens zu kommunizieren.

Natürlich sollten diese Gespräche auch im Zusammenhang mit den Daten geführt werden. A
Business Deep Dive sollte Hand in Hand mit einem Deep Dive der Datenexploration gehen (Schritt
2 von Abbildung 8-2). So vorteilhaft eine Lösung auch sein mag, wenn keine Qualitätsdaten verfügbar sind,
dann gibt es kein Projekt. Oder die Daten sind zwar vorhanden, können aber aufgrund des Datenschutzes
Datenschutzes können sie nicht verwendet werden oder müssen um relevante Informationen bereinigt werden, die für das
Modell. In jedem Fall hängen die Durchführbarkeit eines Projekts und die Erfolgsaussichten von den Daten ab.
den Daten ab. Daher ist es wichtig, dass die Datenverantwortlichen innerhalb der Organisation
frühzeitig in diese Gespräche einzubeziehen.

Die Daten steuern den Prozess, und es ist wichtig, die Qualität der verfügbaren Daten zu verstehen
die verfügbar sind. Wie ist die Verteilung der wichtigsten Merkmale? Wie viele fehlende
Werte sind vorhanden? Wie wird mit fehlenden Werten umgegangen? Gibt es Ausreißer? Sind irgendwelche
Eingabewerte hoch korreliert? Welche Merkmale gibt es in den Eingabedaten und welche Merkmale
turen sollten bearbeitet werden? Viele Modelle des maschinellen Lernens benötigen einen großen Datensatz
für das Training. Sind genügend Daten vorhanden? Wie können wir den Datensatz erweitern? Gibt es Verzerrungen im
dem Datensatz? Dies sind wichtige Fragen, und sie berühren nur die Oberfläche. Eine mögliche
mögliche Entscheidung in dieser Phase ist, dass mehr Daten oder Daten für ein bestimmtes Szenario gesammelt werden müssen
gesammelt werden müssen, bevor das Projekt fortgesetzt werden kann.

Die Datenexploration ist ein wichtiger Schritt zur Beantwortung der Frage, ob Daten von ausreichender
Qualität vorhanden sind. Gespräche allein sind selten ein Ersatz dafür, sich die Hände schmutzig zu machen
und mit den Daten zu experimentieren. Die Visualisierung spielt dabei eine wichtige Rolle
Schritt. Dichte-Diagramme und Histogramme sind hilfreich, um die Streuung der verschiedenen
Eingabewerte zu verstehen. Boxplots können helfen, Ausreißer zu identifizieren. Streudiagramme sind nützlich für die Dis-
Streudiagramme sind nützlich für die Darstellung und Beschreibung bivariater Beziehungen. Perzentile können helfen, den
Bereich für numerische Daten. Durchschnittswerte, Mediane und Standardabweichungen können helfen, die
zentrale Tendenz zu beschreiben. Mit diesen und anderen Techniken lässt sich feststellen, welche
welche Merkmale für das Modell von Vorteil sind und welche Datentransformationen
Daten für die Modellierung vorbereitet werden müssen.

368 | Kapitel 8: Verbundene Patterns

In der Entdeckungsphase kann es hilfreich sein, ein paar Modellierungsexperimente durchzuführen, um zu sehen
ob es wirklich ein "Signal im Rauschen" gibt. An diesem Punkt könnte es von Vorteil sein, eine
Machbarkeitsstudie zum maschinellen Lernen durchzuführen (Schritt 3). Wie es sich anhört, handelt es sich dabei in der Regel um einen kurzen
technischer Sprint von nur wenigen Wochen, dessen Ziel es ist, die Eignung der Daten für die
Daten für die Lösung des Problems. Dies bietet die Möglichkeit, Optionen für die Formulierung des
zu erkunden, mit der Auswahl der Algorithmen zu experimentieren und zu lernen, welche
Schritte des Feature Engineering am vorteilhaftesten wären. Der Schritt der Machbarkeitsstudie in der
Entdeckungsphase ist auch ein guter Zeitpunkt, um eine heuristische Benchmark zu erstellen (siehe
Kapitel 7).

Entwicklung

Nach der Einigung auf die wichtigsten Bewertungskennzahlen und Geschäfts-KPIs beginnt die Entwicklungsphase des
des Lebenszyklus des maschinellen Lernens begonnen. Die Einzelheiten der Entwicklung eines ML-Modells werden
werden in vielen Ressourcen zum maschinellen Lernen ausführlich behandelt. Hier heben wir die wichtigsten
Komponenten.

In der Entwicklungsphase beginnen wir mit dem Aufbau von Datenpipelines und technischen
Features (Schritt 4 in Abbildung 8-2), um die Dateneingaben zu verarbeiten, die in das Modell eingespeist werden sollen.
Die in realen Anwendungen gesammelten Daten können viele Probleme aufweisen, z. B. fehlende
Werte, ungültige Beispiele oder doppelte Datenpunkte. Datenpipelines werden benötigt, um diese
Datenpipelines werden benötigt, um diese Dateneingaben so zu verarbeiten, dass sie vom Modell verwendet werden können. Merkmalstechnik
ist der Prozess der Umwandlung von rohen Eingabedaten in Merkmale, die besser
Lernziel des Modells ausgerichtet sind und in einem Format ausgedrückt werden können, das
dem Modell zum Training zugeführt werden können. Feature-Engineering-Techniken können Folgendes beinhalten
Eingaben, die Konvertierung zwischen Datenformaten, die Tokenisierung und das Stemming von Text, die Erstellung von kat-
egorische Merkmale oder One-Hot-Codierung, Hashing von Eingaben, Erstellen von Merkmalskreuzen und
Merkmalseinbettungen und viele andere. Kapitel 2 dieses Buches behandelt Data Repre-
sentation Design Patterns und deckt viele Datenaspekte ab, die in dieser Phase
des ML-Lebenszyklus auftreten. Kapitel 5 und Kapitel 6 beschreiben Muster, die sich auf die Belastbarkeit und
Reproduzierbarkeit in ML-Systemen, die beim Aufbau von Datenpipelines helfen.

Dieser Schritt kann auch die Entwicklung der Beschriftungen für das Problem und Designentscheidungen
Entscheidungen in Bezug auf die Art der Darstellung des Problems. Bei Zeitreihenproblemen kann dies zum Beispiel
bei Zeitreihenproblemen die Erstellung von Merkmalsfenstern und das Experimentieren mit Verzögerungszeiten
und der Größe der Beschriftungsintervalle. Oder vielleicht ist es hilfreich, ein Regressionsproblem
Klassifizierung umzudeuten und die Darstellung der Beschriftungen vollständig zu ändern. Oder vielleicht ist es
Rebalancing-Techniken anzuwenden, wenn die Verteilung der Ausgangsklassen
von einer einzigen Klasse überrepräsentiert wird. Kapitel 3 dieses Buches befasst sich mit der Problemrepräsentation
Problemdarstellung und behandelt diese und andere wichtige Entwurfsmuster, die mit der
Problem-Rahmung.

Muster innerhalb von ML-Projekten | 369
Der nächste Schritt (Schritt 5 in Abbildung 8-2) der Entwicklungsphase konzentriert sich auf die Erstellung
des ML-Modells. Während dieses Entwicklungsschritts ist es entscheidend, die bewährten Praktiken zu befolgen
der Erfassung von ML-Workflows in einer Pipeline zu beachten: siehe "Entwurfsmuster 25: Workflow-Pipeline"
auf Seite 282 in Kapitel 6. Dazu gehört die Erstellung von wiederholbaren Splits für Trainings-/Validierungs-/Testsätze
Validierungs-/Testsätze zu erstellen, bevor die Modellentwicklung begonnen hat, um sicherzustellen, dass es keine Datenverluste gibt.
Datenverluste gibt. Verschiedene Modellalgorithmen oder Kombinationen von Algorithmen können trainiert werden, um
Algorithmen oder Kombinationen von Algorithmen trainiert werden, um ihre Leistung auf dem Validierungsset zu bewerten und die Qualität ihrer Vorhersagen zu untersuchen.
Vorhersagen zu überprüfen. Parameter und Hyperparameter werden abgestimmt, Regularisierungstechniken werden
Regularisierungstechniken eingesetzt und Grenzfälle untersucht. Die typische ML-Modell-Trainingsschleife wird
Kapitel 4 wird die typische Trainingsschleife eines ML-Modells im Detail beschrieben.
Entwurfsmuster für die Änderung der Trainingsschleife, um bestimmte Ziele zu erreichen.

Viele Schritte des ML-Lebenszyklus sind iterativ, und dies gilt insbesondere für die
Modellentwicklung. In vielen Fällen kann es nach einigen Experimenten notwendig sein
Daten, Geschäftsziele und KPIs zu überdenken. Neue Datenerkenntnisse werden während der
der Modellentwicklung gewonnen, und diese Erkenntnisse können ein zusätzliches Licht darauf werfen, was
möglich ist (und was nicht möglich ist). Es ist nicht ungewöhnlich, dass man viel Zeit in der
Modellentwicklungsphase zu verbringen, insbesondere bei der Entwicklung eines benutzerdefinierten Modells. Kapitel 6
befasst sich mit vielen anderen Entwurfsmustern für die Reproduzierbarkeit, die Herausforderungen angehen, die
die während dieser iterativen Phase der Modellentwicklung auftreten.

Während der gesamten Entwicklung des Modells wird jede neue Anpassung oder jeder neue Ansatz
an den Bewertungsmaßstäben gemessen, die in der Findungsphase festgelegt wurden. Daher ist die erfolgreiche
Daher ist die erfolgreiche Durchführung der Entdeckungsphase entscheidend, und es ist notwendig, die
Entscheidungen, die in dieser Phase getroffen wurden, abzustimmen. Letztendlich gipfelt die Modellentwicklung
Schließlich gipfelt die Modellentwicklung in einem abschließenden Bewertungsschritt (Schritt 6 in Abbildung 8-2). An diesem Punkt wird die Modell
wird die Modellentwicklung beendet und die Leistung des Modells wird anhand der vorher festgelegten
Bewertungsmetriken bewertet.

Eines der Hauptergebnisse der Entwicklungsphase ist die Interpretation und Präsentation der Ergebnisse
(Schritt 7 in Abbildung 8-2) für die Stakeholder und Regelungsgruppen innerhalb des Unternehmens.
Diese Bewertung auf hohem Niveau ist entscheidend und notwendig, um den Wert der
Entwicklungsphase an das Management zu kommunizieren. In diesem Schritt geht es um die Erstellung von Zahlen und Visi
Dieser Schritt konzentriert sich auf die Erstellung von Zahlen und Visionen für die ersten Berichte, die den Interessengruppen innerhalb der Organisation vorgelegt werden.
In Kapitel 7 werden einige gängige Entwurfsmuster erörtert, die sicherstellen, dass KI verantwortungsvoll eingesetzt wird
verantwortungsvoll eingesetzt wird und beim Stakeholder-Management helfen kann. In der Regel ist dies ein wichtiger Entscheidungs
Entscheidung darüber, ob weitere Ressourcen für die letzte Phase des Lebenszyklus
Lebenszyklus, der Produktion und dem Einsatz von maschinellem Lernen, eingesetzt werden.

Einsatz

Nach erfolgreichem Abschluss der Modellentwicklung und dem Nachweis vielversprechender Ergebnisse
Ergebnisse, konzentriert sich die nächste Phase auf die Produktion des Modells, wobei der erste
Der erste Schritt (Schritt 8 in Abbildung 8-2) ist die Planung des Einsatzes.

370 | Kapitel 8: Verbundene Patterns

Das Training eines Modells für maschinelles Lernen erfordert einen erheblichen Arbeitsaufwand, aber um
Aber um den Wert dieses Aufwands voll auszuschöpfen, muss das Modell in der Produktion laufen, um die
Geschäftsaktivitäten, die es verbessern soll. Es gibt verschiedene Ansätze zur Erreichung
dieses Ziel zu erreichen, und der Einsatz kann je nach Anwendungsfall in verschiedenen Organisationen unterschiedlich aussehen.
je nach Anwendungsfall. Produktionsfähige ML-Assets können zum Beispiel die Form von
interaktiven Dashboards, statischen Notebooks, Code, der in eine wiederverwendbare Bibliothek verpackt ist, oder
Endpunkte für Webdienste.

Es gibt viele Überlegungen und Designentscheidungen für die Produktion von Modellen. Wie
viele der Entscheidungen, die in der Erkundungsphase getroffen werden, auch für diesen Schritt
auch diesen Schritt. Wie soll die Umschulung des Modells gehandhabt werden? Müssen die Eingabedaten
einfließen? Soll das Training mit neuen Datenstapeln oder in Echtzeit erfolgen? Was ist mit
Modellinferenz? Sollten wir jede Woche einmalige Batch-Inferenzaufträge planen oder müssen wir
Vorhersage in Echtzeit unterstützen? Gibt es besondere Probleme mit dem Durchsatz oder der Latenz
zu berücksichtigen? Müssen sporadische Arbeitsbelastungen bewältigt werden? Ist eine niedrige Latenz eine Priorität? Ist
Netzwerkkonnektivität ein Problem? Die Entwurfsmuster in Kapitel 5 gehen auf einige der
die bei der Operationalisierung eines ML-Modells auftreten.

Dies sind wichtige Überlegungen, und diese letzte Phase stellt für viele Unternehmen die größte Hürde dar
die größte Hürde für viele Unternehmen dar, da sie eine starke Koordinierung zwischen verschiedenen Teilen der
der Organisation und die Integration einer Vielzahl von technischen Komponenten erfordern kann. Diese Schwierigkeit
Diese Schwierigkeit ist zum Teil auch auf die Tatsache zurückzuführen, dass die Produktionssteigerung die Integration eines neuen Prozesses
Prozess, der sich auf das Modell des maschinellen Lernens stützt, in ein bestehendes System integriert werden muss. Dies kann
Dies kann den Umgang mit Altsystemen erfordern, die zur Unterstützung eines einzigen Ansatzes entwickelt wurden,
oder man muss sich mit komplexen Änderungskontroll- und Produktionsprozessen
innerhalb der Organisation. Außerdem verfügen die bestehenden Systeme oft nicht über einen Mechanismus
Vorhersagen aus einem Modell für maschinelles Lernen zu unterstützen, so dass neue Anwendungen
Anwendungen und Arbeitsabläufe entwickelt werden müssen. Es ist wichtig, diese Herausforderungen zu antizipieren,
und die Entwicklung einer umfassenden Lösung erfordert erhebliche Investitionen auf der
um den Übergang so einfach wie möglich zu gestalten und die Markteinführung zu beschleunigen.
Geschwindigkeit auf den Markt zu bringen.

Der nächste Schritt der Einführungsphase ist die Operationalisierung des Modells (Schritt 9 in
Abbildung 8-2). Dieser Bereich der Praxis wird üblicherweise als MLOps (ML Opera-
tions) bezeichnet und umfasst Aspekte im Zusammenhang mit der Automatisierung, Überwachung, Prüfung, Verwaltung und
Verwaltung und Wartung von Modellen für maschinelles Lernen in der Produktion. Es ist eine notwendige Komponente für
Unternehmen, die die Anzahl der durch maschinelles Lernen gesteuerten Anwendungen
innerhalb ihrer Organisation zu skalieren.

Eines der wichtigsten Merkmale operationalisierter Modelle sind automatisierte Arbeitsabläufe
linien. Die Entwicklungsphase des ML-Lebenszyklus ist ein mehrstufiger Prozess. Der Aufbau von
Pipelines zur Automatisierung dieser Schritte ermöglicht effizientere Arbeitsabläufe und wiederholbare
Prozesse, die die künftige Modellentwicklung verbessern und eine höhere Flexibilität bei der
Lösung auftretender Probleme. Heute bieten Open-Source-Tools wie Kubeflow diese

Muster in ML-Projekten | 371
Funktionalität und viele große Softwareunternehmen haben ihre eigenen End-to-End
End-to-End-ML-Plattformen entwickelt, wie Michelangelo von Uber oder TFX von Google, die ebenfalls Open
Quelle sind.

Eine erfolgreiche Operationalisierung umfasst die Komponenten der kontinuierlichen Integration
und der kontinuierlichen Bereitstellung (CI/CD), die zu den bekannten Best Practices der Software
Entwicklung sind. Diese CI/CD-Praktiken konzentrieren sich auf Zuverlässigkeit, Reproduzierbarkeit,
Geschwindigkeit, Sicherheit und Versionskontrolle bei der Codeentwicklung. ML/AI-Workflows profitieren
Die ML/AI-Workflows profitieren von denselben Überlegungen, obwohl es einige bemerkenswerte Unterschiede gibt. Für
Zum Beispiel ist es wichtig, neben dem Code, der zur Entwicklung des Modells verwendet wird, auch die
CI/CD-Prinzipien auf die Daten anzuwenden, einschließlich Datenbereinigung, Versionierung und
Orchestrierung von Datenpipelines.

Der letzte Schritt, der in der Einführungsphase zu berücksichtigen ist, ist die Überwachung und Pflege
des Modells. Sobald das Modell operationalisiert wurde und in Produktion ist, ist es notwendig, die
ist es notwendig, die Leistung des Modells zu überwachen. Im Laufe der Zeit ändern sich die Datenverteilungen, wodurch
dass das Modell veraltet. Diese Veralterung des Modells (siehe Abbildung 8-3) kann aus
aus vielen Gründen auftreten, von Änderungen im Kundenverhalten bis hin zu Veränderungen in der Umgebung. Aus
Aus diesem Grund ist es wichtig, über Mechanismen zur effizienten Überwachung des
Modell des maschinellen Lernens und all die verschiedenen Komponenten, die zu seiner Leistung beitragen
Komponenten, die zu seiner Leistung beitragen, von der Datenerfassung bis hin zur Qualität der Vorhersagen während der Auslieferung. Die Diskussion
Erörterung von "Entwurfsmuster 18: Fortgesetzte Modellevaluation" auf Seite 220 in
Kapitel 5 behandelt dieses häufige Problem und seine Lösung im Detail.

Abbildung 8-3. Die Veralterung eines Modells kann viele Gründe haben. Regelmäßiges Umlernen der Modelle
kann dazu beitragen, ihre Leistung im Laufe der Zeit zu verbessern.

Es ist zum Beispiel wichtig, die Verteilung der Merkmalswerte zu überwachen, um sie
mit den Verteilungen zu vergleichen, die während der Entwicklungsschritte verwendet wurden. Es ist auch
Es ist auch wichtig, die Verteilung der Kennzeichnungswerte zu überwachen, um sicherzustellen, dass eine Datenabweichung
nicht zu einem Ungleichgewicht oder einer Verschiebung in der Verteilung der Bezeichnungen geführt hat. Oftmals stützt sich ein maschinelles
Lernmodell auf Daten, die von einer externen Quelle stammen. Vielleicht stützt sich unser Modell
auf eine Verkehrs-API eines Drittanbieters, um Wartezeiten für Autoabholungen vorherzusagen, oder verwendet Daten
von einer Wetter-API als Eingabe für ein Modell, das Flugverspätungen vorhersagt. Diese APIs werden nicht
nicht von unserem Team verwaltet. Wenn diese API ausfällt oder sich ihr Ausgabeformat wesentlich ändert
ändert, hat das Konsequenzen für unser Produktionsmodell. In diesem Fall ist es wichtig

372 | Kapitel 8: Verbundene Patterns

eine Überwachung einzurichten, um Änderungen in diesen vorgelagerten Datenquellen zu überprüfen. Und schließlich ist es
ist es wichtig, Systeme zur Überwachung der Vorhersageverteilungen einzurichten und, wenn möglich,
die Qualität dieser Vorhersagen in der Produktionsumgebung zu messen.

Nach Abschluss des Überwachungsschritts kann es von Vorteil sein, den Geschäfts
Anwendungsfall zu überprüfen und objektiv und genau zu bewerten, wie das maschinelle Lernmodell die
die Unternehmensleistung beeinflusst hat. Wahrscheinlich führt dies zu neuen Erkenntnissen und zum Start neuer
ML-Projekte, und der Lebenszyklus beginnt von neuem.

KI-Bereitschaft
Wir stellen fest, dass verschiedene Organisationen, die an der Entwicklung von Lösungen für maschinelles Lernen arbeiten
in unterschiedlichen Stadien der KI-Bereitschaft befinden. Laut einem Whitepaper von Goo-
gle Cloud veröffentlicht wurde, lässt sich der Reifegrad eines Unternehmens bei der Integration von KI in das Unternehmen
in drei Phasen charakterisiert werden: taktisch, strategisch und transformativ. Maschinen
Machine-Learning-Tools in diesen drei Phasen gehen von der primär manuellen Entwicklung
in der taktischen Phase, über die Verwendung von Pipelines in der strategischen Phase bis hin zur vollständig automatisierten
in der Transformationsphase.

Taktische Phase: Entwicklung des Handbuchs

Die taktische Phase der KI-Bereitschaft wird häufig in Unternehmen gesehen, die gerade erst damit beginnen
das Potenzial von KI zu erforschen, wobei der Schwerpunkt auf kurzfristigen Projekten liegt. Hier sind die
KI/ML-Anwendungsfälle eher eng gefasst und konzentrieren sich eher auf Konzeptnachweise oder Pro-
Eine direkte Verbindung zu den Geschäftszielen ist nicht immer klar. In dieser Phase,
erkennen Unternehmen das Potenzial fortschrittlicher Analyseverfahren, aber die Ausführung wird
aber die Ausführung wird in erster Linie von einzelnen Mitarbeitern durchgeführt oder vollständig an Partner ausgelagert.
Zugang zu großen, qualitativ hochwertigen Datensätzen innerhalb der Organisation kann schwierig sein.

In dieser Phase gibt es in der Regel keinen Prozess zur konsistenten Skalierung von Lösungen, und die verwendeten ML
Tools (siehe Abbildung 8-4) werden auf Ad-hoc-Basis entwickelt. Die Daten werden off-
Daten werden offline oder in isolierten Dateninseln gespeichert und manuell für die Datenexploration und -analyse genutzt.
sis. Es gibt keine Werkzeuge zur Automatisierung der verschiedenen Phasen des ML
Entwicklungszyklus zu automatisieren, und es wird wenig Wert auf die Entwicklung wiederholbarer Prozesse
des Arbeitsablaufs. Dies erschwert die gemeinsame Nutzung von Ressourcen innerhalb der Organisation.
und es gibt auch keine spezielle Hardware für die Entwicklung.

Der Umfang von MLOps ist auf einen Bestand an trainierten Modellen beschränkt, und es gibt kaum
zwischen Test- und Produktionsumgebungen, in denen das endgültige Modell
als API-basierte Lösung eingesetzt werden kann.

Muster in ML-Projekten | 373
Abbildung 8-4. Manuelle Entwicklung von KI-Modellen. Abbildung angepasst von Google Cloud-Dokumentation.
umentation.

Strategische Phase: Nutzung von Pipelines

Unternehmen, die sich in der strategischen Phase befinden, haben ihre KI-Bemühungen mit ihren Geschäftszielen und -prioritäten abgeglichen.
und Prioritäten abgestimmt, und ML wird als entscheidender Beschleuniger für das Unternehmen angesehen. Als solche gibt es
gibt es oft die Unterstützung der Geschäftsleitung und ein eigenes Budget für ML-Projekte, die
die von qualifizierten Teams und strategischen Partnern durchgeführt werden. Es gibt eine Infrastruktur, die es
Diese Teams können auf einfache Weise Ressourcen gemeinsam nutzen und ML-Systeme entwickeln, die sowohl fertige
gebrauchsfertige und benutzerdefinierte Modelle nutzen. Es gibt eine klare Unterscheidung zwischen Entwicklungs- und
Produktionsumgebungen.

Die Teams verfügen in der Regel bereits über Kenntnisse in der Datenverarbeitung und über Fachwissen in der deskriptiven und
prädiktiven Analysen. Die Daten werden in einem unternehmensweiten Data Warehouse gespeichert, und es gibt ein
einheitliches Modell für die zentralisierte Verwaltung von Daten und ML-Assets. Die Entwicklung von
ML-Modellen erfolgt in Form eines orchestrierten Experiments. Die ML-Assets und der Quellcode für
für diese Pipelines werden in einem zentralen Quellcode-Repository gespeichert und können von den
Mitgliedern der Organisation geteilt.

Die Datenpipelines für die Entwicklung von ML-Modellen sind automatisiert und nutzen einen vollständig man- gelten
serverlosen Datenservice für die Aufnahme und Verarbeitung automatisiert und sind entweder zeit- oder
ereignisgesteuert. Darüber hinaus wird der ML-Workflow für Training, Auswertung und Batch-Pre
diction durch eine automatisierte Pipeline verwaltet, so dass die Phasen des ML-Lebenszyklus,
von der Datenvalidierung und -vorbereitung bis zum Modelltraining und zur Validierung (siehe
Abbildung 8-5), durch einen Auslöser für die Leistungsüberwachung ausgeführt werden. Diese Modelle werden
Modelle werden in einem zentralisierten Register für trainierte Modelle gespeichert und können automatisch
auf der Grundlage vorher festgelegter Modellvalidierungsmetriken eingesetzt werden.

Es können mehrere ML-Systeme in der Produktion eingesetzt und gewartet werden.
Protokollierung, Leistungsüberwachung und Benachrichtigung. Die ML-Systeme nutzen ein

374 | Kapitel 8: Verbundene Patterns

Modell-API, die in der Lage ist, Datenströme in Echtzeit zu verarbeiten, sowohl für Inferenzen als auch
um Daten zu sammeln, die in die automatisierte ML-Pipeline eingespeist werden, um das Modell für
späteres Training.

Abbildung 8-5. Pipelines-Phase der KI-Entwicklung. Abbildung angepasst von Google Cloud-Dokumentation
umentation.

Transformationsphase: Vollständig automatisierte Prozesse

Unternehmen, die sich in der Transformationsphase der KI-Bereitschaft befinden, nutzen KI aktiv, um
Innovation anzuregen, Agilität zu fördern und eine Kultur des Experimentierens und
Experimentieren und Lernen fortgesetzt wird. Strategische Partnerschaften werden genutzt, um innovativ zu sein, mitzugestalten und
technische Ressourcen innerhalb des Unternehmens zu ergänzen. Viele der Entwurfsmuster
in den Kapiteln 5 und 6, die sich auf Reproduzierbarkeit und Widerstandsfähigkeit beziehen, entstehen in dieser Phase der KI
Bereitschaft.

In dieser Phase ist es üblich, dass produktspezifische KI-Teams in die
Produktteams eingebettet sind und vom Advanced-Analytics-Team unterstützt werden. Auf diese Weise
ML-Expertise in die verschiedenen Geschäftsbereiche des Unternehmens einfließen

Muster in ML-Projekten | 375
Organisation. Die etablierten gemeinsamen Muster und Best Practices sowie die Standard
Werkzeuge und Bibliotheken zur Beschleunigung von ML-Projekten werden von verschiedenen
Gruppen innerhalb der Organisation geteilt.

Die Datensätze werden in einer Plattform gespeichert, auf die alle Teams zugreifen können, so dass es einfach ist, Datensätze und
und die Wiederverwendung von Datensätzen und ML-Assets. Es gibt standardisierte ML-Funktionen
Es gibt standardisierte ML-Funktionsspeicher, und die Zusammenarbeit innerhalb des gesamten Unternehmens wird gefördert. Vollständig auto-
Unternehmen betreiben eine integrierte ML-Experimentier- und Produktionsplattform
Plattform, auf der Modelle erstellt und eingesetzt werden und ML-Praktiken für alle
jedem im Unternehmen zugänglich sind. Diese Plattform wird unterstützt durch skalierbare und serverlose
Berechnungen für Batch- und Online-Dateneingabe und -verarbeitung unterstützt. Spezialisierte ML
Beschleuniger wie GPUs und TPUs sind bei Bedarf verfügbar, und es gibt organisierte
trierte Experimente für End-to-End-Daten und ML-Pipelines.

Die Entwicklungs- und Produktionsumgebungen ähneln der Pipeline-Phase (siehe
Abbildung 8-6), haben aber CI/CD-Praktiken in jede der verschiedenen Phasen des
ihres ML-Workflows integriert. Diese CI/CD-Best-Practices konzentrieren sich auf Zuverlässigkeit, Reproduzierbarkeit
Reproduzierbarkeit und Versionskontrolle für den Code zur Erstellung der ML-Modelle sowie für die Daten
und die Datenpipelines und deren Orchestrierung. Dies ermöglicht das Erstellen, Testen und
Paketierung der verschiedenen Pipeline-Komponenten. Die Modellversionierung wird von einer ML
Model Registry verwaltet, die auch die notwendigen ML-Metadaten und Artefakte speichert.

376 | Kapitel 8: Verbundene Patterns

Abbildung 8-6. Vollständig automatisierte Prozesse unterstützen die KI-Entwicklung. Abbildung angepasst aus
Google Cloud-Dokumentation.

Gemeinsame Muster nach Anwendungsfall und Datentyp
Viele der in diesem Buch besprochenen Entwurfsmuster werden im Laufe eines jeden
jedem Entwicklungszyklus des maschinellen Lernens und werden wahrscheinlich unabhängig vom
Anwendungsfall verwendet werden - zum Beispiel Hyperparameter-Tuning, heuristischer Benchmark,
Wiederholbares Splitting, Modellversionierung, verteiltes Training, Workflow-Pipelines,
oder Prüfpunkte. Andere Entwurfsmuster sind für bestimmte Szenarien besonders nützlich.
bestimmte Szenarien. Hier gruppieren wir häufig verwendete Entwurfsmuster nach
gängigen Anwendungsfällen des maschinellen Lernens.

Verstehen natürlicher Sprache
Das Verstehen natürlicher Sprache (Natural Language Understanding, NLU) ist ein Zweig der künstlichen Intelligenz, bei dem es darum geht, eine
Maschine zu trainieren, die Bedeutung von Text und Sprache zu verstehen. NLU wird verwendet von
Sprachagenten wie Amazons Alexa, Apples Siri und Googles Assistant verwendet, um
Sätze wie "Wie ist die Wettervorhersage für dieses Wochenende?" zu verstehen. Es gibt viele
Anwendungsfälle, die unter das Dach von NLU fallen, und es kann auf eine Vielzahl von

Gemeinsame Muster nach Anwendungsfall und Datentyp | 377
Prozesse wie Textklassifizierung (E-Mail-Filterung), Entitätsextraktion, Beantwortung von Fragen
Beantwortung von Fragen, Spracherkennung, Textzusammenfassung und Stimmungsanalyse.

Einbettungen
Hash-Merkmal
Neutrale Klasse
Multimodale Eingabe
Transfer-Lernen
Zwei-Phasen-Vorhersagen
Kaskade
Gefensterte Inferenz
Computer Vision
Computer Vision ist die allgemeine Bezeichnung für KI, die Maschinen trainiert, visuelle Eingaben zu verstehen
visuelle Eingaben wie Bilder, Videos, Icons und alles, was mit Pixeln zu tun haben könnte
beteiligt sind. Computer-Vision-Modelle zielen darauf ab, jede Aufgabe zu automatisieren, die auf
zu automatisieren, von der Erkennung von Lungenkrebs mit einem MRT bis hin zu selbstfahrenden Autos. Einige
Einige klassische Anwendungen der Computer Vision sind Bildklassifizierung, Video-Bewegungsanalyse
sis, Bildsegmentierung und Bildentrauschung.

Reframing
Neutrale Klasse
Multimodaler Input
Transfer-Lernen
Einbettungen
Mehrstufige
Kaskade
Zwei-Phasen-Vorhersagen
Prädiktive Analytik
Die prädiktive Modellierung verwendet historische Daten, um Muster zu finden und die Wahrscheinlichkeit zu bestimmen
dass ein bestimmtes Ereignis in der Zukunft eintritt. Prädiktive Modelle finden sich in
vielen verschiedenen Industriezweigen zu finden. So können Unternehmen beispielsweise prädiktive Modi verwenden
um Einnahmen genauer vorherzusagen oder die zukünftige Nachfrage nach Produkten zu antizipieren. In der
Medizin können Vorhersagemodelle verwendet werden, um das Risiko der Entwicklung einer
chronische Krankheit zu entwickeln oder vorherzusagen, wann ein Patient nicht zu einem geplanten

378 | Kapitel 8: Verbundene Patterns

Ernennung. Weitere Beispiele sind Energieprognosen, Vorhersage der Kundenabwanderung,
Finanzmodellierung, Wettervorhersage und vorausschauende Wartung.

Feature Store
Merkmal Kreuz
Einbettungen
Ensemble
Transformieren
Umrahmung
Kaskade
Multilabel
Neutrale Klasse
Gefensterte Inferenz
Stapelverarbeitung
Die IoT-Analytik ist ebenfalls eine weit gefasste Kategorie, die zur prädiktiven Analytik gehört. IoT-Modelle
stützen sich auf Daten, die von mit dem Internet verbundenen Sensoren, so genannten IoT-Geräten, gesammelt werden. Nehmen wir ein
Verkehrsflugzeug, das mit Tausenden von Sensoren ausgestattet ist, die täglich mehr als 2 TB an
Daten pro Tag. Das maschinelle Lernen von IoT-Sensordaten kann Vorhersagemodelle liefern
Modelle erstellen, die vor Geräteausfällen warnen, bevor sie auftreten.

Merkmal Laden
Umgestaltung
Reframing
Hashed-Feature
Kaskade
Neutrale Klasse
Zwei-Phasen-Vorhersagen
Zustandslose Serving-Funktion
Gefensterte Inferenz
Empfehlungssysteme
Empfehlungssysteme sind eine der am weitesten verbreiteten Anwendungen des maschinellen Lernens
maschinellen Lernens in der Wirtschaft und treten häufig auf, wenn Benutzer mit Artikeln interagieren. Empfehlungs-
Empfehlungssysteme erfassen Merkmale des bisherigen Verhaltens und ähnlicher Nutzer und empfehlen
die für einen bestimmten Benutzer am relevantesten sind. Denken Sie daran, wie YouTube Ihnen eine Reihe von Videos empfiehlt
eine Reihe von Videos empfiehlt, die Sie sich auf der Grundlage Ihrer Beobachtungshistorie ansehen, oder Amazon empfiehlt

Allgemeine Muster nach Anwendungsfall und Datentyp | 379
Einkäufe auf der Grundlage der Artikel in Ihrem Einkaufswagen. Empfehlungssysteme sind in vielen
in vielen Unternehmen verbreitet, insbesondere für Produktempfehlungen, personalisiertes
personalisiertes und dynamisches Marketing und Streaming-Video- oder Musikplattformen.

Einbettungen
Ensemble
Multilabel
Transfer-Lernen
Merkmalsspeicher
Gehashtes Merkmal
Umstrukturierung
Transformieren
Gefensterte Inferenz
Zwei-Phasen-Vorhersagen
Neutrale Klasse
Multimodale Eingabe
Stapelverarbeitung
Erkennung von Betrug und Anomalien
Viele Finanzinstitute nutzen maschinelles Lernen zur Betrugserkennung, um die Konten ihrer
Konten der Kunden zu schützen. Diese maschinellen Lernmodelle werden so trainiert, dass sie Transaktionen
Transaktionen zu erkennen, die auf der Grundlage bestimmter Merkmale oder Muster, die in den Daten
die aus den Daten gelernt wurden.

Im weiteren Sinne ist die Erkennung von Anomalien eine Technik, die dazu dient, abnormales Verhalten oder
Ausreißerelemente in einem Datensatz zu finden. Anomalien können als Spitzen oder Einbrüche auftreten, die von
normalen Mustern abweichen, oder es kann sich um längerfristige anormale Trends handeln. Die Erkennung von Anomalien
taucht in vielen verschiedenen Anwendungsfällen des maschinellen Lernens auf und kann sogar
in Verbindung mit einem anderen Anwendungsfall verwendet werden. Betrachten wir zum Beispiel ein maschinelles Lern
Modell, das anhand von Bildern anomale Zuggleise identifiziert.

Neugewichtung
Merkmal Kreuz
Einbettungen
Ensemble
Zwei-Phasen-Vorhersagen
Transformieren
380 | Kapitel 8: Verbundene Patterns

Merkmal Laden
Kaskade
Neutrale Klasse
Umstrukturierung
Gemeinsame Muster nach Anwendungsfall und Datentyp | 381
Index.
A
ablation, 356
AdaBoost, 102 , 107
AdaNet, 108
KI-Plattform Notebooks, 336
KI-Plattform Pipelines, 230 , 288
KI-Plattform Vorhersage, 9, 288, 315, 354
KI-Plattform Pusher-Komponente, 288
KI-Plattform Training, 9, 197
KI-Bereitschaft, 373 -376
Alexander, Christoph, 1-2
All-Reduce-Algorithmus, 176
Anomalie-Erkennung, 132 -136, 243, 380
Apache Airflow, 284 , 292
Apache Beam, 217 , 256, 276-278, 297, 309
Apache Flink, 297 , 309
Apache Spark, 217 , 297, 309
Apigee, 314
anwendungsspezifischer integrierter Schaltkreis (siehe
ASIC)
ARIMA, 79 , 275
Arrays, 27 -28
ASIC, 184 , 239
Asynchrone Bedienung, 247
Asynchrone Ausbildung, 179 -181
Zuschreibungswerte, 136
Autoencoder, 49 -51
AutoML-Tabellen, 339
AutoML-Verfahren, 108
Autoregressiver integrierter gleitender Durchschnitt (siehe
ARIMA)
AWS Lambda, 206 , 219, 228, 314
Azure, 184 , 314
Azure-Funktionen, 206 , 228

Azure Maschinelles Lernen, 314
Azure ML Pipelines, 288
B
Bag-of-Words-Ansatz (siehe BOW-Kodierung)
bagging, 100 -101, 104-105, 108
Grundlinie, 330 -336
(siehe auch informative Baseline, uninforma-
tive Baseline)
Batch-Vorhersage, 7, 240, 247
Stapelverarbeitung, 218 -220
Batch Serving Entwurfsmuster, 201 , 213-220, 365
Chargengröße, 185 -186
Stapelverarbeitung, 208
Bayes'sche Optimierung, 193 -194
Balkensuchalgorithmus, 79
BERT, 50 -51, 174, 186
Vorspannung
Daten, 339 , 344, 350-356
(siehe auch Verzerrung bei der Datenerfassung, Datenverteilung
Verzerrung der Datenverteilung, Verzerrung der Datendarstellung,
Experimentator-Verzerrung, implizite Verzerrung,
problematische Verzerrung, stellvertretende Verzerrung, Berichts-
berichtsverzerrung)
menschlich, 12 -13, 343
(siehe auch implizite Voreingenommenheit, problematische Voreingenommenheit,
stellvertretende Voreingenommenheit, )
Modell, 72 , 100, 105, 128, 326, 343, 345-346,
356 -358
(siehe auch Etikettierungsvorurteile)
unfair, 356
Kompromiss zwischen Verzerrung und Varianz, 100 , 108
Bidirektionale Kodierung von Repräsentationen aus
Transformatoren (siehe BERT)
Index | 383
BigQuery
über, 2, 9
Merkmale von, 217 , 219, 297, 309
Leistung von, 280
Verwendung von, 33 , 52, 54-55, 91, 95, 214, 285
BigQuery Machine Learning (siehe BigQuery
ML)
BigQuery ML
über, 9
Merkmale von, 127 , 251-252
Leistung von, 57
Verwendungszwecke, 55-56, 132, 250
BigQueryExampleGen-Komponente, 284 , 286
BigTable, 309
Binäre Klassifizierung, 93-99, 322
Binärer Klassifikator, 98 , 117, 121, 153
binäre Kodierung, 37
Boolesche Variablen, 19
Anreicherung, 102 -102, 104, 105, 108
Bootstrap-Aggregation (siehe Bagging)
Engpass-Schicht, 163 -168
BOW-Kodierung, 67 -70
Box-Cox-Transformation, 27
Bridged Schema Entwurfsmuster, 250 , 266-273,
364 , 365
Bucketing, 30

C
CAIP (siehe Cloud AI Platform)
Kapazität, 100 , 157
Kartesisches Produkt, 62
Kaskade, 110 , 129-132, 271
Kaskadenentwurfsmuster, 79 , 97, 108-117, 272,
364 -365
Kassandra, 297 , 309
kategorische Daten, 6
kategorische Eingaben, 28 -31
CBOW, 50
ZentraleSpeicherStrategie, 177
Zentroid, 133 -134
Chaostheorie, 145
Checkpoint-Auswahl, 155 -157
Checkpointing, 151
Kontrollpunkte, 151 -155
Kontrollpunkt-Entwurfsmuster, 149 -161, 364
CI/CD, 289 , 291-292, 308, 372, 376
Datensatz für zivile Kommentare, 350
Klassifizierungsmodelle, 6
Klassifizierungsschwelle, 353

Ausschnitt, 23 -24
enge Beziehungen, 40 -41, 60
Cloud-KI-Plattform, 2, 9, 221, 223
Cloud-KI-Plattform Pipelines, 284 , 287
Cloud-KI-Plattform Vorhersagen, 219
Cloud-KI-Plattform Training, 285 , 287
Cloud Build, 292
Cloud Composer/Apache Airflow, 230
Cloud-Datenfluss, 219
Cloud-Funktionen, 228 , 291
Cloud-Ausführung, 206 , 315
Wolkenspanner, 219
Clustering, 5
Clustering-Modelle, 5
CNN, 71 , 169-171
Kaltstart, 32 , 35
Kombinatorische Explosion, 189
Vollständigkeit, 12
Komponenten, Definition von, 284
Computervision, 378
Konzeptabweichung, 220 , 231
Vertrauen, 98 , 120, 223
Konfusionsmatrix, 123 , 225
Konsistenz, 12 -13
Behälter, 282 , 284, 288
Kontext-Sprachmodelle, 50 -51
(siehe auch BERT, Word2Vec)
Fortgesetzte Modellauswertung Entwurfsmuster,
201 , 220-231, 314, 320, 355, 365
Kontinuierlicher Beutel mit Wörtern (siehe CBOW)
kontinuierliche Bewertung, 247 -248
Kontinuierliche Integration und kontinuierliche Bereitstellung
(siehe CI/CD)
Faltungsneuronales Netzwerk (siehe CNN)
Coral Edge TPU, 239
kontrafaktische Analyse, 339 -342
kontrafaktische Argumentation, 224
kryptografische Algorithmen, 38
benutzerdefinierte Servierfunktion, 209
D
DAG, 289 , 292
Darwin, Charles, 197
Datengenauigkeit, 11
Datenanalysten, 10
Datenerweiterung, 356
Verzerrung der Datenerfassung, 348 , 350
Verzerrungen bei der Datenverteilung, 344
Datendrift, 14 -15, 220, 231, 243, 310
384 | Index

Dateningenieure, 9, 16, 297
Datenparallelität, 175 -176, 178, 181, 184
Vorverarbeitung von Daten, 6
(siehe auch Datentransformation, Feature-Engineering
neering)
Datendarstellung, 20 -21
Verzerrung der Datendarstellung, 348
Datenwissenschaftler
Rolle von, 9, 16-17, 207, 327
Aufgaben von, 282 , 295, 311
Datentransformation, 7
Datenvalidierung, 7, 231
Data Warehouses, 51-52
Transformationen auf Datensatzebene, 255
Datensätze, Definition von, 6
Datenspeicher, 219
Entscheidungsbäume, 5, 19-21, 107, 135, 139, 327
Tiefes Galerkin-Verfahren, 147 -148
Tiefes Lernen, 4-5, 77
tiefes neuronales Netzwerk (siehe DNN-Modell)
Standard, Definition von, 312
Dichte Schichten, 64 , 76
Entwurfsmuster, Definition von, 1-2
Entwurfsmuster: Elemente wiederverwendbarer objekt-
orientierter Software, 2
Entwickler, 10 , 16
Dimensionalitätsreduktion, 5
gerichteter azyklischer Graph (siehe DAG)
diskrete Wahrscheinlichkeitsverteilung, 81 , 82, 83
Verteilte Datenverarbeitungsinfrastruktur, 214
VerteilteDatenParallel, 178
Verteilungsstrategie-Entwurfsmuster, 175 -187
DNN-Modell, 44 , 57, 107, 139
Docker-Container, 287 , 315
Abtastung, 123 , 125-127, 134-135
Dropout-Technik, 107
Dummy-Codierung, 29

E
frühzeitiges Anhalten, 155
Rand, 232 -233, 238
Einbettungsmuster, 20 -21, 39-52, 62-65,
66 , 363-364
Einbettungen, 167
(siehe auch Engpass-Schicht)
Einbettungen, als Ähnlichkeit, 47
Ensemble-Entwurfsmuster, 79 -80, 99-108, 110,
134 , 365
Ensemble-Methoden, 100

(siehe auch Bagging, Boosting, Stacking)
Epochen
Training, 6, 155
Verwendung, 140 , 150, 159-160
virtuell, 160 , 180, 364
Bewertung, Definition von, 7
Beispielbasierte Erklärung, 339 -342
ExampleGen-Komponenten, 284 -284
BeispielValidator, 284
Experimentator-Verzerrung, 345
Erklärbarkeit, 310 , 327, 329, 335, 357-358, 365
(siehe auch Deep Learning, Post-hoc-Erklärbarkeit
bility-Methode)
Erklärbare KI, 9, 136, 335, 339
Explainable Predictions Entwurfsmuster, 320 ,
326 -343, 365
exportiertes Modell, 150
F
Facetten, 231
Fairness-Indikatoren, 354
Fairness-Objektiv-Entwurfsmuster, 320 , 343-358
Farm Fingerprint Hashing-Algorithmus, 259 , 263,
265
FarmHash, 33
Fest, 298 -309
Merkmalszuweisungen, 329 -339
Merkmalsspalten, 33 , 39, 42, 252-255
Merkmalskreuz-Entwurfsmuster, 21 , 52-62, 363
Merkmalskreuz, Kardinalität, 61
Merkmalstechnik, 6, 20, 257, 295, 368
(siehe auch Datenvorverarbeitung)
Merkmalsextraktion, 21 , 172-173, 260
Feature Store Entwurfsmuster, 250 , 257, 295-310,
364 -365
Merkmal, Definition von, 7, 20
MerkmalSatz, 299 -302
Vorwärtsgerichtete neuronale Netze (siehe neuronale Netze
werke)
Feldprogrammierbares Gate-Array (FPGA), 184
Feinabstimmung, 157 , 172-173, 229
(siehe auch progressive Feinabstimmung)
Fingerabdruck-Hashing-Algorithmus, 38
Fitnessfunktion, 198
flacher Ansatz, 97
Ebene abflachen, 71
FPGA (feldprogrammierbares Gate-Array), 184
Betrugserkennung, 122 -126, 134, 220, 224, 263,
265
Index | 385
G
Gamma, Erich, 2
Gaußscher Prozess, 194
genetische Algorithmen, 194 , 197-198
GitHub-Aktionen, 292
GitLab-Auslöser, 292
GKE, 284 , 287
GLoVE, 51
Google App Engine, 206
Google Bolo, 242
Google Cloud-Funktionen, 206
Öffentliche Google Cloud-Datensätze, 9
Google Container-Register, 287
Google Kubernetes Engine (siehe GKE)
Google Übersetzen, 241
GPU, 162 , 175-178, 184, 186, 213, 287, 376
Gradient-Boosting-Maschinen, 102
Gradientenabstieg (siehe SGD)
Grafikprozessor (siehe GPU)
Gittersuche, 188 -190, 192
Grid-SearchCV, 189
Kennzeichnung der Grundwahrheit, 7, 12, 223-227

H
Hash-Eimer
Kollisionen, 35
leer, 39
Heuristik zur Auswahl von Zahlen, 34
Entwurfsmuster für Hash-Funktionen, 21 , 32-39, 363
Helm, Richard, 2
Heroku, 206
Heuristische Benchmark , 321 -324, 333, 369
Heuristisches Benchmark-Entwurfsmuster, 320 -325
versteckte Schichten, 4
hohe Kardinalität, 32 , 34, 40
Histogramm-Ausgleich, 26
Hive, 297 , 309
Hopsworks, 309
Hyperparameter-Abstimmung, 37 , 160
Hyperparameter-Abstimmung Entwurfsmuster,
187 -198, 363
Hyperparameter, 6, 187

I
Redewendungen, 22 , 28, 31
IG, 336 , 337
Bildeinbettungen, 45
ImageDataGenerator, 236

ImageNet, 45 , 49, 97, 162, 163, 165
implizite Verzerrung, 345
Imputation, 270 -273
Beginn, 45
Inferenz, 8, 143
(siehe auch ML-Approximation)
informative Grundlinie, 330 -333
Eingabe, Definition von, 7, 20
Instanz, Definition von, 7
Transformationen auf Instanzebene, 255
Integrierte Gradienten (siehe IG)
Interpretierbarkeit (siehe Erklärbarkeit)
interpretierbar durch Design, 327
IoT-Analytik, 379
irreduzibler Fehler, 100
J
Jetson Nano, 239
Johnson, Ralph, 2
JSON, 208
K
k-nächste Nachbarn (kNN), 105
Kaggle, 235
Kale, 293
Keras
über, 2, 4, 128
Merkmale von, 127 , 152, 167, 177, 190, 236
Verwendungen von, 42 -45, 64-65, 71, 73, 94, 252-255
Keras ImageDataGenerator, 129
Keras Sequentielle API, 91
Keras Trainingsschleife, 140
Kernelgröße, 72
Leistungsindikator (siehe KPI)
Keyed Predictions Entwurfsmuster, 2, 201,
243 -248, 365
Schlüssel, 243 -248
KFP (siehe Kubeflow-Pipelines)
kNN (k-nächste Nachbarn), 105
KPI , 367 -370
Kubeflow-Pipelines, 113 , 284, 288, 292
L
Etikettenverzerrung, 89
Etikett, Definition von, 7
(siehe auch Ground-Truth-Label, Vorhersage)
Beschriftung, 13 , 118, 223, 324, 345
Etiketten, überlappend, 97-99
386 | Index

LAMB, 186
Lambda-Architektur, 219
Werkzeug zur Sprachinterpretation, 354
Bibliotheksfunktion, 213
Licht auf zwei Seiten eines jeden Raums, 1-2
Stammbaumverfolgung, 294
Lineare Modelle, 5, 322
Modell des langen Kurzzeitgedächtnisses (siehe LSTM)
niedrige Latenzzeit, 8, 209-209, 215, 237, 296-298, 307
LSTM, 135 , 275, 281

M
Ingenieure für maschinelles Lernen (siehe ML-Ingenieure)
Machbarkeitsstudie zum maschinellen Lernen, 369
Rahmen für maschinelles Lernen, 14
Lebenszyklus des maschinellen Lernens (siehe ML-Lebenszyklus)
Modelle für maschinelles Lernen, 4
Probleme des maschinellen Lernens (siehe überwachtes
Lernen; unüberwachtes Lernen)
Maschinelles Lernen, Definition von, 4
MAE (mittlerer absoluter Fehler), 320
MAP (mittlere durchschnittliche Genauigkeit), 321
MapReduce, 216
Matrixfaktorisierung, 79
MD5-Hash, 37
Mittlerer absoluter Fehler (MAE), 320
Mittlere durchschnittliche Genauigkeit (MAP), 321
Maschen-TensorFlow, 184
netzfreie Annäherung, 147
Microservices-Architektur, 283
Min-Max-Skalierung, 23 -25
Gespiegelte Variable, 176
GespiegelteStrategie, 177 , 179
Gemischte Eingabedarstellung, 115
ML-Approximation, 143 , 145
ML-Ingenieure
Rolle der, 9, 16, 207, 319, 327
Aufgaben von, 295 , 297, 311, 314
ML-Lebenszyklus, 366 -367, 369-373
ML-Operationen (siehe MLOps)
ML-Pipelines, 8
ML-Forscher, 319
MLflow, 284
MLOps, 371 , 373
MNIST-Datensatz, 71 , 74
MobileNetV2, 236 , 238
Mockus, Jonas, 193
Modellbauer, 319
(siehe auch Datenwissenschaftler, ML-Forscher)

Modellkarten-Werkzeugsatz, 357
Modellkarten, 357
Modellbewertung, 8, 123, 294, 343, 345
(siehe auch Fortgesetzte Modellevaluation
Entwurfsmuster)
Modellparallelität, 175 , 183-184
Modellparameter, 187 -188
Modellverständnis (siehe Erklärbarkeit)
Entwurfsmuster für Modellversionierung, 250 , 310-317,
365
Modell, vortrainiert, 167 -169, 173, 319, 364
Modell, Textklassifizierung, 203 , 209, 250, 316
monolithische Anwendungen, 283
Monte-Carlo-Ansatz, 146 -147
Multi-Hot-Codierung, 31
Klassifizierungsprobleme mit mehreren Klassen, 90
mehrstufige Klassifizierung, 93 -95, 322
Multilabel-Entwurfsmuster, 79 , 90-99
Multilabel-, Multiklassen-Klassifikation (siehe Multi-
Label-Entwurfsmuster)
Entwurfsmuster für multimodale Eingaben, 62-77, 365
Multimodale Eingaben, Definition von, 65
MultiWorkerMirroredStrategy, 177 , 179
MySQL, 219
MySQL-Cluster, 309
N
Naive Bayes, 105
Verstehen natürlicher Sprache (NLU), 377
Netflix-Preis, 106
Neuronale maschinelle Übersetzung, 183
Neuronale Netze, 4, 147
Neutral Class Entwurfsmuster, 80 , 117-122, 320,
364
NLU (natürliches Sprachverständnis), 377
NNLM, 51
Nichtlineare Transformationen, 26 -27
numerische Daten, 6-7
O
Zielfunktion, 193
OCR (optische Zeichenerkennung), 116
Einer-gegen-Rest-Ansatz, 98
One-Hot-Codierung, 29 -30, 39-40, 48, 267
EinGerätStrategie, 179 , 180
Online-Maschinenlernen, 230
Online-Vorhersage, 7, 247
Online-Aktualisierung, 279
ONNX, 205
Index | 387
optische Zeichenerkennung (OCR), 116
Orchestrierung, Definition von, 293
Ausreißer, 24
Verzerrung der Ausgabeschicht, 128
Überanpassungsmodell, 100 , 142
(siehe auch physikbasiertes Modell)
Überanpassung, 148 -149

P
Parameter-Server-Architektur, 179
Gemeinsame Nutzung von Parametern, 89 -90
ParameterServerStrategie, 180
Partielle Differentialgleichung (siehe PDE)
Parzen-Schätzer, 194
Pattern Language, A, 1
PCA, 41 , 49
PDE, 141 -143, 146, 147
PDF, 82 , 83, 87
Physik-basiertes Modell, 142
Pipeline, 284
Pixelwerte, 71
Post-hoc-Erklärbarkeitsmethode, 329
posteriore Wahrscheinlichkeitsverteilung, 82
Genauigkeit, 124
Vorhersage, 7, 109, 213
(siehe auch Batch-Vorhersage, Inferenz, Online
Vorhersage)
Vorhersagemodellierung, 378
Hauptkomponentenanalyse (siehe PCA)
Wahrscheinlichkeitsdichtefunktion (siehe PDF)
Problematische Verzerrungen, 343 -346
Produktion von Modellen, 371
progressive Feinabstimmung, 172
Proxy-Verzerrung, 345
Pusher-Komponente, 285
PyTorch, 152 , 175, 178, 196

Q
Quantilsregression, 83 , 85
Quantisierung, 233 , 237, 238
Quantisierungsbewusstes Training, 241

R
Zufallsforst, 107 , 189
Zufallssuche, 190 , 192
Zufallsauswahl, 258 -259
RandomForestRegressor, 188
RandomizedSearchCV, 190

Bewertungen, Darstellung von, 66
Strahlenverfolgungsmodell, 143
Rebalancing-Entwurfsmuster, 109 , 115, 122-136,
350 , 365
Rückruf, 124
Empfehlungssysteme
Neukonzipierung als Regression, 84
Verwendungen für, 81 , 89, 379
Redis, 297 , 309
reduzierbarer Fehler, 100
Reframing , 123 , 129-132
Reframing-Entwurfsmuster, 79 -90, 331, 364-365
Regressionsmodelle, 6, 322
Regularisierung, 107 , 141, 146, 149, 155-157, 271
relative Häufigkeit, 31
Wiederholbarkeit, 13
Wiederholbares Splitting-Entwurfsmuster, 250 ,
258 -265, 320, 363, 370
Berichtsverzerrung, 344
Reproduzierbarkeit, 13 -14
Forschungswissenschaftler, 10
ResNet, 45
Verantwortungsvolle KI, 320 , 357, 370
REST API, für die Modellbereitstellung, 213
Auslöser für Umschulung, 228
Rollen, 9-10
Rollen, Auswirkungen der Teamgröße, 10
Runge-Kutta-Verfahren, 146
Läufe, Definition von, 285
S
SageMaker, 207 , 288, 314
Salz, 38
Shapley-Abtastung, 336
GespeicherteModelle, 205 , 212, 219
(siehe auch saved_model_cli)
saved_model_cli, 205
Skalierung, 16 , 22-23
SchemaGen, 284
scikit-learn, 14 , 22, 69, 107, 135, 189, 190
Satzeinbettungen, 174
Sequentielle API, 71 , 73
serverlos, 8, 315
Serverlose Auslöser, 228
Serving, Definition von, 7
SGD, 139 -140, 176
SHAP, 136 , 333-336
Shapley-Wert, 330 , 333
sigmoid, 94 , 99-99
388 | Index

(siehe auch sigmoidale Aktivierung)
sigmoidale Aktivierung, 91-95
Sechs-Fuß-Balkon-Muster, 1-2
Skip-Gramm-Modell, 50
Smart Compose, 356
SMOTE, 128 -129, 134
softmax, 45 , 90, 94
(siehe auch Softmax-Aktivierung)
Softmax-Aktivierung, 92
Software-Zuverlässigkeitsingenieur (SRE), 209
Störkorrelation, 37
SRE (Software-Zuverlässigkeits-Ingenieur), 209
Stapelüberlauf, 67 -69, 70, 91, 95
Stapeln, 103-104, 106-106
Interessengruppen, 319 , 327
zustandsabhängige Stromverarbeitung, 275
zustandsfähige vs. zustandslose Komponenten, 202 -203
zustandslose Funktionen, 202 -203
Entwurfsmuster für zustandslose Serving-Funktionen,
201 -213, 315, 365
StatisticsGen-Komponente, 284
stochastischer Gradientenabstieg (siehe SGD)
Stratified Split, 264
Streaming, Definition von, 8
strukturierte Daten, 6
(siehe auch kategoriale Daten, numerische Daten,
tabellarische Daten)
Überwachtes Lernen, 5
Stützvektormaschine (siehe SVM)
Surrogatfunktion, 194
Theorie des Überlebens des Stärkeren, 197
SVM, 105 , 139
Schwenken, 52
synchrones Training, 176 -181
Synthetische Minderheiten-Überstichproben-Technik (Synthetic Minority Over-sampling Technique)
(siehe SMOTE)

T
TabNet, 50 , 174
tabellarische Daten
über, 6
(siehe auch strukturierte Daten)
Anwendungen für, 91 , 173-174
Darstellung von, 65 -66
Tensor Processing Unit (siehe TPU)
TensorBoard, 354
TensorFlow
über, 2, 4, 107
Merkmale von, 135 , 152, 155, 175, 176, 252, 335

Verwendungen von, 13 , 33, 42, 56
TensorFlow-Datenüberprüfung, 231 , 354
TensorFlow-Datensatz, 158
TensorFlow Erweitert, 256 , 284
TensorFlow-Hub, 52 , 167, 169, 335
Tensorflow Lite, 233 , 237
TensorFlow Lite, 233
TensorFlow-Modellanalyse (siehe TFMA)
TensorFlow-Wahrscheinlichkeit, 86
TensorFlow Dienen, 207 , 256, 315, 354
TensorFlow Transform-Verfahren, 114
(siehe auch Transform-Entwurfsmuster)
Testdaten, 6, 140, 258, 264
Testdatensatz (siehe Testdaten)
Texteinbettungen, 42 -45
TF Hub (siehe TensorFlow Hub)
TF Lite Interpreter, 237 -238
TFMA, 354 -355
TFX, 231 , 284-292, 354
Schwellenwert-Auswahl, 96
Schwellenwert, Definition von, 95
Zeitfenstermittelwert, 218
Aktualität, 13
Tokenisierung, 42 -45
TorchServe, 207
TPAClusterResolver, 185
TPU
über, 2, 186, 376
Merkmale von, 150 , 287
Verwendungen von, 184 , 213
TPUStrategie, 185
Trainer-Komponente, 285 , 288
Trainingsdaten, 6
Trainingsbeispiele, 7
Trainingsschleife, 139 -141, 150, 155
(siehe auch Wohlverhaltens-Trainingsschleife)
Training, Definition von, 7
Training, synchron vs. asynchron,
179 -181
Schräglage bei der Ausbildung, 251 , 257, 309
Transfer Learning Entwurfsmuster, 161 -174, 356,
364 -364
Transformationskomponente, 284
Transform-Entwurfsmuster, 2, 56, 250-258, 309,
365
Versuche, Definition von, 188
Tweedie-Verteilung, 81
Entwurfsmuster für zweiphasige Vorhersagen, 201 ,
232 -243, 282
Index | 389
U
underfit-Modell, 100
Einheitlicher Approximationssatz, 144 -145
uninformative Grundlinie, 330 -332
Universal Sentence Encoder, 174
Ungeprüfte Daten, 125
unstrukturierte Daten, 6, 265
Unüberwachtes Lernen, 5, 132
Upsampling, 123 , 128-129
Nützliches Overfitting-Entwurfsmuster, 141 -149

V
Validierungsdaten, 6, 140
Validierungsdatensatz (siehe Validierungsdaten)
VGG, 164 -168, 239
Vision API, 356 , 357
Vlissides, John, 2
Wortschatz, 29 -33, 34-35, 42, 67-69

W
wohlerzogene Trainingsschleife, 155 , 157

Was-wäre-wenn-Werkzeug, 136 , 339, 347-354
Wheeler, David, 209
Entwurfsmuster "Windowed Inference", 250 ,
273 -282
Winsorisierung, 24 -25
Wort-Index, 68
Wort2Vec, 50 -51
Workflow-Pipeline-Entwurfsmuster, 112 -112,
228 , 250, 282-294, 355, 365
X
XGBoost, 69 , 102, 107, 135, 315
XRAI, 337 , 337
Z
z-Score-Normalisierung, 23 -25
390 | Index

Über die Autoren
Valliappa (Lak) Lakshmanan ist Global Head für Data Analytics und AI-Lösungen bei
Google Cloud. Sein Team entwickelt Softwarelösungen für Unternehmensprobleme unter Verwendung von Goo-
Datenanalyse- und maschinellen Lernprodukten von Google Cloud. Er gründete Googles
Advanced Solutions Lab ML Immersion Programm. Vor Google war Lak Direktor
Director of Data Science bei der Climate Corporation und Forscher bei der NOAA.

Sara Robinson ist Developer Advocate im Cloud Platform-Team von Google und beschäftigt sich
auf maschinelles Lernen. Sie inspiriert Entwickler und Datenwissenschaftler, ML in ihre Anwendungen zu integrieren.
in ihre Anwendungen zu integrieren, durch Demos, Online-Inhalte und Veranstaltungen. Sara hat einen Bakkalaureus
Bachelor-Abschluss von der Brandeis University. Bevor sie zu Google kam, war sie Developer Advocate im
dem Firebase-Team.

Michael Munn ist ein ML Solutions Engineer bei Google, wo er mit Kunden von Google Cloud
Kunden von Google Cloud bei der Entwicklung, Implementierung und Bereitstellung von Modellen für maschinelles Lernen
lernenden Modellen. Außerdem unterrichtet er ein ML Immersion Program im Advanced Solutions
Labor. Michael hat einen Doktortitel in Mathematik von der City University of New York.
Bevor er zu Google kam, arbeitete er als Forschungsprofessor.

Kolophon
Das Tier auf dem Titelbild von Machine Learning Design Patterns ist eine Sonnendommel (Eury-
pyga helias), ein Vogel, der in den tropischen Regionen Amerikas, von Guatemala bis Brasilien, vorkommt.
zil. Der engste lebende Verwandte der Sonnendommel ist der Kagu, ein Vogel, der nur in Neukaledonien
Kaledonien, einer Inselgruppe im südwestlichen Pazifik.

Sonnenhühner sind kryptisch, d. h. ihre Färbung mit subtilen schwarzen, grauen und braunen
Mustern dient als Tarnung in ihrer Umgebung. Ihre Flugfedern sind rot, gelb,
und schwarz, und wenn sie ihre Flügel ganz ausbreiten, sehen diese Federn wie Augenflecken aus. Diese
werden bei der Balz und bei Drohgebärden gezeigt und dienen der Abschreckung von Raubtieren. Die
Vögel haben Puderdaunen, eine spezielle Daunenart, die nur bei wenigen Vogelarten vorkommt.

Männchen und Weibchen wechseln sich beim Ausbrüten der Eier und Füttern der Küken ab. Die
Ernährung besteht aus einem breiten Spektrum von Tieren, darunter Insekten, Krebstiere, Fische und
Amphibien. Obwohl die Vögel nur in Gefangenschaft beobachtet wurden, hat man sie auch schon gesehen
wie sie mit Ködern fischen, um Beute in Schlagdistanz anzulocken.

Der Erhaltungszustand der Sonnendommel ist am wenigsten besorgniserregend. Viele der Tiere auf
O'Reilly sind vom Aussterben bedroht; sie alle sind wichtig für die Welt.

Die Umschlagillustration stammt von Karen Montgomery und basiert auf einem Schwarz-Weiß-Gravur
Stich aus Elements of Ornithology. Die Umschlagschriftarten sind Gilroy Semibold und Guardian
Sans. Die Textschrift ist Adobe Minion Pro, die Überschriftenschrift ist Adobe Myriad Con
densed, und die Codeschrift ist Ubuntu Mono von Dalton Maag.

Es gibt noch viel mehr

woher das kam.

Erleben Sie Bücher, Videos, Live-Online
Schulungen und vieles mehr von O'Reilly
und unseren über 200 Partnern - alles an einem Ort.
Erfahren Sie mehr unter oreilly.com/online-learning

©2019 O'Reilly Media, Inc. O'Reilly ist eine eingetragene Marke von O'Reilly Media, Inc. | 175
Dies ist ein Offline-Tool, Ihre Daten bleiben lokal und werden nicht an einen Server gesendet!
Feedback & Fehlerberichte