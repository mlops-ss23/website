---
title: Data Engineering Fundamentals
subtitle: Lecture 3
author: Jan Kirenz
format:
  revealjs:
    theme: default
    transition: fade
    slide-number: true
    chalkboard:
      buttons: false
    preview-links: auto
    logo: images/logo.png
    css: slides.scss
    footer: MLOps | Jan Kirenz
    incremental: true 
#jupyter: python3
---



## Basics of Data Engineering

*How to collect, process, store, retrieve, and process data*


- Data storage

- Data format

- Data structure

- Data models
  - define how data stored 
  - in a particular data format 
  - is structured


## Data sources {background-color="#0ca37f"}

## Data Sources


- User input data
  - text, images, videos, uploaded files, etc.

- Internal databases (services and applications)
  - Inventory
  - Customer relationship
  - ... 

- System-generated data
  - Logs (tell you how the system is doing) 
  - System outputs (like model predictions)

- Third-party data

:::{.notes}
- Log everything you can (makes debugging easier)
   - What exactly?
   - How?
   - How long?
   - Privacy regulations?
:::

## Data Sources

- First-party data
  - Data that your company already collects about your users or customers S

- Second-party data 
  - Data collected by another company on their own customers that they make available to you, though you’ll probably have to pay for it
  
- Third-party data
  - Companies collect data on the public who aren’t their direct customers


## Data Formats {background-color="#0ca37f"}

## Data Formats 

- How do I store multimodal data, e.g., a sample that might contain both images and texts?

- Where do I store my data so that it’s cheap and still fast to access?

- How do I store complex models so that they can be loaded and run correctly on different hardware?

## Data Formats

- Store ("persist") data

- Data serialization
  - Converting data into a specific format

- Format depends on how data will be used 
  - Human readability
  - Access patterns
  - ...


## Some common data formats

| **Format** | **Binary/Text** | **Human-readable** | **Example** **use** **cases** |
| ---------- | --------------- | ------------------ | ----------------------------- |
| JSON       | Text            | Yes                | Everywhere                    |
| _CSV_      | _Text_          | _Yes_              | _Everywhere_                  |
| _Parquet_  | _Binary_        | _No_               | _Hadoop,_ _Amazon_ _Redshift_ |
| Avro       | Binary primary  | No                 | Hadoop                        |
| Protobuf   | Binary primary  | No                 | Google, TensorFlow (TFRecord) |
| Pickle     | Binary          | No                 | Python, PyTorch serialization |



## Row-Major Versus Column-Major Format

- CSV (comma-separated values) is row-major

- Parquet is column-major

![](images/lec-03/fig-3-1-row-column.png)

- Row-major formats
  - better when you have to do a lot of writes

- Column-major
  - better when you have to do a lot of column-based reads.

## NumPy vs pandas

- pandas: built around DataFrame
  - Column-major

- NumPy: 
  - Major order can be specified

  ![](images/lec-03/fig-3-2-pandas.png)


## Text vs Binary Format

  - Text files
    - Plain text
    - Human-readable
    - Not very efficient
    - E.g. CSV
  
  - Binary format
    - Nontext (only 0s and 1s)
    - Very compact
    - Not human-readable
    - E.g. Parquet files

## Text vs Binary Format

Store number `1000000`

- Text file
  - 7 characters 
  - Each character 1 byte
  - 7 byte
  
- Binary file as int32
  - 32 bits or 4 bytes

## Text vs Binary Format

- File with 17,654 rows and 10 columns
  - CSV: 14 MB
  - Parquet: 6 MB 

- Parquet format is up to 2x faster to unload and consumes up to 6x less storage in Amazon S3, compared to text formats

# Data models {background-color="#0ca37f"}

*Describe how data is represented*

## Relational Models

