---
Titel: "Untitled"
format: revealjs
---

Vorwort
Seit dem ersten Kurs zum maschinellen Lernen, den ich 2017 in Stanford gehalten habe, haben mich viele Menschen
haben mich um Rat gefragt, wie sie ML-Modelle in ihren Unternehmen einsetzen können. Diese
Fragen können allgemeiner Natur sein, wie z. B. "Welches Modell sollte ich verwenden?" "Wie oft sollte ich
mein Modell neu trainieren?" "Wie kann ich Verschiebungen in der Datenverteilung erkennen? "Wie kann ich sicherstellen
dass die beim Training verwendeten Merkmale mit den Merkmalen übereinstimmen, die bei der
Inferenz verwendet werden?"

Diese Fragen können auch spezifisch sein, wie z. B. "Ich bin überzeugt, dass der Wechsel von
Batch-Vorhersage auf Online-Vorhersage umzusteigen, einen Leistungsschub für unser Modell bedeuten würde, aber
aber wie überzeuge ich meinen Vorgesetzten davon, dies zuzulassen?" oder "Ich bin der
Datenwissenschaftler in meinem Unternehmen und wurde kürzlich mit der Einrichtung unserer ersten
Plattform für maschinelles Lernen einzurichten; wo soll ich anfangen?"

Meine kurze Antwort auf all diese Fragen lautet immer: "Es kommt darauf an." Meine langen Antworten erfordern oft
stundenlange Diskussionen, um zu verstehen, woher der Fragesteller kommt, was er
was er eigentlich erreichen will, und die Vor- und Nachteile der verschiedenen Ansätze für
für seinen spezifischen Anwendungsfall.

ML-Systeme sind sowohl komplex als auch einzigartig. Sie sind komplex, weil sie sich aus
vielen verschiedenen Komponenten (ML-Algorithmen, Daten, Geschäftslogik, Bewertungsmetriken,
zugrundeliegende Infrastruktur usw.) und viele verschiedene Interessengruppen einbeziehen (Datenwissenschaftler
(Datenwissenschaftler, ML-Ingenieure, Unternehmensleiter, Nutzer und sogar die Gesellschaft im Allgemeinen). ML-Systeme sind
sind einzigartig, weil sie datenabhängig sind, und die Daten variieren von einem Anwendungsfall zum
dem nächsten.

Zum Beispiel könnten zwei Unternehmen im selben Bereich (E-Commerce) tätig sein und dasselbe Problem haben
dasselbe Problem haben, das sie mit ML lösen wollen (Empfehlungssystem), aber die daraus
ML-Systeme können jedoch eine unterschiedliche Modellarchitektur haben, unterschiedliche Merkmale verwenden, anhand
nach unterschiedlichen Maßstäben bewertet werden und unterschiedliche Investitionsrenditen bringen.

ix
Viele Blogbeiträge und Tutorials zur ML-Produktion konzentrieren sich auf die Beantwortung einer bestimmten
Frage. Diese Fokussierung hilft zwar, den Punkt zu vermitteln, kann aber den Eindruck erwecken
dass es möglich ist, jede dieser Fragen isoliert zu betrachten. In Wirklichkeit werden Änderungen
einer Komponente wahrscheinlich Auswirkungen auf andere Komponenten. Daher ist es notwendig
das System als Ganzes zu betrachten, wenn man versucht, eine Designentscheidung zu treffen.

Dieses Buch verfolgt einen ganzheitlichen Ansatz für ML-Systeme. Es berücksichtigt die verschiedenen
Komponenten des Systems und die Ziele der verschiedenen beteiligten Akteure. Der
Inhalt dieses Buches wird anhand von realen Fallstudien illustriert, von denen ich viele
Fallstudien veranschaulicht, an denen ich persönlich gearbeitet habe, und von ML-Praktikern
sowohl in der Wissenschaft als auch in der Industrie. Abschnitte, die tiefgreifendes Wissen über ein bestimmtes
Themen erfordern - z. B. Batch-Verarbeitung versus Stream-Verarbeitung, Infrastruktur für Speicher und
Rechenleistung und verantwortungsvolle KI - werden zusätzlich von Experten geprüft, deren Arbeit sich auf
auf das jeweilige Thema konzentrieren. Mit anderen Worten: Dieses Buch ist ein Versuch, nuancierte Antworten auf
die oben genannten Fragen und mehr zu geben.

Als ich die Vorlesungsunterlagen schrieb, die die Grundlage für dieses Buch bildeten, dachte ich
dachte ich, ich schreibe sie für meine Studenten, um sie auf die Anforderungen ihrer zukünftigen Jobs
als Datenwissenschaftler und ML-Ingenieure vorzubereiten. Ich merkte jedoch bald, dass ich dabei auch
durch diesen Prozess enorm viel gelernt habe. Die ersten Entwürfe, die ich mit frühen Lesern teilte
entfachten viele Gespräche, die meine Annahmen prüften, mich zwangen, andere
Perspektiven zu betrachten, und ich lernte neue Probleme und Ansätze kennen.

Ich hoffe, dass dieser Lernprozess für mich weitergeht, jetzt, wo das Buch in Ihren Händen liegt.
in der Hand halten, da Sie Erfahrungen und Perspektiven haben, die einzigartig sind. Bitte zögern Sie nicht
Sie mir Ihr Feedback zu diesem Buch über den MLOps Discord
Server, den ich betreibe (wo Sie auch andere Leser dieses Buches finden können), Twitter,
LinkedIn oder über andere Kanäle, die Sie auf meiner Website finden können.

Für wen dieses Buch gedacht ist
Dieses Buch richtet sich an alle, die ML nutzen wollen, um reale Probleme zu lösen.
ML bezieht sich in diesem Buch sowohl auf Deep Learning als auch auf klassische Algorithmen, wobei der Schwerpunkt auf
ML-Systeme im großen Maßstab, wie sie in mittleren bis großen Unternehmen und
schnell wachsende Start-ups. Systeme in kleinerem Maßstab sind in der Regel weniger komplex und können
weniger von dem in diesem Buch dargelegten umfassenden Ansatz profitieren.

Da ich aus dem Ingenieurwesen komme, richtet sich die Sprache dieses Buches an
Ingenieure, einschließlich ML-Ingenieure, Datenwissenschaftler, Dateningenieure, ML-Plattform-Ingenieure
neers und technische Leiter. Vielleicht können Sie sich mit einem der folgenden
Szenarien:

x | Vorwort

-Sie haben ein Geschäftsproblem und eine Menge Rohdaten erhalten. Sie wollen
Sie wollen diese Daten auswerten und die richtigen Metriken zur Lösung des Problems auswählen.
Ihre ersten Modelle haben sich in Offline-Experimenten bewährt und Sie wollen sie einsetzen.
sie einsetzen.
-Sie haben nur wenig Rückmeldung über die Leistung Ihrer Modelle, nachdem diese
Sie möchten eine Möglichkeit finden, Probleme mit Ihren Modellen schnell zu erkennen, zu beheben und
Probleme, auf die Ihre Modelle in der Produktion stoßen könnten, schnell zu erkennen, zu beheben und zu lösen.
-Der Prozess der Entwicklung, Evaluierung, Bereitstellung und Aktualisierung von Modellen für Ihr
Team war bisher meist manuell, langsam und fehleranfällig. Sie möchten diesen Prozess automatisieren und
diesen Prozess verbessern.
-Jeder ML-Anwendungsfall in Ihrem Unternehmen wurde mit einem eigenen Arbeitsablauf implementiert.
Sie möchten eine Grundlage schaffen (z. B. Modellspeicher, Funktionsspeicher), die gemeinsam genutzt werden kann,
Überwachungswerkzeuge), die gemeinsam genutzt und für alle Anwendungsfälle wiederverwendet werden können.
Sie sind besorgt, dass es in Ihren ML-Systemen zu Verzerrungen kommen könnte, und Sie wollen
Ihre Systeme verantwortlich machen!
Sie können auch von diesem Buch profitieren, wenn Sie zu einer der folgenden Gruppen gehören:

Tool-Entwickler, die unterversorgte Bereiche in der ML-Produktion identifizieren und
herausfinden wollen, wie sie ihre Werkzeuge im Ökosystem positionieren können.
-Personen, die nach ML-bezogenen Aufgaben in der Branche suchen.
Technische und geschäftliche Führungskräfte, die die Einführung von ML-Lösungen zur Verbesserung
um ihre Produkte und/oder Geschäftsprozesse zu verbessern. Leser ohne starken technischen
nischen Hintergrund können am meisten von den Kapiteln 1, 2 und 11 profitieren.
Was dieses Buch nicht ist
Dieses Buch ist keine Einführung in ML. Es gibt viele Bücher, Kurse und
Es gibt viele Bücher, Kurse und Ressourcen für ML-Theorien, und deshalb hält sich dieses Buch von diesen
Konzepte und konzentriert sich auf die praktischen Aspekte von ML. Um genau zu sein, geht das Buch davon aus
dass die Leser ein grundlegendes Verständnis der folgenden Themen haben:

-ML-Modelle wie Clustering, logistische Regression, Entscheidungsbäume, kollaboratives Fil- tering
und verschiedene neuronale Netzarchitekturen wie Feed-Forward, Rekursion
rent, Faltungsnetzwerke und Transformatoren
ML-Techniken wie überwachte versus unbeaufsichtigte, Gradientenabstieg, Ziel-/Verlustfunktion
Ziel-/Verlustfunktion, Regularisierung, Generalisierung und Hyperparameterabstimmung
-Metriken wie Genauigkeit, F1, Präzision, Recall, ROC, mittlerer quadratischer Fehler und
Log-Likelihood
Vorwort | xi
-Statistische Konzepte wie Varianz, Wahrscheinlichkeit und Normal-/Langschwanz
Verteilung
-Gängige ML-Aufgaben wie Sprachmodellierung, Erkennung von Anomalien, Objektklassifizierung
kation und maschinelle Übersetzung
Sie müssen diese Themen nicht in- und auswendig kennen - für Konzepte, deren genaue Definitionen
Sie müssen diese Themen nicht in- und auswendig kennen - für Konzepte, deren genaue Definitionen man sich mühsam merken muss, z. B. F1-Score, fügen wir kurze Notizen als Verweise ein.
Aber Sie sollten schon ein grobes Gefühl dafür haben, was sie bedeuten.

In diesem Buch werden zwar aktuelle Tools erwähnt, um bestimmte Konzepte und Lösungen zu veranschaulichen,
ist es kein Lehrbuch. Technologien entwickeln sich mit der Zeit weiter. Tools kommen und gehen schnell aus der Mode
schnell aus der Mode, aber grundlegende Problemlösungsansätze sollten etwas länger Bestand haben. Dieses
Buch bietet Ihnen einen Rahmen, in dem Sie das für Ihren Anwendungsfall am besten geeignete Werkzeug
Anwendungsfälle. Wenn Sie ein bestimmtes Werkzeug verwenden möchten, finden Sie in der Regel problemlos
Tutorials dafür online zu finden. Aus diesem Grund enthält dieses Buch nur wenige Codeschnipsel und konzentriert sich stattdessen
stattdessen auf eine ausführliche Diskussion über Kompromisse, Vor- und Nachteile und konkrete
Beispiele.

Navigation in diesem Buch
Die Kapitel in diesem Buch sind so gegliedert, dass sie die Probleme widerspiegeln, die Datenwissenschaftler
während des Lebenszyklus eines ML-Projekts begegnen können. Die ersten beiden
Kapiteln werden die Grundlagen für den Erfolg eines ML-Projekts gelegt, ausgehend von
der grundlegendsten Frage: Braucht Ihr Projekt ML? Es behandelt auch die Wahl der
Ziele für Ihr Projekt und wie Sie Ihr Problem so formulieren können, dass es zu
einfachere Lösungen ermöglicht. Wenn Sie mit diesen Überlegungen bereits vertraut sind und ungeduldig
zu den technischen Lösungen zu kommen, können Sie die ersten beiden Kapitel überspringen.

Die Kapitel 4 bis 6 behandeln die Phase vor dem Einsatz eines ML-Projekts: von der Erstellung der
Trainingsdaten und der Entwicklung von Funktionen bis hin zur Entwicklung und Evaluierung Ihrer Modelle in einer
einer Entwicklungsumgebung. Dies ist die Phase, in der Fachwissen sowohl über ML als auch über die
Problemdomäne besonders wichtig sind.

Die Kapitel 7 bis 9 behandeln die Einführungs- und Nachbereitungsphase eines ML-Projekts.
Anhand einer Geschichte, mit der sich viele Leser identifizieren können, lernen wir, dass
dass die Bereitstellung eines Modells nicht das Ende des Bereitstellungsprozesses ist. Das eingesetzte Modell
muss überwacht und kontinuierlich an sich ändernde Umgebungen und Geschäftsanforderungen angepasst werden.
Geschäftsanforderungen.

In den Kapiteln 3 und 10 geht es um die Infrastruktur, die erforderlich ist, damit die Beteiligten aus
zusammenarbeiten können, um erfolgreiche ML-Systeme zu entwickeln. Kapitel 3
befasst sich mit Datensystemen, während sich Kapitel 10 mit der Recheninfrastruktur und
ML-Plattformen. Ich habe lange darüber nachgedacht, wie tief ich in die Datensysteme eindringen und
wo ich sie in das Buch einführen soll. Datensysteme, einschließlich Datenbanken, Datenformate,

xii | Vorwort

Datenbewegungen und Datenverarbeitungs-Engines werden in ML-Kursen eher spärlich behandelt
Kursen behandelt, weshalb viele Datenwissenschaftler sie als unbedeutend oder
irrelevant. Nach Rücksprache mit vielen meiner Kollegen habe ich beschlossen, dass ML
Systeme von Daten abhängen, habe ich beschlossen, dass die frühzeitige Behandlung der Grundlagen von Datensystemen uns helfen wird
Daten im weiteren Verlauf des Buches zu diskutieren.

Während wir in diesem Buch viele technische Aspekte eines ML-Systems behandeln, werden ML-Systeme
von Menschen für Menschen gebaut und können einen großen Einfluss auf das Leben vieler Menschen haben. Es wäre
nachlässig, ein Buch über ML-Produktion zu schreiben, ohne ein Kapitel über die menschliche Seite zu schreiben,
und das ist der Schwerpunkt von Kapitel 11, dem letzten Kapitel.

Beachten Sie, dass sich die Rolle des "Datenwissenschaftlers" in den letzten Jahren stark weiterentwickelt hat, und
Es gab viele Diskussionen darüber, was diese Rolle beinhalten sollte.
auf einige dieser Diskussionen in Kapitel 10 eingehen. In diesem Buch verwenden wir den Begriff "Datenwissenschaftler"
als Überbegriff für alle, die an der Entwicklung und dem Einsatz von ML
Modelle arbeiten, einschließlich Personen, deren Berufsbezeichnungen ML-Ingenieure, Dateningenieure,
Datenanalysten, usw.

GitHub-Repository und Gemeinschaft
Dieses Buch wird von einem GitHub-Repository begleitet, das Folgendes enthält:

-Überblick über grundlegende ML-Konzepte
-Eine Liste der in diesem Buch verwendeten Referenzen und anderer fortgeschrittener, aktualisierter Ressourcen
-In diesem Buch verwendete Codeschnipsel
-Eine Liste von Tools, die Sie für bestimmte Probleme verwenden können, die Ihnen in Ihren
Arbeitsabläufe
Ich betreibe auch einen Discord-Server auf MLOps, auf dem Sie diskutieren und Fragen zum Buch stellen können.
Fragen über das Buch zu stellen.

In diesem Buch verwendete Konventionen
Die folgenden typografischen Konventionen werden in diesem Buch verwendet:

Kursiv
Kennzeichnet neue Begriffe, URLs, E-Mail-Adressen, Dateinamen und Dateierweiterungen.

Konstante Breite
Wird für Programmlistings sowie innerhalb von Absätzen verwendet, um auf Programmelemente
Elemente wie Variablen- oder Funktionsnamen, Datenbanken, Datentypen, Umgebungsvariablen
Variablen, Anweisungen und Schlüsselwörter.

Vorwort | xiii
Dieses Element kennzeichnet einen allgemeinen Hinweis.
Dieses Element weist auf eine Warnung oder Vorsicht hin.
Verwendung von Codebeispielen
Wie bereits erwähnt, steht zusätzliches Material (Codebeispiele, Übungen usw.) zum
https://oreil.ly/designing-machine-learning-systems-code heruntergeladen werden.

Wenn Sie eine technische Frage oder ein Problem mit den Codebeispielen haben, senden Sie bitte eine
E-Mail an bookquestions@oreilly.com.

Dieses Buch soll Ihnen helfen, Ihre Arbeit zu erledigen. Generell gilt: Wenn Beispielcode
angeboten wird, können Sie ihn in Ihren Programmen und Ihrer Dokumentation verwenden. Sie
müssen uns nicht um Erlaubnis bitten, es sei denn, Sie reproduzieren einen wesentlichen
Teil des Codes. Wenn Sie zum Beispiel ein Programm schreiben, das mehrere Teile des Codes
aus diesem Buch verwendet, ist keine Genehmigung erforderlich. Der Verkauf oder die Weitergabe von Beispielen aus
O'Reilly-Büchern ist eine Genehmigung erforderlich. Die Beantwortung einer Frage durch Zitieren dieses Buches
und das Zitieren von Beispielcode ist nicht genehmigungspflichtig. Das Einbinden einer signifikanten
Menge an Beispielcode aus diesem Buch in die Dokumentation Ihres Produkts ist
erfordert eine Erlaubnis.

Wir freuen uns über eine Namensnennung, verlangen sie aber im Allgemeinen nicht. Eine Namensnennung umfasst normalerweise
den Titel, den Autor, den Verlag und die ISBN. Zum Beispiel: "Entwurf von maschinellen
Learning Systems" von Chip Huyen (O'Reilly). Urheberrecht 2022 Huyen Thi Khanh
Nguyen, 978-1-098-10796-3."

Wenn Sie der Meinung sind, dass Ihre Verwendung von Code-Beispielen nicht in den Bereich der fairen Nutzung oder der oben genannten Erlaubnis fällt
zuwiderläuft, können Sie uns gerne unter permissions@oreilly.com kontaktieren.

O'Reilly Online Lernen
Seit mehr als 40 Jahren bietet O'Reilly Media technologische und
Technologie- und Geschäftsschulungen, Wissen und Einblicke, die den
Unternehmen zum Erfolg zu verhelfen.
xiv | Vorwort

Unser einzigartiges Netzwerk von Experten und Innovatoren teilt sein Wissen und seine Erfahrung
durch Bücher, Artikel und unsere Online-Lernplattform. Die Online-Lernplattform von O'Reilly
Plattform von O'Reilly bietet Ihnen On-Demand-Zugang zu Live-Trainingskursen, vertieften Lernpfaden
Lernpfaden, interaktiven Programmierumgebungen und einer umfangreichen Sammlung von Texten und Videos von
O'Reilly und 200+ anderen Verlagen. Weitere Informationen finden Sie unter https://oreilly.com.

Wie Sie uns kontaktieren können
Bitte richten Sie Kommentare und Fragen zu diesem Buch an den Herausgeber:

O'Reilly Media, Inc.
1005 Gravenstein Highway Nord
Sebastopol, CA 95472
800-998-9938 (in den Vereinigten Staaten oder Kanada)
707-829-0515 (international oder lokal)
707-829-0104 (Fax)
Wir haben eine Webseite für dieses Buch, auf der wir Errata, Beispiele und zusätzliche Informationen auflisten.
Informationen. Sie können diese Seite unter https://oreil.ly/designing-machine-learning- aufrufen.
Systeme.

Senden Sie eine E-Mail an bookquestions@oreilly.com, um Kommentare oder technische Fragen zu diesem Buch zu stellen.
Buch zu stellen.

Neuigkeiten und Informationen über unsere Bücher und Kurse finden Sie unter https://oreilly.com.

Finden Sie uns auf LinkedIn: https://linkedin.com/company/oreilly-media

Folgen Sie uns auf Twitter: https://twitter.com/oreillymedia

Sehen Sie uns auf YouTube: https://youtube.com/oreillymedia

Danksagungen
Es hat zwei Jahre gedauert, dieses Buch zu schreiben, und viele weitere Jahre, es vorzubereiten.
Wenn ich zurückblicke, bin ich gleichermaßen erstaunt und dankbar für die enorme Hilfe
die ich beim Schreiben dieses Buches erhalten habe. Ich habe mein Bestes getan, um die Namen aller aufzuführen
die mir hier geholfen haben, aber aufgrund der dem menschlichen Gedächtnis innewohnenden Fehlerhaftigkeit habe ich
habe ich zweifelsohne viele vergessen zu erwähnen. Wenn ich vergessen habe, Ihren Namen zu nennen, dann
nicht, weil ich Ihren Beitrag nicht zu schätzen weiß, und erinnern Sie mich bitte freundlich daran
erinnern Sie mich daran, damit ich das so schnell wie möglich nachholen kann!

Zuallererst möchte ich mich bei den Kursleitern bedanken, die mir bei der Entwicklung des Kurses und der
Kurs und den Materialien, auf denen dieses Buch basiert, geholfen haben: Michael Cooper, Xi Yin, Chloe He,
Kinbert Chou, Megan Leszczynski, Karan Goel, und Michele Catasta. Ich möchte

Vorwort | xv
danke ich meinen Professoren Christopher Ré und Mehran Sahami, ohne die der Kurs
überhaupt nicht existieren würde.

Ich möchte mich bei einer langen Liste von Rezensenten bedanken, die mich nicht nur ermutigt, sondern
sondern das Buch auch um viele Größenordnungen verbessert haben: Eugene Yan, Josh Wills,
Han-chung Lee, Thomas Dietterich, Irene Tematelewo, Goku Mohandas, Jacopo
Tagliabue, Andrey Kurenkov, Zach Nussbaum, Jay Chia, Laurens Geffert, Brian Spier-
ing, Erin Ledell, Rosanne Liu, Chin Ling, Shreya Shankar und Sara Hooker.

Ich möchte mich bei allen Lesern bedanken, die die Vorabversion des Buches gelesen und mir
Ideen zur Verbesserung des Buches gegeben haben, darunter Charles Frye, Xintong Yu, Jordan
Zhang, Jonathon Belotti und Cynthia Yu.

Natürlich wäre das Buch ohne das Team bei O'Reilly nicht möglich gewesen, insbesondere
meine Entwicklungsredakteurin, Jill Leonard, und meine Produktionsredakteure, Kristen Brown,
Sharon Tripp, und Gregory Hyman. Ich möchte mich bei Laurence Moroney, Hannes
Hapke und Rebecca Novack, die mir geholfen haben, dieses Buch von einer Idee zu einem Vorschlag zu machen.

Dieses Buch ist schließlich eine Sammlung unschätzbarer Lektionen, die ich im Laufe meiner
meiner bisherigen Karriere gelernt habe. Diese Lektionen verdanke ich meinen äußerst kompetenten und geduldigen Mitarbeitern
und ehemaligen Mitarbeitern bei Claypot AI, Primer AI, Netflix, NVIDIA und Snorkel AI.
Jede Person, mit der ich zusammengearbeitet habe, hat mir etwas Neues darüber beigebracht, wie man ML in die Welt bringt.
die Welt zu bringen.

Mein besonderer Dank gilt meinem Mitbegründer Zhenzhong Xu, der die Feuer in unserem Startup gelöscht hat
Startup gelöscht hat und mir erlaubt hat, Zeit mit diesem Buch zu verbringen. Danke, Luke, dass du immer
dass du mich immer bei allem unterstützt, was ich tun möchte, egal wie ehrgeizig es ist.

xvi | Vorwort

1 Mike Schuster, Melvin Johnson, und Nikhil Thorat, "Zero-Shot Translation with Google's Multilingual Neural
Machine Translation System," Google AI Blog, November 22, 2016, https://oreil.ly/2R1CB.
2 Larry Hardesty, "A Method to Image Black Holes," MIT News, June 6, 2016, https://oreil.ly/HpL2F.
KAPITEL 1

Überblick über maschinelle Lernsysteme
Im November 2016 gab Google bekannt, dass es sein mehrsprachiges
neuronales maschinelles Übersetzungssystem in Google Translate integriert hat und damit eine der ersten
Erfolgsgeschichte von tiefen künstlichen neuronalen Netzwerken im Produktionsmaßstab.^1 Laut Google
Google zufolge hat sich mit diesem Update die Qualität der Übersetzung auf einen Schlag mehr verbessert
als in den vorangegangenen 10 Jahren zusammen.

Dieser Erfolg von Deep Learning hat das Interesse an maschinellem Lernen (ML) im Allgemeinen neu belebt.
Seitdem haben sich immer mehr Unternehmen ML zugewandt, um Lösungen für ihre
schwierigsten Probleme. In nur fünf Jahren hat ML seinen Weg in fast
wie wir auf Informationen zugreifen, wie wir kommunizieren, wie wir arbeiten
wie wir arbeiten, wie wir Liebe finden. Die Verbreitung von ML ist so rasant, dass wir uns ein Leben ohne ML kaum noch
Leben ohne sie vorzustellen. Doch es gibt noch viele weitere Anwendungsfälle für ML, die darauf warten, erforscht zu werden
in Bereichen wie Gesundheitsfürsorge, Verkehr, Landwirtschaft und sogar bei der Erforschung
das Universum zu verstehen.^2

Viele Menschen denken beim Begriff "maschinelles Lernsystem" nur an die verwendeten ML-Algorithmen
rithmen wie logistische Regression oder verschiedene Arten von neuronalen Netzen.
Der Algorithmus ist jedoch nur ein kleiner Teil eines ML-Systems in der Produktion. Das
System umfasst auch die geschäftlichen Anforderungen, die das ML-Projekt überhaupt erst entstehen ließen
die Schnittstelle, über die Benutzer und Entwickler mit Ihrem System interagieren, den
Datenstapel und die Logik für die Entwicklung, Überwachung und Aktualisierung Ihrer Modelle sowie die
sowie die Infrastruktur, die die Bereitstellung dieser Logik ermöglicht. Abbildung 1-1 zeigt Ihnen
die verschiedenen Komponenten eines ML-Systems und in welchen Kapiteln dieses Buches sie
behandelt werden.

1
Die Beziehung zwischen MLOps und ML Systems Design
Ops in MLOps kommt von DevOps, kurz für Developments and
Betrieb. Etwas zu operationalisieren bedeutet, es in die
Produktion zu bringen, was die Bereitstellung, Überwachung und Wartung
es. MLOps ist eine Reihe von Tools und bewährten Verfahren für die Einführung von ML
in Produktion zu bringen.
Das Design von ML-Systemen verfolgt bei MLOps einen Systemansatz, was bedeutet
Das bedeutet, dass ein ML-System ganzheitlich betrachtet wird, um sicherzustellen, dass alle
Komponenten und ihre Akteure zusammenarbeiten können, um die
die festgelegten Ziele und Anforderungen zu erfüllen.
Abbildung 1-1. Verschiedene Komponenten eines ML-Systems. "ML-Algorithmen" ist normalerweise das, woran
wenn man von maschinellem Lernen spricht, aber das ist nur ein kleiner Teil des gesamten
Systems.

Es gibt viele ausgezeichnete Bücher über verschiedene ML-Algorithmen. Dieses Buch behandelt
behandelt keine spezifischen Algorithmen im Detail, sondern hilft dem Leser, das gesamte
ML-System als Ganzes zu verstehen. Mit anderen Worten, das Ziel dieses Buches ist es, Ihnen einen Rahmen
Rahmen für die Entwicklung einer Lösung, die für Ihr Problem am besten geeignet ist, unabhängig davon, welchen
Algorithmus Sie am Ende verwenden werden. Algorithmen können schnell veraltet sein, da
Algorithmen können schnell veralten, da ständig neue Algorithmen entwickelt werden, aber der in diesem
Buch vorgeschlagene Rahmen sollte auch mit neuen Algorithmen funktionieren.

Das erste Kapitel des Buches soll Ihnen einen Überblick darüber geben, was es braucht, um
ein ML-Modell in Produktion zu bringen. Bevor wir auf die Entwicklung eines ML-Systems eingehen, ist es wichtig




2 | Kapitel 1: Überblick über maschinelle Lernsysteme

3 Ich habe nicht gefragt, ob ML ausreicht, denn die Antwort ist immer nein.
Es ist wichtig, die grundsätzliche Frage zu stellen, wann ML verwendet werden soll und wann nicht. Wir werden
werden wir einige der populären Anwendungsfälle von ML behandeln, um diesen Punkt zu veranschaulichen.

Nach den Anwendungsfällen werden wir uns den Herausforderungen beim Einsatz von ML-Systemen zuwenden und
und vergleichen dabei ML in der Produktion mit ML in der Forschung sowie mit herkömmlicher
Software. Wenn Sie sich mit der Entwicklung angewandter ML-Systeme befasst haben, sind Sie vielleicht
mit dem, was in diesem Kapitel steht, bereits vertraut sein. Wenn Sie jedoch bisher nur
Erfahrung mit ML in einem akademischen Umfeld haben, wird Ihnen dieses Kapitel einen ehrlichen Blick auf
ML in der realen Welt und bereitet Ihre erste Anwendung auf den Erfolg vor.

Wann wird maschinelles Lernen eingesetzt?
Mit der zunehmenden Verbreitung in der Industrie hat sich ML als leistungsfähiges Werkzeug
für eine breite Palette von Problemen. Trotz einer unglaublichen Menge an Aufregung und Hype
und Hype, der sowohl innerhalb als auch außerhalb des Fachgebiets erzeugt wurde, ist ML kein magisches Werkzeug
alle Probleme lösen kann. Selbst für Probleme, die ML lösen kann, sind ML-Lösungen möglicherweise nicht
die optimalen Lösungen sein. Bevor Sie ein ML-Projekt starten, sollten Sie sich fragen, ob
ML notwendig oder kosteneffizient ist.^3

Um zu verstehen, was ML leisten kann, sollten wir untersuchen, was ML-Lösungen im Allgemeinen tun:

Maschinelles Lernen ist ein Ansatz zum (1) Lernen (2) komplexer Muster aus (3) vorhandenen
Daten zu lernen und diese Muster zu verwenden, um (4) Vorhersagen über (5) ungesehene Daten zu treffen.
Wir werden uns jeden der kursiv gedruckten Schlüsselbegriffe im obigen Rahmen ansehen, um zu verstehen
Implikationen für die Probleme, die ML lösen kann:

Lernen: das System hat die Fähigkeit zu lernen
Eine relationale Datenbank ist kein ML-System, weil sie nicht die Fähigkeit hat
zu lernen. Sie können die Beziehung zwischen zwei Spalten in einer relationalen Datenbank explizit angeben.
relationalen Datenbank kann man die Beziehung zwischen zwei Spalten explizit angeben, aber es ist unwahrscheinlich, dass sie die Fähigkeit hat, die Beziehung zwischen diesen beiden Spalten selbst herauszufinden.
zwischen diesen beiden Spalten selbst herauszufinden.
Damit ein ML-System lernen kann, muss es etwas geben, aus dem es lernen kann. In den meisten
Fällen lernen ML-Systeme aus Daten. Beim überwachten Lernen, das auf Beispielen
Eingabe- und Ausgabepaaren, lernen ML-Systeme, wie sie Ausgaben für beliebige Eingaben
Eingaben zu erzeugen. Wenn Sie zum Beispiel ein ML-System entwickeln wollen, das lernt, den
den Mietpreis für Airbnb-Angebote vorhersagen kann, müssen Sie einen Datensatz bereitstellen, bei dem jede
Eingabe eine Auflistung mit relevanten Merkmalen ist (Quadratmeterzahl, Anzahl der Zimmer,
Nachbarschaft, Ausstattung, Bewertung des Angebots usw.), und die zugehörige Ausgabe ist der
der Mietpreis für dieses Angebot. Einmal gelernt, sollte dieses ML-System in der Lage sein
in der Lage sein, den Preis für ein neues Angebot anhand seiner Merkmale vorherzusagen.
Wann man maschinelles Lernen einsetzen sollte | 3
4 Muster sind etwas anderes als Verteilungen. Wir kennen die Verteilung der Ergebnisse eines fairen Würfels, aber es gibt
aber es gibt keine Muster in der Art und Weise, wie die Ergebnisse erzeugt werden.
5 Andrej Karpathy, "Software 2.0", Medium, November 11, 2017, https://oreil.ly/yHZrE.
Komplexe Muster: Es gibt Muster zu lernen, und sie sind komplex.
ML-Lösungen sind nur dann sinnvoll, wenn es Muster zu lernen gibt. Vernünftige Menschen
investieren kein Geld in den Aufbau eines ML-Systems zur Vorhersage des nächsten Ergebnisses
eines fairen Würfels vorhersagen kann, weil es kein Muster gibt, wie diese Ergebnisse generiert werden.^4
Allerdings gibt es Muster in der Preisbildung von Aktien, und deshalb haben Unternehmen
Daher haben Unternehmen Milliarden von Dollar in den Aufbau von ML-Systemen investiert, um diese Muster zu lernen.
Ob es ein Muster gibt, ist vielleicht nicht offensichtlich, oder wenn es Muster gibt, sind Ihr Datensatz
oder die ML-Algorithmen möglicherweise nicht aus, um sie zu erfassen. Zum Beispiel könnte es
könnte ein Muster darin bestehen, wie Elon Musks Tweets die Preise von Kryptowährungen beeinflussen. Wie-
Aber das wissen Sie erst, wenn Sie Ihre ML-Modelle auf seine Tweets trainiert und bewertet haben.
Modelle auf seine Tweets trainiert und ausgewertet haben. Selbst wenn alle Ihre Modelle keine vernünftigen Vorhersagen
von Kryptowährungspreisen machen, heißt das nicht, dass es kein Muster gibt.
Betrachten Sie eine Website wie Airbnb mit einer Vielzahl von Wohnungsangeboten; jedes Angebot
mit einer Postleitzahl. Wenn Sie die Angebote nach den Staaten sortieren möchten, in denen sie sich befinden,
bräuchten Sie kein ML-System. Da das Muster einfach ist - jede Postleitzahl
einem bekannten Staat entspricht, können Sie einfach eine Nachschlagetabelle verwenden.
Die Beziehung zwischen einem Mietpreis und allen seinen Merkmalen folgt einem viel
komplexeren Muster, das manuell nur sehr schwer zu spezifizieren wäre. ML
ist eine gute Lösung für diese Aufgabe. Anstatt Ihrem System mitzuteilen, wie es den
Preis aus einer Liste von Merkmalen zu berechnen, können Sie Preise und Merkmale angeben und
lassen Sie Ihr ML-System das Muster herausfinden. Der Unterschied zwischen ML-Lösungen
und der Nachschlagetabellenlösung sowie den allgemeinen traditionellen Softwarelösungen ist
in Abbildung 1-2 dargestellt. Aus diesem Grund wird ML auch als Software 2.0 bezeichnet.^5
ML hat sich bei Aufgaben mit komplexen Mustern wie der Objekterkennung
Objekterkennung und Spracherkennung. Was für Maschinen komplex ist, unterscheidet sich von
was für Menschen komplex ist. Viele Aufgaben, die für Menschen schwer zu bewältigen sind
sind für Maschinen einfach - zum Beispiel das Erhöhen einer Zahl um die Potenz 10. Andererseits
Andererseits können viele Aufgaben, die für Menschen leicht sind, für Maschinen schwer sein.
zum Beispiel zu entscheiden, ob auf einem Bild eine Katze zu sehen ist.
4 | Kapitel 1: Überblick über maschinelle Lernsysteme

6 Wir werden uns in Kapitel 9 mit dem Online-Lernen befassen.
Abbildung 1-2. Anstatt von Hand spezifizierte Muster zur Berechnung der Ausgaben zu benötigen, lernen ML
Lösungen Muster aus Eingaben und Ausgaben lernen
Vorhandene Daten: Daten sind verfügbar, oder es ist möglich, Daten zu sammeln
Da ML aus Daten lernt, muss es Daten geben, aus denen es lernen kann. Es ist amüsant
Es ist amüsant, ein Modell zu entwickeln, das vorhersagt, wie viel Steuern eine Person pro Jahr zahlen muss.
Jahr zahlen sollte, aber das ist nicht möglich, es sei denn, man hat Zugang zu Steuer- und Einkommensdaten einer großen
Bevölkerung.
Im Zusammenhang mit dem Zero-Shot-Learning (manchmal auch als Zero-Data-Learning bezeichnet) ist es
ist es für ein ML-System möglich, gute Vorhersagen für eine Aufgabe zu machen, ohne dass es
auf Daten für diese Aufgabe trainiert wurde. Allerdings wurde dieses ML-System zuvor
Daten für andere Aufgaben trainiert, die oft mit der fraglichen Aufgabe zusammenhängen. Obwohl also
das System zwar keine Daten für die betreffende Aufgabe benötigt, um daraus zu lernen, benötigt es dennoch
benötigt es dennoch Daten zum Lernen.
Es ist auch möglich, ein ML-System ohne Daten zu starten. Zum Beispiel, im
Kontext des kontinuierlichen Lernens können ML-Modelle eingesetzt werden, ohne dass sie
Daten trainiert wurden, aber sie lernen aus den eingehenden Daten in der Produktion.^6
Die Bereitstellung unzureichend trainierter Modelle für die Nutzer birgt jedoch gewisse Risiken,
wie z. B. eine schlechte Kundenerfahrung.
Ohne Daten und ohne kontinuierliches Lernen verfolgen viele Unternehmen einen "fake-it-
til-you make it"-Ansatz: Sie bringen ein Produkt auf den Markt, das von Menschen gemachte Vorhersagen
Vorhersagen, die von Menschen und nicht von ML-Modellen gemacht werden, in der Hoffnung, die generierten Daten später zum Trainieren von
ML-Modelle später zu trainieren.
Wann man maschinelles Lernen einsetzen sollte | 5
7 Steke Bako, Thijs Vogels, Brian McWilliams, Mark Meyer, Jan Novák, Alex Harvill, Pradeep Sen, Tony Derose,
und Fabrice Rousselle, "Kernel-Predicting Convolutional Networks for Denoising Monte Carlo Renderings,"
ACM Transactions on Graphics 36, no. 4 (2017): 97, https://oreil.ly/EeI3j; Oliver Nalbach, Elena Arabadzhiy-
ska, Dushyant Mehta, Hans-Peter Seidel, and Tobias Ritschel, "Deep Shading: Convolutional Neural Networks
for Screen-Space Shading", arXiv, 2016, https://oreil.ly/dSspz.
Vorhersagen: Es ist ein prädiktives Problem
ML-Modelle machen Vorhersagen, daher können sie nur Probleme lösen, die
prädiktive Antworten erfordern. ML kann besonders attraktiv sein, wenn man von einer großen Menge
große Menge an billigen, aber ungefähren Vorhersagen profitieren kann. Im Englischen bedeutet "vorhersagen"
"einen Wert für die Zukunft schätzen". Zum Beispiel: Wie wird das Wetter
morgen sein? Wer wird dieses Jahr den Super Bowl gewinnen? Welchen Film wird sich ein Nutzer
als nächstes sehen?
Da prädiktive Maschinen (z. B. ML-Modelle) immer effektiver werden, werden immer mehr
werden immer mehr Probleme zu Vorhersageproblemen umgewandelt. Welche Frage auch immer
Sie haben, Sie können sie immer so formulieren: "Was wäre die Antwort auf diese Frage?
Frage sein?", unabhängig davon, ob sich die Frage auf etwas in der
Zukunft, der Gegenwart oder sogar der Vergangenheit geht.
Rechenintensive Probleme sind eine Klasse von Problemen, die sehr erfolgreich
erfolgreich als prädiktiv umgestaltet wurden. Anstatt das genaue Ergebnis eines Prozesses zu berechnen
Prozesses zu berechnen, was noch rechenintensiver und zeitaufwändiger sein kann
als ML, können Sie das Problem folgendermaßen formulieren: "Wie würde das Ergebnis dieses Prozesses
Prozesses aussehen?" und es mit Hilfe eines ML-Modells annähern. Die Ausgabe ist eine
eine Annäherung an das exakte Ergebnis, aber oft ist es gut genug. Sie können eine Menge
bei grafischen Renderings, z. B. bei der Bildentrauschung und der Bildschirmschattierung.^7
Ungesehene Daten: Ungesehene Daten teilen Muster mit den Trainingsdaten
Die Muster, die Ihr Modell aus den vorhandenen Daten lernt, sind nur dann nützlich, wenn die ungesehenen Daten
diese Muster ebenfalls aufweisen. Ein Modell, das vorhersagt, ob eine App zu Weihnachten heruntergeladen
Weihnachten 2020 heruntergeladen wird, wird nicht sehr gut funktionieren, wenn es mit Daten aus dem Jahr 2008 trainiert wurde,
als die beliebteste App im App Store Koi Pond war. Was ist Koi Pond?
Ganz genau.
Technisch ausgedrückt bedeutet das, dass Ihre ungesehenen Daten und Ihre Trainingsdaten
aus ähnlichen Verteilungen stammen. Sie fragen sich vielleicht: "Wenn die Daten ungesehen sind, wie können wir
wissen, aus welcher Verteilung sie stammen?" Das wissen wir nicht, aber wir können Vermutungen anstellen.
Vermutungen anstellen, z. B. dass sich das Verhalten der Nutzer morgen nicht allzu sehr
Verhalten der Nutzer nicht allzu sehr von dem der heutigen Nutzer unterscheidet, und hoffen, dass unsere Annahmen zutreffen. Wenn
nicht, haben wir ein Modell, das schlecht abschneidet, was wir vielleicht durch
mit Hilfe der in Kapitel 8 behandelten Überwachung und der in Kapitel 9 behandelten Produktionstests herausfinden.
in Kapitel 9 behandelt wird.
6 | Kapitel 1: Überblick über maschinelle Lernsysteme

Aufgrund der Art und Weise, wie die meisten ML-Algorithmen heute lernen, werden ML-Lösungen besonders gut funktionieren, wenn
Ihr Problem die folgenden zusätzlichen Merkmale aufweist:

Es ist repetitiv
Menschen sind gut darin, aus wenigen Bildern zu lernen: Man kann Kindern ein paar Bilder von Katzen zeigen
und die meisten von ihnen werden eine Katze erkennen, wenn sie das nächste Mal eine sehen. Trotz aufregender
Fortschritte in der Forschung zum "few-shot learning" benötigen die meisten ML-Algorithmen immer noch viele
Beispiele, um ein Muster zu lernen. Wenn sich eine Aufgabe wiederholt, wird jedes Muster mehrfach
wiederholt, was es für Maschinen einfacher macht, es zu lernen.
Die Kosten für falsche Vorhersagen sind gering
Wenn die Leistung Ihres ML-Modells nicht immer 100 % beträgt, was bei sinnvollen Aufgaben höchst
was bei allen sinnvollen Aufgaben sehr unwahrscheinlich ist, wird Ihr Modell Fehler machen. ML ist
besonders geeignet, wenn die Kosten für eine falsche Vorhersage gering sind. Zum Beispiel ist einer der
einer der wichtigsten Anwendungsfälle von ML sind Empfehlungssysteme, denn bei
Empfehlungssystemen wird eine schlechte Empfehlung in der Regel verziehen - der Benutzer klickt einfach
klickt einfach nicht auf die Empfehlung.
Wenn ein einziger Vorhersagefehler katastrophale Folgen haben kann, ist ML vielleicht trotzdem
eine geeignete Lösung sein, wenn die Vorteile richtiger Vorhersagen im Durchschnitt die
Kosten der falschen Vorhersagen überwiegen. Die Entwicklung von selbstfahrenden Autos ist eine Herausforderung, weil ein
algorithmischer Fehler zum Tod führen kann. Dennoch wollen viele Unternehmen
selbstfahrende Autos entwickeln, weil sie das Potenzial haben, viele Leben zu retten, sobald
Selbstfahrende Autos sind statistisch gesehen sicherer als menschliche Fahrer.
Es geht um Größenordnungen
ML-Lösungen erfordern häufig nicht unerhebliche Vorabinvestitionen in Daten, Rechenleistung
Infrastruktur und Talente. Es wäre also sinnvoll, wenn wir diese Lösungen häufig nutzen könnten.
"In großem Maßstab" bedeutet für verschiedene Aufgaben unterschiedliche Dinge, aber im Allgemeinen bedeutet es
viele Vorhersagen machen. Beispiele sind das Sortieren von Millionen von E-Mails
pro Jahr oder die Vorhersage, an welche Abteilungen Tausende von Support-Tickets
weiterzuleiten sind.
Ein Problem mag als einzelne Vorhersage erscheinen, aber in Wirklichkeit ist es eine Reihe von
Vorhersagen. Ein Modell, das vorhersagt, wer die Präsidentschaftswahlen in den USA gewinnen wird
Präsidentschaftswahlen vorhersagt, scheint nur eine Vorhersage alle vier Jahre zu treffen, aber es könnte
tatsächlich jede Stunde oder sogar noch häufiger eine Vorhersage, weil diese
Vorhersage ständig aktualisiert werden muss, um neue Informationen einzubeziehen.
Ein Problem in großem Maßstab bedeutet auch, dass Sie eine große Menge an Daten sammeln müssen,
die für das Training von ML-Modellen nützlich sind.
Wann man maschinelles Lernen einsetzen sollte | 7
Die Muster ändern sich ständig
Kulturen ändern sich. Geschmäcker ändern sich. Technologien ändern sich. Was heute in Mode ist, kann
morgen schon Schnee von gestern sein. Denken Sie an die Klassifizierung von E-Mail-Spam. Heute ist
ein Hinweis auf eine Spam-E-Mail ein nigerianischer Prinz sein, aber morgen könnte es ein
verzweifelter vietnamesischer Schriftsteller sein.
Wenn Ihr Problem ein oder mehrere sich ständig ändernde Muster umfasst, können hart kodierte
Lösungen, wie z. B. handschriftliche Regeln, schnell veraltet sein. Herausfinden
wie sich Ihr Problem verändert hat, damit Sie Ihre handschriftlichen Regeln entsprechend aktualisieren können
entsprechend aktualisieren können, kann zu teuer oder unmöglich sein. Da ML aus Daten lernt,
können Sie Ihr ML-Modell mit neuen Daten aktualisieren, ohne dass Sie herausfinden müssen, wie
die Daten geändert haben. Es ist auch möglich, Ihr System so einzurichten, dass es sich an die
Datenverteilungen anzupassen, ein Ansatz, den wir im Abschnitt "Kontinuierliches
Lernen" auf Seite 264.
Die Liste der Anwendungsfälle lässt sich endlos fortsetzen und wird noch länger werden, wenn die Einführung von ML
in der Branche reift. Auch wenn ML eine Teilmenge von Problemen sehr gut lösen kann,
kann es viele Probleme nicht lösen und/oder sollte nicht für viele Probleme verwendet werden. Die meisten der heutigen ML
Algorithmen sollten unter den folgenden Bedingungen nicht verwendet werden:

-Es ist unethisch. Im Abschnitt "Fallstudie I: Voreingenommenheit des automatischen Beurteilers" wird eine Fallstudie behandelt, bei der der Einsatz von ML-Algorithmen
als unethisch bezeichnet werden kann, wird im Abschnitt "Fallstudie I: Voreingenommenheit des automatisierten Beurteilers" auf
Seite 341.
-Einfachere Lösungen genügen. In Kapitel 6 werden wir die vier Phasen der ML
Modellentwicklung, wobei in der ersten Phase Nicht-ML-Lösungen verwendet werden sollten.
-Es ist nicht kosteneffektiv.
Aber selbst wenn ML Ihr Problem nicht lösen kann, ist es vielleicht möglich, Ihr Problem in kleinere
Problem in kleinere Komponenten aufzuteilen und ML zur Lösung einiger dieser Komponenten einzusetzen. Zum Beispiel,
Wenn Sie nicht in der Lage sind, einen Chatbot zu bauen, der alle Fragen Ihrer Kunden beantwortet, ist es vielleicht möglich
ein ML-Modell zu erstellen, das vorhersagt, ob eine Anfrage mit einer der häufig gestellten
Fragen passt. Wenn ja, leiten Sie den Kunden zur Antwort weiter. Wenn nicht, verweisen Sie ihn an den
Kundendienst.

Ich möchte auch davor warnen, eine neue Technologie zu verwerfen, weil sie nicht so kosteneffizient ist wie die
kostengünstiger ist als die bestehenden Technologien. Die meisten technologischen Fortschritte sind
schrittweise. Eine bestimmte Technologie ist vielleicht jetzt noch nicht effizient, aber mit der Zeit
Zeit mit weiteren Investitionen. Wenn Sie warten, bis sich die Technologie in der Branche bewährt hat
der Branche bewährt hat, bevor Sie auf den Zug aufspringen, könnten Sie am Ende Jahre oder Jahrzehnte hinter
Ihren Konkurrenten.

8 | Kapitel 1: Überblick über maschinelle Lernsysteme

Anwendungsfälle für maschinelles Lernen
ML wird zunehmend sowohl in Unternehmens- als auch in Verbraucheranwendungen eingesetzt. Seit
Mitte der 2010er Jahre hat es eine explosionsartige Zunahme von Anwendungen gegeben, die ML nutzen, um
um den Verbrauchern bessere oder zuvor unmögliche Dienste zu bieten.

Angesichts der explosionsartigen Zunahme von Informationen und Diensten wäre es für uns eine große Herausforderung gewesen
ohne die Hilfe von ML zu finden, was wir suchen, entweder in Form einer Suchmaschine oder eines
Suchmaschine oder einem Empfehlungssystem. Wenn Sie eine Website wie Amazon oder Netflix besuchen,
werden Ihnen Artikel empfohlen, von denen man annimmt, dass sie am besten zu Ihrem Geschmack passen. Wenn Sie keine
Empfehlungen nicht gefallen, möchten Sie vielleicht nach bestimmten Artikeln suchen, und
Ihre Suchergebnisse werden wahrscheinlich von ML unterstützt.

Wenn Sie ein Smartphone besitzen, unterstützt ML Sie wahrscheinlich bereits bei vielen Ihrer täglichen
Aktivitäten. Das Tippen auf Ihrem Telefon wird durch Predictive Typing erleichtert, ein ML-System
das Ihnen Vorschläge macht, was Sie als Nächstes sagen möchten. Ein ML-System könnte
in Ihrer Fotobearbeitungsanwendung ausgeführt werden, um Ihnen Vorschläge zu machen, wie Sie Ihre Fotos am besten verbessern können. Sie könnten
Ihr Telefon mit Ihrem Fingerabdruck oder Ihrem Gesicht authentifizieren, was ein ML
System benötigt, um vorherzusagen, ob ein Fingerabdruck oder ein Gesicht zu Ihnen passt.

Der ML-Anwendungsfall, der mich in das Feld lockte, war die maschinelle Übersetzung, die automatische
die automatische Übersetzung von einer Sprache in eine andere. Sie hat das Potenzial, Menschen aus verschiedenen
verschiedenen Kulturen miteinander zu kommunizieren und die Sprachbarriere zu überwinden. Meine
Eltern sprechen kein Englisch, aber dank Google Translate können sie jetzt meine Texte lesen
Schrift lesen und mit meinen Freunden sprechen, die kein Vietnamesisch sprechen.

ML ist mit intelligenten persönlichen Assistenten wie Alexa und Google Assistant zunehmend in unserem Zuhause präsent.
und Google Assistant. Intelligente Sicherheitskameras können Sie informieren, wenn Ihre Haustiere das
Haustiere das Haus verlassen oder wenn Sie einen ungebetenen Gast haben. Ein Freund von mir war besorgt über seine alternde
Mutter, die alleine lebt - wenn sie stürzt, ist niemand da, der ihr beim Aufstehen hilft -, also verließ er sich
daher auf ein System zur Gesundheitsüberwachung, das vorhersagt, ob jemand im Haus gestürzt ist.
dem Haus gestürzt ist.

Auch wenn der Markt für ML-Anwendungen für Verbraucher boomt, sind die meisten
ML-Anwendungsfälle nach wie vor in der Unternehmenswelt zu finden. ML-Anwendungen für Unternehmen haben in der Regel
haben in der Regel ganz andere Anforderungen und Überlegungen als Verbraucheranwendungen.
Es gibt viele Ausnahmen, aber in den meisten Fällen haben Unternehmensanwendungen vielleicht
strengere Anforderungen an die Genauigkeit, sind aber nachsichtiger mit den Latenzanforderungen. Für
beispielsweise die Verbesserung der Genauigkeit eines Spracherkennungssystems von 95 % auf 95,5 %
für die meisten Verbraucher nicht spürbar sein, aber die Verbesserung der Effizienz eines Ressourcenzuweisungssystems
Effizienz eines Ressourcenzuweisungssystems um nur 0,1 % kann einem Unternehmen wie Google oder General Motors helfen
Millionen von Dollar sparen. Gleichzeitig kann eine Latenzzeit von einer Sekunde einen Verbraucher dazu bringen
ablenken und etwas anderes öffnen, aber Unternehmensnutzer sind vielleicht toleranter
hohe Latenzzeiten. Für Menschen, die daran interessiert sind, Unternehmen aus ML-Anwendungen aufzubauen,

Wann man maschinelles Lernen einsetzen sollte | 9
8 "2020 State of Enterprise Machine Learning", Algorithmia, 2020, https://oreil.ly/wKMZB.
Verbraucher-Apps sind vielleicht leichter zu verbreiten, aber viel schwieriger zu monetarisieren. Wie auch immer,
Die meisten Anwendungsfälle in Unternehmen sind jedoch nicht offensichtlich, es sei denn, man hat sie selbst erlebt.

Laut der Umfrage von Algorithmia zum Stand des maschinellen Lernens in Unternehmen im Jahr 2020 sind ML
Anwendungen in Unternehmen vielfältig und dienen sowohl internen Anwendungsfällen (Kostenreduzierung,
Kundeneinblicke und Intelligenz, interne Prozessautomatisierung) und
externe Anwendungsfälle (Verbesserung der Kundenerfahrung, Kundenbindung, Interaktion mit
mit Kunden), wie in Abbildung 1-3 dargestellt.^8

Abbildung 1-3. Stand des maschinellen Lernens in Unternehmen im Jahr 2020. Quelle: Adaptiert von einem Bild von
Algorithmia

10 | Kapitel 1: Überblick über maschinelle Lernsysteme

9 "Average Mobile App User Acquisition Costs Worldwide from September 2018 to August 2019, by User
Action and Operating System," Statista, 2019, https://oreil.ly/2pTCH.
10 Jeff Henriksen, "Valuing Lyft Requires a Deep Look into Unit Economics", Forbes, 17. Mai 2019,
https://oreil.ly/VeSt4.
11 David Skok, "Startup Killer: The Cost of Customer Acquisition", For Entrepreneurs, 2018,
https://oreil.ly/L3tQ7.
12 Amy Gallo, "The Value of Keeping the Right Customers", Harvard Business Review, 29. Oktober 2014,
https://oreil.ly/OlNkl.

Die Betrugserkennung ist eine der ältesten Anwendungen von ML in der Unternehmenswelt. Wenn
Ihr Produkt oder Ihre Dienstleistung Transaktionen von beliebigem Wert beinhaltet, ist es anfällig für
Betrug. Durch den Einsatz von ML-Lösungen zur Erkennung von Anomalien können Sie Systeme einsetzen, die
aus historischen Betrugstransaktionen lernen und vorhersagen, ob eine zukünftige Transaktion
betrügerisch ist.
Die Entscheidung, wie viel Sie für Ihr Produkt oder Ihre Dienstleistung berechnen sollen, ist wahrscheinlich eine der schwierigsten
schwersten Geschäftsentscheidungen; warum lassen Sie ML das nicht für Sie erledigen? Preisoptimierung ist der
Prozess der Schätzung eines Preises zu einem bestimmten Zeitpunkt, um ein bestimmtes Ziel zu maximieren
Funktion zu maximieren, z. B. die Gewinnspanne des Unternehmens, den Umsatz oder die Wachstumsrate. ML-basierte Preisgestaltung
Optimierung eignet sich am besten für Fälle mit einer großen Anzahl von Transaktionen, bei denen
die Nachfrage schwankt und die Verbraucher bereit sind, einen dynamischen Preis zu zahlen - zum Beispiel,
Internetwerbung, Flugtickets, Unterkunftsbuchungen, Mitfahrgelegenheiten und Veranstaltungen.
Um ein Unternehmen zu führen, ist es wichtig, die Kundennachfrage zu prognostizieren, damit Sie
damit Sie ein Budget aufstellen, Lagerbestände auffüllen, Ressourcen zuweisen und die Preisstrategie aktualisieren können.
Wenn Sie beispielsweise einen Lebensmittelladen betreiben, müssen Sie genügend Waren vorrätig haben, damit die Kunden
damit die Kunden finden, was sie suchen, aber Sie wollen nicht zu viel vorrätig haben, denn sonst könnten Ihre
sonst könnten die Lebensmittel schlecht werden und Sie verlieren Geld.
Die Gewinnung eines neuen Nutzers ist teuer. Ab 2019 werden die durchschnittlichen Kosten für eine App
einen Nutzer zu akquirieren, der einen In-App-Kauf tätigt, bei 86,61 $.^9 Die Akquisitionskosten für Lyft werden
schätzungsweise 158 $/Fahrer.^10 Diese Kosten sind für Unternehmenskunden noch viel höher. Cus-
Kundenakquisitionskosten werden von Investoren als Startup-Killer gepriesen.^11 Die Reduzierung der Kundenakquisitions
Kundenakquisitionskosten um einen kleinen Betrag kann zu einer großen Gewinnsteigerung führen. Dies kann
Dies kann durch eine bessere Identifizierung potenzieller Kunden, die Schaltung gezielterer Werbung
die Gewährung von Rabatten zum richtigen Zeitpunkt usw. - allesamt geeignete Aufgaben für ML.
Nachdem Sie so viel Geld ausgegeben haben, um einen Kunden zu gewinnen, wäre es eine Schande, wenn er
gehen. Die Kosten für die Gewinnung eines neuen Benutzers sind schätzungsweise 5 bis 25 Mal höher
teurer als die Bindung eines bestehenden Kunden.^12 Bei der Abwanderungsvorhersage geht es darum, vorherzusagen, wann ein
Kunden aufhören wird, Ihre Produkte oder Dienstleistungen zu nutzen, so dass Sie geeignete
geeignete Maßnahmen ergreifen können, um ihn zurückzugewinnen. Die Abwanderungsvorhersage kann nicht nur für
Kunden, sondern auch für Mitarbeiter.
Wann man maschinelles Lernen einsetzen sollte | 11
13 Marty Swant, "The World's 20 Most Valuable Brands", Forbes, 2020, https://oreil.ly/4uS5i.

Um zu verhindern, dass Kunden abwandern, ist es wichtig, sie zufrieden zu stellen, indem man sich
indem man sich um ihre Anliegen kümmert, sobald sie auftauchen. Die automatisierte Klassifizierung von Support-Tickets kann dabei helfen
dabei helfen. Wenn ein Kunde früher ein Support-Ticket öffnete oder eine E-Mail schickte,
musste es zunächst bearbeitet und dann an verschiedene Abteilungen weitergeleitet werden, bis es
bis es im Posteingang eines Mitarbeiters ankam, der es bearbeiten konnte. Ein ML-System kann den Inhalt des Tickets analysieren
Inhalt des Tickets analysieren und vorhersagen, wohin es weitergeleitet werden soll, was die Reaktionszeit
und die Kundenzufriedenheit verbessern. Es kann auch dazu verwendet werden, interne IT-Tickets zu klassifizieren.
Ein weiterer beliebter Anwendungsfall von ML in Unternehmen ist die Markenüberwachung. Die Marke ist
Die Marke ist ein wertvoller Vermögenswert eines Unternehmens.^13 Es ist wichtig, zu überwachen, wie die Öffentlichkeit und Ihre Kunden
Kunden Ihre Marke wahrnehmen. Sie möchten vielleicht wissen, wann/wo/wie sie genannt wird.
Sie möchten wissen, wann, wo und wie Ihre Marke genannt wird, sowohl explizit (z. B. wenn jemand "Google" erwähnt) als auch implizit (z. B,
wenn jemand "der Suchgigant" sagt), als auch die damit verbundene Stimmung.
Wenn Ihre Markenerwähnungen plötzlich einen Anstieg der negativen Stimmung aufweisen, sollten Sie
sollten Sie so schnell wie möglich darauf reagieren. Die Stimmungsanalyse ist eine typische ML-Aufgabe.
Eine Reihe von ML-Anwendungsfällen, die in letzter Zeit viel Aufsehen erregt haben, betrifft das Gesundheitswesen.
Es gibt ML-Systeme, die Hautkrebs erkennen und Diabetes diagnostizieren können. Auch wenn
Obwohl sich viele Anwendungen im Gesundheitswesen an Verbraucher richten, sind sie aufgrund ihrer strengen
Anforderungen an Genauigkeit und Datenschutz werden sie in der Regel von einem Gesundheitsdienstleister
Sie werden in der Regel von einem Gesundheitsdienstleister, z. B. einem Krankenhaus, bereitgestellt oder zur Unterstützung von Ärzten bei der Diagnose verwendet.
Verständnis für maschinelle Lernsysteme
Das Verständnis von ML-Systemen ist hilfreich für deren Entwurf und Entwicklung. In diesem
Abschnitt gehen wir darauf ein, wie sich ML-Systeme von ML in der Forschung (oder
Forschung (oder wie sie oft in der Schule gelehrt wird) als auch von traditioneller Software unterscheiden, was den Bedarf für dieses
Buches.
Maschinelles Lernen in der Forschung vs. in der Produktion
Da die Nutzung von ML in der Industrie noch recht neu ist, haben die meisten Menschen mit ML-Kenntnissen
haben sie sich im akademischen Bereich angeeignet: Sie haben Kurse besucht, geforscht und akademische Arbeiten gelesen.
Wenn dies Ihr Hintergrund ist, könnte es für Sie eine steile Lernkurve sein, um
die Herausforderungen beim Einsatz von ML-Systemen in der Praxis zu verstehen und sich in einer
einer überwältigenden Anzahl von Lösungen für diese Herausforderungen. ML in der Produktion ist ganz anders
von ML in der Forschung. Tabelle 1-1 zeigt fünf der wichtigsten Unterschiede.
12 | Kapitel 1: Überblick über maschinelle Lernsysteme
Tabelle 1-1. Hauptunterschiede zwischen ML in der Forschung und ML in der Produktion

Forschung Produktion
Anforderungen State-of-the-Art Modellleistung auf
Benchmark-Datensätzen
Verschiedene Interessengruppen haben unterschiedliche
Anforderungen
Berechnungspriorität Schnelles Training, hoher Durchsatz Schnelle Inferenz, geringe Latenzzeit
Daten Statica Ständig wechselnd
Fairness Oft nicht im Fokus Muss berücksichtigt werden
Interpretierbarkeit Oft nicht im Fokus Muss berücksichtigt werden
a Ein Teilbereich der Forschung konzentriert sich auf kontinuierliches Lernen: Entwicklung von Modellen, die mit sich ändernden Datenverteilungen arbeiten. Wir werden
Kapitel 9 behandeln wir das kontinuierliche Lernen.
Unterschiedliche Interessengruppen und Anforderungen

Die an einem Forschungs- und Leaderboard-Projekt beteiligten Personen sind oft auf ein einziges
Ziel. Das häufigste Ziel ist die Modellleistung - ein Modell zu entwickeln, das
das in Benchmark-Datensätzen die besten Ergebnisse erzielt. Um eine kleine
um eine kleine Leistungsverbesserung zu erreichen, greifen die Forscher oft auf Techniken zurück, die die Modelle zu
els zu komplex machen, um nützlich zu sein.

Es gibt viele Beteiligte, die an der Einführung eines ML-Systems in die Produktion beteiligt sind.
Jeder Beteiligte hat seine eigenen Anforderungen. Unterschiedliche, oft widersprüchliche
Anforderungen kann es schwierig machen, ein ML-Modell zu entwerfen, zu entwickeln und auszuwählen, das
das allen Anforderungen gerecht wird.

Nehmen wir eine mobile App, die ihren Nutzern Restaurants empfiehlt. Die App verdient Geld
indem sie den Restaurants bei jeder Bestellung eine Servicegebühr von 10 % berechnet. Das bedeutet, dass teure
Bestellungen der App mehr Geld einbringen als billige Bestellungen. An dem Projekt sind ML-Ingenieure
neure, Verkäufer, Produktmanager, Infrastrukturingenieure und ein Manager:

ML-Ingenieure
Sie wollen ein Modell, das Restaurants empfiehlt, bei denen die Nutzer mit hoher Wahrscheinlichkeit bestellen werden.
bestellen werden, und sie glauben, dass sie dies mit einem komplexeren Modell mit mehr
Daten.

Verkaufsteam
Möchte ein Modell, das die teureren Restaurants empfiehlt, da diese
Restaurants mehr Servicegebühren einbringen.

Produkt-Team
merkt, dass jede Erhöhung der Latenzzeit zu einem Rückgang der Bestellungen über den
Sie wollen also ein Modell, das die empfohlenen Restaurants in weniger als
weniger als 100 Millisekunden liefert.

Maschinelle Lernsysteme verstehen | 13
14 Es ist nicht ungewöhnlich, dass die ML- und Data-Science-Teams zu den ersten gehören, die bei Massenentlassungen in einem Unternehmen
entlassen werden, wie bei IBM, Uber und Airbnb berichtet wurde. Siehe auch die Analyse von Sejuti Das "How Data Scientists Are Also
Susceptible to the Layoffs Amid Crisis," Analytics India Magazine, May 21, 2020, https://oreil.ly/jobmz.
15 Wikipedia, s.v. "Ensemble learning", https://oreil.ly/5qkgp.

ML-Plattform-Team
Da der Datenverkehr zunimmt, wurde dieses Team mitten in der Nacht geweckt
wegen Problemen mit der Skalierung ihres bestehenden Systems geweckt, so dass sie
Modellaktualisierungen zurückstellen, um der Verbesserung der ML-Plattform Vorrang zu geben.
Leiter
Möchte die Gewinnspanne maximieren, und eine Möglichkeit, dies zu erreichen, könnte die Entlassung des
ML-Team zu entlassen.^14
"Empfehlung der Restaurants, auf die die Nutzer am ehesten klicken" und "Empfehlung der
die Restaurants zu empfehlen, die das meiste Geld für die App einbringen" sind zwei
Ziele, und im Abschnitt "Entkopplung der Ziele" auf Seite 41 werden wir
diskutieren wir, wie man ein ML-System entwickelt, das verschiedene Ziele erfüllt. Spoiler: Wir werden
ein Modell für jedes Ziel entwickeln und ihre Vorhersagen kombinieren.
Nehmen wir einmal an, wir hätten zwei verschiedene Modelle. Modell A ist das Modell, das
die Restaurants empfiehlt, auf die die Nutzer am ehesten klicken werden, und Modell B ist
das Modell, das die Restaurants empfiehlt, die der App das meiste Geld einbringen
App einbringen. A und B können sehr unterschiedliche Modelle sein. Welches Modell sollte den Nutzern zur Verfügung gestellt werden?
Benutzer? Erschwerend kommt hinzu, dass weder A noch B die Anforderung des Produktteams erfüllen
die vom Produktteam festgelegt wurde: Sie können keine Restaurantempfehlungen in weniger als
als 100 Millisekunden liefern.
Bei der Entwicklung eines ML-Projekts ist es für ML-Ingenieure wichtig zu verstehen
Anforderungen aller beteiligten Interessengruppen zu verstehen und zu wissen, wie streng diese Anforderungen sind.
Wenn zum Beispiel die Fähigkeit, Empfehlungen innerhalb von 100 Millisekunden zu liefern, eine
ein Muss ist - das Unternehmen stellt fest, dass, wenn Ihr Modell mehr als 100 Milli-
Sekunden braucht, um Restaurants zu empfehlen, würden 10 % der Nutzer die Geduld verlieren und die App schließen.
App schließen - dann funktionieren weder Modell A noch Modell B. Wenn es sich jedoch nur um eine Nice-to-have
ist, sollten Sie vielleicht trotzdem Modell A oder Modell B in Betracht ziehen.
Dass die Produktion andere Anforderungen als die Forschung hat, ist einer der Gründe, warum
erfolgreiche Forschungsprojekte nicht immer in der Produktion eingesetzt werden können. Ein Beispiel,
Ensembling ist eine Technik, die bei den Gewinnern vieler ML-Wettbewerbe beliebt ist,
einschließlich des berühmten Netflix-Preises in Höhe von 1 Million Dollar, wird aber in der Produktion nicht häufig eingesetzt.
tion. Ensembling kombiniert "mehrere Lernalgorithmen, um eine bessere Vorhersageleistung
Vorhersageleistung zu erzielen, als dies mit einem der einzelnen Lernalgorithmen möglich wäre.
"^15 Ihr ML-System kann dadurch zwar eine kleine Leistungssteigerung erfahren,
Ensembling neigt dazu, ein System zu komplex zu machen, um in der Produktion nützlich zu sein, z. B.,
14 | Kapitel 1: Überblick über maschinelle Lernsysteme
16 Julia Evans, "Machine Learning Isn't Kaggle Competitions", 2014, https://oreil.ly/p8mZq.
17 Lauren Oakden-Rayner, "AI Competitions Don't Produce Useful Models," September 19, 2019,
https://oreil.ly/X6RlT.
18 Kawin Ethayarajh und Dan Jurafsky, "Utility Is in the Eye of the User: A Critique of NLP Leaderboards,"
EMNLP, 2020, https://oreil.ly/4Ud8P.

Die Vorhersagen werden langsamer und die Ergebnisse sind schwieriger zu interpretieren. Wir werden das Ensem-
bling wird im Abschnitt "Ensembles" auf Seite 156 näher erläutert.
Bei vielen Aufgaben kann eine kleine Leistungsverbesserung zu einem enormen Anstieg der
Einnahmen oder Kosteneinsparungen führen. Zum Beispiel kann eine Verbesserung der Klickrate um 0,2 % bei einem
Durchklickrate für ein System zur Produktempfehlung zu einer Umsatzsteigerung in Millionenhöhe
Millionen Dollar an Einnahmen für eine E-Commerce-Website führen. Bei vielen Aufgaben ist jedoch eine kleine Verbesserung
für die Nutzer nicht spürbar sein. Für die zweite Art von Aufgaben, wenn ein einfaches Modell
ein einfaches Modell eine vernünftige Arbeit leisten kann, müssen komplexe Modelle deutlich besser
Komplexität zu rechtfertigen.
Kritik an ML-Ranglisten
In den letzten Jahren hat es viele Kritiker von ML-Ranglisten gegeben, sowohl bei Wettbewerben
wie Kaggle als auch Forschungs-Ranglisten wie ImageNet oder GLUE.
Ein offensichtliches Argument ist, dass in diesen Wettbewerben viele der schwierigen Schritte, die für die
zum Aufbau von ML-Systemen bereits für Sie erledigt sind.^16
Ein weniger offensichtliches Argument ist, dass aufgrund des Szenarios der Mehrfachhypothesentests, das
wenn mehrere Teams mit demselben Hold-Out-Testsatz testen, ein Modell
rein zufällig besser abschneiden kann als der Rest.^17
Die Diskrepanz zwischen den Interessen von Forschung und Produktion wurde von
von Forschern festgestellt. In einem EMNLP 2020-Papier argumentieren Ethayarajh und Jurafsky, dass
Benchmarks zu Fortschritten in der Verarbeitung natürlicher Sprache (NLP) beigetragen haben, indem sie
Anreize für die Erstellung genauerer Modelle auf Kosten anderer Qualitäten
die von Praktikern geschätzt werden, wie Kompaktheit, Fairness und Energieeffizienz.^18
Rechnerische Prioritäten
Beim Entwurf eines ML-Systems machen Personen, die noch kein ML-System eingesetzt haben, oft den Fehler
den Fehler, sich zu sehr auf die Modellentwicklung zu konzentrieren und nicht
und nicht genug auf den Teil der Modellbereitstellung und -pflege.
Während des Modellentwicklungsprozesses trainieren Sie möglicherweise viele verschiedene Modelle, und
jedes Modell durchläuft mehrere Durchläufe der Trainingsdaten. Jedes trainierte Modell erzeugt dann
einmal Vorhersagen für die Validierungsdaten, um die Ergebnisse zu melden. Die Validierungs
Daten sind normalerweise viel kleiner als die Trainingsdaten. Während der Modellentwicklung,
Systeme des maschinellen Lernens verstehen | 15
19 Martin Kleppmann, Designing Data-Intensive Applications (Sebastopol, CA: O'Reilly, 2017).

Die Ausbildung ist der Engpass. Sobald das Modell jedoch eingesetzt wurde, besteht seine Aufgabe darin
Vorhersagen zu erstellen, so dass die Schlussfolgerung der Engpass ist. Die Forschung bevorzugt in der Regel schnelles
Training, während die Produktion in der Regel schnelle Schlussfolgerungen bevorzugt.
Daraus ergibt sich, dass in der Forschung ein hoher Durchsatz im Vordergrund steht, während in der Produktion
eine niedrige Latenzzeit bevorzugt. Falls Sie eine Auffrischung benötigen, bezieht sich Latenz auf die Zeit, die
vom Erhalt einer Anfrage bis zur Rückgabe des Ergebnisses. Der Durchsatz bezieht sich darauf, wie viele
Abfragen innerhalb einer bestimmten Zeitspanne verarbeitet werden.
Terminologie-Kollision
In einigen Büchern wird zwischen Latenzzeit und Antwortzeit unterschieden.
zeit. Nach Martin Kleppmann in seinem Buch Designing Data-
Intensive Applications, "Die Antwortzeit ist das, was der Client sieht:
neben der tatsächlichen Zeit für die Bearbeitung der Anfrage (die Servicezeit),
enthält sie Netzwerkverzögerungen und Verzögerungen durch Warteschlangen. Die Latenzzeit ist die
Dauer, in der eine Anfrage auf ihre Bearbeitung wartet - in der Zeit, in der sie
latent, in Erwartung der Bearbeitung."^19
In diesem Buch wird zur Vereinfachung der Diskussion und zur Übereinstimmung mit der
der in der ML-Gemeinschaft verwendeten Terminologie zu entsprechen, verwenden wir Latenz
die Antwortzeit, d. h. die Latenzzeit einer Anfrage misst die Zeit
zwischen dem Senden der Anfrage und dem Empfang der Antwort.
Die durchschnittliche Latenzzeit von Google Translate ist beispielsweise die durchschnittliche Zeit, die von
wenn ein Nutzer auf Übersetzen klickt, bis die Übersetzung angezeigt wird, und der Durchsatz ist
wie viele Abfragen es pro Sekunde verarbeitet und bedient.
Wenn Ihr System immer nur eine Anfrage auf einmal verarbeitet, bedeutet eine höhere Latenzzeit einen geringeren
Durchsatz. Wenn die durchschnittliche Latenzzeit 10 ms beträgt, d. h. es dauert 10 ms, um eine Anfrage zu bearbeiten
eine Abfrage zu bearbeiten, beträgt der Durchsatz 100 Abfragen/Sekunde. Wenn die durchschnittliche Latenzzeit 100 ms beträgt, ist der
Durchsatz 10 Abfragen/Sekunde.
Da jedoch die meisten modernen verteilten Systeme Abfragen in Stapeln verarbeiten, um sie
zusammen und oft gleichzeitig zu verarbeiten, kann eine höhere Latenz auch einen höheren Durchsatz bedeuten. Wenn Sie
10 Abfragen gleichzeitig verarbeiten und es 10 ms dauert, einen Stapel auszuführen, beträgt die durchschnittliche Latenz
immer noch 10 ms, aber der Durchsatz ist nun 10-mal höher - 1.000 Abfragen/Sekunde. Wenn Sie
50 Abfragen gleichzeitig verarbeiten und 20 ms für die Ausführung eines Stapels benötigen, beträgt die durchschnittliche Latenzzeit jetzt
20 ms und der Durchsatz beträgt 2.500 Abfragen/Sekunde. Sowohl die Latenzzeit als auch der Durchsatz
haben sich erhöht! Der Unterschied bei Latenz und Durchsatz zwischen der Verarbeitung
zwischen der Verarbeitung einzelner Abfragen und der Verarbeitung von Abfragen in Stapeln ist in Abbildung 1-4 dargestellt.
16 | Kapitel 1: Überblick über maschinelle Lernsysteme
20 Akamai Technologies, Akamai Online Retail Performance Report: Milliseconds Are Critical, April 19, 2017,
https://oreil.ly/bEtRu.
21 Lucas Bernardi, Themis Mavridis, und Pablo Estevez, "150 Successful Machine Learning Models: 6 Lessons
Learned at Booking.com," KDD '19, August 4-8, 2019, Anchorage, AK, https://oreil.ly/G5QNA.
22 "Consumer Insights", Think with Google, https://oreil.ly/JCp6Z.

Abbildung 1-4. Wenn eine Abfrage nach der anderen verarbeitet wird, bedeutet eine höhere Latenzzeit einen geringeren Durchsatz.
Durchsatz. Bei der Verarbeitung von Abfragen in Stapeln kann eine höhere Latenz jedoch auch einen höheren
Durchsatz bedeuten.
Noch komplizierter wird es, wenn Sie Online-Abfragen stapelweise verarbeiten wollen. Batching erfordert
Ihr System muss warten, bis genügend Abfragen in einem Stapel angekommen sind, bevor es sie verarbeitet,
was die Latenzzeit weiter erhöht.
In der Forschung geht es mehr darum, wie viele Stichproben Sie in einer Sekunde verarbeiten können
(Durchsatz) und weniger, wie lange es dauert, bis die einzelnen Proben verarbeitet werden
(Latenzzeit). Sie sind bereit, die Latenzzeit zu erhöhen, um den Durchsatz zu steigern, zum Beispiel durch
aggressiver Stapelverarbeitung.
Sobald Sie Ihr Modell jedoch in der realen Welt einsetzen, ist die Latenzzeit sehr wichtig. Unter
2017 fand eine Akamai-Studie heraus, dass eine Verzögerung von 100 ms die Konversionsraten um 7 % senken kann.^20
Im Jahr 2019 stellte Booking.com fest, dass ein Anstieg der Latenz um etwa 30 % die Konversionsraten um etwa 0,5 %
Konversionsraten kostet - "ein relevanter Preis für unser Geschäft".^21 Im Jahr 2016 stellte Google fest, dass
mehr als die Hälfte der mobilen Nutzer eine Seite verlässt, wenn sie mehr als drei Sekunden zum Laden braucht.
^22 Heutzutage sind die Nutzer noch weniger geduldig.
Maschinelle Lernsysteme verstehen | 17
23 Kleppmann, Designing Data-Intensive Applications.

Um die Latenzzeit in der Produktion zu verringern, müssen Sie möglicherweise die Anzahl der Abfragen reduzieren, die
die Sie auf derselben Hardware gleichzeitig verarbeiten können. Wenn Ihre Hardware in der Lage ist
in der Lage ist, viel mehr Abfragen gleichzeitig zu verarbeiten, bedeutet die Verwendung zur Verarbeitung von weniger Abfragen eine Unterauslastung
Ihrer Hardware, wodurch die Kosten für die Verarbeitung jeder Abfrage steigen.
Bei der Betrachtung der Latenz ist es wichtig zu bedenken, dass die Latenz nicht eine
einzelne Zahl, sondern eine Verteilung ist. Es ist verlockend, diese Verteilung zu vereinfachen, indem man
diese Verteilung zu vereinfachen, indem man eine einzelne Zahl wie die durchschnittliche (arithmetische) Latenz aller Anfragen
innerhalb eines Zeitfensters, aber diese Zahl kann irreführend sein. Stellen Sie sich vor, Sie haben 10
Anforderungen mit den Latenzen 100 ms, 102 ms, 100 ms, 100 ms, 99 ms, 104 ms, 110
ms, 90 ms, 3.000 ms, 95 ms. Die durchschnittliche Latenzzeit beträgt 390 ms, so dass Ihr
System langsamer erscheinen lässt, als es tatsächlich ist. Möglicherweise gab es einen
ein Netzwerkfehler aufgetreten ist, der eine Anfrage viel langsamer gemacht hat als andere, und Sie sollten
diese problematische Anfrage untersuchen.
In der Regel ist es besser, in Prozentsätzen zu denken, da diese etwas über einen bestimmten
Prozentsatz Ihrer Anfragen. Das gebräuchlichste Perzentil ist das 50ste Perzentil,
abgekürzt als p50. Er ist auch als Median bekannt. Wenn der Median 100 ms beträgt, dauert die Hälfte der
Anfragen länger als 100 ms, und die Hälfte der Anfragen braucht weniger als 100 ms.
Höhere Perzentile helfen auch bei der Erkennung von Ausreißern, die ein Anzeichen für
etwas Falsches sind. Normalerweise sind die Perzentile p90, p95 und
p99. Das 90. Perzentil (p90) für die 10 obigen Anfragen beträgt 3.000 ms, was ein
Ausreißer.
Höhere Perzentile sind wichtig, denn auch wenn sie nur einen kleinen Prozentsatz
einen kleinen Prozentsatz Ihrer Benutzer ausmachen, können sie manchmal die wichtigsten Benutzer sein.
Auf der Amazon-Website zum Beispiel sind die Kunden mit den langsamsten Anfragen
Kunden mit den langsamsten Anfragen sind oft diejenigen, die die meisten Daten in ihren Konten haben, weil sie viele Käufe getätigt haben.
Einkäufe getätigt haben, d. h. sie sind die wertvollsten Kunden.^23
Es ist eine gängige Praxis, hohe Perzentile zu verwenden, um die Leistungsanforderungen an Ihr System zu spezifizieren.
Es ist üblich, hohe Perzentile zu verwenden, um die Leistungsanforderungen für Ihr System zu spezifizieren; ein Produktmanager könnte zum Beispiel festlegen, dass das 90.
Perzentil oder das 99,9. Perzentil der Latenzzeit eines Systems unter einem bestimmten Wert liegen muss.
Daten
In der Forschungsphase sind die Datensätze, mit denen Sie arbeiten, oft sauber und gut
formatiert, so dass Sie sich auf die Entwicklung von Modellen konzentrieren können. Sie sind von Natur aus statisch, damit
damit die Gemeinschaft sie zum Benchmarking neuer Architekturen und Techniken verwenden kann.
Das bedeutet, dass viele Leute die gleichen Datensätze verwendet und diskutiert haben und
Macken des Datensatzes bekannt sind. Vielleicht finden Sie sogar Open-Source-Skripte zur Verarbeitung
und die Daten direkt in Ihre Modelle einzuspeisen.
18 | Kapitel 1: Überblick über maschinelle Lernsysteme
24 Andrej Karpathy, "Building the Software 2.0 Stack", Spark+AI Summit 2018, Video, 17:54,
https://oreil.ly/Z21Oz.

In der Produktion sind die Daten, sofern vorhanden, viel unübersichtlicher. Sie sind verrauscht, möglicherweise unstrukturiert,
ständig wechselnd. Sie sind wahrscheinlich verzerrt, und Sie wissen wahrscheinlich nicht, wie sie verzerrt sind. Kennzeichnungen,
wenn es welche gibt, können spärlich, unausgewogen oder falsch sein. Geänderte Projekt- oder
Geschäftsanforderungen erfordern möglicherweise die Aktualisierung einiger oder aller Ihrer bestehenden Beschriftungen. Wenn
Sie mit Benutzerdaten arbeiten, müssen Sie sich auch Gedanken über den Datenschutz und rechtliche
Bedenken. Im Abschnitt "Fallstudie II: Die Gefahr der "Anonymisierung"" wird ein Fallbeispiel für einen unangemessenen Umgang mit den Daten der Benutzer erörtert.
Abschnitt "Fallstudie II: Die Gefahr von "anonymisierten" Daten" auf Seite 344.
In der Forschung arbeiten Sie meist mit historischen Daten, d. h. mit Daten, die bereits existieren und
irgendwo gespeichert sind. In der Produktion werden Sie höchstwahrscheinlich auch mit Daten arbeiten müssen, die
die ständig von Benutzern, Systemen und Daten Dritter erzeugt werden.
Abbildung 1-5 wurde von einer großartigen Grafik von Andrej Karpathy, Director of AI
bei Tesla, die die Datenprobleme veranschaulicht, auf die er während seiner Doktorarbeit
mit seiner Zeit bei Tesla.
Abbildung 1-5. Daten in der Forschung versus Daten in der Produktion. Quelle: Angepasst an ein Bild
von Andrej Karpathy^24
Fairness
In der Forschungsphase wird ein Modell noch nicht auf Menschen angewendet, so dass es für Forscher leicht ist, Fairness
Daher ist es für die Forscher leicht, das Thema Fairness auf die lange Bank zu schieben: "Versuchen wir zuerst, den Stand der Technik zu erreichen und
und kümmern uns um Fairness, wenn wir in die Produktion gehen." Wenn es zur Produktion kommt, ist es
zu spät. Wenn Sie Ihre Modelle im Hinblick auf bessere Genauigkeit oder geringere Latenzzeiten optimieren, können Sie
können Sie zeigen, dass Ihre Modelle besser sind als der Stand der Technik. Aber als ich dieses Buch schrieb, gab es noch keinen
keinen gleichwertigen Stand der Technik für Fairness-Metriken.
Systeme des maschinellen Lernens verstehen | 19
25 Khristopher J. Brooks, "Disparity in Home Lending Costs Minorities Millions, Researchers Find," CBS News,
15. November 2019, https://oreil.ly/UiHUB.
26 Cathy O'Neil, Weapons of Math Destruction (New York: Crown Books, 2016).
27 Stanford University Human-Centered Artificial Intelligence (HAI), The 2019 AI Index Report, 2019,
https://oreil.ly/xs8mG.

Vielleicht sind Sie oder jemand in Ihrem Leben bereits Opfer von verzerrten mathematischen
Algorithmen, ohne es zu wissen. Ihr Kreditantrag könnte abgelehnt werden, weil
der ML-Algorithmus Ihre Postleitzahl auswählt, die Vorurteile über den sozioökonomischen Hintergrund einer Person verkörpert.
sozioökonomischen Hintergrund enthält. Ihr Lebenslauf könnte schlechter bewertet werden, weil das
System, das Arbeitgeber verwenden, die Schreibweise Ihres Namens auswertet. Ihre Hypothek könnte
Hypothek einen höheren Zinssatz erhalten, weil er teilweise von Kreditwürdigkeitswerten abhängt, die die
die Reichen begünstigen und die Armen bestrafen. Weitere Beispiele für ML-Verzerrungen in der realen Welt sind
Algorithmen zur vorausschauenden Polizeiarbeit, Persönlichkeitstests, die von potenziellen Arbeitgebern durchgeführt werden
und Hochschulrankings.
Im Jahr 2019 fanden Berkeley-Forscher heraus, dass sowohl persönliche als auch Online-Kreditgeber
insgesamt 1,3 Millionen kreditwürdige schwarze und lateinamerikanische Bewerber zwischen
2008 and 2015." Als die Forscher "die Einkommens- und Kreditwürdigkeitswerte der
abgelehnten Anträge verwendeten, aber die Rassenkennzeichen löschten, wurde der Hypothekenantrag
angenommen."^25 Für noch mehr abschreckende Beispiele empfehle ich Cathy O'Neils Weapons of
Math Destruction.^26
ML-Algorithmen sagen nicht die Zukunft voraus, sondern kodieren die Vergangenheit, so dass sich die
Verzerrungen in den Daten und mehr. Wenn ML-Algorithmen in großem Maßstab eingesetzt werden, können sie
Menschen in großem Umfang diskriminieren. Wenn ein menschlicher Bediener nur pauschale
pauschale Urteile über einige wenige Personen fällen, kann ein ML-Algorithmus
in Sekundenbruchteilen pauschale Urteile über Millionen treffen. Dies kann insbesondere Mitglieder von
Minderheitengruppen schaden, da eine Fehlklassifizierung bei ihnen nur geringe Auswirkungen auf die
die Gesamtleistungskennzahlen der Modelle auswirken.
Wenn ein Algorithmus bereits bei 98 % der Bevölkerung korrekte Vorhersagen machen kann
und die Verbesserung der Vorhersagen für die restlichen 2 % ein Vielfaches an Kosten verursachen würde, könnten einige
könnten sich einige Unternehmen leider dafür entscheiden, dies nicht zu tun. In einer McKinsey & Company
Forschungsstudie im Jahr 2019 gaben nur 13 % der befragten Großunternehmen an, sie würden
Maßnahmen ergreifen, um Risiken für Gerechtigkeit und Fairness, wie algorithmische Verzerrungen und
Diskriminierung.^27 Dies ändert sich jedoch schnell. Wir werden Fairness und andere
Aspekte der verantwortungsvollen KI in Kapitel 11.
Interpretierbarkeit
Anfang 2020 schlug der Turing-Preisträger Professor Geoffrey Hinton eine
hitzig diskutierte Frage nach der Bedeutung der Interpretierbarkeit in ML-Systemen.
"Nehmen wir an, Sie haben Krebs und müssen sich zwischen einem KI-Chirurgen und einer Blackbox entscheiden.
20 | Kapitel 1: Überblick über maschinelle Lernsysteme
28 Tweet von Geoffrey Hinton (@geoffreyhinton), 20. Februar 2020, https://oreil.ly/KdfD8.
29 Für bestimmte Anwendungsfälle in bestimmten Ländern haben Nutzer ein "Recht auf Erklärung": ein Recht darauf, eine Erklärung für eine Ausgabe des Algorithmus zu erhalten.
tion für eine Ausgabe des Algorithmus.
30 Stanford HAI, The 2019 AI Index Report.

der nicht erklären kann, wie er funktioniert, aber eine Heilungsrate von 90 % hat, und ein menschlicher Chirurg mit einer
mit einer Heilungsrate von 80 %. Wollen Sie, dass der KI-Chirurg illegal ist?"^28
Als ich diese Frage ein paar Wochen später einer Gruppe von 30 Führungskräften aus
Technologieunternehmen stellte, wollte nur die Hälfte von ihnen, dass der hocheffektive
hocheffektiven, aber unerklärlichen KI-Chirurgen operiert werden. Die andere Hälfte wollte
den menschlichen Chirurgen.
Während die meisten von uns eine Mikrowelle benutzen können, ohne zu verstehen
Mikrowelle zu benutzen, ohne zu verstehen, wie sie funktioniert, ist das bei KI noch nicht der Fall, vor allem, wenn diese KI
wichtige Entscheidungen über ihr Leben trifft.
Da die meisten ML-Forschungen immer noch anhand eines einzigen Ziels, der Modellleistung, bewertet werden,
haben Forscher keinen Anreiz, an der Interpretierbarkeit von Modellen zu arbeiten. Allerdings ist die Interpretierbarkeit
Interpretierbarkeit ist jedoch für die meisten ML-Anwendungsfälle in der Industrie nicht nur optional, sondern eine Voraussetzung.
Erstens ist die Interpretierbarkeit wichtig für die Benutzer, sowohl für Unternehmensleiter als auch für Endbenutzer, um
zu verstehen, warum eine Entscheidung getroffen wurde, damit sie einem Modell vertrauen können und mögliche
^29 Zweitens ist es für Entwickler wichtig, dass sie ein Modell
ein Modell zu debuggen und zu verbessern.
Nur weil Interpretierbarkeit eine Anforderung ist, heißt das noch lange nicht, dass dies auch alle tun. Bis zum
2019 arbeiten nur 19 % der großen Unternehmen an der Verbesserung der Erklärbarkeit ihrer
Algorithmen.^30
Diskussion
Manche mögen argumentieren, dass es in Ordnung ist, nur die akademische Seite von ML zu kennen, weil es
weil es viele Jobs in der Forschung gibt. Der erste Teil - es ist in Ordnung, nur die akademische Seite
von ML zu kennen, ist wahr. Der zweite Teil ist falsch.
Es ist zwar wichtig, reine Forschung zu betreiben, aber die meisten Unternehmen können es sich nicht leisten, wenn es nicht
wenn es nicht zu kurzfristigen Geschäftsanwendungen führt. Dies gilt insbesondere jetzt, da die Forschungs
Gemeinschaft den Ansatz "größer, besser" gewählt hat. Neue Modelle erfordern oft eine
Datenmengen und zweistellige Millionenbeträge allein für die Datenverarbeitung.
In dem Maße, wie ML-Forschung und Standardmodelle leichter zugänglich werden, werden mehr Menschen und
mehr Menschen und Organisationen Anwendungen für sie finden wollen, was die Nachfrage nach
für ML in der Produktion.
Die überwiegende Mehrheit der ML-bezogenen Arbeitsplätze wird in der Produktion von ML liegen und liegt bereits dort.
Maschinelle Lernsysteme verstehen | 21
31 Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, und Dawn Song, "Targeted Backdoor Attacks on Deep
Learning Systems Using Data Poisoning," arXiv, December 15, 2017, https://oreil.ly/OkAjb.
32 Wir werden Edge-Geräte in Kapitel 7 behandeln.

Maschinelle Lernsysteme im Vergleich zu traditioneller Software
Da ML ein Teil des Software-Engineerings (SWE) ist und Software bereits seit einem halben Jahrhundert erfolgreich
seit mehr als einem halben Jahrhundert erfolgreich in der Produktion eingesetzt wird, könnte man sich fragen, warum wir nicht
warum wir nicht einfach die bewährten Best Practices der Softwaretechnik auf ML anwenden.
Das ist eine ausgezeichnete Idee. In der Tat wäre die ML-Produktion ein viel besserer Ort, wenn ML
Experten bessere Software-Ingenieure wären. Viele traditionelle SWE-Tools können verwendet werden, um
ML-Anwendungen entwickelt und eingesetzt werden.
Viele Herausforderungen sind jedoch einzigartig für ML-Anwendungen und erfordern ihre eigenen Werkzeuge.
Bei SWE geht man davon aus, dass Code und Daten getrennt sind. Tatsächlich ist es so, dass wir in
SWE wollen wir die Dinge so modular und getrennt wie möglich halten (siehe die Wikipedia
Seite über die Trennung von Belangen).
Im Gegenteil: ML-Systeme bestehen zum Teil aus Code, zum Teil aus Daten und zum Teil aus Artefakten, die
aus diesen beiden. Der Trend des letzten Jahrzehnts zeigt, dass Anwendungen, die mit den
den meisten/besten Daten entwickelt werden. Anstatt sich auf die Verbesserung von ML-Algorithmen zu konzentrieren, werden die meisten
Unternehmen auf die Verbesserung ihrer Daten konzentrieren. Da sich Daten schnell ändern können, müssen ML
Anwendungen an die sich verändernde Umgebung angepasst werden, was möglicherweise
schnellere Entwicklungs- und Einsatzzyklen erfordern.
Bei der traditionellen SWE müssen Sie sich nur auf das Testen und die Versionierung Ihres Codes konzentrieren. Bei
ML müssen wir auch unsere Daten testen und versionieren, und das ist der schwierige Teil. Wie versioniert man
große Datensätze? Woher weiß man, ob eine Datenprobe gut oder schlecht für das System ist? Nicht
alle Datenproben sind gleich - einige sind für Ihr Modell wertvoller als andere. Für
Beispiel: Wenn Ihr Modell bereits auf eine Million Scans normaler Lungen und
nur eintausend Scans von krebsbefallenen Lungen trainiert hat, ist ein Scan einer krebsbefallenen Lunge viel
wertvoller als ein Scan einer normalen Lunge. Die wahllose Annahme aller verfügbaren Daten
kann die Leistung Ihres Modells beeinträchtigen und es sogar anfällig für Data Poisoning
Angriffe.^31
Die Größe von ML-Modellen ist eine weitere Herausforderung. Ab 2022 ist es üblich, dass ML-Modelle
Hunderte von Millionen, wenn nicht Milliarden von Parametern haben, was Gigabytes
an Arbeitsspeicher (RAM) benötigt, um sie in den Speicher zu laden. In ein paar Jahren
könnte eine Milliarde Parameter kurios erscheinen - wie der Computer, der die Menschen auf den
Männer auf den Mond geschickt hat, nur 32 MB RAM hatte?"
Im Moment ist es jedoch eine enorme Herausforderung, diese großen Modelle in die Produktion zu bringen, insbesondere auf
^32 ist eine große technische Herausforderung. Und dann ist da noch die Frage, wie man
diese Modelle schnell genug laufen, um nützlich zu sein. Ein Modell zur automatischen Vervollständigung ist nutzlos, wenn
22 | Kapitel 1: Überblick über maschinelle Lernsysteme
33 Jacob Devlin, Ming-Wei Chang, Kenton Lee, und Kristina Toutanova, "BERT: Pre-training of Deep Bidirec-
tional Transformers for Language Understanding," arXiv, October 11, 2018, https://oreil.ly/TG3ZW.
34 Google Search On, 2020, https://oreil.ly/M7YjM.

die Zeit, die für den Vorschlag des nächsten Zeichens benötigt wird, ist länger als die Zeit, die Sie für die Eingabe
zu tippen.
Die Überwachung und Fehlersuche bei diesen Modellen in der Produktion ist ebenfalls nicht trivial. Da ML
Modelle immer komplexer werden, ist es in Verbindung mit der mangelnden Einsicht in ihre Arbeit schwierig
herauszufinden, was schief gelaufen ist, oder schnell genug gewarnt zu werden, wenn etwas schief läuft.
Die gute Nachricht ist, dass diese technischen Herausforderungen in rasantem Tempo angegangen werden.
Tempo angegangen werden. Im Jahr 2018, als die Studie Bidirectional Encoder Representations from Transform-
(BERT) veröffentlicht wurde, hieß es, BERT sei zu groß,
zu komplex und zu langsam, um praktikabel zu sein. Das vortrainierte große BERT-Modell hat
340 Millionen Parameter und ist 1,35 GB groß.^33 Zwei Jahre später werden BERT und seine
Varianten bereits in fast jeder englischen Suche bei Google verwendet.^34
Zusammenfassung
Dieses Eröffnungskapitel sollte den Lesern ein Verständnis dafür vermitteln, was nötig ist, um
ML in die reale Welt zu bringen. Wir begannen mit einem Rundgang durch das breite Spektrum der Anwendungsfälle von ML
in der heutigen Produktion. Während die meisten Menschen mit ML in verbraucherorientierten Anwendungen vertraut sind
Anwendungen vertraut sind, ist die Mehrheit der ML-Anwendungsfälle für Unternehmen bestimmt. Wir haben auch diskutiert, wann
ML-Lösungen geeignet sind. Auch wenn ML viele Probleme lösen kann
sehr gut lösen kann, kann sie nicht alle Probleme lösen und ist sicherlich nicht für alle
Probleme. Bei Problemen, die ML nicht lösen kann, ist es jedoch möglich, dass ML ein Teil der Lösung sein kann.
Teil der Lösung sein.
In diesem Kapitel wurden auch die Unterschiede zwischen ML in der Forschung und ML in der Pro- duktion hervorgehoben.
produktion. Zu den Unterschieden gehören die Einbeziehung von Interessengruppen, die Priorität von Berechnungen,
die Eigenschaften der verwendeten Daten, die Schwere der Fairnessprobleme und die Anforderungen an die
Interpretierbarkeit. Dieser Abschnitt ist am hilfreichsten für diejenigen, die von der Wissenschaft zur ML-Produktion kommen.
aus dem akademischen Bereich kommen. Wir haben auch erörtert, wie sich ML-Systeme von herkömmlichen Software
Systemen unterscheiden, was die Notwendigkeit dieses Buches begründet.
ML-Systeme sind komplex und bestehen aus vielen verschiedenen Komponenten. Datenwissenschaftler
und ML-Ingenieure, die mit ML-Systemen in der Produktion arbeiten, werden wahrscheinlich feststellen, dass
dass es bei weitem nicht ausreicht, sich nur auf den Teil der ML-Algorithmen zu konzentrieren. Es ist wichtig, zu wissen
Es ist wichtig, andere Aspekte des Systems zu kennen, einschließlich des Datenstapels, der Bereitstellung, der Überwachung,
Wartung, Infrastruktur, usw. In diesem Buch wird ein Systemansatz für die Entwicklung von
ML-Systeme, was bedeutet, dass wir alle Komponenten eines Systems ganzheitlich betrachten
anstatt nur ML-Algorithmen zu betrachten. Was dieser ganzheitliche Ansatz im Detail bedeutet, wird
Ansatz bedeutet, wird im nächsten Kapitel erläutert.
Zusammenfassung | 23
KAPITEL 2

Einführung in maschinelles Lernen
Entwurf von Systemen
Nachdem wir uns nun einen Überblick über ML-Systeme in der realen Welt verschafft haben, können wir
können wir uns nun dem unterhaltsamen Teil des Entwurfs eines ML-Systems widmen. Um das erste Kapitel zu wiederholen
ersten Kapitel zu wiederholen, wird beim Entwurf von ML-Systemen ein Systemansatz für MLOps verfolgt, was bedeutet
dass wir ein ML-System ganzheitlich betrachten, um sicherzustellen, dass alle Komponenten - die
Geschäftsanforderungen, der Datenstapel, die Infrastruktur, die Bereitstellung, die Überwachung usw. -
und ihre Beteiligten zusammenarbeiten können, um die festgelegten Ziele und
Anforderungen zu erfüllen.

Wir beginnen das Kapitel mit einer Diskussion über die Ziele. Bevor wir ein ML
System entwickeln, müssen wir verstehen, warum dieses System benötigt wird. Wenn dieses System für ein Unternehmen gebaut wird
Unternehmen gebaut wird, muss es von Geschäftszielen geleitet sein, die in ML-Ziele übersetzt werden müssen
in ML-Ziele übersetzt werden, um die Entwicklung von ML-Modellen zu steuern.

Sobald alle Beteiligten mit den Zielen unseres ML-Systems einverstanden sind, müssen wir
einige Anforderungen festlegen, die die Entwicklung dieses Systems leiten. In diesem Buch werden wir
vier Anforderungen betrachten: Zuverlässigkeit, Skalierbarkeit, Wartbarkeit und Anpassungsfähigkeit
lichkeit. Anschließend werden wir den iterativen Prozess für die Entwicklung von Systemen vorstellen, die diese
Anforderungen.

Sie fragen sich vielleicht: Wenn all diese Ziele, Anforderungen und Prozesse vorhanden sind,
kann ich dann endlich mit dem Aufbau meines ML-Modells beginnen? Nicht so bald! Bevor Sie ML Algo-
Algorithmen zur Lösung Ihres Problems einsetzen, müssen Sie Ihr Problem zunächst in eine Aufgabe umwandeln, die
ML lösen kann. In diesem Kapitel erfahren Sie, wie Sie Ihre ML-Probleme formulieren können. Die
Schwierigkeit Ihrer Aufgabe kann sich erheblich ändern, je nachdem, wie Sie Ihr Problem formulieren.
Problem.

25
1 Eugene Yan hat einen großartigen Beitrag darüber geschrieben, wie Datenwissenschaftler die geschäftlichen Absichten und den Kontext der Projekte, an denen sie arbeiten, verstehen können.
Projekte, an denen sie arbeiten, verstehen können.
2 Milton Friedman, "A Friedman Doctrine-The Social Responsibility of Business Is to Increase Its Profits,"
New York Times Magazine, 13. September 1970, https://oreil.ly/Fmbem.
Da ML ein datengesteuerter Ansatz ist, wäre ein Buch über ML-Systemdesign verfehlt, wenn
wenn es nicht die Bedeutung von Daten in ML-Systemen erörtert. Der letzte Teil dieses Kapitels
berührt eine Debatte, die in den letzten Jahren einen Großteil der ML-Literatur verschlungen hat:
Was ist wichtiger - Daten oder intelligente Algorithmen?

Fangen wir an!

Geschäfts- und ML-Ziele
Zunächst müssen wir die Ziele der vorgeschlagenen ML-Projekte betrachten. Bei der Arbeit
an einem ML-Projekt arbeiten, interessieren sich Datenwissenschaftler in der Regel für die ML-Ziele: die Metriken
die sie über die Leistung ihrer ML-Modelle messen können, wie z. B. Genauigkeit, F1
Score, Inferenzlatenz usw. Sie freuen sich über die Verbesserung der Genauigkeit ihres Modells
von 94 % auf 94,2 % zu verbessern, und geben dafür möglicherweise eine Menge Ressourcen aus - Daten, Rechenleistung und Ingenieurzeit.
Zeit, um dies zu erreichen.

Aber die Wahrheit ist: Die meisten Unternehmen interessieren sich nicht für die ausgefallenen ML-Metriken. Es interessiert sie nicht
die Genauigkeit eines Modells von 94 % auf 94,2 % zu erhöhen, es sei denn, dadurch werden einige
Geschäftsmetriken. Ein Muster, das ich bei vielen kurzlebigen ML-Projekten sehe, ist, dass die Datenwissenschaftler
Datenwissenschaftler sich zu sehr auf das Hacken von ML-Metriken konzentrieren, ohne auf die
Geschäftsmetriken. Ihre Manager interessieren sich jedoch nur für Geschäftskennzahlen und,
wenn sie nicht erkennen, wie ein ML-Projekt dazu beitragen kann, ihre Geschäftsmetriken voranzutreiben, beenden sie die
Projekte vorzeitig ab (und entlassen möglicherweise das beteiligte Data-Science-Team).^1

Welche Metriken sind für Unternehmen also wichtig? Die meisten Unternehmen wollen Sie zwar
Sie vom Gegenteil überzeugen wollen, ist der einzige Zweck von Unternehmen laut dem Wirtschaftsnobelpreisträger
Wirtschaftsnobelpreisträger Milton Friedman, die Gewinnmaximierung für die Aktionäre.^2

Das oberste Ziel eines jeden Projekts in einem Unternehmen ist es daher, den Gewinn zu steigern,
entweder direkt oder indirekt: direkt durch die Steigerung des Umsatzes (Konversionsrate) und
Kostenreduzierung; indirekt durch höhere Kundenzufriedenheit und längere Verweildauer
Verweildauer auf einer Website.

Für den Erfolg eines ML-Projekts in einem Unternehmen ist es entscheidend, die
Leistung eines ML-Systems mit der Gesamtleistung des Unternehmens zu verknüpfen. Welche geschäftlichen
Leistungsmetriken soll das neue ML-System beeinflussen, z. B. die Höhe der
Werbeeinnahmen, die Anzahl der monatlich aktiven Nutzer?

26 | Kapitel 2: Einführung in den Entwurf maschineller Lernsysteme

3 Wir werden die Batch-Vorhersage und die Online-Vorhersage in Kapitel 7 behandeln.
4 Ashok Chandrashekar, Fernando Amat, Justin Basilico, und Tony Jebara, "Artwork Personalization at Netflix,"
Netflix Technology Blog, 7. Dezember 2017, https://oreil.ly/UEDmw.
5 Carlos A. Gomez-Uribe und Neil Hunt, "The Netflix Recommender System: Algorithms, Business Value,
and Innovation," ACM Transactions on Management Information Systems 6, no. 4 (Januar 2016): 13,
https://oreil.ly/JkEPB.
Stellen Sie sich vor, Sie arbeiten für eine E-Commerce-Website, die sich um die Kaufrate kümmert
und Sie möchten Ihr Empfehlungssystem von der Batch-Vorhersage auf die Online-Vorhersage umstellen
Online-Vorhersage umstellen.^3 Sie könnten argumentieren, dass die Online-Vorhersage Empfehlungen
die für die Nutzer relevanter sind, was zu einer höheren Kaufrate führen kann.
Sie können sogar ein Experiment durchführen, um zu zeigen, dass die Online-Vorhersage die
die Vorhersagegenauigkeit Ihres Empfehlungssystems um X % verbessern kann und, historisch gesehen, auf Ihrer Website, jedes
Prozent Erhöhung der Vorhersagegenauigkeit des Empfehlungssystems zu einer bestimmten
Anstieg der Kaufrate.

Einer der Gründe, warum die Vorhersage von Anzeigenklickraten und die Betrugserkennung
zu den beliebtesten Anwendungsfällen für ML gehören, ist, dass sich die Leistung von ML-Modellen leicht
Leistung von ML-Modellen auf Geschäftsmetriken abbilden lässt: Jede Erhöhung der Klickrate führt zu tatsächlichen
Werbeeinnahmen, und jede gestoppte betrügerische Transaktion führt zu tatsächlich eingespartem Geld.

Viele Unternehmen entwickeln ihre eigenen Metriken, um Geschäftsmetriken auf ML-Metriken abzubilden.
Netflix beispielsweise misst die Leistung seines Empfehlungssystems anhand von
Take-Rate: die Anzahl der Qualitätsspiele geteilt durch die Anzahl der Empfehlungen
^4 Je höher die Take-Rate, desto besser das Empfehlungssystem. Netflix
setzt die Take-Rate eines Empfehlungssystems auch in den Kontext seiner anderen Geschäfts
Geschäftskennzahlen wie der Gesamtzahl der Streaming-Stunden und der Kündigungsrate von Abonnements. Sie fanden heraus, dass
eine höhere Einschaltquote auch zu einer höheren Gesamtanzahl an Streaming-Stunden und einer niedrigeren
Kündigungsrate führt.^5

Die Auswirkungen eines ML-Projekts auf die Unternehmensziele sind mitunter schwer zu ermessen. Unter
kann ein ML-Modell, das den Kunden individuellere Lösungen bietet, sie zufriedener
Kunden zufriedener machen, so dass sie mehr Geld für Ihre Dienstleistungen ausgeben. Das gleiche ML
Modell kann ihre Probleme auch schneller lösen, so dass sie weniger Geld für Ihre
Dienstleistungen ausgeben.

Um eine eindeutige Antwort auf die Frage zu erhalten, wie ML-Metriken die Geschäfts
Metriken beeinflussen, sind oft Experimente erforderlich. Viele Unternehmen machen das mit Experimenten
wie A/B-Tests und wählen das Modell, das zu besseren Geschäftskennzahlen führt, unabhängig davon
unabhängig davon, ob dieses Modell bessere ML-Kennzahlen hat.

Geschäfts- und ML-Ziele | 27
6 Parmy Olson, "Nearly Half of All 'AI Startups' Are Cashing In on Hype", Forbes, 4. März 2019,
https://oreil.ly/w5kOr.
7 "2020 State of Enterprise Machine Learning", Algorithmia, 2020, https://oreil.ly/FlIV1.
Doch selbst rigorose Experimente reichen möglicherweise nicht aus, um die Beziehung
zwischen den Ergebnissen eines ML-Modells und den Geschäftskennzahlen zu verstehen. Stellen Sie sich vor, Sie arbeiten für ein
Cybersicherheitsunternehmen, das Sicherheitsbedrohungen aufspürt und stoppt, und ML ist nur eine
Komponente in ihrem komplexen Prozess. Ein ML-Modell wird verwendet, um Anomalien im
Verkehrsmuster zu erkennen. Diese Anomalien durchlaufen dann einen Logiksatz (z. B. eine Reihe von
if-else-Anweisungen), die kategorisiert, ob sie potenzielle Bedrohungen darstellen. Diese
potenziellen Bedrohungen werden dann von Sicherheitsexperten geprüft, um festzustellen, ob es sich um
tatsächliche Bedrohungen sind. Tatsächliche Bedrohungen durchlaufen dann einen weiteren, anderen Prozess, der darauf abzielt
um sie zu stoppen. Wenn dieser Prozess eine Bedrohung nicht aufhalten kann, ist es möglicherweise unmöglich
herauszufinden, ob die ML-Komponente etwas damit zu tun hat.

Viele Unternehmen sagen gerne, dass sie ML in ihren Systemen einsetzen, weil "KI-
angetrieben" zu sein, hilft ihnen bereits, Kunden anzuziehen, unabhängig davon, ob die KI
Teil tatsächlich etwas Nützliches tut.^6

Bei der Bewertung von ML-Lösungen aus der Sicht des Unternehmens ist es wichtig, die
über die erwarteten Erträge. Aufgrund des ganzen Hypes, der um ML gemacht wird, sowohl
Medien und von Fachleuten, die ein persönliches Interesse an der Einführung von ML haben, könnten einige
Unternehmen vielleicht die Vorstellung, dass ML ihr Geschäft auf magische Weise
über Nacht.

Magisch: möglich. Über Nacht: nein.

Es gibt viele Unternehmen, für die sich ML ausgezahlt hat. Zum Beispiel hat ML
Google geholfen, besser zu suchen, mehr Anzeigen zu höheren Preisen zu verkaufen, die Übersetzungsqualität zu
Übersetzungsqualität zu verbessern und bessere Android-Anwendungen zu entwickeln. Aber dieser Gewinn kam nicht über Nacht.
Google investiert schon seit Jahrzehnten in ML.

Die Rendite von Investitionen in ML hängt stark von der Reifephase der Einführung ab. Je
länger Sie ML einsetzen, desto effizienter wird Ihre Pipeline laufen, desto schneller wird Ihr
Je länger Sie ML einsetzen, desto effizienter läuft Ihre Pipeline, desto schneller ist Ihr Entwicklungszyklus, desto weniger Zeit benötigen Sie für die Entwicklung und desto niedriger sind Ihre
Cloud-Rechnungen werden niedriger sein, was alles zu höheren Erträgen führt. Laut einer 2020 durchgeführten Umfrage von
Algorithmia, unter den Unternehmen, die bei der Einführung von ML schon weiter fortgeschritten sind
(mit Modellen, die seit mehr als fünf Jahren in Produktion sind), können fast 75 % ein
Modell in weniger als 30 Tagen einsetzen. Bei den Unternehmen, die gerade erst mit ihrer ML-Pipeline beginnen,
brauchen 60 % mehr als 30 Tage für die Bereitstellung eines Modells (siehe Abbildung 2-1).^7

28 | Kapitel 2: Einführung in den Entwurf maschineller Lernsysteme

Abbildung 2-1. Wie lange ein Unternehmen braucht, um ein Modell in Produktion zu bringen, hängt
proportional dazu, wie lange es ML bereits einsetzt. Quelle: Angepasst an ein Bild von Algorithmia

Anforderungen für ML-Systeme
Wir können nicht sagen, dass wir ein ML-System erfolgreich gebaut haben, ohne zu wissen, welche
Anforderungen das System erfüllen muss. Die spezifizierten Anforderungen an ein ML-System
variieren von Anwendungsfall zu Anwendungsfall. Die meisten Systeme sollten jedoch die folgenden vier Merkmale aufweisen
Zuverlässigkeit, Skalierbarkeit, Wartbarkeit und Anpassungsfähigkeit. Wir werden diese
jedes dieser Konzepte im Detail. Schauen wir uns zunächst die Zuverlässigkeit genauer an.

Verlässlichkeit
Das System sollte auch bei widrigen Umständen (Hardware- oder Softwarefehler und
Leistung auch bei widrigen Umständen (Hardware- oder Softwarefehler und sogar
menschliches Versagen).

Die "Korrektheit" kann bei ML-Systemen schwer zu bestimmen sein. Zum Beispiel könnte Ihr
System könnte die Vorhersagefunktion - z. B. model.predict()- korrekt aufrufen, aber die
Vorhersagen sind jedoch falsch. Wie können wir wissen, ob eine Vorhersage falsch ist, wenn wir keine
mit denen wir sie vergleichen können?

Bei herkömmlichen Softwaresystemen erhalten Sie oft eine Warnung, z. B. einen Systemabsturz
oder Laufzeitfehler oder 404. ML-Systeme können jedoch unbemerkt ausfallen. Die Endnutzer wissen nicht einmal
Endnutzer wissen nicht einmal, dass das System ausgefallen ist, und nutzen es möglicherweise weiter, als ob es funktionieren würde.
Wenn Sie zum Beispiel Google Translate verwenden, um einen Satz in eine Sprache zu übersetzen, die Sie nicht kennen
Sprache zu übersetzen, die Sie nicht kennen, ist es für Sie möglicherweise sehr schwer zu erkennen, ob die Übersetzung falsch ist. Wir werden
wie ML-Systeme in der Produktion versagen, werden wir in Kapitel 8 besprechen.

Anforderungen für ML-Systeme | 29
8 Auf- und Abwärtsskalierung sind zwei Aspekte des "Scaling out", das sich von "Scaling up" unterscheidet. Verkleinern
ist das parallele Hinzufügen weiterer gleichwertiger funktionaler Komponenten, um eine Last zu verteilen. Beim Hochskalieren wird eine
Komponente größer oder schneller zu machen, um eine größere Last zu bewältigen (Leah Schoeb, "Cloud Scalability: Scale Up vs Scale Out,"
Turbonomic Blog, 15. März 2018, https://oreil.ly/CFPtb).
9 Sean Wolfe, "Amazon's One Hour of Downtime on Prime Day May Have Cost It up to $100 Million in Lost
Sales," Business Insider, July 19, 2018, https://oreil.ly/VBezI.
Skalierbarkeit
Es gibt mehrere Möglichkeiten, wie ein ML-System wachsen kann. Es kann an Komplexität zunehmen. Letztes Jahr
haben Sie ein logistisches Regressionsmodell verwendet, das in eine Amazon Web Services (AWS)
Instanz mit 1 GB RAM passte, aber dieses Jahr sind Sie zu einem neuronalen Netzwerk mit 100 Millionen
Parameter neuronales Netzwerk, das 16 GB RAM benötigt, um Vorhersagen zu erstellen.

Ihr ML-System kann das Verkehrsaufkommen erhöhen. Als Sie mit dem Einsatz eines ML
Systems begannen, haben Sie täglich nur 10.000 Vorhersageanfragen bearbeitet. Wenn jedoch die Nutzerbasis Ihres Unternehmens
wächst, schwankt die Anzahl der täglichen Vorhersageanfragen, die Ihr ML-System
zwischen 1 Million und 10 Millionen schwanken.

Ein ML-System kann in der Anzahl der ML-Modelle wachsen. Anfänglich haben Sie vielleicht nur ein
Modell für einen bestimmten Anwendungsfall, z. B. die Erkennung von Trending Hashtags in einem sozialen
Website wie Twitter. Im Laufe der Zeit möchten Sie jedoch weitere Funktionen zu diesem Anwendungsfall hinzufügen.
Sie fügen also ein weiteres Modell hinzu, um NSFW-Inhalte (not safe for work) herauszufiltern und
ein weiteres Modell, um von Bots generierte Tweets herauszufiltern. Dieses Wachstumsmuster ist besonders
bei ML-Systemen, die auf Anwendungsfälle in Unternehmen abzielen. Anfänglich könnte ein Startup
nur einen Unternehmenskunden bedienen, was bedeutet, dass dieses Startup nur ein Modell hat.
Wenn das Startup jedoch mehr Kunden gewinnt, kann es für jeden Kunden ein Modell haben.
Kunden haben. Ein Startup, mit dem ich gearbeitet habe, hatte 8.000 Modelle in Produktion für seine 8.000
Unternehmenskunden.

Unabhängig davon, wie Ihr System wächst, sollte es vernünftige Wege geben, mit diesem Wachstum umzugehen.
mit diesem Wachstum umgehen. Wenn von Skalierbarkeit die Rede ist, denken die meisten Menschen an Ressourcenskalierung,
die aus einer Aufwärtsskalierung (Erweiterung der Ressourcen zur Bewältigung des Wachstums) und einer Abwärtsskalierung
Skalierung (Reduzierung der Ressourcen, wenn sie nicht benötigt werden).^8

In der Spitze könnte Ihr System zum Beispiel 100 GPUs (Grafikprozessoren) benötigen.
Einheiten). Die meiste Zeit benötigt es jedoch nur 10 GPUs. Die ständige Aufrechterhaltung von 100 GPUs
Daher sollte Ihr System auf 10 GPUs herunter skaliert werden können.

Ein unverzichtbares Merkmal vieler Cloud-Dienste ist die automatische Skalierung, d. h. die automatische
die Anzahl der Maschinen je nach Nutzung automatisch nach oben oder unten. Diese Funktion kann kompliziert
zu implementieren. Sogar Amazon fiel dem zum Opfer, als seine Autoscaling-Funktion am Prime Day ausfiel
am Prime Day ausfiel und das System zum Absturz brachte. Eine Stunde Ausfallzeit hat Amazon schätzungsweise
Amazon zwischen 72 und 99 Millionen Dollar gekostet.^9

30 | Kapitel 2: Einführung in den Entwurf maschineller Lernsysteme

Bei der Bewältigung des Wachstums geht es jedoch nicht nur um die Skalierung von Ressourcen, sondern auch um die Verwaltung von Artefakten.
Die Verwaltung von hundert Modellen unterscheidet sich stark von der Verwaltung eines Modells. Bei einem
Modell können Sie vielleicht die Leistung dieses Modells manuell überwachen und das Modell manuell
das Modell mit neuen Daten aktualisieren. Da es nur ein Modell gibt, können Sie einfach eine Datei
die Ihnen hilft, dieses Modell bei Bedarf zu reproduzieren. Bei einhundert
Modellen müssen sowohl die Überwachung als auch die Nachschulung automatisiert werden. Sie werden
Sie brauchen eine Möglichkeit, die Codegenerierung zu verwalten, damit Sie ein Modell bei Bedarf adäquat
Modell bei Bedarf reproduzieren können.

Da die Skalierbarkeit ein so wichtiges Thema im gesamten ML-Projektablauf ist,
werden wir es in verschiedenen Teilen des Buches behandeln. Insbesondere werden wir den Aspekt der Ressourcenskalierung
im Abschnitt "Verteiltes Training" auf Seite 168 , im Abschnitt "Modelloptimierung
Optimierung" auf Seite 216 und im Abschnitt "Ressourcenmanagement" auf Seite 311.
Der Aspekt der Artefaktverwaltung wird in den Abschnitten "Experimentverfolgung und
Versionierung" auf Seite 162 und im Abschnitt "Entwicklungsumgebung" auf Seite 302.

Instandhaltbarkeit
Es gibt viele Personen, die an einem ML-System arbeiten werden. Es sind ML-Ingenieure,
DevOps-Ingenieure und Fachexperten (KMU). Sie kommen vielleicht aus sehr
verschiedenen Hintergründen, mit sehr unterschiedlichen Programmiersprachen und Tools und
können für verschiedene Teile des Prozesses zuständig sein.

Es ist wichtig, Ihre Arbeitslasten zu strukturieren und Ihre Infrastruktur so einzurichten, dass
Infrastruktur so einzurichten, dass verschiedene Beteiligte mit den Werkzeugen arbeiten können, mit denen sie vertraut sind
mit denen sie vertraut sind, anstatt dass eine Gruppe von Mitwirkenden ihre Werkzeuge anderen Gruppen aufzwingt.
Der Code sollte dokumentiert werden. Code, Daten und Artefakte sollten versioniert werden. Modelle
sollten hinreichend reproduzierbar sein, so dass auch bei Abwesenheit der ursprünglichen Autoren
nicht anwesend sind, andere Mitwirkende genügend Kontext haben, um auf ihrer Arbeit aufzubauen. Wenn
Wenn ein Problem auftritt, sollten verschiedene Mitwirkende zusammenarbeiten können, um das Problem zu
das Problem zu identifizieren und eine Lösung zu implementieren, ohne sich gegenseitig die Schuld zuzuschieben.

Wir werden im Abschnitt "Teamstruktur" auf Seite 334 näher darauf eingehen.

Anpassungsfähigkeit
Um sich an wechselnde Datenverteilungen und Geschäftsanforderungen anzupassen, sollte das System
über eine gewisse Fähigkeit verfügen, sowohl Aspekte zur Leistungsverbesserung zu erkennen als auch
Aktualisierungen ohne Dienstunterbrechung zu ermöglichen.

Anforderungen an ML-Systeme | 31
10 Was, wie ein früher Rezensent bemerkte, eine Eigenschaft traditioneller Software ist.
11 Das Beten und Weinen wird nicht vorgestellt, ist aber während des gesamten Prozesses präsent.

Da ML-Systeme zum Teil aus Code und zum Teil aus Daten bestehen und sich Daten schnell ändern können, müssen ML
Systeme schnell weiterentwickelt werden können müssen. Dies ist eng mit der Wartungsfreundlichkeit verbunden.
Wir erörtern wechselnde Datenverteilungen im Abschnitt "Verschiebung der Datenverteilung" auf
Seite 237 , und wie Sie Ihr Modell kontinuierlich mit neuen Daten aktualisieren können, wird im Abschnitt
"Kontinuierliches Lernen" auf Seite 264.
Iterativer Prozess
Die Entwicklung eines ML-Systems ist ein iterativer und in den meisten Fällen nie endender Prozess.^10
Sobald ein System in Betrieb genommen wird, muss es kontinuierlich überwacht und
aktualisiert werden.
Bevor ich mein erstes ML-System in Betrieb nahm, dachte ich, der Prozess würde linear und
geradlinig. Ich dachte, ich müsste nur Daten sammeln, ein Modell trainieren, es einsetzen
dieses Modells und das war's. Ich habe jedoch bald festgestellt, dass der Prozess eher einem
Zyklus mit vielen Hin- und Herbewegungen zwischen verschiedenen Schritten.
Hier ein Beispiel für einen Arbeitsablauf, der beim Aufbau eines ML-Modells auftreten kann
Modells zur Vorhersage, ob eine Anzeige eingeblendet werden soll, wenn Benutzer eine Suchanfrage stellen:^11
1.1. Wählen Sie eine zu optimierende Metrik. Sie könnten zum Beispiel optimieren wollen für
Impressionen - die Anzahl der Anzeigenschaltungen.
2.2. Sammeln Sie Daten und erhalten Sie Labels.
3.3. Funktionen entwickeln.
4.4. Modelle trainieren.
5.5. Bei der Fehleranalyse stellen Sie fest, dass die Fehler durch falsche Beschriftungen verursacht werden, also
Sie die Daten neu beschriften.
6.6. Trainieren Sie das Modell erneut.
7.7. Während der Fehleranalyse stellen Sie fest, dass Ihr Modell immer vorhersagt, dass eine Anzeige
dass eine Anzeige nicht angezeigt werden sollte, und zwar deshalb, weil 99,99 % der Daten
NEGATIVE Labels (Anzeigen, die nicht angezeigt werden sollten). Sie müssen also mehr Daten
Daten von Anzeigen sammeln, die angezeigt werden sollten.
32 | Kapitel 2: Einführung in den Entwurf von Systemen für maschinelles Lernen
8.8. Trainieren Sie das Modell erneut.
9.9. Das Modell schneidet bei Ihren vorhandenen Testdaten, die inzwischen zwei Monate alt sind, gut ab.
alt sind. Es schneidet jedoch bei den Daten von gestern schlecht ab. Ihr Modell ist jetzt
veraltet, so dass Sie es mit neueren Daten aktualisieren müssen.
10.10. Trainieren Sie das Modell erneut.

11.11. Setzen Sie das Modell ein.

12.12. Das Modell scheint gut zu funktionieren, aber dann klopfen die Geschäftsleute
an Ihre Tür und fragen, warum die Einnahmen rückläufig sind. Es stellt sich heraus, dass die
Anzeigen zwar geschaltet werden, aber nur wenige Leute darauf klicken. Also wollen Sie Ihr Modell ändern
Modell ändern, um stattdessen die Klickrate der Anzeigen zu optimieren.

13.13. Gehen Sie zu Schritt 1.

Abbildung 2-2 zeigt in einer stark vereinfachten Darstellung, wie der iterative Prozess zur
Entwicklung von ML-Systemen in der Produktion aus der Sicht eines Datenwissenschaftlers
Wissenschaftlers oder eines ML-Ingenieurs aussieht. Dieser Prozess sieht aus der Perspektive eines
eines ML-Plattform-Ingenieurs oder eines DevOps-Ingenieurs anders aus, da sie möglicherweise nicht so viel
Kontext der Modellentwicklung haben und viel mehr Zeit mit der Einrichtung der
Infrastruktur.

Abbildung 2-2. Der Prozess der Entwicklung eines ML-Systems gleicht eher einem Zyklus mit einer Vielzahl von
hin und her zwischen den Schritten

Iterativer Prozess | 33
In späteren Kapiteln wird näher darauf eingegangen, was jeder dieser Schritte in der Praxis bedeutet.
Hier wollen wir einen kurzen Blick darauf werfen, was sie bedeuten:

Schritt 1. Projekt-Scoping
Ein Projekt beginnt mit dem Projekt-Scoping, bei dem die Ziele, Vorgaben und Bedingungen festgelegt werden.
Belastungen. Die Interessengruppen sollten ermittelt und einbezogen werden. Die Ressourcen sollten
geschätzt und zugewiesen werden. Wir haben bereits in Kapitel 1 die verschiedenen Akteure und einige
die Schwerpunkte für ML-Projekte in der Produktion in Kapitel 1. Wir haben auch schon besprochen
wie man ein ML-Projekt im Kontext eines Unternehmens einteilt, haben wir bereits in diesem Kapitel besprochen.
Wie man Teams organisiert, um den Erfolg eines ML-Projekts zu gewährleisten, besprechen wir in
Kapitel 11.

Schritt 2. Daten-Engineering
Die überwiegende Mehrheit der ML-Modelle lernt heute aus Daten, daher beginnt die Entwicklung von ML-Modellen
beginnt mit der Entwicklung von Daten. In Kapitel 3 werden wir die Grundlagen der Datentechnik
Engineering, das den Umgang mit Daten aus verschiedenen Quellen und Formaten umfasst.
Wenn wir Zugang zu Rohdaten haben, müssen wir aus diesen Daten Trainingsdaten erstellen, indem wir Stichproben
und die Erzeugung von Labels, was in Kapitel 4 behandelt wird.

Schritt 3. Entwicklung eines ML-Modells
Mit dem anfänglichen Satz von Trainingsdaten müssen wir Merkmale extrahieren und
erste Modelle entwickeln, die diese Merkmale nutzen. Dies ist die Phase, die das meiste
ML-Kenntnisse erfordert und am häufigsten in ML-Kursen behandelt wird. In Kapitel 5 werden wir
Feature-Engineering besprechen. In Kapitel 6 besprechen wir die Modellauswahl, das Training
und Auswertung.

Schritt 4. Bereitstellung
Nachdem ein Modell entwickelt wurde, muss es den Benutzern zugänglich gemacht werden. Die Entwicklung von
eines ML-Systems ist wie Schreiben - Sie werden nie den Punkt erreichen, an dem Ihr System
fertig ist. Aber Sie erreichen den Punkt, an dem Sie Ihr System bereitstellen müssen.
In Kapitel 7 werden wir verschiedene Möglichkeiten zur Bereitstellung eines ML-Modells erörtern.

34 | Kapitel 2: Einführung in den Entwurf maschineller Lernsysteme

Schritt 5. Überwachung und kontinuierliches Lernen
Sobald die Modelle in Produktion sind, müssen sie auf Leistungsabfall überwacht und
gewartet werden, damit sie sich an veränderte Umgebungen und Anforderungen anpassen können.
Dieser Schritt wird in den Kapiteln 8 und 9 behandelt.

Schritt 6. Geschäftliche Analyse
Die Leistung des Modells muss anhand der Unternehmensziele bewertet und analysiert werden
um geschäftliche Erkenntnisse zu gewinnen. Diese Erkenntnisse können dann genutzt werden, um unpro-
unproduktive Projekte zu eliminieren oder neue Projekte zu konzipieren. Dieser Schritt steht in engem Zusammenhang mit dem ersten
Schritt.

ML-Probleme einrahmen
Stellen Sie sich vor, Sie sind technischer Leiter für ML in einer Bank, die sich an Millennials richtet.
Eines Tages erfährt Ihr Chef von einer konkurrierenden Bank, die ML einsetzt, um ihren Kundenservice zu beschleunigen.
Kundenbetreuung zu beschleunigen. Angeblich kann die konkurrierende Bank so Kundenanfragen
zwei Mal schneller zu bearbeiten. Er beauftragt Ihr Team, den Einsatz von ML zu prüfen, um auch Ihren
Kundenbetreuung zu beschleunigen.

Langsamer Kundendienst ist ein Problem, aber es ist kein ML-Problem. Ein ML-Problem
wird durch Eingaben, Ausgaben und die Zielfunktion definiert, die den Lernprozess steuert.
Lernprozess steuert - keine dieser drei Komponenten ist aus der Anfrage Ihres Chefs ersichtlich. Es ist
Ihre Aufgabe als erfahrener ML-Ingenieur ist es, Ihr Wissen darüber, welche Probleme ML
lösen kann, um diese Anfrage als ML-Problem zu formulieren.

Bei einer Untersuchung stellen Sie fest, dass der Engpass bei der Beantwortung von Kundenanfragen
in der Weiterleitung von Kundenanfragen an die richtige Abteilung liegt.
Abteilungen: Buchhaltung, Inventar, HR (Human Resources) und IT. Sie können diesen Engpass beheben
Sie können diesen Engpass beseitigen, indem Sie ein ML-Modell entwickeln, das vorhersagt, an welche dieser vier Abteilungen eine Anfrage gehen soll.
Abteilungen eine Anfrage gehen sollte. Es handelt sich also um ein Klassifizierungsproblem. Die Eingabe ist
die Kundenanfrage. Die Ausgabe ist die Abteilung, an die die Anfrage gehen soll. Die
Zielfunktion ist die Minimierung der Differenz zwischen der vorhergesagten Abteilung
und der tatsächlichen Abteilung zu minimieren.

In Kapitel 5 werden wir ausführlich erörtern, wie Sie Merkmale aus Rohdaten extrahieren und in Ihr ML
Modell in Kapitel 5. In diesem Abschnitt werden wir uns auf zwei Aspekte konzentrieren: die Ausgabe Ihres
Modells und die Zielfunktion, die den Lernprozess steuert.

Formulierung von ML-Problemen | 35
Arten von ML-Aufgaben
Die Ausgabe Ihres Modells bestimmt den Aufgabentyp Ihres ML-Problems. Die allgemeinsten
Typen von ML-Aufgaben sind Klassifizierung und Regression. Innerhalb der Klassifizierung gibt es
gibt es weitere Untertypen, wie in Abbildung 2-3 dargestellt. Wir gehen auf jeden dieser Aufgabentypen ein.

Abbildung 2-3. Übliche Aufgabentypen in ML

Klassifizierung versus Regression

Klassifizierungsmodelle klassifizieren Eingaben in verschiedene Kategorien. Sie möchten zum Beispiel
jede E-Mail entweder als Spam oder nicht als Spam klassifizieren. Regressionsmodelle geben einen
kontinuierlichen Wert aus. Ein Beispiel ist ein Modell zur Vorhersage eines Hauses, das den Preis eines
bestimmten Hauses ausgibt.

Ein Regressionsmodell kann leicht in ein Klassifikationsmodell umgewandelt werden und vice versa. Unter
Beispiel: Die Vorhersage von Hauspreisen kann zu einer Klassifizierungsaufgabe werden, wenn wir die Hauspreise
Hauspreise in Kategorien wie unter $100.000, $100.000-$200.000, $200.000-$500.000,
und so weiter, und vorhersagen, in welchem Bereich sich das Haus befinden sollte.

Das E-Mail-Klassifizierungsmodell kann zu einem Regressionsmodell werden, wenn wir es zur Ausgabe von
Werte zwischen 0 und 1 ausgibt und einen Schwellenwert festlegt, um zu bestimmen, welche Werte
SPAM ist (z. B. wenn der Wert über 0,5 liegt, ist die E-Mail Spam), wie in
Abbildung 2-4.

36 | Kapitel 2: Einführung in den Entwurf maschineller Lernsysteme

Abbildung 2-4. Die Aufgabe der E-Mail-Klassifizierung kann auch als Regressionsaufgabe formuliert werden

Binäre versus Mehrklassen-Klassifizierung

Bei Klassifizierungsproblemen gilt: Je weniger Klassen es zu klassifizieren gibt, desto einfacher ist das
Problem ist. Am einfachsten ist die binäre Klassifizierung, bei der es nur zwei mögliche
Klassen gibt. Beispiele für binäre Klassifizierung sind die Klassifizierung, ob ein Kommentar giftig ist
giftig ist, ob ein Lungenscan Anzeichen von Krebs zeigt oder ob eine Transaktion betrügerisch ist.
Es ist unklar, ob diese Art von Problemen in der Branche üblich ist, weil sie
weil sie in der Natur der Sache liegen oder einfach, weil ML-Fachleute am liebsten
Umgang mit ihnen.

Wenn es mehr als zwei Klassen gibt, wird das Problem zu einer Multiklassen-Klassifikation.
Der Umgang mit binären Klassifizierungsproblemen ist viel einfacher als der Umgang mit Mehrklassenproblemen.
Klassenproblemen. Zum Beispiel sind die Berechnung von F1 und die Visualisierung von Konfusionsmatrizen
viel intuitiver, wenn es nur zwei Klassen gibt.

Wenn die Anzahl der Klassen hoch ist, wie bei der Krankheitsdiagnose, wo die Anzahl
Krankheiten in die Tausende gehen kann, oder bei Produktklassifizierungen, wo die Anzahl der
Produkte bis zu Zehntausenden gehen kann, spricht man von einer Klassifizierungsaufgabe mit hoher
Kardinalität. Probleme mit hoher Kardinalität können eine große Herausforderung darstellen. Die erste Herausforderung besteht
in der Datenerfassung. Nach meiner Erfahrung benötigen ML-Modelle in der Regel mindestens 100 Beispiele
für jede Klasse, um zu lernen, diese Klasse zu klassifizieren. Wenn Sie also 1.000 Klassen haben, benötigen Sie bereits
mindestens 100.000 Beispiele benötigt. Die Datenerfassung kann besonders schwierig sein für seltene
Klassen. Wenn Sie Tausende von Klassen haben, ist es wahrscheinlich, dass einige davon selten sind.

Rahmung von ML-Problemen | 37
Wenn die Anzahl der Klassen groß ist, kann eine hierarchische Klassifizierung sinnvoll sein. Bei
hierarchischen Klassifikation wird ein Klassifikator eingesetzt, der jedes Beispiel zunächst in eine der
einer der großen Gruppen. Dann wird ein weiterer Klassifikator eingesetzt, um dieses Beispiel in eine
der Untergruppen einordnet. Bei der Produktklassifizierung können Sie zum Beispiel jedes Produkt zunächst
Produkt in eine der vier Hauptkategorien einordnen: Elektronik, Haushalt und Küche, Mode,
oder Haustierbedarf. Nachdem ein Produkt in eine Kategorie, z. B. Mode, eingeordnet wurde, können Sie
können Sie dieses Produkt mit einem weiteren Klassifikator in eine der Untergruppen einordnen: Schuhe, Hemden,
Jeans oder Accessoires.

Multiklassen- versus Multilabel-Klassifizierung

Sowohl bei der binären als auch bei der Mehrklassen-Klassifikation gehört jedes Beispiel zu genau einer
Klasse. Wenn ein Beispiel zu mehreren Klassen gehören kann, spricht man von einem Multilabel-Klassifizierungsproblem.
Klassifizierungsproblem. Wenn Sie zum Beispiel ein Modell zur Klassifizierung von Artikeln in vier
Themen - Technik, Unterhaltung, Finanzen und Politik - kann ein Artikel sowohl zu Technik als auch zu
Finanzen gehören.

Es gibt zwei Hauptansätze für mehrstufige Klassifizierungsprobleme. Der erste besteht darin
wie bei einer Mehrklassen-Klassifikation zu behandeln. Bei der Mehrklassenklassifizierung, wenn es
vier mögliche Klassen gibt [Technik, Unterhaltung, Finanzen, Politik] und das Label für ein
Beispiel "Unterhaltung" ist, wird diese Bezeichnung durch den Vektor [0, 1, 0, 0] dargestellt. Bei
Multilabel-Klassifikation wird ein Beispiel, das die beiden Labels Unterhaltung und Finanzen hat
Label als [0, 1, 1, 0] dargestellt.

Der zweite Ansatz besteht darin, sie in eine Reihe von binären Klassifizierungsproblemen umzuwandeln. Für das
Artikelklassifizierungsproblem können Sie vier Modelle haben, die vier Themen entsprechen,
jedes Modell gibt aus, ob ein Artikel zu diesem Thema gehört oder nicht.

Von allen Aufgabentypen ist die Multilabel-Klassifizierung in der Regel diejenige, mit der die Unternehmen
Unternehmen die meisten Probleme haben. Multilabel bedeutet, dass die Anzahl der Klassen
eines Beispiels von Beispiel zu Beispiel variieren kann. Dies erschwert zunächst die
der Beschriftung, da es das Problem der Beschriftungsvielfalt, das wir in Kapitel
in Kapitel 4 erörtert wird. Zum Beispiel könnte ein Annotator glauben, dass ein Beispiel zu zwei Klassen gehört
Klassen zugehörig ist, während ein anderer Annotator dasselbe Beispiel vielleicht nur einer
Klasse zugehörig ist, und es könnte schwierig sein, ihre Unstimmigkeiten aufzulösen.

Zweitens macht es diese unterschiedliche Anzahl von Klassen schwierig, Vorhersagen aus der rohen
Wahrscheinlichkeit. Betrachten wir die gleiche Aufgabe, Artikel in vier Themen zu klassifizieren. Stellen Sie sich vor
dass Ihr Modell bei einem Artikel diese rohe Wahrscheinlichkeitsverteilung ausgibt: [0.45, 0.2,
0.02, 0.33]. Wenn Sie wissen, dass ein Beispiel nur einer Kategorie angehören kann, wählen Sie im Mehrklassenmodell
nur zu einer Kategorie gehören kann, wählen Sie einfach die Kategorie mit der höchsten Wahrscheinlichkeit, die in diesem Fall
ist in diesem Fall 0,45. Da man in der Multilabel-Einstellung nicht weiß, zu wie vielen
Kategorien ein Beispiel gehören kann, wählen Sie möglicherweise die beiden Kategorien mit der höchsten Wahrscheinlichkeit
Kategorien (entsprechend 0,45 und 0,33) oder die drei Kategorien mit der höchsten Wahrscheinlichkeit
(die 0,45, 0,2 und 0,33 entsprechen).

38 | Kapitel 2: Einführung in den Entwurf maschineller Lernsysteme

Mehrere Möglichkeiten, ein Problem zu formulieren

Wenn Sie die Art und Weise, wie Sie Ihr Problem formulieren, ändern, wird es möglicherweise deutlich
schwieriger oder einfacher machen. Stellen Sie sich vor, Sie sollen vorhersagen, welche App ein Telefonbenutzer als nächstes nutzen will
als nächstes nutzen möchte. Ein naiver Ansatz wäre es, dies als eine Mehrklassen-Klassifizierungsaufgabe zu betrachten - Verwendung
die Merkmale des Benutzers und der Umgebung (demografische Informationen des Benutzers, Zeit, Ort,
zuvor genutzte Apps) als Eingabe und Ausgabe einer Wahrscheinlichkeitsverteilung für jede einzelne
App auf dem Telefon des Benutzers. N sei die Anzahl der Apps, die Sie einem Nutzer empfehlen wollen.
einem Nutzer empfehlen wollen. In diesem Rahmen gibt es für einen bestimmten Benutzer zu einem bestimmten Zeitpunkt nur
eine Vorhersage zu treffen, und die Vorhersage ist ein Vektor der Größe N. Dieser Aufbau ist
Abbildung 2-5 veranschaulicht.

Abbildung 2-5. Bei dem Problem der Vorhersage der App, die ein Benutzer wahrscheinlich als nächstes öffnen wird,
kann man es als Klassifikationsproblem betrachten. Die Eingabe sind die Merkmale des Benutzers und der Umgebung.
der Umgebung. Die Ausgabe ist eine Verteilung über alle Apps auf dem Telefon.

Dies ist ein schlechter Ansatz, denn jedes Mal, wenn eine neue Anwendung hinzugefügt wird, müssen Sie möglicherweise
Ihr Modell von Grund auf neu trainieren oder zumindest alle Komponenten Ihres Modells neu trainieren, deren Anzahl von N abhängt.
deren Anzahl der Parameter von N abhängt. Ein besserer Ansatz ist es, dies als eine
Regressionsaufgabe. Die Eingabe sind die Merkmale des Benutzers, der Umgebung und der Anwendung. Die
Ausgabe ist ein einzelner Wert zwischen 0 und 1; je höher der Wert, desto wahrscheinlicher wird der
je höher der Wert, desto wahrscheinlicher ist es, dass der Nutzer die App in dem gegebenen Kontext öffnet. In diesem Rahmen gibt es für einen bestimmten Nutzer zu einem bestimmten
N Vorhersagen zu treffen, eine für jede App, aber jede Vorhersage ist nur eine
Zahl. Dieser verbesserte Aufbau wird in Abbildung 2-6 veranschaulicht.

ML-Probleme formulieren | 39
12 Beachten Sie, dass Zielfunktionen mathematische Funktionen sind, die sich von den Geschäfts- und ML
Zielen, die wir weiter oben in diesem Kapitel besprochen haben.

Abbildung 2-6. Bei dem Problem der Vorhersage der App, die ein Benutzer wahrscheinlich als nächstes öffnen wird,
kann man es als ein Regressionsproblem auffassen. Die Eingabe sind die Merkmale des Benutzers, die Merkmale der Umgebung
Merkmale und die Merkmale einer App. Die Ausgabe ist ein einzelner Wert zwischen 0 und 1, der angibt
wie wahrscheinlich es ist, dass der Nutzer die App im gegebenen Kontext öffnet.
In diesem neuen Rahmen müssen Sie bei jeder neuen App, die Sie einem Nutzer empfehlen möchten
empfehlen möchten, müssen Sie einfach neue Eingaben mit der Funktion dieser neuen App verwenden, anstatt
Ihr Modell oder einen Teil Ihres Modells von Grund auf neu trainieren zu müssen.
Objektive Funktionen
Um zu lernen, benötigt ein ML-Modell eine Zielfunktion, die den Lernprozess steuert.^12
Eine Zielfunktion wird auch als Verlustfunktion bezeichnet, da das Ziel des Lernprozesses
Ziel des Lernprozesses ist es in der Regel, den durch falsche Vorhersagen verursachten Verlust zu minimieren (oder zu optimieren).
Bei überwachtem ML kann dieser Verlust durch den Vergleich der Modellausgaben mit
der Grundwahrheit unter Verwendung eines Maßes wie dem mittleren quadratischen Fehler (RMSE) oder
Kreuzentropie.
Um diesen Punkt zu veranschaulichen, kehren wir noch einmal zu der vorherigen Aufgabe zurück, Artikel in vier Themenbereiche zu klassifizieren
in vier Themenbereiche [Technik, Unterhaltung, Finanzen, Politik]. Betrachten wir einen Artikel, der
der Klasse Politik angehört, d. h. sein Ground-Truth-Label ist [0, 0, 0, 1]. Stellen Sie sich das vor,
Ihr Modell gibt für diesen Artikel die folgende rohe Wahrscheinlichkeitsverteilung aus: [0.45, 0.2,
0.02, 0.33]. Der Kreuzentropieverlust dieses Modells ist in diesem Beispiel die Kreuzentropie
Entropie von [0,45, 0,2, 0,02, 0,33] relativ zu [0, 0, 0, 1]. In Python können Sie die
Kreuzentropie mit dem folgenden Code berechnen:
40 | Kapitel 2: Einführung in den Entwurf von Systemen für maschinelles Lernen
13 Joe Kukura, "Facebook Employee Raises Powered by 'Really Dangerous' Algorithm That Favors Angry Posts,"
SFist, 24. September 2019, https://oreil.ly/PXtGi; Kevin Roose, "The Making of a YouTube Radical," New York
Times, 8. Juni 2019, https://oreil.ly/KYqzF.

import numpy as np
def cross_entropy(p, q):
return -sum([p[i] * np.log(q[i]) for i in range(len(p))])
p = [0, 0, 0, 1]
q = [0.45, 0.2, 0.02, 0.33]
cross_entropy(p, q)
Die Wahl einer Zielfunktion ist in der Regel einfach, aber nicht, weil Zielfunktionen
Zielfunktionen einfach sind. Um sinnvolle Zielfunktionen zu finden, sind
Algebra-Kenntnisse, daher verwenden die meisten ML-Ingenieure einfach gängige Verlustfunktionen wie RMSE
oder MAE (mittlerer absoluter Fehler) für die Regression, logistischer Verlust (auch Log Loss) für die binäre
Klassifikation und Kreuzentropie für die Multiklassenklassifikation.
Ziele entkoppeln
Die Formulierung von ML-Problemen kann schwierig sein, wenn Sie mehrere Zielfunktionen minimieren
Funktionen minimieren will. Stellen Sie sich vor, Sie entwickeln ein System, um Artikel in den Newsfeeds der Benutzer zu bewerten. Ihr
ursprüngliche Ziel ist es, das Engagement der Nutzer zu maximieren. Dieses Ziel wollen Sie erreichen durch
die folgenden drei Ziele erreichen:
-Filterung von Spam
Ausfiltern von NSFW-Inhalten
-Einstufung von Beiträgen nach Engagement: Wie wahrscheinlich ist es, dass Nutzer darauf klicken?
Sie haben jedoch schnell gelernt, dass die Optimierung für das Engagement der Nutzer allein zu
zu fragwürdigen ethischen Bedenken führen kann. Denn extreme Beiträge erhalten in der Regel mehr Engagement,
hat Ihr Algorithmus gelernt, extreme Inhalte zu bevorzugen.^13 Sie möchten einen
gesünderen Newsfeed erstellen. Sie haben also ein neues Ziel: Maximieren Sie das Engagement der Nutzer und
und gleichzeitig die Verbreitung von extremen Ansichten und Fehlinformationen zu minimieren. Um dieses Ziel zu erreichen, müssen Sie
fügen Sie zwei neue Ziele zu Ihrem ursprünglichen Plan hinzu:
-Filterung von Spam
-Filtern von NSFW-Inhalten
-Fehlinformationen herausfiltern
-Beiträge nach Qualität einstufen
-Beiträge nach Engagement einstufen: wie wahrscheinlich ist es, dass Nutzer darauf klicken
ML-Probleme einrahmen | 41
14 Der Einfachheit halber nehmen wir an, wir wüssten, wie man die Qualität eines Beitrags messen kann.
15 Wikipedia, s.v. "Pareto-Optimierung", https://oreil.ly/NdApy. Wenn Sie schon dabei sind, sollten Sie vielleicht auch lesen
Jin und Sendhoffs großartiges Papier über die Anwendung der Pareto-Optimierung für ML, in dem die Autoren behaupten, dass
"Maschinelles Lernen ist von Natur aus eine multikriterielle Aufgabe" (Yaochu Jin und Bernhard Sendhoff, "Pareto-Based
Multiobjective Machine Learning: An Overview and Case Studies", IEEE Transactions on Systems, Man, and
Cybernetics-Part C: Applications and Reviews 38, no. 3 [Mai 2008], https://oreil.ly/f1aKk).

Nun stehen zwei Ziele im Widerspruch zueinander. Wenn ein Beitrag ansprechend ist, aber von
fragwürdiger Qualität ist, sollte dieser Beitrag dann hoch oder niedrig eingestuft werden?
Ein Ziel wird durch eine Zielfunktion dargestellt. Um Beiträge nach Qualität einzustufen, müssen Sie
müssen Sie zunächst die Qualität der Beiträge vorhersagen, und Sie möchten, dass die vorhergesagte Qualität so nah wie möglich
der tatsächlichen Qualität so nahe wie möglich kommt. Im Wesentlichen geht es darum, den quality_loss zu minimieren: die
Differenz zwischen der vorhergesagten Qualität eines Beitrags und seiner tatsächlichen Qualität.^14
Um Beiträge nach Engagement zu ordnen, müssen Sie zunächst die Anzahl der Klicks vorhersagen
die jeder Beitrag erhalten wird. Sie möchten den engagement_loss minimieren: die Differenz zwischen
den vorhergesagten Klicks jedes Beitrags und der tatsächlichen Anzahl der Klicks.
Ein Ansatz besteht darin, diese beiden Verluste zu einem Verlust zu kombinieren und ein Modell zu trainieren, um
diesen Verlust zu minimieren:
Verlust = ɑ quality_loss + β engagement_loss
Sie können nach dem Zufallsprinzip verschiedene Werte für α und β ausprobieren, um die Werte zu finden, die
am besten funktionieren. Wenn Sie diese Werte systematischer einstellen wollen, können Sie sich informieren über
Pareto-Optimierung, "ein Bereich der Entscheidungsfindung nach mehreren Kriterien, der sich mit
der sich mit mathematischen Optimierungsproblemen beschäftigt, bei denen mehr als eine Zielfunktion
tion, die gleichzeitig optimiert werden muss."^15
Ein Problem bei diesem Ansatz ist, dass Sie jedes Mal, wenn Sie α und β einstellen - zum Beispiel, wenn die
Wenn beispielsweise die Qualität der Newsfeeds Ihrer Nutzer steigt, aber das Engagement der Nutzer sinkt, könnten Sie
α verringern und β erhöhen wollen, müssen Sie Ihr Modell neu trainieren.
Ein anderer Ansatz besteht darin, zwei verschiedene Modelle zu trainieren, die jeweils einen Verlust optimieren. Sie haben also
haben Sie zwei Modelle:
quality_model
Minimiert quality_loss und gibt die vorhergesagte Qualität jedes Beitrags aus
engagement_model
Minimiert engagement_loss und gibt die vorhergesagte Anzahl der Klicks für jeden Beitrag aus
Beitrag
42 | Kapitel 2: Einführung in den Entwurf von Systemen für maschinelles Lernen
16 Anand Rajaraman, "More Data Usually Beats Better Algorithms", Datawocky, 24. März 2008,
https://oreil.ly/wNwhV.
17 Rich Sutton, "Die bittere Lektion", 13. März 2019, https://oreil.ly/RhOp9.
18 Tweet von Dr. Judea Pearl (@yudapearl), 27. September 2020, https://oreil.ly/wFbHb.

Sie können die Ergebnisse der Modelle kombinieren und die Beiträge nach ihren kombinierten Werten einstufen:
ɑ quality_score + β engagement_score
Jetzt können Sie α und β verändern, ohne Ihre Modelle neu zu trainieren!
Wenn es mehrere Ziele gibt, ist es im Allgemeinen eine gute Idee, diese zunächst zu entkoppeln
zu entkoppeln, da dies die Modellentwicklung und -pflege erleichtert. Erstens ist es einfacher, das
Erstens ist es einfacher, Ihr System zu optimieren, ohne die Modelle neu zu trainieren, wie bereits erläutert. Zweitens ist es
ist es einfacher für die Wartung, da unterschiedliche Ziele unterschiedliche Wartungspläne erfordern können.
Zeitpläne benötigen. Die Spamming-Techniken entwickeln sich viel schneller weiter als die Art und Weise, wie die Qualität der Beiträge wahrgenommen wird.
wahrgenommen wird, so dass Spam-Filtersysteme sehr viel häufiger aktualisiert werden müssen als
Qualitätsbewertungssysteme.
Geist versus Daten
Die Fortschritte des letzten Jahrzehnts zeigen, dass der Erfolg eines ML-Systems weitgehend von den Daten abhängt
von den Daten abhängt, auf denen es trainiert wurde. Anstatt sich auf die Verbesserung von ML-Algorithmen zu konzentrieren, konzentrieren sich die meisten
Unternehmen auf die Verwaltung und Verbesserung ihrer Daten.^16
Trotz des Erfolgs von Modellen, die große Datenmengen verwenden, sind viele skeptisch gegenüber
die Betonung von Daten als Weg in die Zukunft. In den letzten fünf Jahren waren auf jeder akademischen
Konferenz, an der ich teilnahm, gab es immer wieder öffentliche Debatten über die Macht des Geistes
gegenüber Daten. Der Verstand könnte sich als induktive Vorurteile oder intelligente architektonische
Entwürfe. Daten können mit Berechnungen in einen Topf geworfen werden, da mehr Daten in der Regel
mehr Daten mehr Berechnungen erfordern.
Theoretisch können Sie sowohl architektonische Entwürfe verfolgen als auch große Daten und Berechnungen nutzen.
Daten und Berechnungen nutzen, aber wenn man Zeit in das eine investiert, nimmt man oft Zeit für das andere weg.^17
Zu den Befürwortern von Mind-over-Data gehört Dr. Judea Pearl, ein Turing-Preisträger, der
bekannt für seine Arbeit über kausale Schlussfolgerungen und Bayes'sche Netzwerke. Die Einleitung
zu seinem Buch The Book of Why trägt den Titel "Mind over Data", in dem er betont:
"Daten sind zutiefst dumm". In einem seiner kontroverseren Beiträge auf Twitter im Jahr 2020,
äußerte er seine starke Meinung gegen ML-Ansätze, die sich stark auf Daten stützen, und
warnte, dass datenzentrierte ML-Leute in drei bis fünf Jahren arbeitslos sein könnten: "ML
wird in 3 bis 5 Jahren nicht mehr dasselbe sein, und ML-Leute, die weiterhin dem aktuellen
datenzentrierten Paradigma folgen, werden sich als veraltet, wenn nicht sogar arbeitslos betrachten. Merken Sie sich das."^18
Geist gegen Daten | 43
19 "Deep Learning and Innate Priors" (Chris Manning versus Yann LeCun debate), February 2, 2018, video,
1:02:55, https://oreil.ly/b3hb1.
20 Sutton, "The Bitter Lesson".
21 Alon Halevy, Peter Norvig, und Fernando Pereira, "The Unreasonable Effectiveness of Data", IEEE Computer
Society, März/April 2009, https://oreil.ly/WkN6p.

Eine mildere Meinung vertritt Professor Christopher Manning, Direktor des
Stanford Artificial Intelligence Laboratory, der argumentiert, dass enorme Rechenleistung und
eine riesige Datenmenge mit einem einfachen Lernalgorithmus zu unglaublich schlechten
Lernende. Die Struktur ermöglicht es uns, Systeme zu entwickeln, die aus weniger Daten mehr lernen können.
Daten lernen können.^19
In der ML-Branche gibt es heute viele Vertreter der "Data-over-Mind"-Philosophie. Professor Richard Sutton,
Professor für Computerwissenschaften an der University of Alberta und angesehener
Forschungswissenschaftler bei DeepMind, schrieb einen großartigen Blogbeitrag, in dem er behauptete, dass
dass Forscher, die sich für intelligentes Design statt für Methoden zur Nutzung von Kom
eine bittere Lektion lernen werden: "Die größte Lektion, die man aus
aus 70 Jahren KI-Forschung ist, dass allgemeine Methoden, die Berechnungen nutzen
letztlich die effektivsten sind, und zwar mit großem Abstand.... Auf der Suche nach einer Verbesserung, die
Verbesserung, die kurzfristig einen Unterschied macht, versuchen die Forscher, ihr menschliches
Wissen über den Bereich zu nutzen, aber das Einzige, was auf lange Sicht zählt, ist die
Nutzung von Berechnungen."^20
Auf die Frage, warum die Google-Suche so gut abschneidet, betonte Peter Norvig, Direktor für Suchqualität bei Google
Peter Norvig, Direktor für Suchqualität bei Google, betonte die Bedeutung einer großen Datenmenge gegenüber
intelligenten Algorithmen für ihren Erfolg: "Wir haben keine besseren Algorithmen. Wir haben einfach
mehr Daten."^21
Dr. Monica Rogati, ehemalige Vizepräsidentin für Daten bei Jawbone, argumentierte, dass Daten die
Grundlage der Datenwissenschaft, wie in Abbildung 2-7 dargestellt. Wenn Sie Data Science nutzen wollen, eine
Disziplin, zu der auch ML gehört, nutzen möchten, um Ihre Produkte oder Prozesse zu verbessern, müssen Sie
müssen Sie mit dem Aufbau Ihrer Daten beginnen, sowohl in Bezug auf die Qualität als auch auf die Quantität. Ohne
Daten gibt es keine Datenwissenschaft.
Die Debatte dreht sich nicht darum, ob endliche Daten notwendig sind, sondern ob sie ausreichen. Der
Der Begriff "endlich" ist hier wichtig, denn wenn wir unendlich viele Daten hätten, könnten wir vielleicht
wir die Antwort nachschlagen. Viele Daten zu haben ist etwas anderes als unendlich viele Daten zu haben.
44 | Kapitel 2: Einführung in den Entwurf von Systemen für maschinelles Lernen
22 Monica Rogati, "The AI Hierarchy of Needs", Hackernoon Newsletter, 12. Juni 2017, https://oreil.ly/3nxJ8.
23 Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, und Tony Robinson,
"One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling", arXiv, December
11, 2013, https://oreil.ly/1AdO6.

Abbildung 2-7. Die Bedürfnishierarchie der Datenwissenschaft. Quelle: Angepasst an ein Bild von
Monica Rogati^22
Unabhängig davon, welches Lager sich letztendlich als richtig erweisen wird, kann niemand leugnen, dass
Daten unverzichtbar sind, vorerst. Sowohl die Forschung als auch die Branchentrends der letzten Jahrzehnte
zeigen, dass der Erfolg von ML mehr und mehr von der Qualität und Quantität der Daten abhängt.
Die Modelle werden immer größer und nutzen mehr Daten. Im Jahr 2013 waren die Leute begeistert
aufgeregt, als der "One Billion Word Benchmark for Language Modeling" veröffentlicht wurde,
der 0,8 Milliarden Token enthält.^23 Sechs Jahre später verwendete OpenAIs GPT-2 einen Datensatz von
10 Milliarden Token. Und ein weiteres Jahr später verwendete GPT-3 500 Milliarden Token. Die Wachstums
Wachstumsrate der Datensatzgrößen ist in Abbildung 2-8 dargestellt.
Verstand versus Daten | 45
Abbildung 2-8. Größe der für Sprachmodelle verwendeten Datensätze (logarithmische Skala) im Laufe der Zeit

Auch wenn ein Großteil der Fortschritte beim Deep Learning in den letzten zehn Jahren
durch eine immer größere Datenmenge vorangetrieben wurden, führen mehr Daten nicht immer zu einer besseren
Leistung für Ihr Modell. Mehr Daten von geringerer Qualität, z. B. Daten, die veraltet sind
oder Daten mit falschen Bezeichnungen, können die Leistung Ihres Modells sogar beeinträchtigen.

Zusammenfassung
Ich hoffe, dass dieses Kapitel Ihnen eine Einführung in das Design von ML-Systemen gegeben hat und die
Überlegungen, die wir beim Entwurf eines ML-Systems berücksichtigen müssen.

Jedes Projekt muss mit der Frage beginnen, warum dieses Projekt durchgeführt werden muss, und ML-Projekte sind
keine Ausnahme. Wir haben das Kapitel mit der Annahme begonnen, dass die meisten Unternehmen sich nicht
ML-Metriken nur dann interessieren, wenn sie die Geschäftsmetriken beeinflussen können. Wenn also ein ML
System für ein Unternehmen gebaut wird, muss es durch Geschäftsziele motiviert sein, die
die in ML-Ziele übersetzt werden müssen, um die Entwicklung von ML-Modellen zu steuern.

46 | Kapitel 2: Einführung in den Entwurf maschineller Lernsysteme

24 Alex Krizhevsky, Ilya Sutskever, und Geoffrey E Hinton, "ImageNet Classification with Deep Convolutional
Neural Networks," in Advances in Neural Information Processing Systems, vol. 25, ed. F. Pereira, C.J. Burges,
L. Bottou, and K.Q. Weinberger (Curran Associates, 2012), https://oreil.ly/MFYp9; Jacob Devlin, Ming-Wei
Chang, Kenton Lee, und Kristina Toutanova, "BERT: Pre-training of Deep Bidirectional Transformers for
Language Understanding," arXiv, 2019, https://oreil.ly/TN8fN; "Better Language Models and Their Implica-
tions," OpenAI blog, February 14, 2019, https://oreil.ly/SGV7g.

Bevor wir ein ML-System aufbauen, müssen wir die Anforderungen verstehen, die das
System erfüllen muss, um als gutes System zu gelten. Die genauen Anforderungen variieren
von Anwendungsfall zu Anwendungsfall, und in diesem Kapitel konzentrieren wir uns auf die vier allgemeinsten
Anforderungen: Zuverlässigkeit, Skalierbarkeit, Wartbarkeit und Anpassungsfähigkeit. Techniken zur
jeder dieser Anforderungen werden im Laufe des Buches behandelt.
Der Aufbau eines ML-Systems ist keine einmalige Aufgabe, sondern ein iterativer Prozess. In diesem Kapitel,
haben wir den iterativen Prozess zur Entwicklung eines ML-Systems besprochen, das die oben genannten
Anforderungen erfüllt.
Wir haben das Kapitel mit einer philosophischen Diskussion über die Rolle von Daten in ML-Systemen beendet.
Es gibt immer noch viele Menschen, die glauben, dass intelligente Algorithmen letztendlich
schließlich eine große Menge an Daten übertrumpfen. Doch der Erfolg von Systemen wie
AlexNet, BERT und GPT hat jedoch gezeigt, dass der Fortschritt von ML in den letzten zehn Jahren auf dem
Zugang zu einer großen Datenmenge abhängt.^24 Unabhängig davon, ob Daten die
Daten das intelligente Design überwältigen können, kann niemand die Bedeutung von Daten für ML leugnen. Ein nicht trivialer Teil
dieses Buches wird der Beantwortung verschiedener Datenfragen gewidmet sein.
Komplexe ML-Systeme setzen sich aus einfacheren Bausteinen zusammen. Nachdem wir nun einen Überblick über
Überblick über ein ML-System in Produktion erhalten haben, werden wir in den folgenden Kapiteln die
Bausteine in den folgenden Kapiteln, beginnend mit den Grundlagen des Data
Grundlagen des Data Engineering im nächsten Kapitel. Wenn Ihnen eine der in diesem Kapitel genannten Herausforderungen
abstrakt erscheinen, hoffe ich, dass die spezifischen Beispiele in den folgenden Kapiteln
sie konkreter machen.
Zusammenfassung | 47
KAPITEL 3

Grundlagen der Datentechnik
Der Aufstieg von ML in den letzten Jahren ist eng mit dem Aufstieg von Big Data verbunden. Große
Datensysteme, auch ohne ML, sind komplex. Wenn Sie nicht jahrelang damit gearbeitet haben
mit ihnen gearbeitet hat, kann man sich leicht in Akronymen verlieren. Es gibt viele Herausforderungen und
mögliche Lösungen, die diese Systeme mit sich bringen. Industriestandards, wenn es denn welche gibt,
entwickeln sich schnell weiter, wenn neue Tools auf den Markt kommen und sich die Anforderungen der Branche erweitern, wodurch ein
dynamische und sich ständig verändernde Umgebung. Wenn Sie sich den Datenstapel für verschiedene
Technologieunternehmen betrachtet, könnte man meinen, dass jedes sein eigenes Ding macht.

In diesem Kapitel werden wir die Grundlagen der Datentechnik behandeln, die Ihnen hoffentlich
einen festen Stand haben, wenn Sie die Landschaft für Ihre eigenen Bedürfnisse erkunden.
Wir beginnen mit verschiedenen Datenquellen, mit denen Sie in einem typischen ML
Projekt arbeiten. Anschließend werden wir die Formate besprechen, in denen die Daten gespeichert werden können. Das Speichern von
Daten ist nur dann interessant, wenn Sie beabsichtigen, diese Daten später abzurufen. Zum Abrufen gespeicherter
Daten abzurufen, muss man nicht nur wissen, wie sie formatiert sind, sondern auch, wie sie strukturiert sind.
Datenmodelle definieren, wie die in einem bestimmten Datenformat gespeicherten Daten strukturiert sind.

Während Datenmodelle die Daten in der realen Welt beschreiben, legen Datenbanken fest, wie die Daten
auf Maschinen gespeichert werden sollen. Wir werden weiter über Datenspeicher-Engines sprechen,
die auch als Datenbanken bezeichnet werden, für die beiden Hauptverarbeitungsarten: transaktionale und
analytisch.

Wenn Sie mit Daten in der Produktion arbeiten, arbeiten Sie in der Regel mit Daten über mehrere
Prozesse und Dienste. Sie könnten zum Beispiel einen Feature-Engineering-Dienst haben
der Merkmale aus Rohdaten berechnet, und einen Vorhersagedienst, der Vorhersagen
Vorhersagen auf der Grundlage der berechneten Merkmale. Dies bedeutet, dass Sie die berechneten Merkmale
Merkmale vom Feature-Engineering-Dienst an den Vorhersagedienst übergeben. Im folgenden
Abschnitt des Kapitels werden wir verschiedene Arten der prozessübergreifenden Datenübergabe diskutieren.

49
Bei der Erörterung der verschiedenen Arten der Datenübermittlung lernen wir zwei
verschiedene Arten von Daten: historische Daten in Datenspeicher-Engines und Streaming-Daten in
Echtzeit-Transporte. Diese beiden unterschiedlichen Datentypen erfordern unterschiedliche Verarbeitungs
Paradigmen, die im Abschnitt "Batch-Verarbeitung versus Stream-Verarbeitung" auf Seite 78 besprochen werden.
Verarbeitung" auf Seite 78 besprochen werden.

Wer ML-Systeme in der Produktion aufbauen will, muss wissen, wie man eine immer größere Menge an Daten erfasst, verarbeitet, speichert, abruft und verarbeitet.
Datenmengen zu erfassen, zu speichern, abzurufen und zu verarbeiten, ist für Personen, die ML-Systeme in der Produktion aufbauen wollen, unerlässlich.
Wenn Sie bereits mit Datensystemen vertraut sind, sollten Sie direkt zu
Kapitel 4 übergehen, um mehr darüber zu erfahren, wie man Stichproben nimmt und Labels erzeugt, um Trainingsdaten
Daten. Wenn Sie mehr über Data Engineering aus einer Systemperspektive erfahren möchten,
empfehle ich das ausgezeichnete Buch Designing Data-Intensive Applica- tions von Martin Kleppmann
tions (O'Reilly, 2017).

Datenquellen
Ein ML-System kann mit Daten aus vielen verschiedenen Quellen arbeiten. Sie haben unterschiedliche
Eigenschaften, können für unterschiedliche Zwecke verwendet werden und erfordern unterschiedliche
Methoden. Wenn Sie die Quellen Ihrer Daten verstehen, können Sie Ihre Daten
Daten effizienter zu nutzen. Dieser Abschnitt soll all jenen, die mit Produktionsdaten nicht vertraut sind, einen schnellen Überblick über die verschiedenen Datenquellen geben.
Datenquellen geben, wenn Sie mit Daten in der Produktion noch nicht vertraut sind. Wenn Sie bereits eine Weile mit
ML in der Produktion gearbeitet haben, können Sie diesen Abschnitt getrost auslassen.

Eine Quelle sind Nutzereingaben, d. h. Daten, die ausdrücklich von den Nutzern eingegeben werden. Benutzereingaben können sein
Text, Bilder, Videos, hochgeladene Dateien, usw. Wenn es auch nur im Entferntesten möglich ist, dass die Benutzer
falsche Daten einzugeben, werden sie dies auch tun. Infolgedessen können Benutzereingabedaten leicht
falsch formatiert werden. Text kann zu lang oder zu kurz sein. Wo numerische Werte erwartet werden
erwartet werden, könnten die Benutzer versehentlich Text eingeben. Wenn Sie die Benutzer Dateien hochladen lassen, könnten sie
Dateien in den falschen Formaten hochladen. Benutzereingabedaten erfordern eine aufwändigere Prüfung
und Verarbeitung.

Hinzu kommt, dass die Benutzer wenig Geduld haben. Wenn wir Daten eingeben, erwarten wir in den meisten Fällen
erwarten wir, dass wir die Ergebnisse sofort zurückbekommen. Daher erfordern vom Benutzer eingegebene Daten in der Regel eine schnelle
Verarbeitung.

Eine weitere Quelle sind systemgenerierte Daten. Dies sind die Daten, die von verschiedenen
Komponenten Ihrer Systeme erzeugt werden, darunter verschiedene Arten von Protokollen und Systemausgaben
wie z. B. Modellvorhersagen.

50 | Kapitel 3: Grundlagen der Datentechnik

1 "Interessant" bedeutet in der Produktion in der Regel katastrophal, z. B. ein Absturz oder wenn Ihre Cloud-Rechnung einen
astronomischen Betrag erreicht.
2 Ab November 2021 kostet AWS S3 Standard, die Speicheroption, mit der Sie auf Ihre Daten mit einer
Latenzzeit von Millisekunden auf Ihre Daten zugreifen können, kostet etwa fünfmal mehr pro GB als S3 Glacier, die Speicheroption, mit der Sie
Daten mit einer Latenzzeit von 1 Minute bis 12 Stunden abrufen können.
3 Ein ML-Ingenieur sagte mir einmal, dass sein Team nur die historischen Produktbesuche und -käufe der Nutzer
um Empfehlungen zu geben, was sie als nächstes sehen möchten. Ich antwortete ihm: "Sie verwenden also keine
überhaupt keine persönlichen Daten?" Er schaute mich verwirrt an. "Wenn Sie demografische Daten wie das Alter oder den Standort der Nutzer meinen,
dann nein, das tun wir nicht. Aber ich würde sagen, dass die Surf- und Kaufaktivitäten einer Person sehr persönlich sind."
Protokolle können den Zustand und wichtige Ereignisse des Systems aufzeichnen, z. B. die Speichernutzung,
Anzahl der Instanzen, aufgerufene Dienste, verwendete Pakete usw. Sie können die Ergebnisse
Sie können die Ergebnisse verschiedener Aufträge aufzeichnen, einschließlich großer Stapelverarbeitungsaufträge für die Datenverarbeitung und das Modelltraining.
Diese Arten von Protokollen geben Aufschluss über die Leistung des Systems. Der Hauptzweck
Diese Transparenz dient vor allem der Fehlersuche und der möglichen Verbesserung der Anwendung. Meistens
müssen Sie sich diese Arten von Protokollen nicht ansehen, aber sie sind wichtig, wenn
etwas in Flammen steht.

Da die Protokolle vom System generiert werden, ist es viel unwahrscheinlicher, dass sie falsch formatiert werden
wie es bei Benutzereingabedaten der Fall ist. Insgesamt müssen Protokolle nicht sofort nach ihrem Eintreffen verarbeitet werden
verarbeitet werden, wie es bei Benutzereingabedaten der Fall wäre. Für viele Anwendungsfälle ist es
ist es akzeptabel, Protokolle in regelmäßigen Abständen zu verarbeiten, z. B. stündlich oder sogar täglich. Allerdings sollten Sie
möchten Sie Ihre Protokolle vielleicht trotzdem schnell verarbeiten, um zu erkennen und benachrichtigt zu werden, wenn
etwas Interessantes passiert.^1

Da das Debuggen von ML-Systemen schwierig ist, ist es gängige Praxis, alles zu protokollieren, was man
können. Das bedeutet, dass das Volumen der Protokolle sehr, sehr schnell wachsen kann. Dies führt zu
zwei Probleme. Das erste ist, dass es schwierig sein kann, zu wissen, wo man suchen muss, weil Signale
im Rauschen verloren gehen. Es gibt viele Dienste, die Protokolle verarbeiten und analysieren,
wie Logstash, Datadog, Logz.io, usw. Viele von ihnen verwenden ML-Modelle, um Ihnen zu helfen
um Ihnen zu helfen, Ihre riesige Anzahl von Protokollen zu verarbeiten und zu verstehen.

Das zweite Problem ist die Speicherung einer schnell wachsenden Zahl von Protokollen. Glücklicherweise muss man in
Glücklicherweise müssen Sie die Protokolle in den meisten Fällen nur so lange speichern, wie sie nützlich sind, und können sie verwerfen
Sie können sie löschen, wenn sie für die Fehlersuche in Ihrem aktuellen System nicht mehr relevant sind. Wenn Sie
Wenn Sie nicht häufig auf Ihre Protokolle zugreifen müssen, können Sie sie auch in einem Speicher mit geringem Zugriff speichern
gespeichert werden, der viel weniger kostet als ein Speicher mit häufigem Zugriff.^2

Das System generiert auch Daten, um das Verhalten der Nutzer aufzuzeichnen, wie z. B. Klicken, Auswählen eines
scrollen, zoomen, ein Pop-up ignorieren oder ungewöhnlich viel Zeit auf bestimmten
Zeit auf bestimmten Seiten verbringen. Auch wenn es sich dabei um vom System generierte Daten handelt, werden sie als
Teil der Nutzerdaten und unterliegen möglicherweise den Datenschutzbestimmungen.^3

Datenquellen | 51
4 John Koetsier, "Apple Just Crippled IDFA, Sending an $80 Billion Industry Into Upheaval", Forbes, 24. Juni,
2020, https://oreil.ly/rqPX9.
5 Patrick McGee und Yuan Yang, "TikTok Wants to Keep Tracking iPhone Users with State-Backed Work-
around," Ars Technica, 16. März 2021, https://oreil.ly/54pkg.
Es gibt auch interne Datenbanken, die von verschiedenen Diensten und Unternehmensapplikationen
Unternehmen generiert werden. Diese Datenbanken verwalten ihre Werte wie Inventar, Kunden
Kundenbeziehungen, Benutzer und vieles mehr. Diese Art von Daten kann von ML-Modellen
direkt oder von verschiedenen Komponenten eines ML-Systems genutzt werden. Wenn Benutzer zum Beispiel eine
eine Suchanfrage bei Amazon eingeben, verarbeiten ein oder mehrere ML-Modelle diese Anfrage, um die
um die Absicht zu erkennen - wenn jemand "frozen" eingibt, sucht er nach Tiefkühlkost oder
dann muss Amazon seine internen Datenbanken auf die Verfügbarkeit dieser Produkte überprüfen
dann muss Amazon seine internen Datenbanken auf die Verfügbarkeit dieser Produkte überprüfen, bevor es sie einordnet und den Nutzern anzeigt.

Und dann gibt es noch die wunderbar seltsame Welt der Daten Dritter. Erstanbieterdaten sind die
Daten, die Ihr Unternehmen bereits über Ihre Nutzer oder Kunden sammelt. Daten von Drittanbietern
Daten sind die Daten, die ein anderes Unternehmen über seine eigenen Kunden gesammelt hat und die es
Ihnen zur Verfügung stellen, obwohl Sie wahrscheinlich dafür bezahlen müssen. Daten von Drittanbietern
Unternehmen sammeln Daten über die Öffentlichkeit, die nicht ihre direkten Kunden sind.

Mit dem Aufkommen des Internets und der Smartphones ist es viel einfacher geworden, alle Arten von
Daten zu sammeln. Bei Smartphones war es besonders einfach, da jedes Telefon
eine eindeutige Werbe-ID hatte - iPhones mit Apples Kennung für Werbetreibende
(IDFA) und Android-Telefone mit ihrer Android-Werbe-ID (AAID) -, die
als eindeutige ID fungierte, um alle Aktivitäten auf einem Telefon zusammenzufassen. Daten von Apps, Websites,
Check-in-Diensten usw. werden gesammelt und (hoffentlich) anonymisiert, um einen Aktivitätsverlauf
Aktivitätsverlauf für jede Person zu erstellen.

Daten aller Art können gekauft werden, z. B. Aktivitäten in sozialen Medien, Kaufhistorie, Internet
Surfgewohnheiten, Autovermietungen und politische Neigungen für verschiedene demografische Gruppen
bis hin zu Männern im Alter von 25 bis 34 Jahren, die in der Technologiebranche arbeiten und in der Bay Area leben. Aus
diesen Daten können Sie Informationen ableiten, wie zum Beispiel, dass Menschen, die Marke A mögen, auch Marke
B. Diese Daten können besonders für Systeme wie Empfehlungssysteme hilfreich sein, um
Ergebnisse zu generieren, die den Interessen der Nutzer entsprechen. Daten von Drittanbietern werden in der Regel verkauft, nachdem sie
bereinigt und von Anbietern verarbeitet wurden.

Da die Nutzer jedoch mehr Datenschutz fordern, haben die Unternehmen Schritte unternommen
um die Verwendung von Werbe-IDs einzuschränken. Anfang 2021 hat Apple seine IDFA auf Opt-in umgestellt.
Diese Änderung hat die Menge der auf iPhones verfügbaren Daten Dritter erheblich reduziert.
iPhones zur Verfügung stehen, und zwingt viele Unternehmen, sich mehr auf Daten von Erstanbietern zu konzentrieren.^4 Um gegen diese Veränderung anzukämpfen
gegen diese Änderung zu kämpfen, haben Werbetreibende in Umgehungslösungen investiert. Zum Beispiel hat die China
Advertising Association, ein staatlich unterstützter Handelsverband für Chinas Werbeindustrie
in ein System zur Erfassung von Gerätefingern namens CAID investiert, das es Apps
wie TikTok und Tencent, weiterhin iPhone-Nutzer zu verfolgen.^5

52 | Kapitel 3: Grundlagen der Datentechnik

6 "Zugriffsmuster" bezeichnet das Muster, nach dem ein System oder Programm Daten liest oder schreibt.
Datenformate
Wenn Sie einmal Daten haben, möchten Sie sie vielleicht speichern (oder "aufbewahren", wie es in der Fachsprache heißt).
Da Ihre Daten aus verschiedenen Quellen mit unterschiedlichen Zugriffsmustern stammen,^6 ist die Speicherung
Ihrer Daten nicht immer einfach und kann in manchen Fällen auch kostspielig sein. Es ist wichtig
Es ist wichtig, sich Gedanken darüber zu machen, wie die Daten in Zukunft verwendet werden sollen, damit das von Ihnen verwendete Format
sinnvoll ist. Hier sind einige der Fragen, die Sie sich stellen sollten:

-Wie speichere ich multimodale Daten, z. B. eine Probe, die sowohl Bilder
und Texte enthält?
-Wo speichere ich meine Daten so, dass sie kostengünstig und dennoch schnell abrufbar sind?
-Wie speichere ich komplexe Modelle, so dass sie auf unterschiedlicher Hardware korrekt geladen und ausgeführt werden können?
unterschiedlicher Hardware laufen?
Der Prozess der Konvertierung einer Datenstruktur oder eines Objektzustands in ein Format, das
gespeichert oder übertragen und später wieder rekonstruiert werden kann, ist die Serialisierung von Daten. Es gibt viele,
viele Datenserialisierungsformate. Wenn Sie sich für ein Format entscheiden, mit dem Sie arbeiten möchten, sollten Sie
Sie verschiedene Eigenschaften wie die menschliche Lesbarkeit, Zugriffsmuster und
Zugriffsmuster und ob es auf Text oder Binärdaten basiert, was sich auf die Größe der Dateien auswirkt.
Tabelle 3-1 enthält nur einige der gebräuchlichen Formate, die Ihnen bei Ihrer Arbeit begegnen könnten
Ihrer Arbeit begegnen. Eine ausführlichere Liste finden Sie auf der wunderbaren Wikipedia-Seite
"Vergleich von Daten-Serialisierungsformaten".

Tabelle 3-1. Gängige Datenformate und deren Verwendung

Format Binär/Text Menschenlesbar Anwendungsbeispiele
JSON Text Ja Überall
CSV Text Ja Überall
Parquet Binär Nein Hadoop, Amazon Redshift
Avro Binär primär Nein Hadoop
Protobuf Binär primär Nein Google, TensorFlow (TFRecord)
Pickle Binär Nein Python, PyTorch Serialisierung
Wir werden einige dieser Formate durchgehen, beginnend mit JSON. Wir gehen auch auf die beiden
Formate, die weit verbreitet sind und zwei verschiedene Paradigmen darstellen: CSV und Parquet.

Datenformate | 53
JSON
JSON, JavaScript Object Notation, ist allgegenwärtig. Auch wenn sie von JavaScript abgeleitet wurde, ist sie
JavaScript abgeleitet wurde, ist es sprachunabhängig - die meisten modernen Programmiersprachen können
JSON generieren und parsen. Es ist für Menschen lesbar. Sein Schlüssel-Wert-Paar-Paradigma ist einfach
aber leistungsfähig und in der Lage, Daten unterschiedlicher Strukturierungsgrade zu verarbeiten. Für
Beispiel: Ihre Daten können in einem strukturierten Format wie dem folgenden gespeichert werden:

{
"Vorname" : "Boatie",
"Nachname" : "McBoatFace",
"isVibing" : true ,
"Alter" : 12,
"adresse" : {
"streetAddress" : "12 Ocean Drive",
"city" : "Port Royal",
"postalCode" : "10021-3100"
}
}
Die gleichen Daten können auch in einem unstrukturierten Textblob wie dem folgenden gespeichert werden:

{
"text" : "Boatie McBoatFace, 12 Jahre alt, wohnt am 12 Ocean Drive, Port Royal,
10021-3100"
}
Da JSON allgegenwärtig ist, ist auch der Schmerz, den es verursacht, überall zu spüren. Sobald
Sie die Daten in Ihren JSON-Dateien in ein Schema übertragen haben, ist es ziemlich schmerzhaft
im Nachhinein das Schema zu ändern. JSON-Dateien sind Textdateien, was bedeutet
Das bedeutet, dass sie sehr viel Platz benötigen, wie wir im Abschnitt "Text- versus Binärformat" auf
Seite 57.

Zeilen-Hauptformat versus Spalten-Hauptformat
Die beiden gängigen Formate, die zwei unterschiedliche Paradigmen darstellen, sind CSV
und Parquet. CSV (comma-separated values) ist zeilenorientiert, was bedeutet, dass aufeinanderfolgende
Elemente in einer Zeile werden nebeneinander im Speicher abgelegt. Parquet ist spaltenorientiert,
was bedeutet, dass aufeinanderfolgende Elemente in einer Spalte nebeneinander gespeichert werden.

Da moderne Computer sequenzielle Daten effizienter verarbeiten als nicht-sequenzielle Daten, ist
Wenn eine Tabelle zeilenlastig ist, ist der Zugriff auf ihre Zeilen schneller als der Zugriff auf ihre
Spalten schneller als der erwartete Zugriff. Das bedeutet, dass bei zeilenmajorierten Formaten der Zugriff auf Daten über
zeilenweise schneller als spaltenweise sein dürfte.

54 | Kapitel 3: Grundlagen der Datentechnik

Stellen Sie sich vor, wir haben einen Datensatz mit 1.000 Beispielen, und jedes Beispiel hat 10 Merkmale. Wenn
wir jedes Beispiel als eine Zeile und jedes Merkmal als eine Spalte betrachten, wie es in der ML oft der Fall ist
in ML, dann sind die zeilengroßen Formate wie CSV besser für den Zugriff auf Beispiele geeignet, z. B.,
Zugriff auf alle heute gesammelten Beispiele. Spaltenbetonte Formate wie Parquet sind
besser für den Zugriff auf Merkmale geeignet, z. B. für den Zugriff auf die Zeitstempel aller Ihrer Beispiele. Siehe
Abbildung 3-1.

Abbildung 3-1. Zeilenmajor- und Spaltenmajor-Formate

Spalten-Hauptformate ermöglichen flexible spaltenbasierte Lesevorgänge, insbesondere wenn Ihre Daten
mit Tausenden, wenn nicht Millionen von Merkmalen. Nehmen wir an, Sie haben Daten über
Mitfahrgelegenheitstransaktionen mit 1.000 Merkmalen haben, Sie aber nur 4 Merkmale benötigen: Zeit,
Ort, Entfernung, Preis. Mit Spalten-Hauptformaten können Sie die vier Spalten
die diesen vier Merkmalen entsprechen, direkt lesen. Wenn Sie jedoch bei zeilenorientierten Formaten
Sie die Größe der Zeilen nicht kennen, müssen Sie alle Spalten einlesen und dann nach diesen vier Spalten filtern.
bis auf diese vier Spalten filtern. Selbst wenn Sie die Größe der Zeilen kennen, kann das
langsam sein, da Sie im Speicher herumspringen müssen und die Vorteile der Zwischenspeicherung nicht nutzen können.

Zeilen-Hauptformate ermöglichen ein schnelleres Schreiben von Daten. Stellen Sie sich die Situation vor, dass Sie
immer wieder neue Einzelbeispiele zu Ihren Daten hinzufügen müssen. Für jedes einzelne Beispiel wäre es
wäre es viel schneller, es in eine Datei zu schreiben, in der die Daten bereits in einem zeilenorientierten Format vorliegen.

Insgesamt sind zeilenbetonte Formate besser, wenn Sie viele Schreibvorgänge durchführen müssen, während
spaltenbetonte Formate besser sind, wenn Sie viele spaltenbasierte Lesevorgänge durchführen müssen.

Datenformate | 55
7 Weitere Pandas-Macken finden Sie in meinem GitHub-Repository Just pandas Things.
NumPy vs. Pandas
Ein subtiler Punkt, der von vielen Leuten nicht beachtet wird und der zu einem Missbrauch von
pandas führt, ist, dass diese Bibliothek um das Spaltenformat herum aufgebaut ist.
pandas basiert auf DataFrame, einem Konzept, das von R's Data Frame inspiriert ist, das
spaltenbetont ist. Ein DataFrame ist eine zweidimensionale Tabelle mit Zeilen und Spalten.
In NumPy kann die Hauptreihenfolge festgelegt werden. Wenn ein ndarray erstellt wird, ist es standardmäßig zeilen-
major, wenn Sie die Reihenfolge nicht angeben. Leute, die von NumPy zu Pandas kommen
zu Pandas kommen, neigen dazu, DataFrame so zu behandeln, wie sie ndarray behandeln würden, z.B. indem sie versuchen, auf Daten nach
Zeilen zuzugreifen, und finden DataFrame langsam.
Im linken Feld von Abbildung 3-2 können Sie sehen, dass der zeilenweise Zugriff auf einen DataFrame so
viel langsamer ist als der Zugriff auf denselben DataFrame über eine Spalte. Wenn Sie denselben
DataFrame in ein NumPy ndarray konvertiert, wird der Zugriff auf eine Zeile viel schneller, wie Sie im rechten Feld der Abbildung sehen können.
im rechten Feld der Abbildung sehen kann.^7
Abbildung 3-2. (Links) Die spaltenweise Iteration eines Pandas DataFrame dauert 0,07 Sekunden, aber
das Iterieren desselben DataFrame nach Zeilen dauert 2,41 Sekunden. (Rechts) Wenn Sie den
DataFrame in ein NumPy ndarray konvertiert, wird der Zugriff auf die Zeilen viel schneller.
56 | Kapitel 3: Grundlagen der Datentechnik

8 "Ankündigung des Amazon Redshift Data Lake Export: Share Data in Apache Parquet Format," Amazon AWS,
3. Dezember 2019, https://oreil.ly/ilDb6.
Ich verwende CSV als Beispiel für das Row-Major-Format, weil es
weil es beliebt ist und allgemein von jedem, mit dem ich gesprochen habe, erkannt wird
Technik. Einige der ersten Rezensenten dieses Buches wiesen jedoch darauf hin
darauf hin, dass sie CSV für ein schreckliches Datenformat halten. Es serialisiert
Nicht-Text-Zeichen schlecht serialisiert. Wenn Sie zum Beispiel Float-Werte
in eine CSV-Datei schreiben, kann etwas Präzision verloren gehen - 0,12345678901232323
willkürlich als "0.12345678901" aufgerundet werden - wie in einem
in einem Stack Overflow-Thread und einem Microsoft Com- munity-Thread diskutiert.
munity-Thread. Leute auf Hacker News haben sich leidenschaftlich
gegen die Verwendung von CSV.
Text- versus Binärformat
CSV und JSON sind Textdateien, während Parquet-Dateien Binärdateien sind. Textdateien sind
Dateien im Klartext, was normalerweise bedeutet, dass sie für Menschen lesbar sind. Binäre
Dateien sind der Sammelbegriff für alle Nicht-Text-Dateien. Wie der Name schon sagt, sind Binärdateien
typischerweise Dateien, die nur 0en und 1en enthalten, und sind dafür gedacht, von Programmen gelesen oder verwendet zu werden
Programmen gelesen oder verwendet werden, die wissen, wie die rohen Bytes zu interpretieren sind. Ein Programm muss genau wissen
wie die Daten in einer Binärdatei aufgebaut sind, um die Datei nutzen zu können. Wenn Sie Text
Dateien in einem Texteditor (z. B. VS Code, Notepad) öffnen, können Sie die Texte darin
sie lesen. Wenn Sie eine Binärdatei in Ihrem Texteditor öffnen, sehen Sie Zahlenblöcke, wahrscheinlich
in hexadezimalen Werten, für die entsprechenden Bytes der Datei.

Binärdateien sind kompakter. Hier ein einfaches Beispiel, das zeigt, wie Binärdateien
im Vergleich zu Textdateien Platz sparen können. Nehmen wir an, Sie möchten die Zahl

Wenn Sie es in einer Textdatei speichern, werden 7 Zeichen benötigt, und wenn jedes Zeichen
1 Byte ist, werden 7 Bytes benötigt. Wenn Sie sie in einer Binärdatei als int32 speichern, benötigen Sie nur 32 Bits
oder 4 Bytes.
Zur Veranschaulichung verwende ich interviews.csv, eine CSV-Datei (Textformat) mit 17.654 Zeilen
und 10 Spalten. Als ich sie in ein Binärformat (Parquet) konvertierte, sank die Dateigröße
von 14 MB auf 6 MB, wie in Abbildung 3-3 dargestellt.

AWS empfiehlt die Verwendung des Parquet-Formats, da "das Parquet-Format bis zu 2x
schneller zu entladen ist und bis zu 6x weniger Speicherplatz in Amazon S3 verbraucht, verglichen mit Text
Formaten."^8

Datenformate | 57
Abbildung 3-3. Bei Speicherung im CSV-Format ist meine Interviewdatei 14 MB groß. Wenn sie jedoch im
Parquet gespeichert, ist die gleiche Datei 6 MB groß.

Datenmodelle
Datenmodelle beschreiben, wie Daten dargestellt werden. Denken Sie an Autos in der realen Welt. In
einer Datenbank kann ein Auto durch seine Marke, sein Modell, sein Baujahr, seine Farbe und seinen Preis beschrieben werden.
seinen Preis. Diese Attribute bilden ein Datenmodell für Autos. Alternativ können Sie auch
ein Auto anhand seines Besitzers, seines Kennzeichens und seiner Historie der registrierten Adressen beschreiben.
Dies ist ein weiteres Datenmodell für Autos.

58 | Kapitel 3: Grundlagen der Datentechnik

9 Edgar F. Codd, "A Relational Model of Data for Large Shared Data Banks", Communications of the ACM 13,
no. 6 (Juni 1970): 377-87.
10 Für detailorientierte Leser: Nicht alle Tabellen sind Relationen.

Die Art und Weise, wie Sie Daten darstellen, beeinflusst nicht nur die Art und Weise, wie Ihre Systeme aufgebaut sind, sondern
sondern auch die Probleme, die Ihre Systeme lösen können. Die Art und Weise, wie Sie zum Beispiel Autos in
Datenmodell den Autokauf erleichtert, während das zweite Datenmodell den Polizisten die
Datenmodell den Polizeibeamten das Aufspüren von Kriminellen erleichtert.
In diesem Abschnitt werden wir zwei Arten von Modellen untersuchen, die scheinbar gegensätzlich sind, aber
aber in Wirklichkeit konvergieren: relationale Modelle und NoSQL-Modelle. Wir werden Beispiele durchgehen
um zu zeigen, für welche Arten von Problemen jedes Modell geeignet ist.
Relationales Modell
Relationale Modelle gehören zu den hartnäckigsten Ideen in der Computerwissenschaft. Erfunden
von Edgar F. Codd im Jahr 1970,^9 hat sich das relationale Modell bis heute gehalten und wird sogar
immer beliebter. Die Idee ist einfach, aber wirkungsvoll. In diesem Modell werden die Daten in
In diesem Modell werden Daten in Relationen organisiert; jede Relation ist ein Satz von Tupeln. Eine Tabelle ist eine akzeptierte visuelle
einer Beziehung, und jede Zeile einer Tabelle bildet ein Tupel,^10 wie in Abbildung 3-4 gezeigt.
in Abbildung 3-4. Beziehungen sind nicht geordnet. Sie können die Reihenfolge der Zeilen oder die
Reihenfolge der Spalten in einer Relation ändern und es ist immer noch die gleiche Relation. Daten, die dem
relationalen Modell folgen, werden normalerweise in Dateiformaten wie CSV oder Parquet gespeichert.
Abbildung 3-4. In einer Relation spielt weder die Reihenfolge der Zeilen noch die der Spalten eine Rolle
Oft ist es wünschenswert, dass Relationen normalisiert werden. Die Normalisierung von Daten kann folgen
Normalformen wie die erste Normalform (1NF), die zweite Normalform (2NF) usw. folgen,
Interessierte Leser können auf Wikipedia mehr darüber lesen. In diesem Buch werden wir
Beispiel zeigen wir, wie Normalisierung funktioniert und wie sie Datenredundanz reduzieren
Datenredundanz reduzieren und die Datenintegrität verbessern kann.
Betrachten Sie die in Tabelle 3-2 gezeigte Relation Book. Es gibt viele Duplikate in diesen
Daten. Zum Beispiel sind die Zeilen 1 und 2 fast identisch, mit Ausnahme von Format und Preis.
Wenn sich die Verlagsinformationen ändern, z. B. der Name "Banana
Datenmodelle | 59
11 Sie können die Beziehung "Buch" weiter normalisieren, indem Sie beispielsweise das Format in eine separate Beziehung aufteilen.

Press" zu "Pineapple Press" - oder das Land ändert, müssen wir die Zeilen 1, 2,
und 4 aktualisieren. Wenn wir die Verlagsinformationen in eine eigene Tabelle aufteilen, wie in den Tabellen
3-3 und 3-4 gezeigt, müssen wir bei einer Änderung der Verlagsinformationen nur die
Publisher-Relation aktualisieren.^11 Auf diese Weise können wir die Schreibweise desselben Wertes
über verschiedene Spalten hinweg zu vereinheitlichen. Außerdem wird es dadurch einfacher, Änderungen an diesen Werten vorzunehmen,
weil sich die Werte ändern oder wenn sie in verschiedene Sprachen übersetzt werden sollen.
Sprachen.
Tabelle 3-2. Ursprüngliche Buchbeziehung
Titel Autor Format Verlag Land Preis
Harry Potter J.K. Rowling Taschenbuch Banana Press UK $20
Harry Potter J.K. Rowling E-Book Banana Press UK $10
Sherlock Holmes Conan Doyle Taschenbuch Guava Press US $30
Der Hobbit J.R.R. Tolkien Taschenbuch Banana Press UK $30
Sherlock Holmes Conan Doyle Taschenbuch Guava Press US $15
Tabelle 3-3. Aktualisierte Buchbeziehungen
Titel Autor Format Verlag ID Preis
Harry Potter J.K. Rowling Taschenbuch 1 $20
Harry Potter J.K. Rowling E-Book 1 $10
Sherlock Holmes Conan Doyle Taschenbuch 2 $30
Der Hobbit J.R.R. Tolkien Taschenbuch 1 $30
Sherlock Holmes Conan Doyle Taschenbuch 2 $15
Tabelle 3-4. Verhältnis der Verlage
Verlag ID Verlag Land
1 Banana Press UK
2 Guava Press US
Ein großer Nachteil der Normalisierung ist, dass Ihre Daten nun über mehrere Relationen verteilt sind.
Relationen verteilt sind. Sie können die Daten aus verschiedenen Relationen wieder zusammenführen, aber das Zusammenführen kann
bei großen Tabellen teuer sein.
Datenbanken, die auf einem relationalen Datenmodell basieren, sind relationale Datenbanken. Sobald
Sobald Sie Daten in Ihren Datenbanken gespeichert haben, möchten Sie eine Möglichkeit haben, sie abzurufen. Die Sprache
die Sie verwenden können, um die gewünschten Daten aus einer Datenbank zu spezifizieren, nennt man eine
Abfragesprache. Die beliebteste Abfragesprache für relationale Datenbanken ist heute
SQL. Obwohl sie vom relationalen Modell inspiriert ist, hat sich das Datenmodell hinter SQL
von dem ursprünglichen relationalen Modell abgewichen. SQL-Tabellen können zum Beispiel Folgendes enthalten
60 | Kapitel 3: Grundlagen der Datentechnik
12 Greg Kemnitz, ein Mitverfasser des ursprünglichen Postgres-Papiers, teilte auf Quora mit, dass er einmal eine Berichts-SQL
Abfrage schrieb, die 700 Zeilen lang war und 27 verschiedene Tabellen in Lookups oder Joins besuchte. Die Abfrage hatte etwa 1.000
Zeilen mit Kommentaren, die ihm helfen sollten, sich zu erinnern, was er tat. Er brauchte drei Tage zum Verfassen, Debuggen und
abzustimmen.
13 Yannis E. Ioannidis, "Query Optimization," ACM Computing Surveys (CSUR) 28, Nr. 1 (1996): 121-23,
https://oreil.ly/omXMg
14 Ryan Marcus et al., "Neo: A Learned Query Optimizer," arXiv preprint arXiv:1904.03711 (2019),
https://oreil.ly/wHy6p.

Zeilenduplikate, während echte Beziehungen keine Duplikate enthalten können. Dieser subtile Unterschied
Unterschied wird von den meisten Menschen ignoriert.
Das Wichtigste an SQL ist, dass es eine deklarative Sprache ist, im Gegensatz zu
im Gegensatz zu Python, das eine imperative Sprache ist. Im imperativen Paradigma legen Sie
die für eine Aktion erforderlichen Schritte an, und der Computer führt diese Schritte aus, um die
die Ausgaben. Im deklarativen Paradigma geben Sie die gewünschten Ergebnisse an, und der
Computer die Schritte aus, die erforderlich sind, um die abgefragten Ergebnisse zu erhalten.
Bei einer SQL-Datenbank geben Sie das Muster der gewünschten Daten an - die Tabellen, aus denen Sie
die Daten aus den Tabellen, die Bedingungen, die die Ergebnisse erfüllen müssen, die grundlegenden Datenumwandlungen
wie z. B. Join, Sortieren, Gruppieren, Aggregieren usw., aber nicht, wie die Daten abgerufen werden sollen. Es ist Sache des
aber nicht, wie die Daten abgerufen werden. Es ist Sache des Datenbanksystems zu entscheiden, wie die Abfrage in verschiedene Teile zerlegt wird, welche
Methoden zur Ausführung der einzelnen Teile der Abfrage zu verwenden sind und in welcher Reihenfolge die verschiedenen
Teile der Abfrage ausgeführt werden sollen.
Mit bestimmten zusätzlichen Funktionen kann SQL Turing-komplett sein, was bedeutet, dass theoretisch
das heißt, dass SQL theoretisch zur Lösung jedes Rechenproblems verwendet werden kann (ohne eine
über die benötigte Zeit oder den Speicherplatz). In der Praxis ist es jedoch nicht immer
einfach, eine Abfrage zur Lösung einer bestimmten Aufgabe zu schreiben, und es ist nicht immer machbar oder nachvollziehbar, eine
eine Abfrage auszuführen. Jeder, der mit SQL-Datenbanken arbeitet, hat vielleicht alptraumhafte Erinnerungen an
Erinnerungen an quälend lange SQL-Abfragen, die unmöglich zu verstehen sind und die niemand
sich nicht traut, sie anzufassen, aus Angst, dass etwas kaputtgehen könnte.^12
Herauszufinden, wie man eine beliebige Abfrage ausführt, ist der schwierige Teil, und das ist die Aufgabe
von Abfrageoptimierern. Ein Abfrageoptimierer untersucht alle möglichen Wege zur Ausführung einer
Abfrage auszuführen und findet den schnellsten Weg.^13 Es ist möglich, ML zu verwenden, um Abfrage
Abfrageoptimierer zu verbessern, indem man aus eingehenden Abfragen lernt.^14 Die Abfrageoptimierung ist eines der
ist eines der schwierigsten Probleme in Datenbanksystemen, und Normalisierung bedeutet
Normalisierung bedeutet, dass die Daten auf mehrere Relationen verteilt sind, was das Zusammenführen der Daten noch
erschwert. Auch wenn die Entwicklung eines Abfrageoptimierers schwierig ist, ist die gute Nachricht, dass man
dass Sie im Allgemeinen nur einen Abfrageoptimierer brauchen und alle Ihre Anwendungen ihn nutzen können.
Datenmodelle | 61
15 Matthias Boehm, Alexandre V. Evfimievski, Niketan Pansare, und Berthold Reinwald, "Deklaratives Maschinelles
Learning-A Classification of Basic Properties and Types," arXiv, May 19, 2016, https://oreil.ly/OvW07.

Von deklarativen Datensystemen zu deklarativen ML-Systemen
Möglicherweise inspiriert durch den Erfolg von deklarativen Datensystemen, haben sich viele Menschen auf
auf deklaratives ML gefreut.^15 Bei einem deklarativen ML-System muss der Benutzer nur das
Schema der Merkmale und die Aufgabe deklarieren, und das System findet das beste Modell
Modell, das diese Aufgabe mit den gegebenen Merkmalen erfüllt. Die Benutzer müssen keinen Code schreiben
um Modelle zu konstruieren, zu trainieren und abzustimmen. Beliebte Frameworks für deklarative ML sind
Ludwig, entwickelt bei Uber, und H2O AutoML. In Ludwig können die Benutzer die
Modellstruktur angeben, z. B. die Anzahl der vollständig verbundenen Schichten und die Anzahl der
versteckten Einheiten - zusätzlich zum Schema und zur Ausgabe der Merkmale angeben. In H2O AutoML müssen Sie nicht
die Modellstruktur oder Hyperparameter nicht angeben. Es experimentiert mit mehreren
Modellarchitekturen und wählt das beste Modell für die Merkmale und die Aufgabe aus.
Hier ist ein Beispiel, das zeigt, wie H2O AutoML funktioniert. Sie geben dem System Ihre Daten
(Eingaben und Ausgaben) und geben die Anzahl der Modelle an, die Sie ausprobieren möchten. Es wird
Es experimentiert mit dieser Anzahl von Modellen und zeigt Ihnen das Modell mit der besten Leistung:
# Identifizieren Sie Prädiktoren und Reaktion
x = train.columns
y = "Antwort"
x.remove(y)
# Für die binäre Klassifizierung sollte die Antwort ein Faktor sein.
train[y] = train[y].asfactor()
test[y] = test[y].asfactor()
# AutoML für 20 Basismodelle ausführen
aml = H2OAutoML(max_models=20, seed=1)
aml.train(x=x, y=y, training_frame=train)
# Anzeige der besten Modelle auf dem AutoML-Leaderboard
lb = aml.leaderboard
# Ermittelt das leistungsstärkste Modell
aml.leader
Während deklarative ML in vielen Fällen nützlich sein kann, lässt sie die größten
Herausforderungen von ML in der Produktion. Deklarative ML-Systeme abstrahieren heute den
Modellentwicklung ab, und wie wir in den nächsten sechs Kapiteln behandeln werden, wird die
immer mehr zur Massenware werden, ist die Modellentwicklung oft der einfachere Teil. Der
schwierige Teil liegt im Feature-Engineering, der Datenverarbeitung, der Modellbewertung, der
Erkennung, kontinuierliches Lernen und so weiter.
62 | Kapitel 3: Grundlagen des Data Engineering
16 James Phillips, "Surprises in Our NoSQL Adoption Survey", Couchbase, 16. Dezember 2014,
https://oreil.ly/ueyEX.
17 Martin Kleppmann, Designing Data-Intensive Applications (Sebastopol, CA: O'Reilly, 2017).

NoSQL
Das relationale Datenmodell konnte für viele Anwendungsfälle verallgemeinert werden, von
E-Commerce über Finanzen bis hin zu sozialen Netzwerken. Für bestimmte Anwendungsfälle kann dieses Modell jedoch
restriktiv sein. Es verlangt zum Beispiel, dass Ihre Daten einem strengen Schema folgen,
und das Schema-Management ist mühsam. In einer Umfrage von Couchbase im Jahr 2014 war die Frustration
die Frustration über das Schema-Management der Grund Nr. 1 für die Einführung ihrer nicht-relationalen
Datenbank.^16 Es kann auch schwierig sein, SQL-Abfragen für spezielle Anwendungen zu schreiben und auszuführen.
Anwendungen.
Die neueste Bewegung gegen das relationale Datenmodell ist NoSQL. Ursprünglich begann
Ursprünglich als Hashtag für ein Treffen zur Diskussion über nicht-relationale Datenbanken entstanden, wurde NoSQL retro-
aktiv umgedeutet in Not Only SQL,^17 da viele NoSQL-Datensysteme auch relationale Modelle unterstützen.
relationale Modelle unterstützen. Zwei wichtige Arten von nicht-relationalen Modellen sind das Dokumentenmodell
und das Graphenmodell. Das Dokumentenmodell zielt auf Anwendungsfälle ab, bei denen die Daten in
in sich abgeschlossenen Dokumenten vorliegen und Beziehungen zwischen einem Dokument und einem anderen
selten sind. Das Graphenmodell geht in die entgegengesetzte Richtung und zielt auf Anwendungsfälle ab, in denen
Beziehungen zwischen Datenelementen üblich und wichtig sind. Wir werden jedes dieser
Wir werden diese beiden Modelle untersuchen und mit dem Dokumentenmodell beginnen.
Dokumentenmodell
Das Dokumentenmodell basiert auf dem Konzept des "Dokuments". Ein Dokument ist oft
eine einzelne fortlaufende Zeichenkette, kodiert als JSON, XML oder ein binäres Format wie BSON
(Binary JSON). Es wird davon ausgegangen, dass alle Dokumente in einer Dokumentendatenbank
demselben Format kodiert sind. Jedes Dokument hat einen eindeutigen Schlüssel, der dieses Dokument repräsentiert,
der zum Abrufen des Dokuments verwendet werden kann.
Eine Sammlung von Dokumenten kann analog zu einer Tabelle in einer relationalen Datenbank betrachtet werden.
Datenbank betrachtet werden, und ein Dokument analog zu einer Zeile. In der Tat kann man auf diese Weise eine Relation in
eine Sammlung von Dokumenten umwandeln. Zum Beispiel können Sie die Buchdaten in den
Tabellen 3-3 und 3-4 in drei JSON-Dokumente konvertieren, wie in den Beispielen 3-1, 3-2 und
3-3. Eine Sammlung von Dokumenten ist jedoch viel flexibler als eine Tabelle. Alle Zeilen
in einer Tabelle müssen demselben Schema folgen (z. B. dieselbe Reihenfolge der Spalten haben),
während Dokumente in derselben Sammlung völlig unterschiedliche Schemata haben können.
Datenmodelle | 63
Beispiel 3-1. Dokument 1: harry_potter.json

{
"Titel" : "Harry Potter",
"Autor" : "J. K. Rowling",
"Verlag" : "Banana Press",
"Land" : "Großbritannien",
"Verkauft als" : [
{ "Format" : "Taschenbuch", "Preis" : "$20"},
{ "Format" : "E-Book", "Preis" : "$10"}
]
}

Beispiel 3-2. Dokument 2: sherlock_holmes.json

{
"Titel" : "Sherlock Holmes",
"Autor" : "Conan Doyle",
"Verlag" : "Guava Press",
"Land" : "US",
"Verkauft als" : [
{ "Format" : "Paperback", "Preis" : "$30"},
{ "Format" : "E-Book", "Preis" : "$15"}
]
}

Beispiel 3-3. Dokument 3: the_hobbit.json

{
"Titel" : "Der Hobbit",
"Autor" : "J.R.R. Tolkien",
"Verlag" : "Banana Press",
"Land" : "UK",
"Verkauft als" : [
{ "Format" : "Taschenbuch", "Preis" : "$30"},
]
}

Da das Dokumentenmodell kein Schema erzwingt, wird es oft als schema- los bezeichnet.
maless bezeichnet. Dies ist irreführend, denn wie bereits erwähnt, werden die in Dokumenten gespeicherten Daten
später gelesen werden. Die Anwendung, die die Dokumente liest, geht normalerweise von einer
eine Art Struktur der Dokumente an. Dokumentendatenbanken verlagern lediglich die Verantwortung
der Annahme von Strukturen von der Anwendung, die die Daten schreibt, auf die Anwendung
die die Daten liest.

Das Dokumentenmodell hat eine bessere Lokalisierung als das relationale Modell. Betrachten Sie das Buch
Datenbeispiel in den Tabellen 3-3 und 3-4, wo die Informationen über ein Buch
sowohl über die Tabelle Buch als auch über die Tabelle Verlag (und möglicherweise auch über die Tabelle Format
Tabelle). Um Informationen über ein Buch abzurufen, müssen Sie mehrere Tabellen abfragen.

64 | Kapitel 3: Grundlagen der Datentechnik

Im Dokumentenmodell können alle Informationen über ein Buch in einem Dokument gespeichert werden,
was das Auffinden wesentlich erleichtert.

Im Vergleich zum relationalen Modell ist es jedoch schwieriger und weniger effizient, Joins
Joins über Dokumente auszuführen, als über Tabellen. Wenn Sie zum Beispiel
alle Bücher finden wollen, deren Preis unter $25 liegt, müssen Sie alle Dokumente lesen, die
Preise extrahieren, sie mit $25 vergleichen und alle Dokumente zurückgeben, die die Bücher mit
Preisen unter $25 enthalten.

Aufgrund der unterschiedlichen Stärken des Dokumentenmodells und des relationalen Datenmodells ist es
ist es üblich, beide Modelle für verschiedene Aufgaben in denselben Datenbanksystemen zu verwenden. Mehr
mehr und mehr Datenbanksysteme, wie PostgreSQL und MySQL, unterstützen beide Modelle.

Graphisches Modell

Das Graphenmodell basiert auf dem Konzept eines "Graphen". Ein Graph besteht aus Knoten
und Kanten, wobei die Kanten die Beziehungen zwischen den Knoten darstellen. Eine Datenbank
die Graphenstrukturen zur Speicherung ihrer Daten verwendet, wird als Graphdatenbank bezeichnet. Wenn in Dokument
Datenbanken der Inhalt der einzelnen Dokumente im Vordergrund steht, so sind es bei Graphdatenbanken die
Beziehungen zwischen den Datenelementen im Vordergrund.

Da die Beziehungen in Graphenmodellen explizit modelliert werden, ist es schneller, Daten
Daten auf der Grundlage von Beziehungen. Betrachten Sie ein Beispiel für eine Graphdatenbank in Abbildung 3-5.
Die Daten aus diesem Beispiel könnten von einem einfachen sozialen Netzwerk stammen. Unter
diesem Graphen können die Knoten verschiedene Datentypen haben: Person, Stadt, Land, Unternehmen usw.

Abbildung 3-5. Ein Beispiel für eine einfache Graphdatenbank

Stellen Sie sich vor, Sie wollen alle Personen finden, die in den USA geboren wurden. Bei diesem Graphen können Sie
können Sie vom Knoten USA ausgehen und den Graphen entlang der Kanten "innerhalb" und "geboren_in" durchlaufen
und "born_in", um alle Knoten vom Typ "Person" zu finden. Stellen Sie sich nun vor, dass statt
das Graphenmodell zur Darstellung dieser Daten verwenden, sondern das relationale Modell. Es gäbe
Es gäbe keine einfache Möglichkeit, eine SQL-Abfrage zu schreiben, um alle Personen zu finden, die in den USA geboren wurden,

Datenmodelle | 65
18 In diesem konkreten Beispiel wurde das Problem durch das Ersetzen der Nullwerte für das Alter durch -1 gelöst.

vor allem, wenn man bedenkt, dass es eine unbekannte Anzahl von Sprüngen zwischen Land und Person gibt
Person gibt - es gibt drei Sprünge zwischen Zhenzhong Xu und den USA, während es nur
zwei Sprünge zwischen Chloe He und den USA. Auch für diese Art von Abfrage gibt es keine einfache Möglichkeit
Abfrage mit einer Dokumentendatenbank.
Viele Abfragen, die in einem Datenmodell leicht zu bewerkstelligen sind, sind in einem anderen Datenmodell schwieriger zu bewerkstelligen.
Modell. Die Wahl des richtigen Datenmodells für Ihre Anwendung kann Ihnen das Leben sehr viel
einfacher machen.
Strukturierte Daten im Vergleich zu unstrukturierten Daten
Strukturierte Daten folgen einem vordefinierten Datenmodell, das auch als Datenschema bezeichnet wird. Für
kann das Datenmodell beispielsweise festlegen, dass jedes Datenelement aus zwei Werten besteht: Der
erste Wert, "Name", ist eine Zeichenkette mit höchstens 50 Zeichen, und der zweite Wert, "Alter", ist
eine 8-Bit-Ganzzahl im Bereich zwischen 0 und 200. Die vordefinierte Struktur macht Ihre
Daten leichter zu analysieren. Wenn Sie das Durchschnittsalter der Personen in der Datenbank wissen wollen,
brauchen Sie nur alle Alterswerte zu extrahieren und den Durchschnitt zu bilden.
Der Nachteil von strukturierten Daten ist, dass Sie Ihre Daten in ein vorgegebenes Schema einbinden müssen.
festgelegtes Schema. Wenn sich Ihr Schema ändert, müssen Sie alle Daten nachträglich aktualisieren.
Daten aktualisieren, was oft zu mysteriösen Fehlern führt. Zum Beispiel haben Sie nie die
E-Mail-Adressen Ihrer Benutzer gespeichert, jetzt aber schon, so müssen Sie die
E-Mail-Informationen für alle früheren Benutzer aktualisieren. Einer der seltsamsten Fehler, auf den einer meiner
Kollegen war, als er das Alter der Benutzer nicht mehr für seine Transaktionen verwenden konnte
Transaktionen nicht mehr verwenden konnten, und ihr Datenschema ersetzte alle Null-Altersangaben durch 0, und ihr ML
Modell glaubte, dass die Transaktionen von Personen durchgeführt wurden, die 0 Jahre alt waren.^18
Da sich die Geschäftsanforderungen im Laufe der Zeit ändern, kann die Festlegung auf ein vordefiniertes Datenschema
Datenschema zu restriktiv werden. Oder Sie haben vielleicht Daten aus mehreren Datenquellen
Datenquellen, die sich Ihrer Kontrolle entziehen, und es ist unmöglich, sie an das
gleichen Schema folgen. An dieser Stelle werden unstrukturierte Daten interessant. Unstrukturierte Daten
halten sich nicht an ein vordefiniertes Datenschema. Es handelt sich in der Regel um Text, aber auch um Zahlen,
Daten, Bilder, Audio usw. sein. Zum Beispiel ist eine Textdatei mit Protokollen, die von Ihrem ML-Modell
erzeugt wurden, sind unstrukturierte Daten.
Auch wenn unstrukturierte Daten keinem Schema folgen, können sie dennoch
intrinsische Muster enthalten, die Ihnen helfen, Strukturen zu extrahieren. Zum Beispiel ist der folgende Text
ist unstrukturiert, aber Sie können das Muster erkennen, dass jede Zeile zwei Werte enthält
enthält, die durch ein Komma getrennt sind. Der erste Wert ist ein Text, der zweite ein numerischer Wert.
Es gibt jedoch keine Garantie, dass alle Zeilen diesem Format entsprechen müssen. Sie können eine
neue Zeile zu diesem Text hinzufügen, auch wenn diese Zeile nicht diesem Format entspricht.
66 | Kapitel 3: Grundlagen der Datentechnik
Lisa, 43
Bube, 23
Huyen, 59
Unstrukturierte Daten ermöglichen auch flexiblere Speicheroptionen. Wenn zum Beispiel Ihre
Speicherung einem Schema folgt, können Sie nur Daten speichern, die diesem Schema entsprechen. Aber wenn Ihr
Speicherung keinem Schema folgt, können Sie jede Art von Daten speichern. Sie können alle Daten konvertieren
Daten, unabhängig von Typ und Format, in Bytestrings umwandeln und zusammen speichern.

Ein Repository zur Speicherung strukturierter Daten wird als Data Warehouse bezeichnet. Ein Repositorium für
Speicherung unstrukturierter Daten wird als Data Lake bezeichnet. Data Lakes werden in der Regel zur Speicherung von
Rohdaten vor der Verarbeitung. Data Warehouses dienen der Speicherung von Daten, die bereits
in verwendungsfertige Formate aufbereitet werden. Tabelle 3-5 zeigt eine Zusammenfassung der wichtigsten
Unterschiede zwischen strukturierten und unstrukturierten Daten.

Tabelle 3-5. Die wichtigsten Unterschiede zwischen strukturierten und unstrukturierten Daten

Strukturierte Daten Unstrukturierte Daten
Schema klar definiert Daten müssen nicht einem Schema folgen
Einfach zu suchen und zu analysieren Schnelles Eintreffen
Kann nur Daten mit einem bestimmten Schema verarbeiten Kann Daten aus jeder Quelle verarbeiten
Schema-Änderungen verursachen eine Menge Probleme Man muss sich (noch) nicht um Schema-Änderungen kümmern, denn die Sorge wird auf die
nachgelagerten Anwendungen, die diese Daten verwenden
Gespeichert in Data Warehouses Gespeichert in Data Lakes
Datenspeichermaschinen und Verarbeitung
Datenformate und Datenmodelle legen die Schnittstelle fest, über die Benutzer Daten speichern und abrufen können.
Daten abrufen können. Speicher-Engines, auch bekannt als Datenbanken, sind die Implementierung von
wie Daten auf Maschinen gespeichert und abgerufen werden. Es ist nützlich, verschiedene Arten von Datenbanken zu verstehen
von Datenbanken zu verstehen, da Ihr Team oder Ihr benachbartes Team möglicherweise eine Datenbank auswählen muss
die für Ihre Anwendung geeignet ist.

Typischerweise gibt es zwei Arten von Arbeitslasten, für die Datenbanken optimiert sind: Transaktions- und
tionalen Verarbeitung und die analytische Verarbeitung, und es gibt einen großen Unterschied zwischen ihnen,
den wir in diesem Abschnitt behandeln werden. Anschließend werden wir die Grundlagen des ETL-Prozesses (extract,
Transformieren, Laden), auf den Sie unweigerlich stoßen werden, wenn Sie ein ML
Systems in der Produktion begegnen.

Transaktionsbezogene und analytische Verarbeitung
Traditionell bezeichnet eine Transaktion den Vorgang des Kaufs oder Verkaufs von etwas. In
der digitalen Welt bezieht sich eine Transaktion auf jede Art von Handlung: Tweeten, eine
Mitfahrdienstes, das Hochladen eines neuen Modells, das Ansehen eines YouTube-Videos
Video und so weiter. Auch wenn diese verschiedenen Transaktionen unterschiedliche Arten von

Datenspeichermaschinen und -verarbeitung | 67
19 Dieser Abschnitt, wie auch viele Teile dieses Kapitels, ist inspiriert von Martin Kleppmann's Designing Data-
Intensive Anwendungen.
20 Kleppmann, Designing Data-Intensive Applications.

Die Art und Weise, wie die Daten verarbeitet werden, ist in allen Anwendungen ähnlich. Die Transaktionen werden
eingefügt, wenn sie erzeugt werden, und gelegentlich aktualisiert, wenn sich etwas ändert, oder
gelöscht, wenn sie nicht mehr benötigt werden.^19 Diese Art der Verarbeitung ist bekannt als Online
Transaktionsverarbeitung (OLTP) bezeichnet.
Da an diesen Transaktionen häufig Benutzer beteiligt sind, müssen sie schnell verarbeitet werden (geringe
Latenzzeit) verarbeitet werden, damit sie die Benutzer nicht warten lassen. Die Verarbeitungsmethode muss
hohe Verfügbarkeit, d. h. das Verarbeitungssystem muss jederzeit verfügbar sein, wenn ein
Benutzer eine Transaktion durchführen möchte. Wenn Ihr System eine Transaktion nicht verarbeiten kann, wird diese
wird die Transaktion nicht durchgeführt.
Transaktionsdatenbanken sind für die Verarbeitung von Online-Transaktionen konzipiert und erfüllen die
Anforderungen an niedrige Latenzzeiten und hohe Verfügbarkeit erfüllen. Wenn man von transaktionalen Daten
Datenbanken hören, denken sie in der Regel an ACID (Atomarität, Konsistenz, Isolation, Dauerhaftigkeit). Hier
sind die Definitionen für diejenigen, die eine kurze Erinnerung benötigen:
Atomarität
Die Garantie, dass alle Schritte einer Transaktion als Gruppe erfolgreich abgeschlossen werden.
Gruppe abgeschlossen werden. Wenn ein Schritt in der Transaktion fehlschlägt, müssen alle anderen Schritte ebenfalls fehlschlagen. Für
Beispiel: Wenn die Zahlung eines Benutzers fehlschlägt, wollen Sie diesem Benutzer nicht noch einen Fahrer zuweisen.
Benutzer zuweisen.
Konsistenz
Um zu gewährleisten, dass alle Transaktionen, die durchlaufen werden, vordefinierten
Regeln folgen. Zum Beispiel muss eine Transaktion von einem gültigen Benutzer durchgeführt werden.
Isolierung
Garantiert, dass zwei Transaktionen zur gleichen Zeit stattfinden, als wären sie
isoliert. Zwei Benutzer, die auf dieselben Daten zugreifen, können diese nicht gleichzeitig ändern. Für
Sie wollen beispielsweise nicht, dass zwei Benutzer gleichzeitig denselben Fahrer buchen.
Dauerhaftigkeit
Die Garantie, dass eine einmal festgeschriebene Transaktion auch im Falle eines
auch im Falle eines Systemausfalls. Zum Beispiel, nachdem Sie eine Fahrt bestellt haben
und Ihr Telefon streikt, wollen Sie trotzdem, dass Sie abgeholt werden.
Transaktionsdatenbanken müssen jedoch nicht unbedingt ACID sein, und manche Entwickler finden ACID
opers finden ACID zu restriktiv. Laut Martin Kleppmann sind "Systeme, die
die die ACID-Kriterien nicht erfüllen, werden manchmal BASE genannt, was für Basically
Available, Soft state, and Eventual consistency. Dies ist sogar noch vager als die
Definition von ACID."^20
68 | Kapitel 3: Grundlagen der Datentechnik
Da jede Transaktion oft als eine von anderen Transaktionen getrennte Einheit verarbeitet wird, sind transaktionale
tionen verarbeitet wird, sind transaktionale Datenbanken oft zeilenschwer. Das bedeutet auch, dass transaktionale
Transaktionsdatenbanken möglicherweise nicht effizient für Fragen wie "Wie hoch ist der Durchschnittspreis für
alle Fahrten im September in San Francisco?" Diese Art von analytischer Frage erfordert
Daten in Spalten über mehrere Datenzeilen hinweg zu aggregieren. Analytische Datenbanken sind
für diesen Zweck konzipiert. Sie sind effizient mit Abfragen, die es Ihnen ermöglichen
Daten aus verschiedenen Blickwinkeln zu betrachten. Wir nennen diese Art der Verarbeitung Online Analytical
Verarbeitung (OLAP).

Die beiden Begriffe OLTP und OLAP sind jedoch veraltet, wie in
Abbildung 3-6, aus drei Gründen überholt. Erstens war die Trennung von transaktionalen und analytischen Daten
Datenbanken auf technologische Beschränkungen zurückzuführen - es war schwierig, Datenbanken zu haben, die
die sowohl transaktionale als auch analytische Abfragen effizient verarbeiten konnten. Doch diese Trennung
wird jedoch aufgehoben. Heute gibt es Transaktionsdatenbanken, die analytische Abfragen verarbeiten können
Abfragen verarbeiten können, wie z. B. CockroachDB. Außerdem gibt es analytische Datenbanken, die
transaktionale Abfragen verarbeiten können, wie Apache Iceberg und DuckDB.

Abbildung 3-6. OLAP und OLTP sind laut Google Trends veraltete Begriffe (Stand 2021)

Zweitens sind bei den traditionellen OLTP- oder OLAP-Paradigmen Speicherung und Verarbeitung
eng miteinander gekoppelt - wie die Daten gespeichert werden, so werden auch die Daten verarbeitet. Dies kann
Dies kann dazu führen, dass dieselben Daten in mehreren Datenbanken gespeichert werden und unterschiedliche
Verarbeitungs-Engines zur Lösung verschiedener Arten von Abfragen. Ein interessantes Paradigma der letzten
Paradigma des letzten Jahrzehnts ist die Entkopplung von Speicherung und Verarbeitung (auch bekannt als Compute),
Dies wurde von vielen Datenanbietern übernommen, darunter BigQuery von Google, Snowflake und IBM,

Datenspeichermaschinen und -verarbeitung | 69
21 Tino Tereshko, "Separation of Storage and Compute in BigQuery", Google Cloud Blog, 29. November,
2017, https://oreil.ly/utf7z; Suresh H., "Snowflake Architecture and Key Concepts: A Comprehensive Guide,"
Hevo-Blog, 18. Januar 2019, https://oreil.ly/GyvKl; Preetam Kumar, "Cutting the Cord: Separating Data
from Compute in Your Data Lake with Object Storage", IBM Blog, 21. September 2017, https://oreil.ly/.
Nd3xD; "The Power of Separating Cloud Compute and Cloud Storage", Teradata, letzter Zugriff im April 2022,
https://oreil.ly/f82gP.
22 Wikipedia, s.v. "Nearline Storage", letzter Zugriff im April 2022, https://oreil.ly/OCmiB.

und Teradata.^21 Bei diesem Paradigma können die Daten am selben Ort gespeichert werden, wobei eine
Verarbeitungsschicht, die für verschiedene Arten von Abfragen optimiert werden kann.
Drittens ist "online" ein überladener Begriff geworden, der viele verschiedene Bedeutungen haben kann.
Früher bedeutete "online" einfach "mit dem Internet verbunden". Dann wurde der Begriff erweitert und bedeutete
"in Produktion" - wir sagen, eine Funktion ist online, nachdem sie in der Produktion eingesetzt wurde.
Produktion.
In der Datenwelt von heute könnte sich online auf die Geschwindigkeit beziehen, mit der Ihre Daten
verarbeitet und zur Verfügung gestellt werden: online, nearline oder offline. Laut Wikipedia bedeutet
Online-Verarbeitung bedeutet, dass die Daten sofort für die Eingabe/Ausgabe verfügbar sind. Nearline,
Nearline, eine Abkürzung für Near-Online, bedeutet, dass die Daten nicht sofort verfügbar sind, aber
schnell und ohne menschliches Zutun online gestellt werden. Offline bedeutet, dass die Daten nicht sofort
Offline bedeutet, dass Daten nicht sofort verfügbar sind und ein menschliches Eingreifen erfordern, um online zu werden.^22
ETL: Extrahieren, Transformieren und Laden
In den Anfängen des relationalen Datenmodells waren die Daten meist strukturiert. Wenn Daten
Daten aus verschiedenen Quellen extrahiert werden, werden sie zunächst in das gewünschte Format umgewandelt, bevor sie
in das gewünschte Format umgewandelt, bevor sie in den Zielort, z. B. eine Datenbank oder ein Data Warehouse, geladen werden. Dieser
Prozess wird ETL genannt, was für Extrahieren, Transformieren und Laden steht.
Schon vor ML war ETL in der Datenwelt der letzte Schrei, und es ist auch heute noch relevant
für ML-Anwendungen. ETL bezieht sich auf die allgemeine Verarbeitung und Aggregation von
Daten in der gewünschten Form und dem gewünschten Format.
Extrahieren bedeutet, die gewünschten Daten aus all Ihren Datenquellen zu extrahieren. Einige von ihnen werden
beschädigt oder schlecht formatiert sein. In der Extraktionsphase müssen Sie Ihre Daten validieren
und die Daten zurückweisen, die nicht Ihren Anforderungen entsprechen. Bei abgelehnten Daten müssen Sie möglicherweise
müssen Sie die Quellen benachrichtigen. Da dies der erste Schritt des Prozesses ist, kann eine korrekte Durchführung
kann Ihnen im weiteren Verlauf viel Zeit ersparen.
Die Transformation ist der eigentliche Teil des Prozesses, in dem der Großteil der Datenverarbeitung stattfindet.
Vielleicht möchten Sie Daten aus mehreren Quellen zusammenführen und bereinigen. Vielleicht möchten Sie
die Wertebereiche standardisieren (z. B. könnte eine Datenquelle "männlich" und "weiblich" für
verwenden, während eine andere Quelle "M" und "F" oder "1" und "2" verwendet). Sie können Operationen anwenden wie
wie Transponieren, Deduplizieren, Sortieren, Aggregieren, Ableiten neuer Merkmale, weitere Daten
validieren, usw.
70 | Kapitel 3: Grundlagen der Datentechnik
23 Im ersten Entwurf dieses Buches hatte ich die Kosten als Grund angegeben, warum man nicht alles speichern sollte. Heutzutage ist jedoch
Heute ist die Speicherung jedoch so billig geworden, dass die Speicherkosten kaum noch ein Problem darstellen.

Beim Laden wird entschieden, wie und wie oft die transformierten Daten in das Ziel geladen
Ziel, das eine Datei, eine Datenbank oder ein Data Warehouse sein kann.
Die Idee von ETL klingt einfach, ist aber sehr wirkungsvoll, und sie ist die Grundstruktur der
der Datenschicht in vielen Unternehmen. Ein Überblick über den ETL-Prozess ist in
Abbildung 3-7.
Abbildung 3-7. Ein Überblick über den ETL-Prozess
Als das Internet allgegenwärtig wurde und die Hardware viel leistungsfähiger wurde
leistungsfähiger wurde, wurde das Sammeln von Daten plötzlich sehr viel einfacher. Die Menge der Daten
wuchs schnell. Und nicht nur das, auch die Art der Daten änderte sich. Die Anzahl der Daten
Quellen wurde größer, und die Datenschemata entwickelten sich weiter.
Da es schwierig war, die Daten strukturiert zu halten, hatten einige Unternehmen die Idee: "Warum
Warum nicht einfach alle Daten in einem Data Lake speichern, damit wir uns nicht mit Schemaänderungen herumschlagen müssen?
Welche Anwendung auch immer Daten benötigt, kann einfach Rohdaten von dort abrufen und verarbeiten.
sie verarbeiten." Dieser Prozess, bei dem die Daten zunächst in den Speicher geladen und später verarbeitet werden, wird manchmal
ELT (Extrahieren, Laden, Transformieren) genannt. Dieses Paradigma ermöglicht ein schnelles Eintreffen der Daten
da vor der Speicherung der Daten nur eine geringe Verarbeitung erforderlich ist.
Mit zunehmendem Datenaufkommen wird diese Idee jedoch immer unattraktiver. Es ist ineffizient
Es ist ineffizient, eine riesige Menge an Rohdaten nach den gewünschten Daten zu durchsuchen.^23
Gleichzeitig gehen Unternehmen immer mehr dazu über, Anwendungen in der Cloud auszuführen.
Datenspeichermaschinen und -verarbeitung | 71
und die Infrastrukturen standardisiert werden, werden auch die Datenstrukturen standardisiert.
Die Übergabe von Daten an ein vordefiniertes Schema wird dadurch leichter möglich.

Da Unternehmen die Vor- und Nachteile der Speicherung strukturierter Daten gegenüber der Speicherung
unstrukturierter Daten abwägen, entwickeln Anbieter hybride Lösungen, die die Flexibilität von
Flexibilität von Data Lakes und den Datenverwaltungsaspekt von Data Warehouses kombinieren. Zum Beispiel,
Databricks und Snowflake bieten beide Data Lakehouse-Lösungen an.

Modi des Datenflusses
In diesem Kapitel haben wir uns mit Datenformaten, Datenmodellen, Datenspeicherung und
Verarbeitung von Daten, die im Rahmen eines einzelnen Prozesses verwendet werden. Meistens hat man in der
Produktion nicht nur einen, sondern mehrere Prozesse. Es stellt sich die Frage: Wie können
Daten zwischen verschiedenen Prozessen, die sich keinen Speicher teilen, weitergeben?

Wenn Daten von einem Prozess an einen anderen weitergegeben werden, sagen wir, dass die Daten von
von einem Prozess zu einem anderen, was einen Datenfluss darstellt. Es gibt drei Hauptarten von
Datenflusses:

-Datenübermittlung durch Datenbanken
-Datenübermittlung durch Dienste unter Verwendung von Anfragen wie den Anfragen, die von
REST- und RPC-APIs (z. B. POST/GET-Anfragen)
-Datenübermittlung über einen Echtzeittransport wie Apache Kafka und Amazon
Kinesis
In diesem Abschnitt gehen wir auf jede dieser Möglichkeiten ein.

Datenübermittlung durch Datenbanken
Der einfachste Weg, Daten zwischen zwei Prozessen zu übertragen, sind Datenbanken, die wir
im Abschnitt "Datenspeichermaschinen und -verarbeitung" auf Seite 67 beschrieben. Für
Beispiel: Um Daten von Prozess A an Prozess B weiterzugeben, kann Prozess A diese Daten in eine Datenbank schreiben
eine Datenbank schreiben, und Prozess B liest einfach aus dieser Datenbank.

Dieser Modus funktioniert jedoch nicht immer, und zwar aus zwei Gründen. Erstens erfordert er
dass beide Prozesse auf dieselbe Datenbank zugreifen können müssen. Dies ist unter Umständen nicht machbar,
vor allem, wenn die beiden Prozesse von zwei verschiedenen Unternehmen betrieben werden.

Zweitens müssen beide Prozesse auf Daten aus Datenbanken zugreifen, und das Lesen/Schreiben
von Datenbanken kann langsam sein, so dass es für Anwendungen mit strengen Latenzanforderungen ungeeignet ist
Anforderungen - z. B. fast alle verbraucherorientierten Anwendungen - ungeeignet ist.

72 | Kapitel 3: Grundlagen der Datentechnik

Datenübermittlung durch Dienste
Eine Möglichkeit, Daten zwischen zwei Prozessen weiterzugeben, besteht darin, Daten direkt durch ein
Netzwerk zu senden, das diese beiden Prozesse miteinander verbindet. Um Daten von Prozess B an Prozess A weiterzuleiten
A weiterzuleiten, sendet Prozess A zunächst eine Anfrage an Prozess B, in der die von A benötigten Daten angegeben sind, und
B sendet die angeforderten Daten über das gleiche Netzwerk zurück. Da Prozesse durch Anfragen kommunizieren
Da Prozesse über Anfragen kommunizieren, sprechen wir von einem anfragegesteuerten Prozess.

Diese Art der Datenübermittlung ist eng mit der dienstorientierten Architektur verbunden.
Ein Dienst ist ein Prozess, auf den aus der Ferne, z. B. über ein Netz, zugegriffen werden kann. In diesem
Beispiel ist B für A als Dienst verfügbar, an den A Anfragen senden kann. Damit B in der Lage ist
Daten von A anzufordern, muss A auch für B als Dienst zugänglich sein.

Zwei Dienste, die miteinander kommunizieren, können von verschiedenen Unternehmen
in verschiedenen Anwendungen betrieben werden. Zum Beispiel könnte ein Dienst von einer Börse betrieben werden
der die aktuellen Aktienkurse verfolgt. Ein anderer Dienst könnte von einer
Investmentfirma betrieben werden, die die aktuellen Aktienkurse abfragt und sie zur Vorhersage zukünftiger
Aktienkurse.

Zwei Dienste, die miteinander kommunizieren, können auch Teil derselben Anwendung sein.
Anwendung sein. Die Strukturierung verschiedener Komponenten Ihrer Anwendung als separate Dienste
ermöglicht es, jede Komponente unabhängig voneinander zu entwickeln, zu testen und zu pflegen.
voneinander entwickelt, getestet und gewartet werden. Durch die Strukturierung einer Anwendung als separate Dienste erhalten Sie eine Microservice
Architektur.

Um die Microservice-Architektur in den Kontext von ML-Systemen zu stellen, stellen Sie sich vor, Sie sind
ein ML-Ingenieur, der an einem Preisoptimierungsproblem für ein Unternehmen arbeitet, das
das eine Mitfahrgelegenheit wie Lyft betreibt. In Wirklichkeit hat Lyft Hunderte von Diensten in
seiner Microservice-Architektur, aber der Einfachheit halber betrachten wir nur drei
Dienste:

Fahrer-Management-Dienst
Sagt voraus, wie viele Fahrer in der nächsten Minute in einem bestimmten Gebiet verfügbar sein werden.

Fahrtenmanagement-Dienst
Sagt voraus, wie viele Fahrten in der nächsten Minute in einem bestimmten Gebiet angefordert werden.

Preisoptimierungsdienst
Sagt den optimalen Preis für jede Fahrt voraus. Der Preis für eine Fahrt sollte so niedrig sein
niedrig genug sein, damit die Fahrer bereit sind zu zahlen, aber hoch genug, damit die Fahrer bereit sind zu
Fahrer bereit sind zu fahren und das Unternehmen einen Gewinn erzielt.

Modi des Datenflusses | 73
24 In der Praxis muss die Preisoptimierung möglicherweise nicht jedes Mal die vorhergesagte Anzahl von Fahrten/Fahrern abfragen, wenn
wenn sie eine Preisvorhersage machen muss. Es ist gängige Praxis, die zwischengespeicherte vorausgesagte Zahl der Fahrten/Fahrer zu verwenden
zu verwenden und etwa jede Minute neue Vorhersagen anzufordern.
25 Kleppmann, Designing Data-Intensive Applications.
26 Tyson Trautmann, "Debunking the Myths of RPC and REST", Ethereal Bits, 4. Dezember 2012 (Zugriff über
das Internet Archive), https://oreil.ly/4sUrL.

Da der Preis vom Angebot (den verfügbaren Fahrern) und der Nachfrage (den nachgefragten Fahrten) abhängt, benötigt der
nachgefragten Fahrten) abhängt, benötigt der Preisoptimierungsdienst Daten sowohl von der Fahrerverwaltung als auch von der
Fahrerverwaltung als auch von den Fahrtenverwaltungsdiensten. Jedes Mal, wenn ein Nutzer eine Fahrt anfordert, fragt der Preis
Preisoptimierungsdienst die voraussichtliche Anzahl der Fahrten und die voraussichtliche Anzahl der
Fahrer ab, um den optimalen Preis für diese Fahrt zu ermitteln.^24
Die gebräuchlichsten Arten von Anfragen für die Übermittlung von Daten über Netze sind REST
(Representational State Transfer) und RPC (Remote Procedure Call). Ihre detaillierte
Analyse würde den Rahmen dieses Buches sprengen, aber ein wesentlicher Unterschied besteht darin, dass REST
für Anfragen über Netzwerke entwickelt wurde, während RPC "versucht, eine Anfrage an einen
einen entfernten Netzwerkdienst so aussehen zu lassen, als würde man eine Funktion oder Methode in seiner
Programmiersprache." Aus diesem Grund "scheint REST der vorherrschende Stil
für öffentliche APIs zu sein. Das Hauptaugenmerk der RPC-Frameworks liegt auf Anfragen zwischen Diensten
die derselben Organisation gehören, typischerweise innerhalb desselben Rechenzentrums."^25
Implementierungen einer REST-Architektur werden als RESTful bezeichnet. Auch wenn viele
Obwohl viele Menschen bei REST an HTTP denken, bedeutet REST nicht unbedingt HTTP, da HTTP
nur eine Implementierung von REST ist.^26
Datenübermittlung durch Echtzeit-Transport
Um die Motivation für Echtzeittransporte zu verstehen, kehren wir zum vorangegangenen
Beispiel der Mitfahr-App mit drei einfachen Diensten zurück: Fahrerverwaltung, Fahrten
Verwaltung und Preisoptimierung. Im letzten Abschnitt haben wir besprochen, wie der Preisoptimierungs
Preisoptimierungsdienst Daten von den Fahrten- und Fahrermanagementdiensten benötigt, um
um den optimalen Preis für jede Fahrt zu ermitteln.
Stellen Sie sich nun vor, dass der Fahrermanagementdienst auch die Anzahl der
Fahrten vom Fahrtenmanagementdienst benötigt, um zu wissen, wie viele Fahrer zu mobilisieren sind. Er
will auch die vom Preisoptimierungsdienst vorhergesagten Preise kennen, um sie
um sie als Anreiz für potenzielle Fahrer zu nutzen (z. B. wenn Sie sich jetzt auf den Weg machen, können Sie
einen doppelten Preisaufschlag erhalten). In ähnlicher Weise könnte der Fahrtenmanagementdienst auch Daten
von den Diensten für Fahrermanagement und Preisoptimierung. Wenn wir Daten durch
wie im vorherigen Abschnitt beschrieben, muss jeder dieser Dienste Anfragen an die
Anforderungen an die beiden anderen Dienste senden, wie in Abbildung 3-8 dargestellt.
74 | Kapitel 3: Grundlagen der Datentechnik
Abbildung 3-8. In der anfragegesteuerten Architektur muss jeder Dienst Anfragen an zwei
andere Dienste

Mit nur drei Diensten wird die Datenübermittlung bereits kompliziert. Stellen Sie sich vor, Sie hätten
Hunderte, wenn nicht Tausende von Diensten, wie sie große Internetunternehmen haben.
Der Datenaustausch zwischen den Diensten kann zu einem Engpass werden, der das gesamte System verlangsamt.
gesamte System verlangsamen.

Die anforderungsgesteuerte Datenübermittlung ist synchron: Der Zieldienst muss auf die
Anfrage zuhören, damit die Anfrage bearbeitet werden kann. Wenn der Preisoptimierungsdienst Daten
Daten vom Fahrermanagementdienst an und der Fahrermanagementdienst ist nicht erreichbar, sendet der
Preisoptimierungsdienst die Anfrage so lange erneut, bis die Zeit abgelaufen ist. Und wenn
der Preisoptimierungsdienst ausfällt, bevor er eine Antwort erhält, geht die Antwort
verloren gehen. Ein ausgefallener Dienst kann dazu führen, dass alle Dienste, die Daten von ihm benötigen, ausfallen
ausfallen.

Was wäre, wenn es einen Broker gäbe, der den Datenaustausch zwischen den Diensten koordiniert? Anstatt dass
Dienste Daten direkt voneinander anzufordern und ein Netz komplexer
Datenweitergabe zwischen den Diensten zu schaffen, muss jeder Dienst nur mit dem Broker kommunizieren, wie
in Abbildung 3-9 dargestellt. Anstatt beispielsweise andere Dienste die Fahrerverwaltungsdienste
die voraussichtliche Anzahl der Fahrer für die nächste Minute anzufordern, was wäre, wenn
wenn jedes Mal, wenn der Fahrermanagementdienst eine Vorhersage macht, diese Vorhersage
an einen Makler übermittelt? Welcher Dienst auch immer Daten vom Fahrermanagementdienst wünscht
Dienstes wünscht, kann bei diesem Makler die letzte Vorhersage über die Anzahl der Fahrer abfragen.
Ähnlich verhält es sich, wenn der Preisoptimierungsdienst eine Vorhersage über den Preisanstieg
Gebühr für die nächste Minute macht, wird diese Vorhersage an den Makler übermittelt.

Modi des Datenflusses | 75
Abbildung 3-9. Bei einem Broker muss ein Dienst nur mit dem Broker kommunizieren und nicht
mit anderen Diensten

Technisch gesehen kann eine Datenbank ein Broker sein - jeder Dienst kann Daten in eine Datenbank schreiben
und andere Dienste, die diese Daten benötigen, können sie aus dieser Datenbank lesen. Wie jedoch
Abschnitt "Datenweitergabe über Datenbanken" auf Seite 72 erwähnt, sind Lesen und
Schreiben aus Datenbanken zu langsam für Anwendungen mit strengen Latenzanforderungen.
Statt Datenbanken für die Vermittlung von Daten zu verwenden, nutzen wir In-Memory-Speicher für die Vermittlung von Daten.
Echtzeittransporte können als In-Memory-Speicher für die Datenübermittlung zwischen
Diensten.

Ein Datenpaket, das an einen Echtzeittransport gesendet wird, wird als Ereignis bezeichnet. Diese Architektur
wird daher auch als ereignisgesteuert bezeichnet. Ein Echtzeittransport wird manchmal auch als
Ereignis-Bus genannt.

Die anforderungsgesteuerte Architektur eignet sich gut für Systeme, die sich mehr auf Logik als auf
Daten. Eine ereignisgesteuerte Architektur eignet sich besser für Systeme, die datenlastig sind.

Die beiden gängigsten Arten von Echtzeittransporten sind Pubsub, eine Abkürzung für
publish-subscribe, und die Nachrichtenwarteschlange. Im Pubsub-Modell kann jeder Dienst
an verschiedene Topics in einem Echtzeittransport veröffentlichen, und jeder Dienst, der ein Topic abonniert
kann alle Ereignisse in diesem Thema lesen. Die Dienste, die Daten produzieren, kümmern sich nicht darum
welche Dienste ihre Daten konsumieren. Pubsub-Lösungen verfügen häufig über eine Aufbewahrungsrichtlinie -
Die Daten werden für eine bestimmte Zeit (z. B. sieben Tage) im Echtzeittransport aufbewahrt, bevor sie gelöscht oder in den
Tage) aufbewahrt, bevor sie gelöscht oder in einen permanenten Speicher (wie Amazon S3) verschoben werden. Siehe .
Abbildung 3-10.

76 | Kapitel 3: Grundlagen der Datentechnik

27 Wenn Sie mehr über die Funktionsweise von Apache Kafka erfahren möchten, bietet Mitch Seymour eine großartige Animation zur Erklärung
mit Ottern!

Abbildung 3-10. Eingehende Ereignisse werden im Arbeitsspeicher gespeichert, bevor sie verworfen oder
in einen permanenten Speicher verschoben werden
In einem Modell mit Nachrichtenwarteschlangen hat ein Ereignis oft bestimmte Verbraucher (ein Ereignis mit
(ein Ereignis mit vorgesehenen Verbrauchern wird als Nachricht bezeichnet), und die Nachrichtenwarteschlange ist dafür verantwortlich
die Nachricht zu den richtigen Verbrauchern zu bringen.
Beispiele für Pubsub-Lösungen sind Apache Kafka und Amazon Kinesis.^27 Beispiele für
Nachrichten-Warteschlangen sind Apache RocketMQ und RabbitMQ. Beide Paradigmen haben in den letzten
in den letzten Jahren stark an Bedeutung gewonnen. Abbildung 3-11 zeigt einige der Unternehmen, die
Apache Kafka und RabbitMQ verwenden.
Abbildung 3-11. Unternehmen, die Apache Kafka und RabbitMQ verwenden. Quelle: Screenshot
von Stackshare
Modi des Datenflusses | 77
Batch-Verarbeitung vs. Stream-Verarbeitung
Sobald Ihre Daten in Datenspeichern wie Datenbanken, Data Lakes oder Data Warehouses ankommen
Warehouses ankommen, werden sie zu historischen Daten. Dies steht im Gegensatz zu Streaming-Daten (Daten, die
noch einströmen). Historische Daten werden oft in Batch-Jobs verarbeitet - Jobs, die
in regelmäßigen Abständen gestartet werden. Zum Beispiel könnten Sie einmal am Tag einen Batch-Job starten
Job starten, um den durchschnittlichen Preisaufschlag für alle Fahrten des letzten Tages zu berechnen.

Wenn Daten in Batch-Jobs verarbeitet werden, spricht man von Batch-Verarbeitung. Die Stapelverarbeitung
ist seit vielen Jahrzehnten ein Forschungsthema, und Unternehmen haben verteilte
verteilte Systeme wie MapReduce und Spark entwickelt, um Stapeldaten effizient zu verarbeiten.

Wenn Sie Daten in Echtzeit-Transporten wie Apache Kafka und Amazon Kinesis haben,
sprechen wir von Streaming-Daten. Stream-Processing bezieht sich auf die Ausführung von Berechnungen
auf Streaming-Daten. Berechnungen auf Streaming-Daten können auch periodisch ausgelöst werden.
Die Zeiträume sind jedoch in der Regel viel kürzer als die Zeiträume für Batch-Jobs (z.B.,
alle fünf Minuten statt jeden Tag). Berechnungen mit Streaming-Daten können auch
gestartet werden, wenn der Bedarf besteht. Zum Beispiel, wenn ein Benutzer eine Fahrt anfordert,
Sie verarbeiten Ihren Datenstrom, um zu sehen, welche Fahrer derzeit verfügbar sind.

Die Stream-Verarbeitung kann, wenn sie richtig gemacht wird, eine niedrige Latenzzeit bieten, da Sie Daten verarbeiten können
Daten verarbeitet werden können, sobald sie erzeugt werden, ohne dass sie erst in Datenbanken geschrieben werden müssen. Viele
glauben, dass die Stream-Verarbeitung weniger effizient ist als die Stapelverarbeitung, weil
man Tools wie MapReduce oder Spark nicht nutzen kann. Das ist nicht immer der Fall, und zwar aus
zwei Gründe. Erstens haben sich Streaming-Technologien wie Apache Flink als hoch
skalierbar und vollständig verteilt, was bedeutet, dass sie Berechnungen parallel durchführen können.
Zweitens liegt die Stärke der Stream-Verarbeitung in der zustandsorientierten Berechnung. Betrachten Sie den
Fall, dass Sie das Engagement der Nutzer während einer 30-tägigen Testphase verarbeiten möchten. Wenn Sie
Wenn Sie diesen Stapelverarbeitungsauftrag jeden Tag starten, müssen Sie jeden Tag Berechnungen über die letzten 30 Tage durchführen.
Tag durchführen. Mit Stream Processing ist es möglich, nur die neuen Daten zu berechnen
jeden Tag zu berechnen und die Berechnung der neuen Daten mit der Berechnung der älteren Daten zu verbinden,
Redundanz zu vermeiden.

Da die Stapelverarbeitung viel seltener stattfindet als die Stromverarbeitung, wird die
ML wird die Stapelverarbeitung in der Regel zur Berechnung von Merkmalen verwendet, die sich seltener ändern, wie
wie z. B. die Bewertungen von Fahrern (wenn ein Fahrer Hunderte von Fahrten absolviert hat, ist es unwahrscheinlich, dass sich seine Bewertung
sich von einem Tag auf den anderen erheblich ändern). Batch-Merkmale - durch Stapelverarbeitung extrahierte Merkmale
durch Stapelverarbeitung extrahiert werden, werden auch als statische Merkmale bezeichnet.

Stream Processing wird verwendet, um Merkmale zu berechnen, die sich schnell ändern, z. B. wie
wie viele Fahrer im Moment verfügbar sind, wie viele Fahrten in der letzten Minute angefordert wurden
Minute angefordert wurden, wie viele Fahrten in den nächsten zwei Minuten beendet werden, der Medianpreis
der letzten 10 Fahrten in diesem Gebiet, usw. Merkmale über den aktuellen Zustand des Systems
sind wichtig, um optimale Preisvorhersagen treffen zu können. Streaming-Merkmale -
Merkmale, die durch die Verarbeitung von Datenströmen extrahiert werden, sind auch als dynamische Merkmale bekannt.

78 | Kapitel 3: Grundlagen der Datentechnik

28 Kostas Tzoumas, "Batch Is a Special Case of Streaming", Ververica, September 15, 2015, https://oreil.ly/IcIl2.

Für viele Probleme benötigen Sie nicht nur Batch- oder Streaming-Funktionen, sondern beides.
Sie brauchen eine Infrastruktur, die es Ihnen ermöglicht, sowohl Streaming-Daten als auch Stapeldaten zu verarbeiten
Daten zu verarbeiten und sie miteinander zu verbinden, um sie in Ihre ML-Modelle einzuspeisen. Wir werden mehr darüber diskutieren, wie
Batch-Features und Streaming-Features zusammen verwendet werden können, um Vorhersagen zu generieren, wird in
Kapitel 7.
Um Berechnungen mit Datenströmen durchzuführen, benötigen Sie eine Stream-Computation-Engine (so wie
so wie Spark und MapReduce Stapelverarbeitungsmaschinen sind). Für einfache Streaming
Berechnungen können Sie vielleicht mit der eingebauten Stream-Berechnungsleistung
Kapazität von Echtzeittransporten wie Apache Kafka auskommen, aber die Kafka-Stream-Verarbeitung
Kafka-Stream-Verarbeitung ist jedoch nur begrenzt in der Lage, mit verschiedenen Datenquellen umzugehen.
Bei ML-Systemen, die Streaming-Funktionen nutzen, sind die Streaming-Berechnungen
selten einfach. Die Anzahl der Streaming-Funktionen, die in einer Anwendung wie der Betrugs
Betrugserkennung und Kreditwürdigkeitsprüfung kann in die Hunderte, wenn nicht Tausende gehen. Die Stream
Extraktionslogik kann komplexe Abfragen mit Verknüpfung und Aggregation entlang
verschiedenen Dimensionen. Um diese Merkmale zu extrahieren, sind effiziente Stream Processing
Engines. Zu diesem Zweck sollten Sie sich mit Tools wie Apache Flink, KSQL,
und Spark Streaming. Von diesen drei Engines sind Apache Flink und KSQL
in der Branche anerkannt und bieten eine schöne SQL-Abstraktion für Datenwissenschaftler.
Die Stream-Verarbeitung ist schwieriger, da die Datenmenge unbegrenzt ist und die
Daten mit unterschiedlichen Raten und Geschwindigkeiten eintreffen. Es ist einfacher, einen Stream-Prozessor
Stapelverarbeitung zu machen als einen Stapelprozessor für die Stromverarbeitung. Apache
Flink-Maintainer argumentieren seit Jahren, dass die Stapelverarbeitung ein Spezialfall der
Fall von Stromverarbeitung ist.^28
Zusammenfassung
Dieses Kapitel baut auf den Grundlagen auf, die in Kapitel 2 über die Wichtigkeit von Daten bei der Entwicklung von ML-Systemen gelegt wurden.
Bedeutung von Daten bei der Entwicklung von ML-Systemen. In diesem Kapitel haben wir gelernt, dass es wichtig ist
dass es wichtig ist, das richtige Format für die Speicherung unserer Daten zu wählen, damit sie in
Zukunft zu erleichtern. Wir haben verschiedene Datenformate und die Vor- und Nachteile von zeilenmajorierten
gegenüber spaltenbetonten Formaten sowie Text- gegenüber Binärformaten.
Wir befassten uns weiterhin mit den drei wichtigsten Datenmodellen: relationales Modell, Dokumentenmodell und Graph.
Obwohl das relationale Modell aufgrund der Popularität von SQL das bekannteste ist, werden alle drei Modelle häufig verwendet.
SQL am bekanntesten ist, sind alle drei Modelle heute weit verbreitet, und jedes eignet sich für eine bestimmte
Aufgaben.
Wenn man über das relationale Modell im Vergleich zum Dokumentenmodell spricht, denken viele
ersteres als strukturiert und letzteres als unstrukturiert angesehen. Die Unterteilung
Zusammenfassung | 79
zwischen strukturierten und unstrukturierten Daten ist recht fließend - die Hauptfrage ist, wer
wer die Verantwortung für die Strukturierung der Daten zu übernehmen hat. Strukturierte Daten
bedeutet, dass der Code, der die Daten schreibt, die Struktur übernehmen muss. Unstrukturierte
Daten bedeutet, dass der Code, der die Daten liest, die Struktur übernehmen muss.

Wir setzten das Kapitel mit Datenspeichermaschinen und Datenverarbeitung fort. Wir untersuchten Daten-
Datenbanken, die für zwei verschiedene Arten der Datenverarbeitung optimiert sind: transaktionale Verarbeitung
und analytische Verarbeitung. Wir haben Datenspeichermotoren und Datenverarbeitung zusammen untersucht
denn traditionell ist die Speicherung mit der Verarbeitung gekoppelt: transaktionale Datenbanken für
Transaktionsdatenbanken für die transaktionale Verarbeitung und analytische Datenbanken für die analytische Verarbeitung. Allerdings,
In den letzten Jahren haben jedoch viele Anbieter an der Entkopplung von Speicherung und Verarbeitung gearbeitet.
Heute gibt es transaktionale Datenbanken, die analytische Abfragen verarbeiten können, und analytische
cal-Datenbanken, die transaktionale Abfragen verarbeiten können.

Bei der Erörterung von Datenformaten, Datenmodellen, Datenspeicher-Engines und der Verarbeitung,
wird davon ausgegangen, dass sich die Daten innerhalb eines Prozesses befinden. Wenn Sie jedoch in der Produktion arbeiten, werden Sie
mit mehreren Prozessen arbeiten und müssen wahrscheinlich Daten zwischen ihnen übertragen
ihnen übertragen. Wir haben drei Arten der Datenübergabe besprochen. Der einfachste Modus ist die Weitergabe
durch Datenbanken. Die beliebteste Art der Datenweitergabe für Prozesse ist die Daten
Weitergabe durch Dienste. In diesem Modus wird ein Prozess als Dienst dargestellt, an den ein anderer
Prozess Anfragen nach Daten senden kann. Dieser Modus der Datenweitergabe ist eng gekoppelt mit
Microservice-Architekturen gekoppelt, bei denen jede Komponente einer Anwendung als
Dienst eingerichtet ist.

Eine Art der Datenübermittlung, die in den letzten zehn Jahren immer beliebter geworden ist, ist
die Datenübermittlung über einen Echtzeittransport wie Apache Kafka und RabbitMQ. Diese
Art der Datenweitergabe liegt irgendwo zwischen der Weitergabe über Datenbanken und der Weitergabe
durch Dienste: Sie ermöglicht eine asynchrone Datenübermittlung mit einer relativ geringen Latenzzeit.

Da Daten in Echtzeittransporten andere Eigenschaften haben als Daten in Datenbanken, erfordern sie
Daten in Echtzeittransporten andere Eigenschaften haben als Daten in Datenbanken, erfordern sie andere Verarbeitungstechniken, wie im Abschnitt "Stapelverarbeitung
Versus Stream Processing" auf Seite 78 beschrieben. Daten in Datenbanken werden oft in Batch-Jobs verarbeitet
Batch-Jobs verarbeitet und erzeugen statische Merkmale, während Daten in Echtzeittransporten oft
Daten in Echtzeittransporten oft mit Stream Computing Engines verarbeitet werden und dynamische Merkmale erzeugen. Einige
argumentieren, dass die Batch-Verarbeitung ein Spezialfall der Stream-Verarbeitung ist, und Stream
Computation Engines können zur Vereinheitlichung beider Verarbeitungspipelines verwendet werden.

Sobald wir unsere Datensysteme im Griff haben, können wir Daten sammeln und Trainingsdaten erstellen.
Daten erstellen, die im Mittelpunkt des nächsten Kapitels stehen werden.

80 | Kapitel 3: Grundlagen der Datentechnik

KAPITEL 4

Daten trainieren
In Kapitel 3 haben wir den Umgang mit Daten aus der Systemperspektive behandelt. In diesem
Kapitel gehen wir auf den Umgang mit Daten aus der Perspektive der Datenwissenschaft ein. Trotz
Trotz der Bedeutung von Trainingsdaten für die Entwicklung und Verbesserung von ML-Modellen sind ML-Lehrpläne
Lehrpläne stark auf die Modellierung ausgerichtet, die von vielen Praktikern als der
den "spaßigen" Teil des Prozesses. Die Entwicklung eines modernen Modells ist interessant. Verbringen Sie -
Tage damit zu verbringen, sich mit einer riesigen Menge schlecht formatierter Daten herumzuschlagen, die nicht einmal
nicht einmal in den Speicher Ihres Rechners passen, ist frustrierend.

Daten sind unübersichtlich, komplex, unberechenbar und potenziell tückisch. Wenn sie nicht
nicht richtig gehandhabt werden, können sie leicht Ihr gesamtes ML-Vorhaben zum Scheitern bringen. Aber genau das ist der Grund
warum Datenwissenschaftler und ML-Ingenieure lernen sollten, wie man mit Daten richtig umgeht.
Zeit und Kopfschmerzen zu sparen.

In diesem Kapitel werden wir uns mit Techniken befassen, um gute Trainingsdaten zu erhalten oder zu erstellen.
Unter Trainingsdaten werden in diesem Kapitel alle Daten verstanden, die in der Entwicklungsphase
von ML-Modellen verwendet werden, einschließlich der verschiedenen Splits, die für Training, Validierung und Test
(Train-, Validierungs- und Test-Splits). Dieses Kapitel beginnt mit verschiedenen Stichprobentechniken
quen zur Auswahl von Daten für das Training. Dann werden wir uns mit den üblichen Herausforderungen bei der Erstellung von
Trainingsdaten, einschließlich des Problems der Label-Multiplikation, des Problems des Mangels an Labels, des
Klassenungleichgewichtsproblem und Techniken zur Datenerweiterung, um das Problem der fehlenden
Datenproblem.

Wir verwenden den Begriff "Trainingsdaten" anstelle von "Trainingsdatensatz", weil "Datensatz" eine Menge bezeichnet
einen Satz, der endlich und stationär ist. Daten in der Produktion sind weder endlich noch stationär, ein
Dieses Phänomen wird im Abschnitt "Verschiebung der Datenverteilung" auf Seite 237 behandelt.
Wie andere Schritte beim Aufbau von ML-Systemen ist auch die Erstellung von Trainingsdaten ein iterativer Prozess.
Da sich Ihr Modell im Laufe des Projektlebenszyklus weiterentwickelt, werden sich wahrscheinlich auch Ihre Trainingsdaten
weiterentwickeln.

81
1 Einige Leser könnten einwenden, dass dieser Ansatz bei großen Modellen nicht funktioniert, da bestimmte große Modelle nicht
für kleine Datensätze nicht funktionieren, aber mit viel mehr Daten gut funktionieren. In diesem Fall ist es dennoch wichtig, mit
Datensätzen unterschiedlicher Größe zu experimentieren, um herauszufinden, wie sich die Größe des Datensatzes auf Ihr Modell auswirkt.
Bevor wir weitermachen, möchte ich noch ein Wort der Vorsicht anbringen, das schon oft gesagt wurde
oft gesagt wurde, aber immer noch nicht genug ist. Daten sind voller potenzieller Verzerrungen. Diese Verzerrungen
haben viele mögliche Ursachen. Es gibt Verzerrungen, die bei der Erhebung, der Probenahme oder der
Beschriftung. Historische Daten können mit menschlichen Verzerrungen behaftet sein, und ML-Modelle,
die auf diesen Daten trainiert wurden, können diese Vorurteile noch verstärken. Nutzen Sie Daten, aber vertrauen Sie ihnen nicht zu sehr!

Probenahme
Sampling ist ein integraler Bestandteil des ML-Arbeitsablaufs, der in typischen ML-Kursen leider oft übersehen wird.
in typischen ML-Kursen übersehen wird. Sampling findet in vielen Schritten des Lebenszyklus eines ML-Projekts
Lebenszyklus, wie z.B. Sampling aus allen möglichen realen Daten, um Trainingsdaten zu erstellen;
Stichproben aus einem gegebenen Datensatz, um Splits für Training, Validierung und Tests zu erstellen; oder
Stichproben aus allen möglichen Ereignissen, die in Ihrem ML-System auftreten, zur Überwachung
Zwecke. In diesem Abschnitt werden wir uns auf Sampling-Methoden zur Erstellung von Trainingsdaten konzentrieren,
aber diese Sampling-Methoden können auch für andere Schritte im Lebenszyklus eines ML-Projekts verwendet werden.
Lebenszyklus.

In vielen Fällen ist eine Stichprobe erforderlich. Zum Beispiel, wenn man keinen Zugang zu allen
Daten in der realen Welt haben, sind die Daten, die Sie zum Trainieren Ihres Modells verwenden, eine Teilmenge
der realen Daten, die durch die eine oder andere Stichprobenmethode erstellt wurde. Ein anderer Fall ist, wenn
wenn es nicht möglich ist, alle Daten zu verarbeiten, auf die man Zugriff hat, weil es zu viel Zeit oder Ressourcen erfordert.
zu viel Zeit oder Ressourcen erfordert, so dass Sie eine Stichprobe dieser Daten erstellen müssen, um eine Teilmenge zu
die man verarbeiten kann. In vielen anderen Fällen ist eine Stichprobe hilfreich, da sie es ermöglicht
eine Aufgabe schneller und kostengünstiger zu bewältigen. Wenn Sie zum Beispiel ein neues Modell in Betracht ziehen,
ein schnelles Experiment mit einer kleinen Teilmenge Ihrer Daten durchführen, um zu sehen, ob das
ob das neue Modell vielversprechend ist, bevor Sie es mit Ihren gesamten Daten trainieren.^1

Ein Verständnis der verschiedenen Stichprobenverfahren und ihrer Verwendung in unserem
Arbeitsablaufs kann uns erstens dabei helfen, potenzielle Stichprobenfehler zu vermeiden, und zweitens bei der
die Methoden auszuwählen, die die Effizienz der von uns entnommenen Daten verbessern.

Es gibt zwei Arten von Stichproben: Nichtwahrscheinlichkeitsstichproben und Zufallsstichproben.
Wir beginnen mit den Methoden der Nichtwahrscheinlichkeitsstichproben, gefolgt von mehreren gängigen
dom-Stichprobenverfahren.

82 | Kapitel 4: Trainingsdaten

2 James J. Heckman, "Sample Selection Bias as a Specification Error", Econometrica 47, Nr. 1 (Januar 1979):
153-61, https://oreil.ly/I5AhM.
Nichtwahrscheinlichkeitsstichproben
Von einer Nichtwahrscheinlichkeitsstichprobe spricht man, wenn die Auswahl der Daten nicht auf der Grundlage von Wahrscheinlichkeit
Kriterien beruht. Hier sind einige der Kriterien für Nichtwahrscheinlichkeitsstichproben:

Zufallsstichproben
Stichproben von Daten werden auf der Grundlage ihrer Verfügbarkeit ausgewählt. Diese Stichprobenmethode ist
beliebt, weil sie, nun ja, bequem ist.

Schneeballsystem
Künftige Stichproben werden auf der Grundlage bestehender Stichproben ausgewählt. Zum Beispiel, um
legitimen Twitter-Konten ohne Zugang zu Twitter-Datenbanken, beginnt man
man mit einer kleinen Anzahl von Konten und scrappt dann alle Konten, denen sie folgen,
und so weiter.

Urteilsfähige Stichproben
Experten entscheiden, welche Stichproben einbezogen werden sollen.

Quoten-Stichproben
Sie wählen Stichproben auf der Grundlage von Quoten für bestimmte Datenabschnitte ohne Zufallsauswahl.
ierung. Zum Beispiel könnten Sie bei einer Umfrage 100 Antworten aus jeder
Altersgruppen: unter 30 Jahren, zwischen 30 und 60 Jahren und über 60 Jahren
über 60 Jahre alt, unabhängig von der tatsächlichen Altersverteilung.

Die nach Nicht-Wahrscheinlichkeitskriterien ausgewählten Stichproben sind nicht repräsentativ für die realen
Weltdaten und sind daher mit Auswahlfehlern behaftet.^2 Aufgrund dieser Fehler,
könnte man meinen, dass es eine schlechte Idee ist, Daten für das Training von ML-Modellen mit dieser Familie von
Stichprobenverfahren zu trainieren. Da haben Sie recht. Leider ist die Auswahl von Daten für ML-Modelle in vielen Fällen
für ML-Modelle immer noch aus Bequemlichkeit getroffen.

Ein Beispiel für diese Fälle ist die Sprachmodellierung. Sprachmodelle werden oft trainiert
nicht mit Daten trainiert, die repräsentativ für alle möglichen Texte sind, sondern mit Daten, die leicht zu sammeln sind
gesammelt werden können - Wikipedia, Common Crawl, Reddit.

Ein weiteres Beispiel sind Daten zur Stimmungsanalyse von allgemeinen Texten. Ein Großteil dieser Daten
werden aus Quellen mit natürlichen Bezeichnungen (Bewertungen) gesammelt, wie z. B. IMDB-Bewertungen und
Amazon-Bewertungen. Diese Datensätze werden dann für andere Aufgaben der Stimmungsanalyse verwendet.
IMDB-Rezensionen und Amazon-Rezensionen sind voreingenommen gegenüber Benutzern, die bereit sind
und sind nicht unbedingt repräsentativ für Menschen, die keinen Zugang zum Internet haben
die keinen Zugang zum Internet haben oder die nicht bereit sind, Bewertungen online zu stellen.

Stichprobe | 83
3 Rachel Lerman, "Google Is Testing Its Self-Driving Car in Kirkland", Seattle Times, 3. Februar 2016,
https://oreil.ly/3IA1V.
4 Population bezieht sich hier auf eine "statistische Population", eine (potenziell unendliche) Menge aller möglichen Stichproben, die
Stichproben.
Ein drittes Beispiel sind Daten für das Training selbstfahrender Autos. Ursprünglich stammten die Daten, die für
selbstfahrende Autos gesammelt wurden, stammten hauptsächlich aus zwei Gebieten: Phoenix, Arizona (wegen seiner laxen
Vorschriften) und der Bay Area in Kalifornien (weil hier viele Unternehmen, die
selbstfahrende Autos bauen, hier ansässig sind). In beiden Gebieten herrscht in der Regel sonniges Wetter. Im Jahr 2016,
Waymo seinen Betrieb nach Kirkland, Washington, ausgeweitet, speziell wegen des
Regenwetter in Kirkland,^3 aber es gibt immer noch viel mehr Daten von selbstfahrenden Autos bei sonnigem Wetter als
für regnerisches oder verschneites Wetter.

Nichtwahrscheinlichkeitsstichproben können eine schnelle und einfache Möglichkeit sein, erste Daten zu sammeln, um
Ihr Projekt auf den Weg zu bringen. Für zuverlässige Modelle sollten Sie jedoch eine
Wahrscheinlichkeitsstichproben verwenden, die wir als nächstes behandeln werden.

Einfache Zufallsstichprobe
Bei der einfachsten Form der Zufallsstichprobe geben Sie allen Stichproben in der Grundgesamtheit
^4 Man wählt zum Beispiel 10 % der Grundgesamtheit nach dem Zufallsprinzip aus.
Grundgesamtheit nach dem Zufallsprinzip aus, so dass alle Mitglieder dieser Grundgesamtheit die gleiche 10%ige Chance haben, ausgewählt zu werden.
ausgewählt zu werden.

Der Vorteil dieser Methode ist, dass sie einfach zu implementieren ist. Der Nachteil ist, dass
seltene Datenkategorien möglicherweise nicht in Ihrer Auswahl erscheinen. Betrachten Sie den Fall, dass
eine Klasse nur in 0,01 % Ihrer Datenpopulation vorkommt. Wenn Sie zufällig 1 % Ihrer Daten auswählen
Ihrer Daten auswählen, ist es unwahrscheinlich, dass Proben dieser seltenen Klasse ausgewählt werden. Auf diese Auswahl trainierte Modelle
Auswahl trainiert wurden, könnten denken, dass diese seltene Klasse nicht existiert.

Geschichtete Stichproben
Um den Nachteil einer einfachen Zufallsstichprobe zu vermeiden, können Sie Ihre Population zunächst in
in die für Sie wichtigen Gruppen unterteilen und aus jeder Gruppe eine separate Stichprobe ziehen. Für
Beispiel: Um 1 % der Daten mit den zwei Klassen A und B zu beproben, können Sie 1 % der Klasse A und 1 % der Klasse B beproben.
Klasse A und 1 % der Klasse B. Auf diese Weise stellen Sie sicher, dass unabhängig davon, wie selten Klasse A oder B ist
dass Stichproben aus dieser Klasse in die Auswahl einbezogen werden. Jede Gruppe wird als Schicht bezeichnet,
und diese Methode wird als geschichtete Stichprobe bezeichnet.

84 | Kapitel 4: Trainingsdaten

5 Multilabel-Aufgaben sind Aufgaben, bei denen ein Beispiel mehrere Labels haben kann.
Ein Nachteil dieser Stichprobenmethode ist, dass sie nicht immer möglich ist, z. B. wenn
es unmöglich ist, alle Proben in Gruppen aufzuteilen. Dies ist besonders schwierig, wenn
eine Probe zu mehreren Gruppen gehören kann, wie im Fall von Multilabel-Aufgaben.^5
kann eine Stichprobe beispielsweise sowohl der Klasse A als auch der Klasse B angehören.

Gewichtete Stichprobe
Bei der gewichteten Stichprobe wird jeder Stichprobe ein Gewicht zugewiesen, das die Wahrscheinlichkeit
Wahrscheinlichkeit bestimmt, dass sie ausgewählt wird. Wenn Sie beispielsweise drei Stichproben, A, B und C, haben und
mit einer Wahrscheinlichkeit von 50 %, 30 % bzw. 20 % ausgewählt werden sollen, können Sie
Sie können ihnen die Gewichte 0,5, 0,3 und 0,2 geben.

Diese Methode ermöglicht es Ihnen, Ihr Fachwissen zu nutzen. Zum Beispiel, wenn Sie wissen, dass
Sie wissen, dass eine bestimmte Teilpopulation von Daten, z. B. neuere Daten, für Ihr Modell wertvoller ist
Modell wertvoller ist und eine größere Chance haben soll, ausgewählt zu werden, können Sie ihr ein höheres
gewichten.

Dies ist auch dann hilfreich, wenn die Daten aus einer anderen Verteilung stammen als die
tion im Vergleich zu den echten Daten stammen. Wenn zum Beispiel in Ihren Daten rote Proben zu
25% und blaue Stichproben 75% ausmachen, Sie aber wissen, dass in der realen Welt Rot und Blau die gleiche Wahrscheinlichkeit haben
gleiche Wahrscheinlichkeit haben, können Sie rote Stichproben dreimal höher gewichten
höher gewichten als blaue Stichproben.

In Python können Sie gewichtete Stichproben mit random.choices wie folgt durchführen:

# Wählen Sie zwei Elemente aus der Liste, so dass 1, 2, 3, 4 jeweils eine
# 20 % Chance hat, ausgewählt zu werden, während 100 und 1000 jeweils nur eine 10 %ige Chance haben.
importiere random
random.choices(population=[1, 2, 3, 4, 100, 1000],
gewichte=[0,2, 0,2, 0,2, 0,2, 0,1, 0,1],
k=2)
# Dies ist gleichbedeutend mit folgendem
random.choices(population=[1, 1, 2, 2, 3, 3, 4, 4, 100, 1000],
k=2)
Ein gängiges Konzept in der ML, das eng mit der gewichteten Stichprobenziehung zusammenhängt, sind Stichproben
Gewichte. Gewichtetes Sampling wird verwendet, um Stichproben zum Trainieren des Modells auszuwählen,
während die Gewichte der Stichproben dazu dienen, den Trainingsstichproben "Gewichte" oder "Wichtigkeit" zuzuweisen
Proben. Stichproben mit höheren Gewichten wirken sich stärker auf die Verlustfunktion aus. Das Ändern von Stichproben
Gewichte können die Entscheidungsgrenzen Ihres Modells erheblich verändern, wie in
Abbildung 4-1.

Probenahme | 85
6 "SVM: Weighted Samples," scikit-learn, https://oreil.ly/BDqbk.
Abbildung 4-1. Stichprobengewichte können die Entscheidungsgrenze beeinflussen. Links ist zu sehen, wenn alle
Stichproben gleich gewichtet sind. Rechts ist zu sehen, wenn die Stichproben unterschiedlich
gewichtet werden. Quelle: scikit-learn^6

Reservoir Sampling
Reservoir Sampling ist ein faszinierender Algorithmus, der besonders nützlich ist, wenn man
mit Streaming-Daten zu tun haben, was in der Regel in der Produktion der Fall ist.

Stellen Sie sich vor, Sie haben einen eingehenden Strom von Tweets und möchten eine bestimmte
Anzahl, k, von Tweets für die Analyse oder das Training eines Modells. Sie wissen nicht, wie viele
Sie wissen nicht, wie viele Tweets es sind, aber Sie wissen, dass Sie nicht alle im Speicher unterbringen können, was bedeutet
Sie wissen also nicht im Voraus, mit welcher Wahrscheinlichkeit ein Tweet ausgewählt werden sollte. Sie wollen
das sicherstellen:

-Jeder Tweet hat die gleiche Wahrscheinlichkeit, ausgewählt zu werden.
-Sie können den Algorithmus jederzeit stoppen und die Tweets werden mit der
richtigen Wahrscheinlichkeit.
Eine Lösung für dieses Problem ist das Reservoir Sampling. Der Algorithmus umfasst ein Reservoir
Der Algorithmus umfasst ein Reservoir, das ein Array sein kann, und besteht aus drei Schritten:

1.1. Lege die ersten k Elemente in das Reservoir.
2.2. Für jedes eingehende n-te Element wird eine Zufallszahl i generiert, so dass 1 ≤ i ≤ n.
3.3. Wenn 1 ≤ i ≤ k: Ersetze das i-te Element im Vorratsbehälter durch das n-te Element. Andernfalls
nichts.
86 | Kapitel 4: Trainingsdaten

Das bedeutet, dass jedes eingehende n-te Element mit einer Wahrscheinlichkeit von kn in der Reser-

voir. Sie können auch nachweisen, dass jedes Element im Reservoir eine kn-Wahrscheinlichkeit von

vorhanden ist. Dies bedeutet, dass alle Proben die gleiche Chance haben, ausgewählt zu werden. Wenn
wir den Algorithmus zu irgendeinem Zeitpunkt anhalten, wurden alle Proben im Reservoir
mit der richtigen Wahrscheinlichkeit. Abbildung 4-2 zeigt ein anschauliches Beispiel dafür, wie das Reservoir
Probenahme funktioniert.

Abbildung 4-2. Veranschaulichung der Funktionsweise der Lagerstättenprobenahme

Wichtigkeitsstichproben
Importance Sampling ist eine der wichtigsten Stichprobenmethoden, nicht nur in der
ML. Sie ermöglicht es uns, aus einer Verteilung eine Stichprobe zu ziehen, wenn wir nur Zugang zu einer anderen
Verteilung.

Stellen Sie sich vor, Sie müssen eine Stichprobe von x aus einer Verteilung P(x) ziehen, aber P(x) ist sehr teuer,
langsam oder undurchführbar. Sie haben jedoch eine Verteilung Q(x), die viel
einfacher zu stichprobenartig zu erfassen ist. Sie ziehen also eine Stichprobe von x aus Q(x) und gewichten diese Stichprobe mit
Px
Qx. Q(x) wird als Vorschlagsverteilung oder Wichtigkeitsverteilung bezeichnet. Q(x) kann
eine beliebige Verteilung sein, solange Q(x) > 0 ist, wenn P(x) ≠ 0. Die folgende Gleichung
zeigt, dass das aus P(x) entnommene x erwartungsgemäß gleich dem aus Q(x) entnommenen x ist

gewichtet mit PQxx:

EPx x =∑
x
Pxx=∑
x
QxxPx
Qx
=EQx xPx
Qx
Ein Beispiel für die Verwendung von Wichtigkeitssampling in ML ist das richtlinienbasierte Verstärkungslernen.
ment-Lernen. Nehmen wir den Fall an, dass Sie Ihre Strategie aktualisieren möchten. Sie möchten
Sie möchten die Wertfunktionen der neuen Strategie schätzen, aber die Berechnung der Gesamtbelohnungen für
aber die Berechnung der Gesamtbelohnungen für eine Aktion kann kostspielig sein, da alle möglichen Ergebnisse berücksichtigt werden müssen
bis zum Ende des Zeithorizonts nach dieser Aktion. Wenn jedoch die neue Politik
relativ nahe an der alten Strategie liegt, können Sie die Gesamtbelohnungen auf der Grundlage der alten
Politik berechnen und sie entsprechend der neuen Politik neu gewichten. Die Belohnungen aus der
alten Richtlinie bilden die Vorschlagsverteilung.

Stichprobenprüfung | 87
7 Xiaojin Zhu, "Semi-Supervised Learning with Graphs" (Dissertation, Carnegie Mellon University, 2005),
https://oreil.ly/VYy4C.
Kennzeichnung
Trotz der vielversprechenden Möglichkeiten der unüberwachten ML sind die meisten ML-Modelle in der Produktion heute
überwacht, was bedeutet, dass sie zum Lernen markierte Daten benötigen. Die Leistung
eines ML-Modells hängt immer noch stark von der Qualität und Quantität der gelabelten Daten ab
mit denen es trainiert wird.

In einem Vortrag vor meinen Studenten erzählte Andrej Karpathy, Direktor für KI bei Tesla, eine Anekdote
als er beschloss, ein internes Etikettierungsteam zu gründen, fragte sein Personalvermittler
für wie lange er dieses Team brauchen würde. Er antwortete: "Wie lange brauchen wir ein Ingenieursteam?
ingenieurteam?" Die Datenbeschriftung hat sich von einer Hilfsaufgabe zu einer Kernfunktion
Kernfunktion vieler ML-Teams in der Produktion.

In diesem Abschnitt werden wir uns mit der Herausforderung befassen, Etiketten für Ihre Daten zu erhalten. Wir werden
zunächst die Beschriftungsmethode, die Datenwissenschaftlern in der Regel als erstes in den Sinn kommt
in den Sinn kommt, wenn es um Beschriftung geht: Handbeschriftung. Anschließend werden wir Aufgaben mit natürlichen
Aufgaben mit natürlichen Beschriftungen, d. h. Aufgaben, bei denen die Beschriftungen vom System abgeleitet werden können, ohne dass
Anschließend wird erörtert, was zu tun ist, wenn natürliche und manuelle Beschriftungen fehlen.

Hand-Etiketten
Jeder, der schon einmal mit Daten in der Produktion arbeiten musste, hat es wahrscheinlich am eigenen Leib erfahren
gefühlt: Die Beschaffung von Handbeschriftungen für Ihre Daten ist aus vielen, vielen Gründen schwierig.
Erstens kann die manuelle Beschriftung von Daten teuer sein, insbesondere wenn Fachwissen
erforderlich ist. Um zu klassifizieren, ob ein Kommentar Spam ist, können Sie vielleicht 20
Kommentatoren auf einer Crowdsourcing-Plattform zu finden und sie in 15 Minuten zu schulen, um Ihre Daten zu
Daten. Wenn Sie jedoch Röntgenaufnahmen der Brust beschriften wollen, müssen Sie zertifizierte Radiologen finden
Radiologen finden, deren Zeit begrenzt und teuer ist.

Zweitens stellt die Handbeschriftung eine Gefahr für den Datenschutz dar. Handbeschriftung bedeutet, dass jemand
jemand Ihre Daten ansehen muss, was nicht immer möglich ist, wenn Ihre Daten strengen Datenschutz
Anforderungen haben. Sie können zum Beispiel nicht einfach die medizinischen Daten Ihrer Patienten oder
oder die vertraulichen Finanzdaten Ihres Unternehmens zur Kennzeichnung an einen Drittanbieter schicken. Unter
vielen Fällen dürfen Ihre Daten nicht einmal Ihr Unternehmen verlassen, und Sie
Sie müssen möglicherweise Kommentatoren einstellen oder unter Vertrag nehmen, um Ihre Daten vor Ort zu kennzeichnen.

Drittens ist die manuelle Beschriftung langsam. Zum Beispiel kann die genaue Transkription einer Sprachäußerung
auf phonetischer Ebene 400 Mal länger dauern als die Dauer der Äußerung.^7 Wenn man also
Wenn man also 1 Stunde Sprache annotieren will, braucht eine Person 400 Stunden oder fast 3 Monate
Person dafür. In einer Studie zur Verwendung von ML für die Klassifizierung von Lungenkrebs anhand von Röntgenbildern mussten meine
mussten meine Kollegen fast ein Jahr warten, bis sie genügend Etiketten erhielten.

88 | Kapitel 4: Trainingsdaten

Langsame Beschriftung führt zu langsamer Iterationsgeschwindigkeit und macht Ihr Modell weniger anpassungsfähig an
veränderte Umgebungen und Anforderungen. Wenn sich die Aufgabe oder die Daten ändern, müssen Sie
müssen Sie warten, bis Ihre Daten neu beschriftet sind, bevor Sie Ihr Modell aktualisieren können. Stellen Sie sich das
Szenario, wenn Sie ein Stimmungsanalysemodell haben, das die Stimmung jedes
Tweet zu analysieren, in dem Ihre Marke erwähnt wird. Es hat nur zwei Klassen: NEGATIV und POSITIV.
Nach der Einführung stellt Ihr PR-Team jedoch fest, dass der größte Schaden durch
wütenden Tweets entsteht, und sie wollen sich schneller um wütende Nachrichten kümmern. Also müssen Sie Ihr
Ihr Stimmungsanalysemodell aktualisieren, um drei Klassen zu erhalten: NEGATIV, POSITIV und
WÜTEND. Dazu müssen Sie sich Ihre Daten erneut ansehen, um festzustellen, welche vorhandenen
Trainingsbeispiele in ANGRY umbenannt werden sollten. Wenn Sie nicht genügend ANGRY
Beispiele haben, müssen Sie mehr Daten sammeln. Je länger dieser Prozess dauert, desto mehr
desto mehr wird sich die Leistung Ihres bestehenden Modells verschlechtern.

Etikettvielfalt

Um genügend beschriftete Daten zu erhalten, müssen Unternehmen oft Daten aus mehreren Quellen verwenden
Quellen verwenden und sich auf mehrere Kommentatoren mit unterschiedlichem Fachwissen verlassen. Diese
verschiedenen Datenquellen und Kommentatoren haben auch unterschiedliche Genauigkeitsgrade. Dies führt
zu dem Problem der Mehrdeutigkeit von Bezeichnungen oder der Bezeichnungsvielfalt: Was ist zu tun, wenn es
mehrere widersprüchliche Bezeichnungen für eine Dateninstanz gibt.

Betrachten Sie diese einfache Aufgabe der Entitätserkennung. Sie geben drei Annotatoren die folgende
Beispiel und bitten sie, alle Entitäten, die sie finden können, zu kommentieren:

Darth Sidious, einfach der Imperator genannt, war ein Dunkler Lord der Sith, der als Galaktischer Imperator des Ersten Galaktischen Imperiums über die Galaxis herrschte.
als Galaktischer Imperator des Ersten Galaktischen Imperiums über die Galaxis herrschte.
Sie erhalten drei verschiedene Lösungen zurück, wie in Tabelle 4-1 dargestellt. Drei Kommentatoren
haben verschiedene Entitäten identifiziert. Auf welche sollte Ihr Modell trainieren? Ein Modell
das auf Daten von Kommentator 1 trainiert wurde, wird ganz anders funktionieren als ein Modell
das auf Daten trainiert wurde, die von Kommentator 2 beschriftet wurden.

Tabelle 4-1. Die von verschiedenen Kommentatoren identifizierten Entitäten können sehr unterschiedlich sein

Annotator # Entitäten Annotation
1 3 [Darth Sidious], einfach der Imperator genannt, war ein [Dunkler Lord der Sith], der als [Galaktischer Imperator des Ersten Galaktischen Imperiums] über die
Galaxis als [Galaktischer Imperator des Ersten Galaktischen Imperiums] regierte.
2 6 [Darth Sidious], einfach der [Imperator] genannt, war ein [Dunkler Lord] der [Sith], der über die
Galaxis als [Galaktischer Imperator] des [Ersten Galaktischen Imperiums].
3 4 [Darth Sidious], einfach der [Imperator] genannt, war ein [Dunkler Lord der Sith], der über die
Galaxis als [Galaktischer Imperator des Ersten Galaktischen Imperiums].
Kennzeichnung | 89
8 Wenn etwas so offensichtlich zu beschriften ist, braucht man kein Fachwissen.
Unstimmigkeiten zwischen Kommentatoren sind sehr häufig. Je höher der Grad der
je mehr Fachwissen erforderlich ist, desto größer ist das Potenzial für Unstimmigkeiten bei der Beschriftung.^8 Wenn
ein Experte der Meinung ist, dass das Label A sein sollte, während ein anderer der Meinung ist, dass es B sein sollte,
wie lässt sich dieser Konflikt lösen, um eine einzige Grundwahrheit zu erhalten? Wenn menschliche Experten
sich nicht auf eine Bezeichnung einigen können, was bedeutet dann überhaupt Leistung auf menschlicher Ebene?

Um die Meinungsverschiedenheiten zwischen den Kommentatoren zu minimieren, ist es wichtig, zunächst eine klare
Problemdefinition. In der vorangegangenen Aufgabe zur Erkennung von Entitäten zum Beispiel hätten einige Unstimmigkeiten
einige Unstimmigkeiten beseitigt werden können, wenn wir klarstellen, dass im Falle mehrerer möglicher
Entitäten diejenige Entität ausgewählt wird, die den längsten Teilstring enthält. Das bedeutet Galaktisch
Imperator des Ersten Galaktischen Imperiums anstelle von Galaktischer Imperator und Erstes Galaktisches
Imperium. Zweitens müssen Sie diese Definition in die Schulung der Annotatoren einbeziehen
um sicherzustellen, dass alle Annotatoren die Regeln verstehen.

Datenherkunft

Die wahllose Verwendung von Daten aus mehreren Quellen, die mit unterschiedlichen Annotatoren
toren erzeugt wurden, ohne deren Qualität zu prüfen, kann dazu führen, dass Ihr Modell auf mysteriöse Weise versagt.
Nehmen wir den Fall an, dass Sie ein mäßig gutes Modell mit 100K Daten
Stichproben trainiert. Ihre ML-Ingenieure sind zuversichtlich, dass mehr Daten die Leistung des Modells verbessern werden.
Leistung des Modells verbessern, also geben Sie viel Geld aus, um Kommentatoren für die Beschriftung einer weiteren Million
Datenproben zu beschriften.

Die Leistung des Modells nimmt jedoch ab, nachdem es mit den neuen Daten trainiert wurde.
Daten trainiert wurde. Der Grund dafür ist, dass die neue Million Stichproben an Kommentatoren weitergereicht wurden
die die Daten mit viel geringerer Genauigkeit beschriftet haben als die Originaldaten. Es kann besonders
besonders schwierig sein, wenn Sie Ihre Daten bereits gemischt haben und die neuen Daten nicht
Daten von alten Daten unterscheiden können.

Es ist eine gute Praxis, den Ursprung jeder Ihrer Datenproben sowie deren
Beschriftungen zu verfolgen, eine Technik, die als Data Lineage bekannt ist. Die Datenreihenfolge hilft Ihnen dabei, potenzielle
Verzerrungen in Ihren Daten zu erkennen und Ihre Modelle zu debuggen. Wenn Ihr Modell zum Beispiel bei den meisten
Datenproben versagt, sollten Sie untersuchen, wie die neuen Daten erfasst wurden.
erfasst wurden. Bei mehr als einer Gelegenheit haben wir festgestellt, dass das Problem nicht
mit unserem Modell lag, sondern an der ungewöhnlich hohen Anzahl falscher Bezeichnungen in den Daten
die wir kürzlich erworben hatten.

90 | Kapitel 4: Trainingsdaten

Natürliche Etiketten
Handetikettierung ist nicht die einzige Quelle für Etiketten. Vielleicht haben Sie Glück und arbeiten
an Aufgaben mit natürlichen Kennzeichnungen zu arbeiten. Aufgaben mit natürlichen Labels sind Aufgaben, bei denen
die Vorhersagen des Modells automatisch oder teilweise vom System ausgewertet werden können.
System ausgewertet werden können. Ein Beispiel ist das Modell, das die Ankunftszeit für eine bestimmte Route auf
Google Maps. Wenn Sie diese Route nehmen, weiß Google Maps am Ende Ihrer Reise, wie
wie lange die Fahrt tatsächlich gedauert hat, und kann daher die Genauigkeit der vorhergesagten Ankunftszeit
Ankunftszeit bewerten. Ein weiteres Beispiel ist die Vorhersage von Aktienkursen. Wenn Ihr Modell den Kurs einer Aktie
Kurs einer Aktie in den nächsten zwei Minuten vorhersagt, können Sie nach zwei Minuten den vorhergesagten
Preis mit dem tatsächlichen Preis vergleichen.

Das kanonische Beispiel für Aufgaben mit natürlichen Bezeichnungen sind Empfehlungssysteme. Das Ziel
eines Empfehlungssystems ist es, den Benutzern für sie relevante Elemente zu empfehlen. Ob
ein Benutzer auf das empfohlene Objekt klickt oder nicht, kann als Feedback für diese
Empfehlung angesehen werden. Bei einer Empfehlung, die angeklickt wird, kann davon ausgegangen werden, dass sie
als gut (d.h. das Label ist POSITIV) und eine Empfehlung, die nicht angeklickt wird
nicht angeklickt wird, kann als schlecht angesehen werden (d. h. die Kennzeichnung ist
NEGATIV).

Viele Aufgaben lassen sich als Empfehlungsaufgaben formulieren. Zum Beispiel können Sie die Aufgabe
Aufgabe der Vorhersage der Klickraten von Anzeigen als Empfehlung der relevantesten Anzeigen
basierend auf der Aktivitätshistorie und den Profilen der Nutzer. Natürliche Bezeichnungen, die aus
wie Klicks und Bewertungen abgeleitet werden, werden auch als Verhaltenslabels bezeichnet.

Selbst wenn Ihre Aufgabe nicht von Natur aus natürliche Bezeichnungen hat, kann es möglich sein
Ihr System so einzurichten, dass Sie ein gewisses Feedback zu Ihrem Modell einholen können.
Wenn Sie zum Beispiel ein maschinelles Übersetzungssystem wie Google Translate,
können Sie der Community die Möglichkeit geben, alternative Übersetzungen für schlechte Übersetzungen einzureichen.
Diese alternativen Übersetzungen können verwendet werden, um die nächste Iteration Ihres Modells zu trainieren.
Ihrer Modelle verwendet werden (obwohl Sie diese Übersetzungsvorschläge vielleicht erst einmal überprüfen sollten).
Das Ranking von Newsfeeds ist keine Aufgabe mit inhärenten Labels, aber durch das Hinzufügen des Like-Buttons
und anderen Reaktionen zu jedem Newsfeed-Element ist Facebook in der Lage, Feedback zu
ihren Ranking-Algorithmus.

Aufgaben mit natürlichen Bezeichnungen sind in der Industrie recht häufig. In einer Umfrage unter 86
Unternehmen in meinem Netzwerk habe ich festgestellt, dass 63 % von ihnen mit Aufgaben mit natürlichen Bezeichnungen arbeiten
Bezeichnungen arbeiten, wie in Abbildung 4-3 dargestellt. Das bedeutet nicht, dass 63 % der Aufgaben, die von ML-Lösungen profitieren können
von ML-Lösungen profitieren können, natürliche Bezeichnungen haben. Wahrscheinlicher ist, dass es für die Unternehmen
dass es für Unternehmen einfacher und billiger ist, zunächst mit Aufgaben zu beginnen, die natürliche Bezeichnungen haben.

Kennzeichnung | 91
9 Auf programmatische Etiketten wird im Abschnitt "Schwache Überwachung" auf Seite 95 eingegangen.
Abbildung 4-3. Dreiundsechzig Prozent der Unternehmen in meinem Netzwerk arbeiten an Aufgaben mit natürlichen
Bezeichnungen. Die Prozentsätze ergeben nicht die Summe 1, da ein Unternehmen an Aufgaben mit verschiedenen Labelquellen arbeiten kann.
verschiedenen Label-Quellen arbeiten kann.^9

Im vorherigen Beispiel kann eine Empfehlung, die nach einer gewissen Zeit nicht angeklickt wird
Zeit nicht angeklickt wird, kann als schlecht angesehen werden. Dies wird als implizites Label bezeichnet, da dieses negative
Etikett aus dem Fehlen eines positiven Etiketts abgeleitet wird. Es unterscheidet sich von expliziten Kennzeichnungen
wo die Nutzer ihr Feedback zu einer Empfehlung explizit zeigen, indem sie ihr eine
niedrige Bewertung oder ein Downvoting.

Länge der Rückkopplungsschleife

Bei Aufgaben mit natürlichen Grundwahrheitsbezeichnungen ist die Zeit, die von der Abgabe einer Vorhersage bis zur
Vorhersage bis zur Rückmeldung dauert, ist die Länge der Rückkopplungsschleife
Länge. Aufgaben mit kurzen Rückkopplungsschleifen sind Aufgaben, bei denen die Beschriftungen im Allgemeinen innerhalb von Minuten verfügbar sind.
innerhalb von Minuten verfügbar sind. Viele Empfehlungssysteme haben kurze Feedback-Schleifen. Wenn die empfohlenen
Produkte auf Amazon oder Personen, denen man auf Twitter folgen sollte, ist die
Zeit zwischen der Empfehlung und dem Anklicken des Artikels, wenn überhaupt, kurz.
überhaupt angeklickt wird, ist kurz.

92 | Kapitel 4: Trainingsdaten

Allerdings haben nicht alle Empfehlungssysteme minutenlange Feedbackschleifen. Wenn Sie
mit längeren Inhalten wie Blogbeiträgen, Artikeln oder YouTube-Videos arbeiten, kann die
kann die Feedbackschleife Stunden dauern. Wenn Sie ein System zur Empfehlung von Kleidung für Nutzer entwickeln, wie
wie das von Stitch Fix, erhalten Sie erst dann ein Feedback, wenn die Nutzer die Artikel erhalten
und anprobiert haben, was Wochen später sein kann.

Verschiedene Arten von Benutzerfeedback
Wenn Sie aus dem Nutzerfeedback Labels extrahieren wollen, ist es wichtig zu wissen, dass es
verschiedene Arten von Benutzerfeedback gibt. Sie können in verschiedenen Phasen während der
und unterscheiden sich durch Lautstärke, Stärke des Signals und Länge der Feedbackschleife.
Länge.
Nehmen wir zum Beispiel eine E-Commerce-Anwendung, ähnlich wie bei Amazon. Arten
Feedbacks, die ein Benutzer in dieser Anwendung geben kann, sind z. B. das Klicken auf eine Produktempfehlung
Empfehlung, Hinzufügen eines Produkts zum Warenkorb, Kauf eines Produkts, Bewertung, Hinterlassen einer Rezension,
und die Rückgabe eines zuvor gekauften Produkts.
Der Klick auf ein Produkt erfolgt viel schneller und häufiger (und verursacht daher
(und verursacht daher ein höheres Volumen) als der Kauf eines Produkts. Allerdings ist der Kauf eines Produkts ein
ein viel stärkeres Signal dafür, ob ein Nutzer das Produkt mag, als wenn er es nur anklickt.
auf das Produkt.
Beim Aufbau eines Produktempfehlungssystems konzentrieren sich viele Unternehmen auf die Optimierung
auf Klicks, die ihnen eine größere Menge an Feedback zur Bewertung ihrer Modelle liefern.
Einige Unternehmen konzentrieren sich jedoch auf Käufe, was ihnen ein stärkeres Signal liefert
ein stärkeres Signal liefert, das auch stärker mit ihren Geschäftskennzahlen korreliert (z. B. Einnahmen aus Produktverkäufen).
Verkäufen). Beide Ansätze sind gültig. Es gibt keine eindeutige Antwort darauf, welche Art von Feedback
Sie für Ihren Anwendungsfall optimieren sollten, und es lohnt sich, ernsthafte Diskussionen zwischen allen
beteiligten Interessengruppen.
Die Wahl der richtigen Fensterlänge erfordert eine gründliche Überlegung, da sie einen
Abwägung zwischen Geschwindigkeit und Genauigkeit. Eine kurze Fensterlänge bedeutet, dass Sie
Kennzeichnungen schneller erfassen, so dass Sie diese Kennzeichnungen nutzen können, um Probleme mit Ihrem Modell zu erkennen und
diese Probleme so schnell wie möglich zu beheben. Allerdings bedeutet eine kurze Fensterlänge auch
dass Sie eine Empfehlung möglicherweise vorzeitig als schlecht einstufen, bevor sie angeklickt wird.

Unabhängig davon, wie lang Sie die Länge Ihres Fensters einstellen, kann es immer noch zu vorzeitigen
negative Anzeigen. Anfang 2021 ergab eine Studie des Ads-Teams bei Twitter, dass
obwohl die meisten Klicks auf Anzeigen innerhalb der ersten fünf Minuten erfolgen, einige Klicks

Kennzeichnung | 93
10 Sofia Ira Ktena, Alykhan Tejani, Lucas Theis, Pranay Kumar Myana, Deepak Dilipkumar, Ferenc Huszar,
Steven Yoo, und Wenzhe Shi, "Addressing Delayed Feedback for Continuous Training with Neural Networks
in CTR Prediction," arXiv, July 15, 2019, https://oreil.ly/5y2WA.

Stunden nach Erscheinen der Anzeige erfolgen.^10 Das bedeutet, dass diese Art von Kennzeichnung tendenziell
zu einer Unterschätzung der tatsächlichen Klickrate führt. Wenn Sie nur 1.000
POSITIVE-Kennzeichnungen aufzeichnen, könnte die tatsächliche Anzahl der Klicks bei etwas über 1.000 liegen.
Bei Aufgaben mit langen Feedbackschleifen kann es vorkommen, dass natürliche Kennzeichnungen erst nach Wochen oder sogar
Monate. Die Betrugserkennung ist ein Beispiel für eine Aufgabe mit langen Feedbackschleifen. Für eine
einer bestimmten Zeit nach einer Transaktion können die Nutzer bestreiten, dass diese Transaktion
betrügerisch ist oder nicht. Wenn ein Kunde zum Beispiel seine Kreditkartenabrechnung liest
und eine Transaktion sieht, die er nicht kennt, kann er sie bei seiner Bank anfechten,
Dadurch erhält die Bank die Rückmeldung, dass die Transaktion als betrügerisch einzustufen ist. Ein typisches
Zeitfenster beträgt ein bis drei Monate. Wenn nach Ablauf der Widerspruchsfrist keine
Wenn der Nutzer keinen Widerspruch einlegt, können Sie davon ausgehen, dass die Transaktion rechtmäßig ist.
Kennzeichnungen mit langen Rückkopplungsschleifen sind hilfreich für die Berichterstattung über die Leistung eines Modells in
vierteljährlichen oder jährlichen Geschäftsberichten. Sie sind jedoch nicht sehr hilfreich, wenn Sie
Probleme mit Ihren Modellen so schnell wie möglich erkennen wollen. Wenn es ein Problem mit Ihrem
Betrugserkennungsmodells auftritt und Sie Monate brauchen, um es zu erkennen, sind in der Zeit, in der das Problem behoben wird
bis das Problem behoben ist, könnten alle betrügerischen Transaktionen, die Ihr fehlerhaftes Modell durchgelassen hat, dazu geführt haben, dass
ein kleines Unternehmen in den Bankrott getrieben.
Umgang mit dem Mangel an Etiketten
Da es schwierig ist, ausreichend hochwertige Etiketten zu beschaffen, wurden viele Techniken
entwickelt worden, um die daraus resultierenden Probleme zu lösen. In diesem Abschnitt werden wir
vier von ihnen behandeln: schwache Überwachung, Semi-Supervision, Transfer-Lernen und aktives
Lernen. Eine Zusammenfassung dieser Methoden ist in Tabelle 4-2 dargestellt.
Tabelle 4-2. Zusammenfassungen von vier Techniken für den Umgang mit dem Mangel an handbeschrifteten Daten
Methode Wie viele Grundwahrheiten sind erforderlich?
Schwache
Überwachung
Nutzung von (oft verrauschten) Heuristiken zur
Beschriftungen zu generieren
Nein, aber eine kleine Anzahl von Kennzeichnungen wird empfohlen, um die
Entwicklung von Heuristiken
Semi-
Überwachung
Nutzt strukturelle Annahmen zur
Beschriftungen zu generieren
Ja, eine kleine Anzahl von Anfangsbeschriftungen als Grundlage für die Erzeugung weiterer
Etiketten
Übertragung
Lernen
Nutzt Modelle, die für eine andere
einer anderen Aufgabe für Ihre neue Aufgabe
Nein für Zero-Shot-Lernen
Ja für die Feinabstimmung, obwohl die Anzahl der erforderlichen Grundwahrheiten
oft viel kleiner ist als die Anzahl der Grundwahrheiten, die benötigt werden würde
wenn Sie das Modell von Grund auf trainieren
Aktives Lernen Markiert Datenproben, die für das
für Ihr Modell am nützlichsten sind
Ja
94 | Kapitel 4: Trainingsdaten
11 Alexander Ratner, Stephen H. Bach, Henry Ehrenberg, Jason Fries, Sen Wu, und Christopher Ré, "Snorkel:
Rapid Training Data Creation with Weak Supervision," Proceedings of the VLDB Endowment 11, no. 3 (2017):
269-82, https://oreil.ly/vFPjk.

Schwache Überwachung
Wenn die Handetikettierung so problematisch ist, was wäre dann, wenn wir überhaupt keine Handetiketten verwenden? Ein
Ansatz, der an Popularität gewonnen hat, ist die schwache Überwachung. Eines der beliebtesten
Open-Source-Tools für schwache Überwachung ist Snorkel, das am Stanford AI Lab entwickelt wurde.^11
Die Erkenntnis hinter der schwachen Überwachung ist, dass Menschen sich auf Heuristiken verlassen, die
Fachwissen entwickelt werden können, um Daten zu kennzeichnen. Ein Arzt könnte zum Beispiel
die folgenden Heuristiken verwenden, um zu entscheiden, ob der Fall eines Patienten als dringend eingestuft werden sollte
Dringend:
Wenn in der Notiz der Krankenschwester eine ernste Erkrankung wie eine Lungenentzündung erwähnt wird, sollte der Fall des Patienten
vorrangig behandelt werden.
Bibliotheken wie Snorkel basieren auf dem Konzept einer Beschriftungsfunktion (LF): eine
Funktion, die Heuristiken kodiert. Die vorangehende Heuristik kann durch die folgende Funktion ausgedrückt werden
folgende Funktion ausgedrückt werden:
def labeling_function(note):
if "pneumonia" in note:
return "EMERGENT"
LFs können viele verschiedene Arten von Heuristiken kodieren. Hier sind einige von ihnen:
Schlüsselwort-Heuristik
Wie im vorangegangenen Beispiel
Reguläre Ausdrücke
z. B. ob die Notiz mit einem bestimmten regulären Ausdruck übereinstimmt oder nicht übereinstimmt
Datenbankabfrage
z. B. wenn die Notiz die in der Liste der gefährlichen Krankheiten aufgeführte Krankheit enthält
Die Ergebnisse anderer Modelle
Zum Beispiel, wenn ein bestehendes System die Notiz als EMERGENT klassifiziert.
Nachdem Sie LFs geschrieben haben, können Sie sie auf die Proben anwenden, die Sie kennzeichnen möchten.
Da LFs Heuristiken kodieren und Heuristiken verrauscht sind, sind die von LFs erzeugten Labels
verrauscht. Mehrere LFs können auf dieselben Datenbeispiele angewandt werden, und sie können zu
widersprüchliche Bezeichnungen. Eine Funktion könnte die Notiz einer Krankenschwester als EMERGENT einstufen, eine andere
Funktion könnte sie für nicht dringend halten. Eine Heuristik könnte viel genauer sein als
eine andere Heuristik, was Sie vielleicht nicht wissen, weil Sie keine Grundwahrheit
mit denen man sie vergleichen kann. Es ist wichtig, alle LFs zu kombinieren, zu entrauschen und neu zu gewichten, um
Kennzeichnung | 95
12 Ratner et al., "Snorkel: Rapid Training Data Creation with Weak Supervision".

einen Satz von höchstwahrscheinlich korrekten Bezeichnungen zu erhalten. Abbildung 4-4 zeigt in groben Zügen, wie LFs
funktionieren.
Abbildung 4-4. Überblick über die Kombination von Beschriftungsfunktionen auf hoher Ebene. Quelle:
Adaptiert von einer Abbildung von Ratner et al.^12
Theoretisch braucht man für eine schwache Überwachung keine Handbeschriftungen. Um jedoch ein Gefühl dafür zu bekommen
ein Gefühl dafür zu bekommen, wie genau Ihre LFs sind, wird eine kleine Anzahl von Handbeschriftungen empfohlen.
Diese Hand Labels können Ihnen helfen, Muster in Ihren Daten zu entdecken, um bessere LFs zu schreiben.
Eine schwache Überwachung kann besonders nützlich sein, wenn Ihre Daten strenge Datenschutzanforderungen erfüllen.
Anforderungen haben. Sie müssen nur eine kleine, freigegebene Teilmenge der Daten sehen, um LFs zu schreiben, die
die auf den Rest Ihrer Daten angewendet werden können, ohne dass jemand sie sieht.
Mit LFs kann Fachwissen versioniert, wiederverwendet und weitergegeben werden. Fachwissen
das einem Team gehört, kann kodiert und von einem anderen Team genutzt werden. Wenn sich Ihre Daten ändern
oder Ihre Anforderungen ändern, können Sie die LFs einfach erneut auf Ihre Datenproben anwenden. Der
Ansatz, LFs zur Erzeugung von Labels für Ihre Daten zu verwenden, wird auch als programmatische
Beschriftung. Tabelle 4-3 zeigt einige der Vorteile der programmatischen Beschriftung gegenüber der manuellen
Beschriftung.
Tabelle 4-3. Die Vorteile der programmatischen Beschriftung gegenüber der manuellen Beschriftung
Handbeschriftung Programmatische Beschriftung
Teuer: Besonders wenn Fachwissen
erforderlich
Kostenersparnis: Fachwissen kann versioniert, gemeinsam genutzt und innerhalb einer Organisation wiederverwendet werden
Organisation wiederverwendet werden
Mangelnder Datenschutz: Daten müssen an menschliche
Annotatoren
Datenschutz: Erstellung von LFs anhand einer freigegebenen Datenunterstichprobe und anschließende Anwendung
LFs auf andere Daten anwenden, ohne einzelne Proben zu betrachten
Langsam : Die benötigte Zeit skaliert linear mit der Anzahl der
benötigten Labels
Schnell: Einfache Skalierung von 1K bis 1M Stichproben
Nicht anpassungsfähig: Jede Änderung erfordert eine neue Beschriftung der
Daten
Adaptiv: Bei Änderungen einfach die LFs neu anwenden!
96 | Kapitel 4: Trainingsdaten
13 Jared A. Dunnmon, Alexander J. Ratner, Khaled Saab, Matthew P. Lungren, Daniel L. Rubin, und Christopher
Ré, "Cross-Modal Data Programming Enables Rapid Medical Machine Learning", Patterns 1, no. 2 (2020):
100019, https://oreil.ly/nKt8E.
14 Die beiden Aufgaben in dieser Studie verwenden nur 18 bzw. 20 LFs. In der Praxis habe ich Teams gesehen, die Hunderte von
LFs für jede Aufgabe verwenden.
15 Dummon et al., "Cross-Modal Data Programming".

Die folgende Fallstudie soll zeigen, wie gut eine schwache Aufsicht in der Praxis funktioniert. In einer Studie
mit Stanford Medicine,^13 wurden Modelle mit schwach überwachten Beschriftungen trainiert, die von einem
einem einzelnen Radiologen nach acht Stunden Schreiben von LFs gewonnen wurden, eine vergleichbare Leistung wie
Modellen, die mit Daten trainiert wurden, die fast ein Jahr lang von Hand beschriftet wurden, wie in
Abbildung 4-5. Es gibt zwei interessante Fakten zu den Ergebnissen des Experiments. Erstens,
verbesserten sich die Modelle mit mehr unbeschrifteten Daten auch ohne weitere LFs.
Zweitens wurden die LFs aufgabenübergreifend wiederverwendet. Die Forscher waren in der Lage, sechs
LFs zwischen der CXR-Aufgabe (Röntgenaufnahmen des Brustkorbs) und der EXR-Aufgabe (Röntgenaufnahmen der Extremitäten) wiederverwenden.^14
Abbildung 4-5. Vergleich der Leistung eines Modells, das mit vollständig überwachten Etiketten
(FS) und eines Modells, das mit programmatischen Etiketten (DP) für CXR- und EXR-Aufgaben trainiert wurde.
Quelle: Dunnmon et al.^15
Meine Studenten fragen oft, warum wir ML-Modelle brauchen, wenn Heuristiken so gut funktionieren, um Daten zu beschriften?
ML-Modelle? Ein Grund dafür ist, dass LFs möglicherweise nicht alle Datenproben abdecken, so dass wir
ML-Modelle auf Daten trainieren, die programmatisch mit LFs beschriftet wurden, und dieses trainierte Modell verwenden, um
Vorhersagen für Stichproben zu erstellen, die von keiner LF abgedeckt werden.
Schwache Überwachung ist ein einfaches, aber leistungsstarkes Paradigma. Es ist jedoch nicht perfekt. In
einigen Fällen können die durch schwache Überwachung erhaltenen Kennzeichnungen zu verrauscht sein, um nützlich zu sein.
Aber selbst in diesen Fällen kann die schwache Überwachung ein guter Einstieg sein, wenn man
Kennzeichnung | 97
16 Avrim Blum und Tom Mitchell, "Combining Labeled and Unlabeled Data with Co-Training," in Proceedings of
the Eleventh Annual Conference on Computational Learning Theory (Juli 1998): 92-100, https://oreil.ly/T79AE.

Sie die Wirksamkeit von ML erforschen möchten, ohne vorab zu viel in die
Handbeschriftung zu investieren.
Semi-Supervision
Während die schwache Überwachung Heuristiken nutzt, um verrauschte Beschriftungen zu erhalten, nutzt die Semi-Supervision
strukturelle Annahmen, um neue Beschriftungen auf der Grundlage eines kleinen Satzes von anfänglichen
Beschriftungen. Im Gegensatz zur schwachen Überwachung erfordert die Semi-Supervision einen Anfangssatz von Beschriftungen.
Semi-überwachtes Lernen ist eine Technik, die bereits in den 90er Jahren eingesetzt wurde,^16 und seither
Seitdem wurden viele Semi-Supervision-Methoden entwickelt. Eine umfassende Übersicht
über semi-supervised learning würde den Rahmen dieses Buches sprengen. Wir gehen auf eine kleine
eine kleine Auswahl dieser Methoden, um den Lesern einen Eindruck davon zu vermitteln, wie sie eingesetzt werden. Für eine ausführliche
hensive Übersicht empfehle ich "Semi-Supervised Learning Literature Survey" (Xiaojin
Zhu, 2008) und "A Survey on Semi-Supervised Learning" (Engelen und Hoos, 2018).
Eine klassische Semi-Supervision-Methode ist das Selbsttraining. Sie beginnen mit dem Training eines Modells auf
Sie beginnen mit dem Training eines Modells auf Ihrem bestehenden Satz beschrifteter Daten und verwenden dieses Modell, um Vorhersagen für unbeschriftete
Stichproben. Unter der Annahme, dass Vorhersagen mit hoher Wahrscheinlichkeit richtig sind, fügen Sie
fügen Sie die mit hoher Wahrscheinlichkeit vorhergesagten Bezeichnungen zu Ihrem Trainingssatz hinzu und trainieren ein neues
Modell auf dieser erweiterten Trainingsmenge. So geht es weiter, bis Sie mit der Leistung Ihres Modells zufrieden sind.
Leistung zufrieden sind.
Eine andere Semi-Supervision-Methode geht davon aus, dass Datenproben mit ähnlichen
Merkmale aufweisen, die gleichen Bezeichnungen erhalten. Die Ähnlichkeit kann offensichtlich sein, wie zum Beispiel bei der Aufgabe
der Klassifizierung des Themas von Twitter-Hashtags. Sie können damit beginnen, den Hashtag
"#AI" als Informatik bezeichnen. Unter der Annahme, dass Hashtags, die im selben Tweet oder Profil erscheinen
Profil auftauchen, sich wahrscheinlich um dasselbe Thema drehen, wie das Profil von MIT CSAIL in Abbildung 4-6,
können Sie auch die Hashtags "#ML" und "#BigData" als Informatik kennzeichnen.
Abbildung 4-6. Da #ML und #BigData im selben Twitter-Profil wie #AI erscheinen, können wir
können wir davon ausgehen, dass sie zum selben Thema gehören
98 | Kapitel 4: Trainingsdaten
17 Avital Oliver, Augustus Odena, Colin Raffel, Ekin D. Cubuk, und Ian J. Goodfellow, "Realistic Evaluation of
Deep Semi-Supervised Learning Algorithms", NeurIPS 2018 Proceedings, https://oreil.ly/dRmPV.
18 Ein Token kann ein Wort, ein Zeichen oder ein Teil eines Wortes sein.

In den meisten Fällen kann die Ähnlichkeit nur durch komplexere Methoden ermittelt werden.
Sie müssen beispielsweise eine Clustering-Methode oder einen K-Nächste-Nachbarn
Algorithmus verwenden, um Proben zu finden, die zum selben Cluster gehören.
Eine Semi-Supervisionsmethode, die in den letzten Jahren an Popularität gewonnen hat, ist die
störungsbasierte Methode. Sie basiert auf der Annahme, dass kleine Störungen
einer Stichprobe deren Bezeichnung nicht verändern sollten. Sie wenden also kleine Störungen auf Ihre Trainingsinstanzen
Trainingsinstanzen an, um neue Trainingsinstanzen zu erhalten. Die Störeinflüsse können
direkt auf die Proben (z. B. Hinzufügen von weißem Rauschen zu Bildern) oder auf ihre Repräsentationen
(z. B. Hinzufügen kleiner Zufallswerte zu den Einbettungen von Wörtern). Die gestörten Proben
haben die gleichen Bezeichnungen wie die ungestörten Proben. Mehr darüber erfahren Sie in dem
Abschnitt "Perturbation" auf Seite 114.
In einigen Fällen haben Semi-Supervision-Ansätze die Leistung von reinem
überwachten Lernens erreicht, selbst wenn ein erheblicher Teil der Bezeichnungen in einem bestimmten Datensatz
verworfen wurde.^17
Semi-Supervision ist am nützlichsten, wenn die Anzahl der Trainingsmarker begrenzt ist.
Bei der Semi-Supervision mit begrenzten Daten ist unter anderem zu überlegen, wie viel
dieser begrenzten Daten verwendet werden sollte, um mehrere Modellkandidaten zu bewerten und das beste Modell auszuwählen.
das beste Modell auszuwählen. Wenn Sie eine kleine Menge verwenden, könnte das beste Modell auf dieser kleinen
das Modell mit der besten Leistung auf dieser kleinen Auswertungsmenge sein, das sich am besten an diese Menge anpasst. Auf der anderen Seite,
wenn Sie eine große Menge an Daten für die Auswertung verwenden, wird der Leistungszuwachs durch
Auswahl des besten Modells auf der Grundlage dieses Evaluierungssatzes geringer sein als die Steigerung
der durch das Hinzufügen der Auswertungsmenge zur begrenzten Trainingsmenge erzielt wird. Viele Unternehmen
Unternehmen überwinden diesen Zielkonflikt, indem sie das beste Modell auf der Grundlage einer relativ großen Auswertungsmenge auswählen
Modell auszuwählen und dann das Champion-Modell mit der Auswertungsmenge weiter zu trainieren.
Transfer-Lernen
Transferlernen bezieht sich auf die Familie der Methoden, bei denen ein für eine Aufgabe entwickeltes Modell
als Ausgangspunkt für ein Modell für eine zweite Aufgabe wiederverwendet wird. Zunächst wird das Basismodell
für eine Basisaufgabe trainiert. Bei der Basisaufgabe handelt es sich in der Regel um eine Aufgabe, für die billige und reichhaltige
Trainingsdaten hat. Die Sprachmodellierung ist ein hervorragender Kandidat, da sie keine
beschriftete Daten benötigt. Sprachmodelle können auf jedem beliebigen Textkorpus trainiert werden - Bücher, Wikipedia
Artikeln, Chatverläufen - trainiert werden, und die Aufgabe lautet: Bei einer Folge von Zeichen,^18 das nächste Zeichen vorhersagen.
Token. Bei der Sequenz "Ich habe NVIDIA-Aktien gekauft, weil ich an die
Bedeutung von", könnte ein Sprachmodell "Hardware" oder "GPU" als nächstes Token ausgeben.
Token ausgeben.
Kennzeichnung | 99
19 Jeremy Howard und Sebastian Ruder, "Universal Language Model Fine-tuning for Text Classification," arXiv,
18. Januar 2018, https://oreil.ly/DBEbw.
20 Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, und Graham Neubig, "Pre-train,
Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing," arXiv, July
28, 2021, https://oreil.ly/0lBgn.
21 Jacob Devlin, Ming-Wei Chang, Kenton Lee, und Kristina Toutanova, "BERT: Pre-training of Deep Bidirec-
tional Transformers for Language Understanding," arXiv, October 11, 2018, https://oreil.ly/RdIGU; Tom B.
Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan,
et al. "Language Models Are Few-Shot Learners", OpenAI, 2020, https://oreil.ly/YVmrr.

Das trainierte Modell kann dann für die Aufgabe verwendet werden, an der Sie interessiert sind - eine
Stream-Aufgabe, wie z. B. Stimmungsanalyse, Absichtserkennung oder Beantwortung von Fragen. Unter
einigen Fällen, wie z. B. in Zero-Shot-Learning-Szenarien, können Sie das Basismodell
Modell direkt für eine nachgelagerte Aufgabe verwenden. In vielen Fällen müssen Sie eine Feinabstimmung
des Basismodells. Feinabstimmung bedeutet, dass kleine Änderungen am Basismodell vorgenommen werden, wie z. B.
Fortsetzen des Trainings des Basismodells oder eines Teils des Basismodells mit Daten aus einer bestimmten
nachgelagerten Aufgabe.^19
Manchmal müssen Sie die Eingaben mithilfe einer Vorlage ändern, damit das Basismodell die gewünschten Ergebnisse erzeugt.
die gewünschten Ausgaben zu generieren.^20 Um beispielsweise ein Sprachmodell als
ein Sprachmodell als Basismodell für eine Aufgabe zur Beantwortung von Fragen zu verwenden, könnten Sie diese Aufforderung verwenden:
F: Wann wurden die Vereinigten Staaten gegründet?
A: Am 4. Juli 1776.
F: Wer hat die Unabhängigkeitserklärung verfasst?
A: Thomas Jefferson.
F: In welchem Jahr wurde Alexander Hamilton geboren?
A:
Wenn Sie diese Frage in ein Sprachmodell wie GPT-3 eingeben, gibt es möglicherweise
das Jahr, in dem Alexander Hamilton geboren wurde.
Transferlernen ist besonders attraktiv für Aufgaben, die nicht viele beschriftete Daten enthalten.
Daten haben. Selbst bei Aufgaben mit vielen beschrifteten Daten kann die Verwendung eines vortrainierten Modells als
Modells als Ausgangspunkt kann die Leistung im Vergleich zum Training von Grund auf erheblich gesteigert
von Grund auf.
Das Transfer-Lernen hat in den letzten Jahren aus den richtigen Gründen stark an Interesse gewonnen.
Es hat viele Anwendungen ermöglicht, die zuvor aufgrund des Mangels an
Trainingsbeispiele unmöglich waren. Ein nicht unerheblicher Teil der ML-Modelle in der Produktion sind heute
sind die Ergebnisse des Transfer-Lernens, einschließlich Objekterkennungsmodellen, die mit Hilfe von
Modelle zur Objekterkennung, die auf ImageNet trainiert wurden, und Modelle zur Textklassifizierung, die auf vortrainierten
Sprachmodelle wie BERT oder GPT-3 nutzen.^21 Transferlernen senkt auch die Einstiegskosten
100 | Kapitel 4: Trainingsdaten
22 Burr Settles, Active Learning (Williston, VT: Morgan & Claypool, 2012).

Barrieren für ML, da es dazu beiträgt, die Vorlaufkosten für die Kennzeichnung von Daten zur Erstellung von
ML-Anwendungen.
In den letzten fünf Jahren hat sich der Trend herauskristallisiert, dass (normalerweise) die Leistung bei nachgelagerten Aufgaben umso besser ist, je größer das vor
Basismodell, desto besser ist seine Leistung bei nachgelagerten Aufgaben. Große Modelle
sind teuer in der Ausbildung. Ausgehend von der Konfiguration von GPT-3 wird geschätzt, dass die
Kosten für das Training dieses Modells auf mehrere zehn Millionen USD geschätzt. Viele haben die Hypothese aufgestellt
dass es sich in Zukunft nur noch eine Handvoll Unternehmen leisten können, große
vortrainierte Modelle zu trainieren. Der Rest der Branche wird diese vorab trainierten Modelle direkt verwenden
oder sie für ihre spezifischen Bedürfnisse feinabstimmen.
Aktives Lernen
Aktives Lernen ist eine Methode zur Verbesserung der Effizienz von Datenetiketten. Die Hoffnung
ist, dass ML-Modelle mit weniger Trainingsetiketten eine höhere Genauigkeit erreichen können, wenn sie
sie auswählen können, aus welchen Datenproben sie lernen sollen. Aktives Lernen wird manchmal
Abfrage-Lernen genannt - obwohl dieser Begriff zunehmend unpopulär wird -, weil
ein Modell (aktiver Lerner) Abfragen in Form von unmarkierten Stichproben zurückschickt, die
von Kommentatoren (in der Regel Menschen) beschriftet werden.
Anstatt Datenproben zufällig zu beschriften, beschriften Sie die Proben, die für Ihre Modelle am hilfreichsten sind.
die für Ihre Modelle am hilfreichsten sind, anhand einiger Metriken oder Heuristiken. Die einfachste
Metrik ist die Unsicherheitsmessung: Sie kennzeichnen die Beispiele, bei denen Ihr Modell am wenigsten sicher ist
am unsichersten ist, in der Hoffnung, dass sie Ihrem Modell helfen, die Entscheidungsgrenze besser zu erlernen.
ter. Bei Klassifizierungsproblemen beispielsweise, bei denen Ihr Modell rohe
Wahrscheinlichkeiten für verschiedene Klassen ausgibt, könnte es die Datenproben mit den niedrigsten
Wahrscheinlichkeiten für die vorhergesagte Klasse. Abbildung 4-7 veranschaulicht, wie gut diese Methode funktioniert
an einem Spielzeugbeispiel.
Abbildung 4-7. Wie das auf Unsicherheit basierende aktive Lernen funktioniert. (a) Ein Spielzeugdatensatz von 400
Instanzen, die gleichmäßig aus zwei Gauß-Klassen abgetastet wurden. (b) Ein Modell, das auf 30 Stichproben
trainiert wurde, erreicht eine Genauigkeit von 70%. (c) Ein Modell, das anhand von 30 Stichproben
aktiven Lernens trainiert wurde, erreicht eine Genauigkeit von 90%. Quelle: Burr Settles^22
Kennzeichnung | 101
23 Wir werden Ensembles in Kapitel 6 behandeln.
24 Dana Angluin, "Queries and Concept Learning," Machine Learning 2 (1988): 319-42, https://oreil.ly/0uKs4.
25 Vielen Dank an Eugene Yan für dieses wunderbare Beispiel!

Eine weitere gängige Heuristik basiert auf der Uneinigkeit zwischen mehreren Kandidaten
Modellen. Diese Methode wird Query-by-Committee genannt, ein Beispiel für eine Ensemble-Methode
Beispiel für eine Ensemble-Methode.^23 Sie benötigen ein Komitee aus mehreren Modellkandidaten, die in der Regel das gleiche Modell sind, das mit verschiedenen
dasselbe Modell, das mit verschiedenen Sätzen von Hyperparametern trainiert wurde, oder dasselbe Modell, das
auf verschiedenen Datenabschnitten trainiert wurde. Jedes Modell kann ein Votum darüber abgeben, welche Proben als nächstes beschriftet werden sollen.
als nächstes beschriftet werden soll, und es kann seine Entscheidung davon abhängig machen, wie unsicher es bezüglich der Vorhersage ist. Sie können dann
beschriften Sie dann die Stichproben, bei denen sich der Ausschuss am meisten uneinig ist.
Es gibt noch andere Heuristiken, wie z. B. die Auswahl von Stichproben, die, wenn auf sie trainiert wird, die
die höchsten Gradientenaktualisierungen ergeben oder den Verlust am stärksten reduzieren. Für eine umfassende
Überblick über aktive Lernmethoden finden Sie in "Active Learning Literature Survey"
(Siedler 2010).
Die zu beschriftenden Proben können aus verschiedenen Datenregimen stammen. Sie können synthe-
Sie können synthe- tisiert werden, wobei Ihr Modell Stichproben in dem Bereich des Eingaberaums erzeugt, über den es
unsichersten Bereich des Eingaberaums generiert.^24 Sie können aus einer stationären Verteilung stammen, bei der Sie
Sie können aus einer stationären Verteilung stammen, bei der Sie bereits eine Menge unbeschrifteter Daten gesammelt haben und Ihr Modell
Pool zum Beschriften aus. Sie können aus einer realen Verteilung stammen, bei der Sie einen
Datenstrom, wie in der Produktion, und Ihr Modell wählt Stichproben aus diesem
Datenstrom aus, um ihn zu beschriften.
Ich finde aktives Lernen am spannendsten, wenn ein System mit Echtzeitdaten arbeitet. Daten
ändern sich ständig, ein Phänomen, das wir in Kapitel 1 kurz angeschnitten haben und
in Kapitel 8 näher erläutern. Aktives Lernen in diesem Datenregime wird es Ihrem Modell ermöglichen
effektiver in Echtzeit zu lernen und sich schneller an veränderte Umgebungen anzupassen.
Ungleichgewicht zwischen den Klassen
Ein Klassenungleichgewicht bezieht sich in der Regel auf ein Problem bei Klassifizierungsaufgaben, bei dem es einen
einen erheblichen Unterschied in der Anzahl der Stichproben in jeder Klasse der Trainingsdaten gibt. Zum Beispiel
Beispiel: In einem Trainingsdatensatz für die Erkennung von Lungenkrebs anhand von Röntgenbildern,
99,99 % der Röntgenbilder könnten normale Lungen sein, und nur 0,01 % könnten
Krebszellen enthalten.
Ein Klassenungleichgewicht kann auch bei Regressionsaufgaben auftreten, bei denen die Kennzeichnungen kontinuierlich sind.
sind. Nehmen wir die Aufgabe der Schätzung von Gesundheitsrechnungen.^25 Gesundheitsrechnungen sind stark
Der Median der Rechnung ist niedrig, aber das 95. Perzentil der Rechnung ist astronomisch. Wenn
Vorhersage von Krankenhausrechnungen könnte es wichtiger sein, die Rechnungen am
Perzentil genau vorherzusagen als den Median. Eine Abweichung von 100 % bei einer Rechnung von 250 $ ist
akzeptabel (tatsächliche 500 $, vorhergesagte 250 $), aber eine 100 %ige Abweichung bei einer Rechnung über 10 000 $ ist nicht
102 | Kapitel 4: Trainingsdaten
26 Andrew Ng, "Bridging AI's Proof-of-Concept to Production Gap" (HAI Seminar, 22. September 2020), Video,
1:02:07, https://oreil.ly/FSFWS.
27 Aus diesem Grund ist die Genauigkeit eine schlechte Metrik für Aufgaben mit Klassenungleichgewicht, wie wir im Abschnitt
"Umgang mit Klassenungleichgewichten" auf Seite 105.

(tatsächliche $20k, vorhergesagte $10k). Daher müssen wir das Modell möglicherweise so trainieren, dass es
Perzentil-Rechnungen besser vorhersagen zu können, auch wenn sich dadurch die Gesamtkennzahlen verringern.
Herausforderungen des Klassenungleichgewichts
ML, insbesondere Deep Learning, funktioniert gut in Situationen, in denen die Datenverteilung ausgewogen ist.
ausgeglichener ist, und normalerweise nicht so gut, wenn die Klassen stark unausgewogen sind, wie
in Abbildung 4-8 dargestellt. Ein Ungleichgewicht zwischen den Klassen kann das Lernen aus folgenden Gründen erschweren
drei Gründe.
Abbildung 4-8. ML funktioniert gut in Situationen, in denen die Klassen ausgeglichen sind. Quelle: Angepasst
von einem Bild von Andrew Ng^26
Der erste Grund ist, dass ein Ungleichgewicht der Klassen oft bedeutet, dass das Signal für
Signal für Ihr Modell, um die Minderheitsklassen zu erkennen. In dem Fall, dass es eine kleine
Anzahl von Instanzen in der Minderheitenklasse gibt, wird das Problem zu einem "few-shot" Lernproblem
Problem, bei dem Ihr Modell die Minderheitenklasse nur ein paar Mal zu sehen bekommt, bevor es
bevor es eine Entscheidung über sie treffen muss. In dem Fall, dass es keine Instanz der seltenen
Klassen in Ihrem Trainingssatz gibt, könnte Ihr Modell annehmen, dass diese seltenen Klassen nicht existieren.
Der zweite Grund ist, dass ein Ungleichgewicht der Klassen es Ihrem Modell leichter macht
in einer nicht-optimalen Lösung stecken bleibt, indem es eine einfache Heuristik ausnutzt, anstatt
etwas Nützliches über das zugrunde liegende Muster der Daten zu lernen. Betrachten Sie das vorangegangene Beispiel der Lungen
Beispiel für die Erkennung von Lungenkrebs. Wenn Ihr Modell lernt, immer die Mehrheitsklasse auszugeben,
beträgt seine Genauigkeit bereits 99,99 %.^27 Diese Heuristik kann für den Gradientenabstieg sehr schwierig sein
Ungleichgewicht der Klassen | 103
28 Ich stellte mir vor, dass es einfacher wäre, die ML-Theorie zu lernen, wenn ich nicht herausfinden müsste, wie man mit Klassenungleichgewicht umgeht.
29 The Nilson Report, "Payment Card Fraud Losses Reach $27.85 Billion," PR Newswire, November 21, 2019,
https://oreil.ly/NM5zo.
30 "Job Market Expert Explains Why Only 2% of Job Seekers Get Interviewed," WebWire, January 7, 2014,
https://oreil.ly/UpL8S.
31 "Email and Spam Data," Talos Intelligence, letzter Zugriff Mai 2021, https://oreil.ly/lI5Jr.

Algorithmen zu schlagen, weil eine kleine Menge an Zufälligkeit zu dieser Heuristik hinzugefügt
zu einer schlechteren Genauigkeit führen kann.
Der dritte Grund ist, dass das Ungleichgewicht der Klassen zu asymmetrischen Fehlerkosten führt - die Kosten
einer falschen Vorhersage bei einer Stichprobe der seltenen Klasse viel höher sein können als eine
falschen Vorhersage bei einer Stichprobe der Mehrheitsklasse.
Zum Beispiel ist eine falsche Klassifizierung auf einem Röntgenbild mit Krebszellen viel gefährlicher
gefährlicher als eine Fehlklassifizierung bei einer Röntgenaufnahme einer normalen Lunge. Wenn Ihre Verlustfunktion nicht
Wenn Ihre Verlustfunktion nicht so konfiguriert ist, dass sie diese Asymmetrie berücksichtigt, wird Ihr Modell alle Proben gleich behandeln.
Infolgedessen erhalten Sie möglicherweise ein Modell, das sowohl in der Mehrheitsklasse als auch in der Minderheitsklasse gleich gut abschneidet.
und Minderheitsklassen gleich gut abschneidet, während Sie ein Modell bevorzugen, das in der
der Mehrheitsklasse, aber viel besser in der Minderheitsklasse.
Als ich in der Schule war, hatten die meisten Datensätze, die ich bekam, mehr oder weniger ausgeglichene Klassen.^28
Es war ein Schock für mich, als ich anfing zu arbeiten und feststellte, dass ein Ungleichgewicht der Klassen die Norm ist. In
der realen Welt sind seltene Ereignisse oft interessanter (oder gefährlicher) als
als regelmäßige Ereignisse, und viele Aufgaben konzentrieren sich auf die Erkennung dieser seltenen Ereignisse.
Das klassische Beispiel für Aufgaben mit Klassenungleichgewicht ist die Betrugserkennung. Die meisten Kredit
Kartentransaktionen sind nicht betrügerisch. Im Jahr 2018 sind 6,8¢ pro 100 $ an Ausgaben des Karteninhabers
Ausgaben eines Karteninhabers betrügerisch.^29 Ein weiteres Beispiel ist die Vorhersage von Abwanderung. Die meisten Ihrer Kunden
Kunden haben wahrscheinlich nicht vor, ihr Abonnement zu kündigen. Wenn doch, hat Ihr
Unternehmen mehr Sorgen machen als Algorithmen zur Abwanderungsvorhersage. Andere Beispiele
Krankheitsscreening (die meisten Menschen sind zum Glück nicht unheilbar krank) und
das Screening von Lebensläufen (98 % der Arbeitssuchenden werden bei der ersten Durchsicht des Lebenslaufs aussortiert^30 ).
Ein weniger offensichtliches Beispiel für eine Aufgabe mit Klassenungleichgewicht ist die Objekterkennung. Objekt
Erkennungsalgorithmen arbeiten derzeit mit der Erzeugung einer großen Anzahl von Bounding Boxes
über ein Bild und sagen dann voraus, in welchen Boxen sich am ehesten Objekte befinden.
Die meisten Bounding Boxes enthalten kein relevantes Objekt.
Abgesehen von den Fällen, in denen das Ungleichgewicht der Klassen dem Problem inhärent ist, kann ein Ungleichgewicht der Klassen
auch durch Verzerrungen während des Stichprobenverfahrens verursacht werden. Betrachten Sie den Fall, dass Sie
Trainingsdaten erstellen möchten, um zu erkennen, ob eine E-Mail Spam ist oder nicht. Sie beschließen
alle anonymisierten E-Mails aus der E-Mail-Datenbank Ihres Unternehmens zu verwenden. Nach Angaben von
Talos Intelligence sind im Mai 2021 fast 85 % aller E-Mails Spam.^31 Aber die meisten Spam
104 | Kapitel 4: Trainingsdaten
32 Nathalie Japkowciz und Shaju Stephen, "The Class Imbalance Problem: A Systematic Study", 2002,
https://oreil.ly/d7lVu.
33 Nathalie Japkowicz, "The Class Imbalance Problem: Significance and Strategies," 2000, https://oreil.ly/Ma50Z.
34 Wan Ding, Dong-Yan Huang, Zhuo Chen, Xinguo Yu, and Weisi Lin, "Facial Action Recognition Using
Very Deep Networks for Highly Imbalanced Class Distribution," 2017 Asia-Pacific Signal and Information
Processing Association Annual Summit and Conference (APSIPA ASC), 2017, https://oreil.ly/WeW6J.

E-Mails herausgefiltert wurden, bevor sie die Datenbank Ihres Unternehmens erreichten, so dass in Ihrem
Datensatz nur ein kleiner Prozentsatz Spam ist.
Eine weitere, wenn auch weniger häufige Ursache für ein Ungleichgewicht zwischen den Klassen sind Beschriftungsfehler.
Die Kommentatoren könnten die Anweisungen falsch gelesen haben oder die falschen Anweisungen befolgt haben (weil sie dachten, es gäbe nur zwei Arten von Spam).
befolgt haben (sie dachten, es gäbe nur zwei Klassen, POSITIV und NEGATIV, während es
während es in Wirklichkeit drei gibt), oder sie haben sich einfach geirrt. Wann immer man mit dem Problem des Klassenungleichgewichts
Klassenungleichgewicht konfrontiert ist, ist es wichtig, Ihre Daten zu untersuchen, um die Ursachen dafür zu verstehen.
Umgang mit Klassenungleichgewichten
Aufgrund ihrer Häufigkeit in realen Anwendungen wurde das Klassenungleichgewicht in den letzten zwei Jahren gründlich untersucht.
^32 Klassenungleichgewicht wirkt sich je nach Grad des Ungleichgewichts unterschiedlich auf Aufgaben aus.
je nach dem Grad der Unausgewogenheit. Einige Aufgaben reagieren empfindlicher auf Klassenungleichgewicht
als andere. Japkowicz zeigte, dass die Empfindlichkeit gegenüber Ungleichgewicht mit der
Komplexität des Problems zunimmt und dass nichtkomplexe, linear trennbare Probleme
nicht von allen Stufen des Klassenungleichgewichts betroffen sind.^33 Klassenungleichgewicht bei binären Klassifizierungs
Klassifizierungsproblemen ist ein viel einfacheres Problem als ein Klassenungleichgewicht bei Mehrklassen
Problemen. Ding et al. zeigten, dass sehr tiefe neuronale Netze - wobei "sehr tief"
d. h. mit mehr als 10 Schichten - im Jahr 2017 bei unausgewogenen Daten viel besser abschneiden
als flachere neuronale Netze.^34
Es wurden viele Techniken vorgeschlagen, um die Auswirkungen des Klassenungleichgewichts abzumildern.
Da neuronale Netze jedoch immer größer und tiefer werden und über mehr Lernkapazität verfügen
mehr Lernkapazität haben, könnten einige argumentieren, dass man nicht versuchen sollte, das Klassenungleichgewicht zu "beheben".
wenn die Daten in der realen Welt so aussehen. Ein gutes Modell sollte lernen, dieses
dieses Ungleichgewicht zu modellieren. Die Entwicklung eines Modells, das dafür gut genug ist, kann jedoch eine
schwierig sein, so dass wir immer noch auf spezielle Trainingstechniken zurückgreifen müssen.
In diesem Abschnitt werden wir drei Ansätze zur Behandlung von Klassenungleichgewicht behandeln: Auswahl
Auswahl der richtigen Metriken für Ihr Problem; Methoden auf Datenebene, d. h. Änderung der
Datenverteilung ändern, um sie weniger unausgewogen zu machen, und Methoden auf Algorithmus-Ebene, die
Methoden auf Algorithmenebene, d. h. Änderung der Lernmethode, um sie robuster gegenüber Klassenungleichgewicht zu machen.
Diese Techniken sind zwar notwendig, aber nicht ausreichend. Für einen umfassenden Überblick,
empfehle ich "Survey on Deep Learning with Class Imbalance" (Johnson und Khosh-
goftaar 2019).
Class Imbalance | 105
Verwendung der richtigen Bewertungsmetriken

Das Wichtigste bei einer Aufgabe mit unausgewogenen Klassen ist die Wahl
geeigneten Bewertungsmaßstäben. Falsche Metriken vermitteln Ihnen falsche Vorstellungen davon
wie Ihre Modelle abschneiden, und können Ihnen folglich nicht helfen, Modelle zu entwickeln oder
Modelle auszuwählen, die für Ihre Aufgabe geeignet sind.

Die Gesamtgenauigkeit und die Fehlerrate sind die am häufigsten verwendeten Messgrößen zur Angabe der
der Leistung von ML-Modellen. Diese Kennzahlen sind jedoch unzureichend für Aufgaben mit
unausgewogenen Klassen, da sie alle Klassen gleich behandeln, was bedeutet, dass die Leistung des
Modells in der Mehrheitsklasse diese Metriken dominieren wird. Dies ist besonders schlecht
wenn die Mehrheitsklasse nicht das ist, was Sie interessiert.

Betrachten Sie eine Aufgabe mit zwei Etiketten: CANCER (die positive Klasse) und NORMAL (die
negative Klasse), wobei 90 % der beschrifteten Daten NORMAL sind. Betrachten Sie zwei Modelle, A
und B, mit den in den Tabellen 4-4 und 4-5 dargestellten Konfusionsmatrizen.

Tabelle 4-4. Konfusionsmatrix von Modell A; Modell A kann 10 von 100 KREBS-Fällen erkennen

Modell A Tatsächlicher KREBS Tatsächlicher NORMALER
Vorhersage CANCER 10 10
Vorhersage NORMAL 90 890
Tabelle 4-5. Konfusionsmatrix von Modell B; Modell B kann 90 von 100 CANCER-Fällen erkennen

Modell B Tatsächlicher KREBS Tatsächlicher NORMALER
Vorhersage CANCER 90 90
Vorhersage NORMAL 10 810
Wenn Sie wie die meisten Menschen sind, würden Sie wahrscheinlich Modell B bevorzugen, um Vorhersagen für Sie zu treffen
da es eine bessere Chance hat, Ihnen zu sagen, ob Sie tatsächlich Krebs haben. Allerdings haben beide
haben beide die gleiche Genauigkeit von 0,9.

Metriken, die Ihnen helfen, die Leistung Ihres Modells in Bezug auf bestimmte
Klassen zu verstehen, wären die bessere Wahl. Die Genauigkeit kann immer noch eine gute Metrik sein, wenn Sie sie für
jede Klasse einzeln verwenden. Die Genauigkeit von Modell A für die Klasse KREBS beträgt 10 % und
die Genauigkeit von Modell B für die Klasse CANCER beträgt 90 %.

106 | Kapitel 4: Trainingsdaten

35 Ab Juli 2021 ist pos_label bei der Verwendung von scikit-learn.metrics.f1_score standardmäßig auf 1 gesetzt, aber Sie können
Sie können es jedoch auf 0 ändern, wenn Sie 0 als positive Bezeichnung verwenden möchten.

F1, Precision und Recall sind Metriken, die die Leistung Ihres Modells in Bezug auf die
in Bezug auf die positive Klasse bei binären Klassifizierungsproblemen, da sie auf wahrer
positiv - ein Ergebnis, bei dem das Modell die positive Klasse korrekt vorhersagt.^35
Präzision, Rückruf und F1
Für Leser, die eine Auffrischung benötigen, werden die Werte für Präzision, Rückruf und F1 für binäre Aufgaben
werden anhand der Anzahl wahrer Positiver, wahrer Negativer, falscher Positiver und falscher
Negative. Die Definitionen für diese Begriffe sind in Tabelle 4-6 aufgeführt.
Tabelle 4-6. Definitionen von Wahr Positiv, Falsch Positiv, Falsch Negativ,
und Wahr-Negativ in einer binären Klassifizierungsaufgabe
Vorausgesagte Positive Vorausgesagte Negative
Positives Label Wahres Positiv (Treffer) Falsches Negativ (Fehler vom Typ II, Fehlschlag)
Negativkennzeichen Falsch Positiv (Fehler vom Typ I, falscher Alarm) Wahr Negativ (korrekte Ablehnung)
Präzision = Richtig Positiv / (Richtig Positiv + Falsch Positiv)
Rückruf = Wahrer Positivwert / (Wahrer Positivwert + Falscher Negativwert)
F1 = 2 × Präzision × Rückruf / (Präzision + Rückruf)
F1, Präzision und Recall sind asymmetrische Metriken, was bedeutet, dass sich ihre Werte
sich ändern, je nachdem, welche Klasse als die positive Klasse betrachtet wird. In unserem Fall, wenn wir
Wenn wir in unserem Fall CANCER als die positive Klasse betrachten, beträgt der F1-Wert von Modell A 0,17. Wenn wir jedoch
NORMAL als Positivklasse betrachtet wird, beträgt der F1-Wert von Modell A 0,95. Genauigkeit, Präzision, Erinnerungswert und F1
Werte von Modell A und Modell B, wenn CANCER die positive Klasse ist, sind in
Tabelle 4-7.
Tabelle 4-7. Beide Modelle haben die gleiche Genauigkeit, obwohl ein Modell eindeutig überlegen ist
CANCER (1) NORMAL (0) Genauigkeit Präzision Rückruf F1
Modell A 10/100 890/900 0,9 0,5 0,1 0,17
Modell B 90/100 810/900 0,9 0,5 0,9 0,64
Viele Klassifizierungsprobleme können als Regressionsprobleme modelliert werden. Ihr Modell
kann eine Wahrscheinlichkeit ausgeben, und auf der Grundlage dieser Wahrscheinlichkeit klassifizieren Sie die Stichprobe. Für
Beispiel: Wenn der Wert größer als 0,5 ist, ist es ein positives Label, und wenn er kleiner als oder
Klassenungleichgewicht | 107
36 Jesse Davis und Mark Goadrich, "The Relationship Between Precision-Recall and ROC Curves," Proceedings of
International Conference on Machine Learning, 2006, https://oreil.ly/s40F3.

gleich 0,5 ist, handelt es sich um eine negative Kennzeichnung. Das bedeutet, dass Sie den Schwellenwert so einstellen können
den Schwellenwert so einstellen, dass die Rate der echten Positiven (auch Recall genannt) steigt und die Rate der falschen Positiven
Rate (auch als Fehlalarmwahrscheinlichkeit bezeichnet) zu senken und andersherum. Wir können die wahr-positive
Positiv-Rate gegen die Falsch-Positiv-Rate für verschiedene Schwellenwerte darstellen. Diese Darstellung ist bekannt als
als ROC-Kurve (Receiver Operating Characteristics) bekannt. Wenn Ihr Modell perfekt ist,
beträgt der Recall 1,0, und die Kurve ist nur eine Linie an der Spitze. Diese Kurve zeigt Ihnen, wie
wie sich die Leistung Ihres Modells in Abhängigkeit vom Schwellenwert ändert, und hilft Ihnen bei der
Schwellenwert zu wählen, der für Sie am besten geeignet ist. Je näher an der perfekten Linie, desto besser ist die Leistung Ihres
die Leistung Ihres Modells.
Die Fläche unter der Kurve (AUC) misst die Fläche unter der ROC-Kurve. Da
je näher an der perfekten Linie, desto besser, und je größer diese Fläche ist, desto besser, wie in
Abbildung 4-9.
Abbildung 4-9. ROC-Kurve
Wie F1 und Recall konzentriert sich auch die ROC-Kurve nur auf die positive Klasse und zeigt nicht an
wie gut Ihr Modell in der negativen Klasse abschneidet. Davis und Goadrich schlugen vor, dass
dass wir stattdessen die Präzision gegen den Recall auftragen sollten, was sie als Präzisions- und
Recall-Kurve. Sie argumentierten, dass diese Kurve ein informativeres Bild der Leistung eines
die Leistung eines Algorithmus bei Aufgaben mit starkem Klassenungleichgewicht.^36
108 | Kapitel 4: Trainingsdaten
37 Rafael Alencar, "Resampling Strategies for Imbalanced Datasets", Kaggle, https://oreil.ly/p8Whs.
38 Ivan Tomek, "An Experiment with the Edited Nearest-Neighbor Rule," IEEE Transactions on Systems, Man,
and Cybernetics (Juni 1976): 448-52, https://oreil.ly/JCxHZ.
39 N.V. Chawla, K.W. Bowyer, L.O. Hall, und W.P. Kegelmeyer, "SMOTE: Synthetic Minority Over-sampling
Technique", Journal of Artificial Intelligence Research 16 (2002): 341-78, https://oreil.ly/f6y46.

Methoden auf Datenebene: Resampling
Methoden auf Datenebene verändern die Verteilung der Trainingsdaten, um das Ausmaß der Unausgewogenheit zu
Ungleichgewicht zu reduzieren, um dem Modell das Lernen zu erleichtern. Eine gemeinsame Familie von Techniken
ist das Resampling. Resampling umfasst Oversampling, bei dem mehr Instanzen aus den
Minderheitsklassen hinzugefügt werden, und Unterproben, bei denen Instanzen der Mehrheitsklassen entfernt werden. Die
einfachste Art der Unterstichprobe ist das zufällige Entfernen von Instanzen aus der Mehrheitsklasse
Klasse zu entfernen, während die einfachste Art der Überstichprobe darin besteht, zufällig Kopien der
Minderheitsklasse zu erstellen, bis man ein zufriedenstellendes Verhältnis erreicht hat. Abbildung 4-10 zeigt eine
Visualisierung von Oversampling und Subsampling.
Abbildung 4-10. Veranschaulichung der Funktionsweise von Undersampling und Oversampling. Quelle:
Adaptiert von einem Bild von Rafael Alencar^37
Eine beliebte Methode zur Unterabtastung niedrigdimensionaler Daten, die bereits 1976 entwickelt wurde, ist Tomek Links.
1976 entwickelt wurde, ist Tomek links.^38 Mit dieser Technik findet man Paare von Proben aus entgegengesetzten
Klassen, die nahe beieinander liegen, und entfernen die Stichprobe der Mehrheitsklasse in jedem
Paar.
Dadurch wird die Entscheidungsgrenze zwar klarer und die Modelle können die Grenze besser erlernen.
die Grenze besser zu lernen, aber es kann das Modell weniger robust machen, weil das Modell nicht
von den Feinheiten der wahren Entscheidungsgrenze zu lernen.
Eine beliebte Methode für das Oversampling niedrigdimensionaler Daten ist SMOTE (synthetic
minority oversampling technique).^39 Sie synthetisiert neue Stichproben der Minderheitenklasse
Klassenungleichgewicht | 109
40 "Konvex" bedeutet hier ungefähr "linear".
41 Jianping Zhang und Inderjeet Mani, "kNN Approach to Unbalanced Data Distributions: A Case Study
involving Information Extraction" (Workshop on Learning from Imbalanced Datasets II, ICML, Washington,
DC, 2003), https://oreil.ly/qnpra; Miroslav Kubat und Stan Matwin, "Addressing the Curse of Imbalanced
Training Sets: One-Sided Selection", 2000, https://oreil.ly/8pheJ.
42 Hansang Lee, Minseok Park, und Junmo Kim, "Plankton Classification on Imbalanced Large Scale Database
via Convolutional Neural Networks with Transfer Learning," 2016 IEEE International Conference on Image
Processing (ICIP), 2016, https://oreil.ly/YiA8p.
43 Samira Pouyanfar, Yudong Tao, Anup Mohan, Haiman Tian, Ahmed S. Kaseb, Kent Gauen, Ryan Dailey, et
al., "Dynamic Sampling in Convolutional Neural Networks for Imbalanced Data Classification," 2018 IEEE
Conference on Multimedia Information Processing and Retrieval (MIPR), 2018, https://oreil.ly/D3Ak5.

durch Stichproben aus konvexen Kombinationen vorhandener Datenpunkte innerhalb der Minderheitenklasse
Klasse.^40
Sowohl SMOTE als auch Tomek-Links haben sich nur bei niedrigdimensionalen
Daten bewährt. Viele der ausgefeilten Resampling-Techniken, wie Near-Miss und
einseitige Auswahl,^41 erfordern die Berechnung des Abstands zwischen Instanzen oder zwischen
Instanzen und den Entscheidungsgrenzen, was bei hochdimensionalen Daten oder bei
hochdimensionalen Daten oder im hochdimensionalen Merkmalsraum, wie es bei
großen neuronalen Netzen.
Wenn Sie Ihre Trainingsdaten neu abtasten, evaluieren Sie Ihr Modell niemals auf neu abgetasteten
Daten evaluieren, da dies zu einer Überanpassung Ihres Modells an die neu abgetastete Verteilung führt.
Bei Unterabtastung besteht die Gefahr, dass wichtige Daten durch das Entfernen von Daten verloren gehen. Überabtastung -
pling birgt die Gefahr der Überanpassung der Trainingsdaten, insbesondere wenn die hinzugefügten Kopien der
Minderheitenklasse Repliken der vorhandenen Daten sind. Viele ausgeklügelte Stichprobenverfahren
wurden entwickelt, um diese Risiken zu mindern.
Eine dieser Techniken ist das Zwei-Phasen-Lernen.^42 Sie trainieren Ihr Modell zunächst auf den resam-
pelten Daten. Diese neu gesampelten Daten können durch eine zufällige Unterauswahl großer
Klassen, bis jede Klasse nur noch N Instanzen aufweist. Anschließend erfolgt die Feinabstimmung des Modells anhand der
Originaldaten.
Eine andere Technik ist das dynamische Sampling: Oversampling der wenig leistungsfähigen Klassen und
Unterstichproben der leistungsstarken Klassen während des Trainingsprozesses. Eingeführt von
Pouyanfar et al.^43 eingeführt, zielt diese Methode darauf ab, dem Modell weniger von dem zu zeigen, was es bereits gelernt hat
und mehr von dem, was es noch nicht gelernt hat.
Methoden auf Algorithmus-Ebene
Während Methoden auf Datenebene das Problem des Klassenungleichgewichts durch Veränderung der Verteilung der Trainingsdaten entschärfen, können Algorithmen
Verteilung der Trainingsdaten ändern, behalten Methoden auf Algorithmenebene die Verteilung der Trainingsdaten
Methoden auf Algorithmus-Ebene behalten die Verteilung der Trainingsdaten bei, ändern aber den Algorithmus, um ihn robuster gegen Klassenungleichheit zu machen.
110 | Kapitel 4: Trainingsdaten
44 Charles Elkan, "The Foundations of Cost-Sensitive Learning", Proceedings of the Seventeenth International
Joint Conference on Artificial Intelligence (IJCAI'01), 2001, https://oreil.ly/WGq5M.

Da die Verlustfunktion (oder die Kostenfunktion) den Lernprozess steuert, beinhalten viele
Methoden auf Algorithmenebene eine Anpassung der Verlustfunktion. Der Grundgedanke ist
Wenn es zwei Instanzen gibt, x 1 und x 2 , und der Verlust, der aus der falschen Vorhersage von x 1 resultiert
Vorhersage für x 1 höher ist als für x 2, wird das Modell der richtigen Vorhersage für x 1 den Vorzug
Vorhersage für x 1 gegenüber der richtigen Vorhersage für x 2. Indem es die Trainingsinstanzen, die uns wichtig sind
Instanzen, die uns wichtig sind, ein höheres Gewicht geben, können wir das Modell dazu bringen, sich mehr auf
Lernen dieser Instanzen konzentrieren.
Lx;θ sei der durch die Instanz x verursachte Verlust für das Modell mit dem Parametersatz θ.
Der Verlust des Modells wird oft als der durchschnittliche Verlust aller Instanzen definiert. N bezeichnet
die Gesamtzahl der Trainingsstichproben.
LX;θ = ∑xN^1 Lx;θ
Diese Verlustfunktion bewertet den durch alle Instanzen verursachten Verlust gleich, auch wenn falsche
Vorhersagen auf einigen Instanzen viel teurer sein können als falsche Vorhersagen auf
anderen Instanzen. Es gibt viele Möglichkeiten, diese Kostenfunktion zu modifizieren. In diesem Abschnitt werden wir
werden wir uns auf drei von ihnen konzentrieren, beginnend mit dem kostensensitiven Lernen.
Kostensensitives Lernen. Im Jahr 2001 schlug Elkan auf der Grundlage der Erkenntnis, dass die Fehlklassifizierung von
verschiedenen Klassen unterschiedliche Kosten verursacht, schlug Elkan das kostensensitive Lernen vor, bei dem
bei dem die individuelle Verlustfunktion modifiziert wird, um diese unterschiedlichen Kosten zu berücksichtigen.^44
Die Methode begann mit der Verwendung einer Kostenmatrix zur Angabe von Cij: die Kosten, wenn Klasse i als Klasse j klassifiziert wird.
Wenn i = j ist, handelt es sich um eine korrekte Klassifizierung, und die Kosten sind normalerweise 0. Wenn nicht, handelt es sich um eine
Fehlklassifizierung. Wenn die Klassifizierung von POSITIVEN Beispielen als NEGATIV doppelt so kostspielig ist
als umgekehrt, können Sie C 10 doppelt so hoch ansetzen wie C 01.
Wenn Sie zum Beispiel zwei Klassen haben, POSITIV und NEGATIV, kann die Kostenmatrix
wie die in Tabelle 4-8 dargestellte aussehen.
Tabelle 4-8. Beispiel für eine Kostenmatrix
Tatsächliches NEGATIV Tatsächliches POSITIV
Vorhersage NEGATIV C(0, 0) = C 00 C(1, 0) = C 10
Vorhersage POSITIV C(0, 1) = C 01 C(1, 1) = C 11
Ungleichgewicht der Klassen | 111
45 Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie, "Class-Balanced Loss Based on
Effective Number of Samples," Proceedings of the Conference on Computer Vision and Pattern, 2019,
https://oreil.ly/jCzGH.
46 Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár, "Focal Loss for Dense Object
Detection," arXiv, August 7, 2017, https://oreil.ly/Km2dF.

Der Verlust, der durch den Fall x der Klasse i verursacht wird, wird zum gewichteten Durchschnitt aller
möglichen Klassifizierungen von Instanz x.
Lx;θ = ∑jCijPjx;θ
Das Problem bei dieser Verlustfunktion ist, dass die Kostenmatrix manuell definiert werden muss.
Matrix manuell definieren muss, die für verschiedene Aufgaben in verschiedenen Größenordnungen unterschiedlich ist.
Klassenausgeglichener Verlust. Bei einem Modell, das auf einem unausgewogenen Datensatz trainiert wurde, kann es passieren
unausgewogenen Datensatz trainiert wurde, kann dazu führen, dass es die Mehrheitsklassen bevorzugt und falsche Vorhersagen für Minderheitsklassen macht.
Klassen. Was wäre, wenn wir das Modell dafür bestrafen, dass es falsche Vorhersagen für Minderheitsklassen macht
Klassen bestrafen, um diese Verzerrung zu korrigieren?
In seiner einfachen Form können wir die Gewichtung jeder Klasse umgekehrt proportional zur
Anzahl der Proben in dieser Klasse, so dass die selteneren Klassen höher gewichtet werden. In der
folgenden Gleichung steht N für die Gesamtzahl der Trainingsstichproben:
Wi=Anzahl der Stichproben der Klasse iN
Der Verlust, der durch den Fall x der Klasse i verursacht wird, wird wie folgt berechnet, wobei Loss(x, j) für
der Verlust, wenn x als Klasse j klassifiziert wird. Es kann die Kreuzentropie oder eine andere Verlustfunktion
Funktion sein.
Lx;θ =Wi∑jPjx;θLossx,j
Eine ausgefeiltere Version dieses Verlustes kann die Überschneidung zwischen
vorhandenen Stichproben berücksichtigen, wie z. B. der klassengleiche Verlust auf der Grundlage der effektiven Anzahl der Stichproben.^45
Fokaler Verlust. In unseren Daten sind einige Beispiele leichter zu klassifizieren als andere, und unser
Modell könnte lernen, sie schnell zu klassifizieren. Wir möchten unser Modell dazu anregen
sich auf das Lernen der Beispiele zu konzentrieren, bei denen es noch Schwierigkeiten bei der Klassifizierung hat. Was wäre, wenn wir den
Verlust so anpassen, dass eine Probe, die eine geringere Wahrscheinlichkeit hat, richtig zu sein, ein höheres
gewichtet? Genau das tut der fokale Verlust.^46 Die Gleichung für den fokalen Verlust und seine
Leistung im Vergleich zum Kreuzentropieverlust ist in Abbildung 4-11 dargestellt.
112 | Kapitel 4: Trainingsdaten
47 Mikel Galar, Alberto Fernandez, Edurne Barrenechea, Humberto Bustince, und Francisco Herrera, "A Review
on Ensembles for the Class Imbalance Problem: Bagging-, Boosting-, and Hybrid-Based Approaches," IEEE
Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews) 42, no. 4 (July 2012):
463-84, https://oreil.ly/1ND4g.

In der Praxis hat sich gezeigt, dass Ensembles bei der Lösung des Problems des Klassenungleichgewichts hilfreich sind.^47
wir in diesem Abschnitt nicht auf Ensembles eingehen, da das Ungleichgewicht der Klassen normalerweise nicht
warum Ensembles verwendet werden. Ensembletechniken werden in Kapitel 6 behandelt.
Abbildung 4-11. Das mit Focal Loss (FL) trainierte Modell zeigt geringere Verlustwerte im Vergleich
dem mit Cross-Entropie-Verlust (CE) trainierten Modell. Quelle: Angepasst an ein Bild von Lin
et al.
Datenerweiterung
Bei der Datenerweiterung handelt es sich um eine Reihe von Techniken, die dazu dienen, die Menge der
von Trainingsdaten zu erhöhen. Traditionell werden diese Techniken für Aufgaben verwendet, die nur begrenzte
Trainingsdaten haben, wie zum Beispiel in der medizinischen Bildgebung. In den letzten Jahren hat sich jedoch gezeigt, dass sie
in den letzten Jahren jedoch gezeigt, dass sie auch dann nützlich sind, wenn wir viele Daten haben - erweiterte Daten können unsere
Modelle robuster gegen Rauschen und sogar gegen Angriffe von außen machen.
Die Datenerweiterung ist zu einem Standardschritt bei vielen Aufgaben der Computer Vision geworden und
und findet ihren Weg in die Verarbeitung natürlicher Sprache (NLP). Die Techniken
hängen stark vom Datenformat ab, da sich die Bildverarbeitung von der Textverarbeitung
Manipulation. In diesem Abschnitt werden wir drei Hauptarten der Datenerweiterung behandeln:
einfache beschriftungserhaltende Transformationen; Perturbation, ein Begriff für "Hinzufügen".
Datenerweiterung | 113
48 Alex Krizhevsky, Ilya Sutskever, und Geoffrey E. Hinton, "ImageNet Classification with Deep Convolutional
Neural Networks", 2012, https://oreil.ly/aphzA.
49 Jiawei Su, Danilo Vasconcellos Vargas, und Sakurai Kouichi, "One Pixel Attack for Fooling Deep Neural
Networks," IEEE Transactions on Evolutionary Computation 23, no. 5 (2019): 828-41, https://oreil.ly/LzN9D.

Geräusche"; und Datensynthese. Für jede Art von Daten werden wir Beispiele sowohl für die Computer
Vision und NLP.
Einfache kennzeichenerhaltende Transformationen
Beim Computer Vision besteht die einfachste Technik zur Datenerweiterung darin, ein Bild zufällig zu verändern
eines Bildes unter Beibehaltung seiner Beschriftung. Sie können das Bild durch Beschneiden, Spiegeln,
Drehen, Invertieren (horizontal oder vertikal), Löschen eines Teils des Bildes und vieles mehr.
Dies ist sinnvoll, denn ein gedrehtes Bild eines Hundes ist immer noch ein Hund. Übliche ML-Frame-
Werke wie PyTorch, TensorFlow und Keras haben alle Unterstützung für Bildvergrößerungen.
Laut Krizhevsky et al. in ihrem legendären AlexNet-Papier "werden die transformierten
Bilder werden in Python-Code auf der CPU generiert, während die GPU mit dem
den vorherigen Stapel von Bildern trainiert. Diese Datenerweiterungsschemata sind also in der Tat,
rechnerisch kostenlos."^48
In der NLP kann man ein Wort nach dem Zufallsprinzip durch ein ähnliches Wort ersetzen, unter der Annahme, dass diese
Ersetzung die Bedeutung oder die Stimmung des Satzes nicht verändern würde, wie
in Tabelle 4-9. Ähnliche Wörter können entweder mit einem Wörterbuch mit synonymen Wörtern gefunden werden
Wörter oder durch die Suche nach Wörtern, deren Einbettungen nahe beieinander liegen, in einem Wort
Einbettungsraum liegen.
Tabelle 4-9. Drei aus einem Originalsatz generierte Sätze
Originalsatz Ich bin so froh, dich zu sehen.
Generierte Sätze Ich bin so froh, dich zu sehen.
Ich bin so froh, euch zu sehen.
Ich bin sehr froh, dich zu sehen.
Diese Art der Datenerweiterung ist ein schneller Weg, um Ihre Trainingsdaten zu verdoppeln oder zu verdreifachen.
Perturbation
Perturbation ist ebenfalls eine markierungserhaltende Operation, aber da sie manchmal dazu verwendet wird
Modelle dazu zu bringen, falsche Vorhersagen zu treffen, habe ich gedacht, dass sie einen eigenen Abschnitt verdient.
Neuronale Netze sind im Allgemeinen empfindlich gegenüber Rauschen. Im Fall von Computer Vision bedeutet dies
bedeutet dies, dass das Hinzufügen einer kleinen Menge Rauschen zu einem Bild dazu führen kann, dass ein neuronales Netz
das Bild falsch klassifiziert. Su et al. zeigten, dass 67,97 % der natürlichen Bilder im Kaggle
CIFAR-10-Testdatensatzes und 16,04 % der ImageNet-Testbilder falsch klassifiziert werden können, wenn
Änderung von nur einem Pixel falsch klassifiziert werden können (siehe Abbildung 4-12).^49
114 | Kapitel 4: Trainingsdaten
50 Su et al., "One Pixel Attack".

Abbildung 4-12. Die Änderung eines Pixels kann dazu führen, dass ein neuronales Netz falsche Vorhersagen macht.
Die drei verwendeten Modelle sind AllConv, NiN und VGG. Die ursprünglichen Beschriftungen, die von diesen
Modelle erstellten Beschriftungen liegen über den Beschriftungen, die nach der Änderung eines Pixels erstellt wurden. Quelle: Su et al.^50
Datenerweiterung | 115
51 Ian J. Goodfellow, Jonathon Shlens, und Christian Szegedy, "Explaining and Harnessing Adversarial Exam-
ples," arXiv, March 20, 2015, https://oreil.ly/9v2No; Ian J. Goodfellow, David Warde-Farley, Mehdi Mirza,
Aaron Courville, und Yoshua Bengio, "Maxout Networks, arXiv, February 18, 2013, https://oreil.ly/L8mch.
52 Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, und Pascal Frossard, "DeepFool: A Simple and Accurate
Method to Fool Deep Neural Networks," in Proceedings of IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2016, https://oreil.ly/dYVL8.
53 Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii, "Virtual Adversarial Training: A Regula-
rization Method for Supervised and Semi-Supervised Learning," IEEE Transactions on Pattern Analysis and
Machine Intelligence, 2017, https://oreil.ly/MBQeu.
54 Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding".

Die Verwendung trügerischer Daten, um ein neuronales Netz zu falschen Vorhersagen zu verleiten, wird
nennt man gegnerische Angriffe. Das Hinzufügen von Rauschen zu Stichproben ist eine gängige Technik zur Erzeugung
Proben zu erzeugen. Der Erfolg dieser Angriffe ist umso größer, je höher die
Auflösung der Bilder zunimmt.
Das Hinzufügen von verrauschten Proben zu den Trainingsdaten kann den Modellen helfen, die Schwachstellen in
Schwachstellen in ihrer gelernten Entscheidungsgrenze zu erkennen und ihre Leistung zu verbessern.^51 Verrauschte Stichproben können
entweder durch Hinzufügen von Zufallsrauschen oder durch eine Suchstrategie erzeugt werden. Moosavi-Dezfooli
et al. schlugen einen Algorithmus namens DeepFool vor, der die kleinstmögliche Rauschinjektion findet
Rauschinjektion findet, die erforderlich ist, um eine Fehlklassifizierung mit hoher Sicherheit zu verursachen.^52 Diese Art der
Augmentation wird als adversarische Augmentation bezeichnet.^53
Adversarial augmentation ist im NLP weniger verbreitet (ein Bild eines Bären mit zufällig
(ein Bild eines Bären mit zufällig hinzugefügten Pixeln sieht immer noch wie ein Bär aus, aber das Hinzufügen zufälliger Zeichen zu einem zufälligen
Satz wird durch das Hinzufügen zufälliger Zeichen wahrscheinlich zu Kauderwelsch), aber Perturbation wurde verwendet, um
Modelle robuster zu machen. Eines der bemerkenswertesten Beispiele ist BERT, bei dem das Modell
15 % aller Token in jeder Sequenz zufällig auswählt und 10 % der Token durch zufällige Wörter ersetzt.
der ausgewählten Token durch zufällige Wörter ersetzt. Zum Beispiel, wenn der Satz "Mein Hund ist
haarig", und das Modell ersetzt zufällig "haarig" durch "Apfel", wird der Satz zu
"Mein Hund ist ein Apfel". 1,5 % aller Token könnten also zu einer unsinnigen Bedeutung führen. Ihre
Ablationsstudien zeigen, dass ein kleiner Teil der zufälligen Ersetzung ihrem Modell einen
kleinen Leistungsschub gibt.^54
In Kapitel 6 werden wir uns damit beschäftigen, wie man Perturbation nicht nur als Mittel zur Verbesserung der
Leistung Ihres Modells zu verbessern, sondern auch als Möglichkeit, seine Leistung zu bewerten.
Datensynthese
Da das Sammeln von Daten teuer und langsam ist und viele potenzielle Probleme mit dem Datenschutz mit sich bringt, wäre es
wäre es ein Traum, wenn wir dies ganz umgehen und unsere Modelle mit synthetischen Daten trainieren könnten.
Daten trainieren. Auch wenn wir noch weit davon entfernt sind, alle Trainingsdaten synthetisieren zu können, ist es
ist es möglich, einige Trainingsdaten zu synthetisieren, um die Leistung eines Modells zu verbessern.
116 | Kapitel 4: Trainingsdaten
55 Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin und David Lopez-Paz, "mixup: Beyond Empirical Risk
Minimization," ICLR 2018, https://oreil.ly/lIM5E.
56 Veit Sandfort, Ke Yan, Perry J. Pickhardt, und Ronald M. Summers, "Data Augmentation Using Generative
Adversarial Networks (CycleGAN) to Improve Generalizability in CT Segmentation Tasks," Scientific Reports
9, no. 1 (2019): 16884, https://oreil.ly/TDUwm.

Im NLP können Vorlagen eine kostengünstige Möglichkeit sein, ein Modell zu erstellen. Ein Team, mit dem ich gearbeitet habe
mit dem ich zusammengearbeitet habe, verwendete Vorlagen, um Trainingsdaten für seine Konversations-KI (Chatbot) zu erstellen.
Eine Vorlage könnte wie folgt aussehen: "Finde ein [GESCHMACK] Restaurant innerhalb von [ANZAHL]
Meilen von [STANDORT]" (siehe Tabelle 4-10). Mit Listen aller möglichen Küchen, vernünftigen
Zahlen (Sie würden wahrscheinlich nie nach Restaurants suchen wollen, die weiter als 1.000
Meilen) und Standorten (Wohnort, Büro, Wahrzeichen, genaue Adressen) für jede Stadt können Sie
Tausende von Trainingsabfragen aus einer Vorlage generieren.
Tabelle 4-10. Drei aus einer Vorlage generierte Sätze
Vorlage Finde ein [GESCHMACK] Restaurant innerhalb von [ANZAHL] Meilen von [ORT].
Generierte Abfragen Finde ein vietnamesisches Restaurant im Umkreis von 2 Meilen von meinem Büro.
Finde ein thailändisches Restaurant im Umkreis von 5 Meilen von meinem Zuhause.
Finde ein mexikanisches Restaurant im Umkreis von 3 Meilen von der Google-Zentrale.
In der Computer Vision besteht eine einfache Möglichkeit, neue Daten zu synthetisieren, darin, vorhandene Beispiele mit
Beispiele mit diskreten Bezeichnungen zu kombinieren, um kontinuierliche Bezeichnungen zu erzeugen. Betrachten wir eine Aufgabe zur
Klassifizierung von Bildern mit zwei möglichen Bezeichnungen: DOG (kodiert als 0) und CAT (kodiert
als 1). Aus dem Beispiel x 1 mit dem Label DOG und dem Beispiel x 2 mit dem Label CAT kann man x'
wie z.B.:
x′=γx 1 +1 -γx 2
Das Etikett von x' ist eine Kombination aus den Etiketten von x 1 und x 2: γ× 0 + 1 -γ × 1. Diese
Methode wird Mixup genannt. Die Autoren haben gezeigt, dass Mixup die Generalisierung der Modelle verbessert
die Generalisierbarkeit der Modelle verbessert, ihre Merkfähigkeit für fehlerhafte Kennzeichnungen verringert
und das Training von generativen adversen Netzen stabilisiert.^55
Die Verwendung neuronaler Netze zur Synthese von Trainingsdaten ist ein spannender Ansatz, der
aktiv erforscht wird, aber in der Produktion noch nicht weit verbreitet ist. Sandfort et al. zeigten
dass sie durch Hinzufügen von mit CycleGAN generierten Bildern zu ihren ursprünglichen Trainingsdaten
die Leistung ihres Modells bei der Segmentierung von Computertomographen (CT) deutlich
Computertomographie (CT)-Segmentierungsaufgaben erheblich verbessern konnten.^56
Wenn Sie mehr über Datenerweiterung für Computer Vision erfahren möchten, lesen Sie "A
Survey on Image Data Augmentation for Deep Learning" (Shorten und Khoshgoftaar
2019) ist eine umfassende Übersicht.
Datenerweiterung | 117
Zusammenfassung
Trainingsdaten bilden nach wie vor die Grundlage für moderne ML-Algorithmen. Egal wie
Ihre Algorithmen auch sein mögen, wenn Ihre Trainingsdaten schlecht sind, werden Ihre Algorithmen nicht
Algorithmen keine gute Leistung erbringen können. Es lohnt sich, Zeit und Mühe zu investieren, um Trainingsdaten zu kuratieren und zu erstellen
Trainingsdaten zu erstellen, die es Ihren Algorithmen ermöglichen, etwas Sinnvolles zu lernen.

In diesem Kapitel haben wir die verschiedenen Schritte zur Erstellung von Trainingsdaten besprochen. Zunächst haben wir
verschiedene Stichprobenverfahren, sowohl Nichtwahrscheinlichkeitsstichproben als auch Zufallsstichproben, behandelt
pling, die uns helfen können, die richtigen Daten für unser Problem zu sammeln.

Die meisten ML-Algorithmen, die heute verwendet werden, sind überwachte ML-Algorithmen, so dass die Beschaffung von
ein integraler Bestandteil der Erstellung von Trainingsdaten. Viele Aufgaben, wie z. B. die Schätzung von Lieferzeiten
oder Empfehlungssysteme, haben natürliche Bezeichnungen. Natürliche Kennzeichnungen sind in der Regel verzögert, und
die Zeit, die von der Zustellung einer Vorhersage bis zur Rückmeldung über diese Vorhersage
ist die Länge der Rückkopplungsschleife. Aufgaben mit natürlichen Bezeichnungen sind relativ häufig
Aufgaben mit natürlichen Bezeichnungen sind in der Branche recht häufig, was bedeuten könnte, dass Unternehmen lieber mit Aufgaben
Aufgaben mit natürlichen Bezeichnungen gegenüber Aufgaben ohne natürliche Bezeichnungen.

Bei Aufgaben, für die es keine natürlichen Bezeichnungen gibt, verlassen sich die Unternehmen in der Regel auf menschliche Annotatoren
um ihre Daten zu beschriften. Die manuelle Beschriftung ist jedoch mit vielen Nachteilen verbunden. Für
zum Beispiel kann die manuelle Beschriftung teuer und langsam sein. Um den Mangel an Handbeschriftungen zu bekämpfen,
haben wir Alternativen diskutiert, darunter schwache Überwachung, Semi-Supervision, Transfer
Lernen und aktives Lernen.

ML-Algorithmen funktionieren gut in Situationen, in denen die Datenverteilung eher ausgeglichen ist,
und nicht so gut, wenn die Klassen stark unausgewogen sind. Leider sind Probleme
mit Klassenungleichgewichten in der realen Welt leider die Regel. Im folgenden Abschnitt haben wir
erörtert, warum ein Klassenungleichgewicht das Lernen für ML-Algorithmen erschwert. Wir haben auch
verschiedene Techniken zur Behandlung des Klassenungleichgewichts, von der Wahl der richtigen
Auswahl der richtigen Metriken, der Neuauswahl von Daten und der Modifizierung der Verlustfunktion, um das Modell dazu zu
bestimmten Proben Aufmerksamkeit zu schenken.

Wir haben das Kapitel mit einer Diskussion über Techniken zur Datenerweiterung abgeschlossen, die
die zur Verbesserung der Leistung und Generalisierung eines Modells sowohl für Computer Vision
und NLP-Aufgaben.

Sobald Sie Ihre Trainingsdaten haben, müssen Sie daraus Merkmale extrahieren, um Ihre ML-Modelle zu trainieren.
Ihre ML-Modelle zu trainieren, was wir im nächsten Kapitel behandeln werden.

118 | Kapitel 4: Trainingsdaten

KAPITEL 5

Technische Merkmale
Im Jahr 2014 wurde in dem Papier "Practical Lessons from Predicting Clicks on Ads at Facebook"
behauptet, dass die richtigen Merkmale das Wichtigste bei der Entwicklung ihrer
ML-Modelle. Seitdem haben viele der Unternehmen, mit denen ich zusammengearbeitet habe, immer wieder festgestellt
immer wieder festgestellt, dass sie, sobald sie ein brauchbares Modell haben, mit den richtigen Funktionen
den größten Leistungsschub im Vergleich zu cleveren algorithmischen Techniken
Techniken wie Hyperparameter-Tuning. Modernste Modellarchitekturen können
immer noch schlecht abschneiden, wenn sie nicht die richtigen Merkmale verwenden.

Aufgrund seiner Bedeutung besteht ein großer Teil der Aufgaben im Bereich ML Engineering und Data Science
ist es, neue nützliche Funktionen zu entwickeln. In diesem Kapitel werden wir uns mit gängigen
Techniken und wichtige Überlegungen im Hinblick auf die Entwicklung von Merkmalen. Wir
einen Abschnitt widmen wir einem subtilen, aber verhängnisvollen Problem, das
viele ML-Systeme in der Produktion zum Scheitern gebracht hat: Datenlecks und wie man sie erkennt und
vermeiden.

Am Ende des Kapitels wird erörtert, wie man gute Merkmale entwickelt, wobei
sowohl die Bedeutung als auch die Generalisierung von Merkmalen berücksichtigt. Wenn man über Feature
Engineering sprechen, denken manche Leute vielleicht an Merkmalspeicher. Da Merkmalspeicher näher an der
Infrastruktur zur Unterstützung mehrerer ML-Anwendungen sind, werden wir Feature Stores in
Kapitel 10.

119
1 Loris Nanni, Stefano Ghidoni und Sheryl Brahnam, "Handcrafted vs. Non-handcrafted Features for Com- puter Vision Classification.
puter Vision Classification", Pattern Recognition 71 (November 2017): 158-72, https://oreil.ly/CGfYQ; Wikipe-
dia, s.v. "Feature learning," https://oreil.ly/fJmwN.
Gelernte Merkmale versus konstruierte Merkmale
Wenn ich dieses Thema im Unterricht behandle, fragen meine Studenten häufig: "Warum müssen wir uns
über Feature-Engineering Gedanken machen? Verspricht uns Deep Learning nicht, dass wir keine Features mehr
Features entwickeln müssen?"

Sie haben Recht. Das Versprechen des Deep Learning besteht darin, dass wir Funktionen nicht mehr von Hand
Merkmale. Aus diesem Grund wird Deep Learning manchmal auch als Feature Learning bezeichnet.^1 Viele
Merkmale können von Algorithmen automatisch erlernt und extrahiert werden. Wir sind jedoch
noch weit von dem Punkt entfernt, an dem alle Merkmale automatisiert werden können. Ganz zu schweigen davon
dass zum Zeitpunkt der Erstellung dieses Dokuments die meisten ML-Anwendungen in der Produktion nicht auf Deep Learning
Lernen. Gehen wir ein Beispiel durch, um zu verstehen, welche Merkmale automatisch extrahiert werden können
extrahiert werden können und welche Merkmale noch von Hand erstellt werden müssen.

Stellen Sie sich vor, Sie wollen einen Sentiment-Analyse-Klassifikator erstellen, um festzustellen, ob ein
Kommentar Spam ist oder nicht. Vor dem Deep Learning hätten Sie bei einem Textstück
klassische Textverarbeitungstechniken wie die Lemmatisierung anwenden,
Expandieren von Kontraktionen, Entfernen von Interpunktion und Kleinschreibung. Nach
können Sie Ihren Text in n-Gramme mit n Werten Ihrer Wahl aufteilen.

Für diejenigen, die damit nicht vertraut sind, ist ein n-Gramm eine zusammenhängende Sequenz von n Elementen aus einer bestimmten
Textprobe. Bei den Elementen kann es sich um Phoneme, Silben, Buchstaben oder Wörter handeln. Ein Beispiel,
Bei dem Beitrag "Ich mag Essen" sind die 1-Gramme auf Wortebene ["Ich", "mag", "Essen"] und die
2-Gramme auf Wortebene sind ["Ich mag", "wie Essen"]. Die Menge der n-Gramm-Merkmale dieses Satzes, wenn
wir wollen, dass n 1 und 2 ist, ist: ["ich", "mögen", "Essen", "ich mag", "wie Essen"].

Abbildung 5-1 zeigt ein Beispiel für klassische Textverarbeitungstechniken, die Sie verwenden können, um
n-Gramm-Merkmale für Ihren Text zu erstellen.

120 | Kapitel 5: Feature Engineering

Abbildung 5-1. Ein Beispiel für Techniken, die Sie verwenden können, um n-gram Merkmale für Ihren Text zu erstellen
Ihren Text

Sobald Sie n-Gramme für Ihre Trainingsdaten erzeugt haben, können Sie ein Vokabular erstellen
erstellen, das jedes n-Gramm einem Index zuordnet. Dann können Sie jeden Beitrag in einen Vektor umwandeln
basierend auf den Indizes der n-Gramme konvertieren. Wenn wir zum Beispiel ein Vokabular mit sieben n-Grammen haben
wie in Tabelle 5-1 gezeigt, kann jeder Beitrag ein Vektor aus sieben Elementen sein. Jedes Element
entspricht der Anzahl, wie oft das n-Gramm mit diesem Index im Beitrag vorkommt. "I
like food" wird als Vektor [1, 1, 0, 1, 1, 0, 1] kodiert. Dieser Vektor kann dann
als Eingabe für ein ML-Modell verwendet werden.

Tabelle 5-1. Beispiel für ein 1-Gramm- und 2-Gramm-Vokabular

Ich mag gutes Essen Ich mag gutes Essen wie Essen
0 1 2 3 4 5 6
Gelernte Merkmale versus konstruierte Merkmale | 121
Das Feature-Engineering erfordert Kenntnisse über domänenspezifische Techniken - in diesem Fall,
ist die Domäne die natürliche Sprachverarbeitung (NLP) und die Muttersprache des Textes.
Es handelt sich in der Regel um einen iterativen Prozess, der spröde sein kann. Als ich diese Methode
für eines meiner ersten NLP-Projekte verfolgte, musste ich meinen Prozess immer wieder neu beginnen, weil ich entweder
vergessen hatte, eine Technik anzuwenden, oder weil sich eine Technik, die ich verwendet hatte, als
schlecht funktionierte und ich sie wieder rückgängig machen musste.

Seit dem Aufkommen des Deep Learning hat sich dieses Problem jedoch weitgehend erledigt. Stattdessen
müssen Sie sich nicht mehr um Lemmatisierung, Interpunktion oder die Entfernung von Stoppwörtern kümmern, sondern können
können Sie Ihren Rohtext einfach in Wörter aufteilen (d. h. Tokenisierung), ein Vokabular aus diesen
Vokabular erstellen und jedes Ihrer Wörter unter Verwendung dieses Vokabulars in One-Shot-Vektoren umwandeln.
Ihr Modell wird hoffentlich lernen, daraus nützliche Merkmale zu extrahieren. Bei dieser neuen
Methode ist ein Großteil der Merkmalstechnik für Text automatisiert worden. Ähnliche Fortschritte
wurden auch für Bilder gemacht. Anstatt manuell Merkmale aus Rohbildern zu extrahieren
Rohbildern zu extrahieren und diese Merkmale in Ihre ML-Modelle einzugeben, können Sie einfach
Bilder direkt in Ihre Deep-Learning-Modelle eingeben.

Ein ML-System wird jedoch wahrscheinlich Daten benötigen, die nicht nur aus Text und Bildern bestehen. Für
Bei der Erkennung, ob es sich bei einem Kommentar um Spam handelt oder nicht, können zusätzlich zum Text im
Kommentar selbst auch andere Informationen über:

Der Kommentar
Wie viele Upvotes/Downvotes hat er?

Der Benutzer, der diesen Kommentar gepostet hat
Wann wurde dieses Konto erstellt, wie oft wird es gepostet, und wie viele Upvotes/
Downvotes hat er?

Das Thema, in dem der Kommentar gepostet wurde
Wie viele Aufrufe hat der Kommentar? Beliebte Themen ziehen in der Regel mehr Spam an.

Es gibt viele Möglichkeiten, die Sie in Ihrem Modell verwenden können. Einige von ihnen sind in
in Abbildung 5-2 dargestellt. Der Prozess der Auswahl der zu verwendenden Informationen und der Extraktion
diese Informationen in ein für Ihre ML-Modelle verwendbares Format zu extrahieren, ist Feature Engineering.
Bei komplexen Aufgaben wie der Empfehlung von Videos, die sich Nutzer als Nächstes auf TikTok ansehen sollen,
kann die Anzahl der verwendeten Merkmale in die Millionen gehen. Für bereichsspezifische Aufgaben wie
wie die Vorhersage, ob eine Transaktion betrügerisch ist, benötigen Sie unter Umständen
Fachwissen über Banken und Betrug, um nützliche Funktionen zu entwickeln.

122 | Kapitel 5: Feature Engineering

2 Meiner Erfahrung nach korreliert die Art und Weise, wie gut eine Person mit fehlenden Werten für einen bestimmten Datensatz während der Befragung umgeht, stark
korreliert stark damit, wie gut sie in ihrer täglichen Arbeit zurechtkommen werden.
Abbildung 5-2. Einige der möglichen Merkmale eines Kommentars, eines Threads oder eines Benutzers, die
in Ihr Modell aufzunehmen

Gemeinsame Feature-Engineering-Vorgänge
Aufgrund der Bedeutung und der Allgegenwärtigkeit des Feature Engineering in ML-Projekten,
wurden viele Techniken entwickelt, um diesen Prozess zu rationalisieren. In diesem Abschnitt,
werden wir einige der wichtigsten Operationen besprechen, die Sie bei der
bei der Entwicklung von Merkmalen aus Ihren Daten berücksichtigen sollten. Dazu gehören der Umgang mit fehlenden
Umgang mit fehlenden Werten, Skalierung, Diskretisierung, Kodierung kategorischer Merkmale und die Generierung von
aber immer noch sehr effektiven Kreuzmerkmale sowie die neueren und aufregenden posi-
tionalen Merkmale. Diese Liste ist bei weitem nicht vollständig, aber sie umfasst
einige der gängigsten und nützlichsten Operationen, um Ihnen einen guten Ausgangspunkt zu bieten.
Tauchen wir ein!

Umgang mit fehlenden Werten
Eines der ersten Dinge, die Ihnen beim Umgang mit Daten in der Produktion auffallen könnten
ist, dass einige Werte fehlen. Was jedoch viele ML-Ingenieure, die ich befragt habe
nicht wissen, ist, dass nicht alle Arten von fehlenden Werten gleich sind.^2 Zur Veranschaulichung
um diesen Punkt zu veranschaulichen, betrachten wir die Aufgabe, vorherzusagen, ob jemand in den nächsten 12 Monaten ein Haus
nächsten 12 Monaten ein Haus kaufen wird. Ein Teil der Daten, die uns vorliegen, ist in Tabelle 5-2 dargestellt.

Gemeinsame Vorgänge bei der Featureentwicklung | 123
Tabelle 5-2. Beispieldaten für die Vorhersage des Hauskaufs in den nächsten 12 Monaten

ID Alter Geschlecht Jahreseinkommen Familienstand Anzahl der Kinder Beruf Kaufen?
1 A 150.000 1 Ingenieur Nein
2 27 B 50.000 Lehrerin Nein
3 A 100.000 Verheiratet 2 Ja
4 40 B 2 Ingenieur Ja
5 35 B Alleinstehend 0 Arzt Ja
6 A 50.000 0 Lehrerin Nein
7 33 B 60.000 Alleinstehende Lehrerin Nein
8 20 B 10.000 Student Nein
Es gibt drei Arten von fehlenden Werten. Die offiziellen Bezeichnungen für diese Typen sind ein wenig
ein wenig verwirrend, daher gehen wir auf detaillierte Beispiele ein, um die Verwirrung zu mildern.

Nicht zufällig fehlende Werte (MNAR)
Dies ist der Fall, wenn der Grund für einen fehlenden Wert im wahren Wert selbst liegt.
In diesem Beispiel könnten wir feststellen, dass einige Befragte ihr Einkommen nicht angegeben haben.
Einkommen. Bei der Untersuchung kann sich herausstellen, dass das Einkommen der Befragten, die
das Einkommen der Befragten, die keine Angaben gemacht haben, tendenziell höher ist als das derjenigen, die Angaben gemacht haben. Die Einkommen
Werte fehlen aus Gründen, die mit den Werten selbst zusammenhängen.

Zufällig fehlende Werte (MAR)
Dies ist der Fall, wenn der Grund für das Fehlen eines Wertes nicht der Wert selbst ist, sondern
auf eine andere beobachtete Variable zurückzuführen ist. In diesem Beispiel könnten wir feststellen, dass Alterswerte
für Befragte des Geschlechts "A" oft fehlen, was daran liegen könnte, dass die
Personen des Geschlechts "A" in dieser Umfrage ihr Alter nicht gerne preisgeben.

Völlig zufällig fehlende Werte (MCAR)
Dies ist der Fall, wenn es kein Muster für das Fehlen eines Wertes gibt. In diesem Beispiel könnte man
könnte man meinen, dass die fehlenden Werte für die Spalte "Stelle" völlig zufällig sind
zufällig sind, nicht wegen der Stelle selbst und auch nicht wegen einer anderen Variablen. Die Menschen
Die Leute vergessen einfach manchmal ohne besonderen Grund, diesen Wert einzutragen. Allerdings
ist diese Art des Fehlens sehr selten. Normalerweise gibt es Gründe, warum bestimmte Werte
fehlen, und Sie sollten dem nachgehen.

Wenn Sie auf fehlende Werte stoßen, können Sie entweder die fehlenden Werte mit bestimmten Werten auffüllen
mit bestimmten Werten auffüllen (Imputation) oder die fehlenden Werte entfernen (Löschung). Wir gehen auf
beides.

124 | Kapitel 5: Feature Engineering

3 Rachel Bogardus Drew, "3 Facts About Marriage and Homeownership", Joint Center for Housing Studies der
Harvard University, 17. Dezember 2014, https://oreil.ly/MWxFp.
Löschung

Wenn ich die Bewerber in Vorstellungsgesprächen frage, wie sie mit fehlenden Werten umgehen sollen, neigen viele
tendieren viele dazu, das Löschen zu bevorzugen, nicht weil es eine bessere Methode ist, sondern weil es einfacher zu machen ist.

Eine Möglichkeit des Löschens ist die Spaltenlöschung: Wenn eine Variable zu viele fehlende Werte hat, entfernen Sie einfach
diese Variable entfernen. Im obigen Beispiel fehlen zum Beispiel über 50 % der Werte für
der Variable "Familienstand" fehlen, so dass Sie versucht sein könnten, diese
Variable aus Ihrem Modell zu entfernen. Der Nachteil dieses Ansatzes ist, dass Sie möglicherweise
wichtige Informationen zu entfernen und die Genauigkeit Ihres Modells zu verringern. Der Familienstand könnte
hoch mit dem Kauf von Häusern korreliert sein, da verheiratete Paare sehr viel häufiger Hausbesitzer sind als
Hauseigentümer sind als Alleinstehende.^3

Eine andere Möglichkeit des Löschens ist das Löschen von Zeilen: Wenn eine Stichprobe fehlende(n) Wert(e) aufweist, entfernen Sie einfach
diese Stichprobe. Diese Methode kann funktionieren, wenn die fehlenden Werte völlig zufällig sind
zufällig sind (MCAR) und die Anzahl der Beispiele mit fehlenden Werten gering ist, z. B.
weniger als 0,1 %. Sie sollten keine Zeilen löschen, wenn dies bedeutet, dass 10 % Ihrer Daten
Stichproben entfernt werden.

Durch das Entfernen von Datenzeilen können jedoch auch wichtige Informationen entfernt werden, die Ihr
Modell benötigt, um Vorhersagen zu treffen, insbesondere wenn die fehlenden Werte nicht zufällig sind
(MNAR). Sie möchten zum Beispiel keine Stichproben von Befragten des Geschlechts B
mit fehlendem Einkommen entfernen, weil die Tatsache, dass das Einkommen fehlt, selbst eine Information ist
(ein fehlendes Einkommen kann ein höheres Einkommen bedeuten und damit eine stärkere Korrelation zum Kauf eines
Hauskauf korreliert) und zur Erstellung von Vorhersagen verwendet werden kann.

Darüber hinaus kann das Entfernen von Datenzeilen zu Verzerrungen in Ihrem Modell führen, insbesondere wenn
die fehlenden Werte zufällig sind (MAR). Wenn Sie zum Beispiel alle Beispiele entfernen
fehlenden Alterswerte in den Daten in Tabelle 5-2 entfernen, entfernen Sie alle Befragten mit
Geschlecht A aus Ihren Daten, und Ihr Modell ist nicht in der Lage, gute Vorhersagen für
Befragte mit Geschlecht A.

Anrechnung

Auch wenn das Löschen verlockend ist, weil es einfach ist, kann das Löschen von Daten dazu führen
Verlust wichtiger Informationen führen und Verzerrungen in Ihr Modell einbringen. Wenn Sie nicht
nicht löschen wollen, müssen Sie die fehlenden Werte imputieren, d. h. sie mit
bestimmte Werte". Die Entscheidung, welche "bestimmten Werte" verwendet werden sollen, ist der schwierige Teil.

Gemeinsame Operationen bei der Merkmalstechnik | 125
4 Die Skalierung von Merkmalen hat die Leistung meines Modells einmal um fast 10 % erhöht.
Eine gängige Praxis ist es, fehlende Werte mit ihren Standardwerten zu ergänzen. Zum Beispiel, wenn
der Job fehlt, können Sie ihn mit einer leeren Zeichenfolge "" auffüllen. Eine andere gängige Praxis
ist es, fehlende Werte mit dem Mittelwert, dem Median oder dem Modus (dem häufigsten Wert) aufzufüllen.
Wenn zum Beispiel der Temperaturwert für eine Datenstichprobe fehlt, deren Monatswert
Juli ist, ist es keine schlechte Idee, ihn mit der Mediantemperatur des Monats Juli aufzufüllen.

Beide Praktiken funktionieren in vielen Fällen gut, aber manchmal können sie zu haarsträubenden
Bugs. Bei einem der Projekte, an denen ich mitgearbeitet habe, haben wir festgestellt, dass das
Modell Müll ausspuckte, weil das Frontend der App die Benutzer nicht mehr zur
Alter einzugeben, so dass Alterswerte fehlten und das Modell sie mit 0 füllte. Aber das
Modell sah während des Trainings nie den Alterswert 0, so dass es keine vernünftigen
Vorhersagen machen.

Im Allgemeinen sollten Sie vermeiden, fehlende Werte mit möglichen Werten aufzufüllen, wie z. B.
Auffüllen der fehlenden Anzahl der Kinder mit 0-0 ist ein möglicher Wert für die Anzahl der
Kinder. Dies erschwert die Unterscheidung zwischen Personen, deren Informationen fehlen
und Personen, die keine Kinder haben, zu unterscheiden.

Mehrere Techniken können gleichzeitig oder nacheinander eingesetzt werden, um fehlende
Werte für einen bestimmten Datensatz zu behandeln. Unabhängig davon, welche Techniken Sie verwenden, ist eines
ist eines sicher: Es gibt keine perfekte Methode für den Umgang mit fehlenden Werten. Mit der Löschung riskieren Sie
riskiert man, wichtige Informationen zu verlieren oder Verzerrungen zu verstärken. Bei der Imputation riskieren Sie
Sie riskieren, Ihre eigenen Verzerrungen in Ihre Daten einzubringen und diese zu verrauschen, oder schlimmer noch, Datenverluste. Wenn
Sie nicht wissen, was ein Datenleck ist, keine Panik, wir behandeln es im Abschnitt "Datenleck
Leckagen" auf Seite 135.

Skalierung
Betrachten wir die Aufgabe, vorherzusagen, ob jemand in den nächsten 12 Monaten ein Haus kaufen wird
Monaten kaufen wird, und die in Tabelle 5-2 dargestellten Daten. Die Werte der Variablen Alter in unseren Daten
reichen von 20 bis 40, während die Werte der Variable Jahreseinkommen von
10.000 bis 150.000. Wenn wir diese beiden Variablen in ein ML-Modell eingeben, wird es nicht
nicht verstehen, dass 150.000 und 40 verschiedene Dinge darstellen. Es wird sie einfach beide
als Zahlen, und da die Zahl 150.000 viel größer ist als die Zahl 40, wird sie möglicherweise
mehr Bedeutung beimessen, unabhängig davon, welche Variable für die Erstellung von
Vorhersagen.

Vor der Eingabe von Merkmalen in Modelle ist es wichtig, sie so zu skalieren, dass sie ähnliche
Bereiche. Dieser Vorgang wird als Feature-Skalierung bezeichnet. Dies ist eines der einfachsten Dinge, die Sie
die oft zu einer Leistungssteigerung Ihres Modells führen. Wird dies vernachlässigt, kann
dazu führen, dass Ihr Modell unsinnige Vorhersagen macht, insbesondere bei klassischen Algorithmen
wie gradient-boosted trees und logistische Regression.^4

126 | Kapitel 5: Feature Engineering

5 Changyong Feng, Hongyue Wang, Naiji Lu, Tian Chen, Hua He, Ying Lu, und Xin M. Tu, "Log-
Transformation and Its Implications for Data Analysis", Shanghai Archives of Psychiatry 26, no. 2 (April
2014): 105-9, https://oreil.ly/hHJjt.
Eine intuitive Methode zur Skalierung Ihrer Merkmale besteht darin, sie in den Bereich [0, 1] zu legen. Angesichts einer
Variable x können ihre Werte mit der folgenden Formel so skaliert werden, dass sie in diesem Bereich liegen:

x′=maxx- minx- minx x
Wenn x der Höchstwert ist, ist der skalierte Wert x′ gleich 1. Wenn x der
Minimalwert ist, ist der skalierte Wert x′ gleich 0.

Wenn Sie möchten, dass Ihr Merkmal in einem beliebigen Bereich [a, b] liegt - empirisch gesehen finde ich den
Bereich [-1, 1] besser funktioniert als der Bereich [0, 1], können Sie die folgende Formel verwenden:

x′=a+ xmax- minx- minx b-xa
Die Skalierung auf einen beliebigen Bereich funktioniert gut, wenn Sie keine Annahmen über Ihre Variablen machen wollen.
über Ihre Variablen machen wollen. Wenn Sie glauben, dass Ihre Variablen einer Normalverteilung folgen könnten
Normalverteilung folgen, könnte es hilfreich sein, sie so zu normalisieren, dass sie einen Mittelwert von Null und eine
Einheit Varianz haben. Dieser Vorgang wird als Standardisierung bezeichnet:

x′=x-σx,
wobei x der Mittelwert der Variablen x und σ ihre Standardabweichung ist.

In der Praxis neigen ML-Modelle dazu, mit Merkmalen zu kämpfen, die einer schiefen Verteilung folgen.
Verteilung folgen. Um die Schiefe abzuschwächen, wird üblicherweise die Log-Transformation verwendet.
Formation: Anwendung der Log-Funktion auf Ihr Merkmal. Ein Beispiel dafür, wie die log
Transformation Ihre Daten weniger schief werden lassen kann, ist in Abbildung 5-3 dargestellt. Während diese
Technik in vielen Fällen zu einem Leistungsgewinn führen kann, funktioniert sie nicht in allen Fällen, und
Sie sollten sich vor der Analyse hüten, die auf log-transformierten Daten statt auf den
Originaldaten.^5

Gemeinsame Vorgänge bei der Featureentwicklung | 127
Abbildung 5-3. In vielen Fällen kann die Logarithmentransformation dazu beitragen, die Schiefe der
Ihrer Daten

Bei der Skalierung gibt es zwei wichtige Dinge zu beachten. Erstens ist sie eine häufige
Quelle für Datenlecks ist (dies wird ausführlicher im Abschnitt "Datenlecks
Leckagen" auf Seite 135). Zum anderen sind oft globale Statistiken erforderlich - Sie müssen
Sie müssen sich die gesamten oder eine Teilmenge der Trainingsdaten ansehen, um deren Minimum, Maximum oder Mittelwert zu berechnen.
Während der Inferenz verwenden Sie die Statistiken, die Sie während des Trainings erhalten haben, erneut, um die
neuen Daten. Wenn sich die neuen Daten im Vergleich zu den Trainingsdaten stark verändert haben, sind diese
Statistiken nicht sehr nützlich sein. Daher ist es wichtig, Ihr Modell häufig neu zu trainieren, um
diese Änderungen zu berücksichtigen.

Diskretisierung
Diese Technik ist der Vollständigkeit halber in diesem Buch enthalten, obwohl ich in der Praxis
Diskretisierung nur selten als hilfreich empfunden. Stellen Sie sich vor, wir haben ein Modell mit den Daten
in Tabelle 5-2. Während des Trainings hat unser Modell die Jahreseinkommenswerte von
"150.000", "50.000", "100.000" und so weiter. Während der Inferenz stößt unser Modell auf ein
Beispiel mit einem Jahreseinkommen von "9.000,50".

Intuitiv wissen wir, dass $9.000,50 pro Jahr nicht viel anders sind als $10.000/Jahr, und
wir wollen, dass unser Modell beide gleich behandelt. Aber das Modell weiß das nicht
das nicht. Unser Modell weiß nur, dass sich 9.000,50 $ von 10.000 $ unterscheiden, und es wird
sie unterschiedlich behandeln.

Diskretisierung ist der Prozess der Umwandlung eines kontinuierlichen Merkmals in ein diskretes Merkmal.
Dieser Prozess wird auch als Quantisierung oder Binning bezeichnet. Dies geschieht durch die Erstellung von
Bereichen für die gegebenen Werte. Für das Jahreseinkommen können Sie die Werte wie folgt in
drei Bereiche wie folgt gruppieren:

128 | Kapitel 5: Feature Engineering

-Geringes Einkommen: weniger als $35.000/Jahr
-Mittleres Einkommen: zwischen $35.000 und $100.000/Jahr
-Oberes Einkommen: mehr als $100.000/Jahr
Anstatt eine unendliche Anzahl möglicher Einkommen lernen zu müssen, kann sich unser Modell
kann sich unser Modell auf das Erlernen von nur drei Kategorien konzentrieren, was sehr viel einfacher zu erlernen ist. Diese
Technik soll bei begrenzten Trainingsdaten hilfreicher sein.

Auch wenn die Diskretisierung per Definition für kontinuierliche Merkmale gedacht ist, kann sie auch für
auch für diskrete Merkmale verwendet werden. Die Altersvariable ist diskret, aber es könnte trotzdem nützlich sein
die Werte wie folgt in Bereiche zu gruppieren:

-weniger als 18
-zwischen 18 und 22
-Zwischen 22 und 30
-Zwischen 30 und 40
-Zwischen 40 und 65
-über 65
Der Nachteil ist, dass diese Kategorisierung zu Diskontinuitäten an den Grenzen der Kategorien führt
Grenzen einführt - $34.999 wird nun völlig anders behandelt als $35.000, die
gleich behandelt wird wie $100.000. Die Wahl der Grenzen der Kategorien ist vielleicht nicht ganz
nicht so einfach. Sie können versuchen, die Histogramme der Werte darzustellen und die Grenzen zu wählen
die sinnvoll sind. Im Allgemeinen helfen gesunder Menschenverstand, grundlegende Quantile und manchmal auch Fachwissen.
Fachwissen helfen.

Kodierung kategorischer Merkmale
Wir haben darüber gesprochen, wie man kontinuierliche Merkmale in kategoriale Merkmale umwandelt. In diesem
Abschnitt wird erörtert, wie kategorische Merkmale am besten behandelt werden können.

Menschen, die noch nicht mit Daten in der Produktion gearbeitet haben, neigen zu der Annahme, dass Kategorien
statisch sind, was bedeutet, dass sich die Kategorien im Laufe der Zeit nicht ändern. Dies gilt für viele
Kategorien. Zum Beispiel ist es unwahrscheinlich, dass sich Alters- und Einkommensklassen ändern,
und Sie wissen im Voraus genau, wie viele Kategorien es gibt. Die Handhabung dieser
Kategorien ist ganz einfach. Sie können jeder Kategorie einfach eine Nummer zuweisen und sind
fertig.

In der Produktion ändern sich die Kategorien jedoch. Stellen Sie sich vor, Sie bauen ein Empfehlungssystem
System, das vorhersagen soll, welche Produkte die Benutzer bei Amazon kaufen möchten. Eines der
Merkmale, die Sie verwenden möchten, ist die Produktmarke. Bei der Betrachtung von Amazons historischen

Common Feature Engineering Operations | 129
6 "Zwei Millionen Marken auf Amazon", Marketplace Pulse, 11. Juni 2019, https://oreil.ly/zrqtd.
7 Wikipedia, s.v. "Feature Hashing", https://oreil.ly/tINTc.
Daten, erkennt man, dass es eine Menge Marken gibt. Selbst im Jahr 2019 gab es bereits
über zwei Millionen Marken auf Amazon!^6

Die Anzahl der Marken ist überwältigend, aber Sie denken: "Das schaffe ich noch". Sie
kodieren jede Marke als Zahl, so dass Sie jetzt zwei Millionen Zahlen haben, von 0 bis
1.999.999, die zwei Millionen Marken entsprechen. Ihr Modell schneidet spektakulär ab bei
und Sie erhalten die Genehmigung, es mit 1 % des heutigen Verkehrsaufkommens zu testen.

In der Produktion stürzt Ihr Modell ab, weil es auf eine Marke stößt, die es noch nie gesehen hat
und daher nicht kodieren kann. Ständig kommen neue Marken zu Amazon hinzu. Um dieses Problem zu lösen,
erstellen Sie eine Kategorie UNKNOWN mit dem Wert 2.000.000, um alle Marken zu erfassen, die
die Ihr Modell während des Trainings noch nicht gesehen hat.

Ihr Modell stürzt nicht mehr ab, aber Ihre Verkäufer beschweren sich, dass ihre neuen
Marken keine Besucher erhalten. Das liegt daran, dass Ihr Modell die Kategorie
UNKNOWN in der Zugpackung nicht erkannt hat und deshalb keine Produkte der
UNBEKANNTEN Marke. Sie können dies beheben, indem Sie nur die 99% der beliebtesten Marken kodieren
und die untersten 1 % der Marken als UNBEKANNT kodieren. Auf diese Weise weiß Ihr Modell zumindest
weiß, wie es mit UNBEKANNTEN Marken umzugehen hat.

Ihr Modell scheint etwa eine Stunde lang gut zu funktionieren, dann sinkt die Klickrate auf
Produktempfehlungen stark ab. In der letzten Stunde sind 20 neue Marken auf Ihrer
einige von ihnen sind neue Luxusmarken, einige von ihnen sind skizzenhafte Imitationen,
einige von ihnen sind etablierte Marken. Ihr Modell behandelt sie jedoch alle auf dieselbe
wie es unpopuläre Marken in den Trainingsdaten behandelt.

Dies ist kein extremes Beispiel, das nur bei Amazon vorkommt. Dieses
Problem tritt recht häufig auf. Wenn Sie zum Beispiel vorhersagen wollen, ob ein Kommentar
Spam ist, können Sie das Konto, das diesen Kommentar gepostet hat, als Merkmal verwenden,
und es werden ständig neue Konten erstellt. Das Gleiche gilt für neue Produkt
neue Produkttypen, neue Website-Domänen, neue Restaurants, neue Unternehmen, neue IP-Adressen und
und so weiter. Wenn Sie mit einem dieser Konten arbeiten, werden Sie mit diesem Problem konfrontiert.

Eine Lösung für dieses Problem zu finden, erweist sich als erstaunlich schwierig. Sie wollen nicht
Sie wollen sie nicht in eine Reihe von Bereichen einteilen, weil das sehr schwierig sein kann - wie würden Sie
neue Benutzerkonten überhaupt in verschiedene Gruppen einteilen?

Eine Lösung für dieses Problem ist der Hash-Trick, der durch das Paket Vowpal
Wabbit, das bei Microsoft entwickelt wurde.^7 Der Kern dieses Tricks besteht darin, dass Sie eine Hash-Funktion verwenden
um einen Hash-Wert für jede Kategorie zu erzeugen. Der gehashte Wert wird zum Index
der jeweiligen Kategorie. Da Sie den Hash-Bereich angeben können, können Sie die Anzahl der
kodierter Werte für ein Merkmal im Voraus festlegen, ohne dass man wissen muss, wie viele

130 | Kapitel 5: Feature Engineering

8 Lucas Bernardi, "Lassen Sie sich nicht durch den Hashing-Trick täuschen", Booking.com, 10. Januar 2018,
https://oreil.ly/VZmaY.
Kategorien gibt. Wenn Sie zum Beispiel einen Hash-Raum von 18 Bits wählen, was
was 2^18 = 262.144 möglichen Hash-Werten entspricht, werden alle Kategorien, auch die, die
die Ihr Modell noch nie gesehen hat, werden durch einen Index zwischen 0 und
262,143.

Ein Problem bei Hash-Funktionen ist die Kollision: zwei Kategorien werden demselben Index zugewiesen.
gleichen Index. Bei vielen Hash-Funktionen sind die Kollisionen jedoch zufällig; neue
Marken können sich einen Index mit einer beliebigen der vorhandenen Marken teilen, anstatt sich immer einen Index mit
einen Index mit unbeliebten Marken zu teilen, was passiert, wenn wir die vorangehende
Kategorie UNBEKANNT. Die Auswirkungen kollidierender gehashter Merkmale sind zum Glück nicht
nicht so schlimm. Untersuchungen von Booking.com haben ergeben, dass selbst bei 50 % kollidierender Merkmale der
Leistungsverlust weniger als 0,5 %, wie in Abbildung 5-4 dargestellt.^8

Abbildung 5-4. Bei einer Kollisionsrate von 50 % erhöht sich der Stammverlust nur um weniger als 0,5 %.
Quelle: Lucas Bernardi

Sie können einen ausreichend großen Hash-Bereich wählen, um die Kollisionen zu reduzieren. Sie können auch
eine Hash-Funktion mit den von Ihnen gewünschten Eigenschaften wählen, z. B. eine ortsabhängige
Hash-Funktion, bei der ähnliche Kategorien (z. B. Websites mit ähnlichen Namen)
in nahe beieinander liegende Werte gehasht werden.

Gemeinsame Operationen der Merkmalstechnik | 131
9 Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He, "DeepFM: A Factorization-
Machine Based Neural Network for CTR Prediction," Proceedings of the Twenty-Sixth International Joint
Conference on Artificial Intelligence (IJCAI, 2017), https://oreil.ly/1Vs3v; Jianxun Lian, Xiaohuan Zhou, Fuz-
heng Zhang, Zhongxia Chen, Xing Xie, and Guangzhong Sun, "xDeepFM: Combining Explicit and Implicit
Feature Interactions for Recommender Systems", arXiv, 2018, https://oreil.ly/WFmFt.
Da es sich um einen Trick handelt, wird er von Akademikern oft als "hacky" betrachtet und aus den ML
Lehrplänen ausgeschlossen. Aber die breite Akzeptanz in der Industrie ist ein Beweis dafür, wie effektiv der
Trick ist. Er ist ein wesentlicher Bestandteil von Vowpal Wabbit und gehört zu den Frameworks von scikit-learn,
TensorFlow und Gensim. Er kann vor allem in kontinuierlichen Lernumgebungen nützlich sein
wo Ihr Modell aus eingehenden Beispielen in der Produktion lernt. Wir werden das kontinuierliche
uales Lernen in Kapitel 9.

Feature-Kreuzung
Das Kreuzen von Merkmalen ist eine Technik zur Kombination von zwei oder mehr Merkmalen zur Erzeugung
neue Merkmale zu erzeugen. Diese Technik ist nützlich, um die nichtlinearen Beziehungen zwischen
Merkmalen. Wenn man zum Beispiel vorhersagen will, ob jemand in den nächsten 12 Monaten ein Haus kaufen möchte
Haus in den nächsten 12 Monaten kaufen will, vermuten Sie, dass es eine nichtlineare Beziehung gibt
zwischen dem Familienstand und der Anzahl der Kinder bestehen könnte, also kombiniert man sie zu einem neuen Merkmal
Merkmal "Ehe und Kinder" wie in Tabelle 5-3.

Tabelle 5-3. Beispiel dafür, wie zwei Features zu einem neuen Feature kombiniert werden können

Ehe Alleinstehend Verheiratet Alleinstehend Verheiratet
Kinder 0 2 1 0 1
Heirat und Kinder Alleinstehend, 0 Verheiratet, 2 Alleinstehend, 1 Alleinstehend, 0 Verheiratet, 1
Da das Kreuzen von Merkmalen bei der Modellierung nichtlinearer Beziehungen zwischen Variablen hilft, ist es
wesentlich für Modelle, die nichtlineare Beziehungen nicht lernen können oder schlecht darin sind,
wie z. B. lineare Regression, logistische Regression und baumbasierte Modelle. Sie ist weniger wichtig
in neuronalen Netzen weniger wichtig, kann aber dennoch nützlich sein, weil die explizite Kreuzung von Merkmalen
gelegentlich dazu beiträgt, dass neuronale Netze nichtlineare Beziehungen schneller lernen. DeepFM und
xDeepFM sind die Modellfamilie, die explizite Merkmalsinteraktionen erfolgreich für Empfehlungssysteme
Interaktionen für Empfehlungssysteme und die Vorhersage von Klickraten genutzt haben.^9

Ein Nachteil von Feature-Crossing ist, dass es den Feature-Bereich aufblähen kann. Stellen Sie sich vor:
Merkmal A hat 100 mögliche Werte und Merkmal B hat 100 mögliche Merkmale; die Kreuzung
dieser beiden Merkmale ergibt ein Merkmal mit 100 × 100 = 10.000 möglichen Werten. Sie
brauchen viel mehr Daten, damit die Modelle all diese möglichen Werte lernen können. Ein weiterer Vorbehalt
ist, dass die Kreuzung von Merkmalen die Anzahl der von den Modellen verwendeten Merkmale erhöht und
die Modelle übermäßig an die Trainingsdaten anpassen.

132 | Kapitel 5: Feature Engineering

10 Flavian Vasile, Elena Smirnova, and Alexis Conneau, "Meta-Prod2Vec-Product Embeddings Using Side-
Information for Recommendation," arXiv, July 25, 2016, https://oreil.ly/KDaEd; "Product Embeddings and
Vectors," Coveo, https://oreil.ly/ShaSY.
11 Andrew Zhai, "Representation Learning for Recommender Systems," August 15, 2021, https://oreil.ly/OchiL.

Diskrete und kontinuierliche Positionseinbettungen
Die Positionseinbettung wurde der Deep-Learning-Gemeinschaft erstmals in der Arbeit "Attention Is All
You Need" (Vaswani et al. 2017) vorgestellt wurde, hat sich die Positionseinbettung zu einer Standard
Datenverarbeitungstechnik für viele Anwendungen im Bereich Computer Vision und NLP geworden. Wir werden
ein Beispiel durchgehen, um zu zeigen, warum positionale Einbettung notwendig ist und wie
es tun.
Nehmen wir die Aufgabe der Sprachmodellierung, bei der Sie das nächste Token vorhersagen wollen
(z. B. ein Wort, ein Zeichen oder ein Teilwort) auf der Grundlage der vorherigen Tokenfolge vorhersagen möchten. In
der Praxis kann eine Sequenz bis zu 512 Zeichen lang sein, wenn nicht sogar länger. Aber der Einfachheit halber
lassen Sie uns jedoch der Einfachheit halber Wörter als Token und eine Sequenzlänge von 8 verwenden. Bei einer beliebigen
Sequenz von 8 Wörtern, wie z. B. "Manchmal ist alles, was ich wirklich will,", wollen wir das nächste Wort
das nächste Wort vorhersagen.
Einbettungen
Eine Einbettung ist ein Vektor, der einen Teil der Daten darstellt. Wir nennen die Menge aller möglichen
Einbettungen, die durch denselben Algorithmus für einen Datentyp erzeugt werden, "Einbettungsraum".
Alle Einbettungsvektoren im gleichen Raum haben die gleiche Größe.
Eine der häufigsten Verwendungen von Einbettungen sind Worteinbettungen, bei denen man jedes Wort durch einen Vektor darstellen kann.
jedes Wort durch einen Vektor darstellen kann. Aber auch Einbettungen für andere Datentypen werden
immer beliebter. So bieten beispielsweise E-Commerce-Lösungen wie Criteo und Coveo
Einbettungen für Produkte.^10 Pinterest verfügt über Einbettungen für Bilder, Graphen, Abfragen und
und sogar Benutzer.^11 Da es so viele Arten von Daten mit Einbettungen gibt, besteht
Interesse an der Erstellung universeller Einbettungen für multimodale Daten.
Wenn wir ein rekurrentes neuronales Netz verwenden, verarbeitet es Wörter in sequenzieller Reihenfolge, was
das heißt, die Reihenfolge der Wörter wird implizit eingegeben. Wenn wir jedoch ein Modell wie einen
Transformator, werden die Wörter parallel verarbeitet, so dass die Positionen der Wörter explizit eingegeben werden müssen
eingegeben werden, damit unser Modell die Reihenfolge dieser Wörter kennt ("ein Hund beißt ein Kind"
ist etwas ganz anderes als "ein Kind beißt einen Hund"). Wir wollen nicht die absoluten
Positionen 0, 1, 2, ..., 7 in unser Modell eingeben, weil neuronale Netze erfahrungsgemäß nicht
nicht gut mit Eingaben arbeiten, die keine Einheitsvarianz haben (deshalb skalieren wir unsere Merkmale, wie
wie zuvor im Abschnitt "Skalierung" auf Seite 126 beschrieben).
Allgemeine Operationen bei der Feature-Entwicklung | 133
Wenn wir die Positionen auf einen Wert zwischen 0 und 1 umskalieren, also 0, 1, 2, ..., 7 zu 0, 0,143, 0,286,
..., 1 werden, sind die Unterschiede zwischen den beiden Positionen zu gering, als dass neuronale Netze
um zu lernen, sie zu unterscheiden.

Eine Möglichkeit, mit Positionseinbettungen umzugehen, besteht darin, sie so zu behandeln, wie wir die Einbettung von Wörtern behandeln würden.
ding. Bei der Worteinbettung verwenden wir eine Einbettungsmatrix mit der Vokabulargröße als
der Anzahl der Spalten, und jede Spalte ist die Einbettung für das Wort mit dem Index
der jeweiligen Spalte. Bei der Positionseinbettung entspricht die Anzahl der Spalten der Anzahl der
Stellen. Da wir in unserem Fall nur mit der bisherigen Sequenzgröße von 8 arbeiten, gehen die
Positionen von 0 bis 7 (siehe Abbildung 5-5).

Die Einbettungsgröße für Positionen ist in der Regel die gleiche wie die Einbettungsgröße für Wörter
so dass sie summiert werden können. Zum Beispiel ist die Einbettung für das Wort "Essen" an
Position 0 die Summe aus dem Einbettungsvektor für das Wort "food" und dem Einbettungsvektor für Position 0.
Dies ist die Art und Weise, wie Positionseinbettungen in Hugging Face's BERT implementiert sind.
in Hugging Face's BERT ab August 2021 implementiert. Da sich die Einbettungen ändern, wenn die
Modellgewichte aktualisiert werden, sagen wir, dass die Positionseinbettungen gelernt werden.

Abbildung 5-5. Eine Möglichkeit, Positionen einzubetten, besteht darin, sie so zu behandeln, wie man es mit Wort
Einbettungen

Auch Positionseinbettungen können festgelegt werden. Die Einbettung für jede Position ist immer noch
ein Vektor mit S Elementen (S ist die Größe der Positionseinbettung), aber jedes Element ist
aber jedes Element ist durch eine Funktion vordefiniert, normalerweise Sinus und Kosinus. In der ursprünglichen Transformer
Papier wird Sinus verwendet, wenn das Element einen geraden Index hat. Andernfalls wird Kosinus verwendet. Siehe Abbildung 5-6.

134 | Kapitel 5: Feature Engineering

Abbildung 5-6. Beispiel für die Einbettung mit fester Position. H ist die Dimension der Ausgaben
die das Modell erzeugt.

Die feste Positionseinbettung ist ein Spezialfall der so genannten Fourier-Merkmale.
Wenn Positionen in Positionseinbettungen diskret sind, können Fourier-Merkmale auch kontinuierlich sein.
kontinuierlich sein. Betrachten wir eine Aufgabe, bei der es um die Darstellung von 3D-Objekten, wie z. B. einer Teekanne, geht.
Jede Position auf der Oberfläche der Teekanne wird durch eine dreidimensionale
Koordinate dargestellt, die kontinuierlich ist. Wenn die Positionen kontinuierlich sind, wäre es sehr schwer
eine Einbettungsmatrix mit kontinuierlichen Spaltenindizes zu erstellen, aber feste Positions
Einbettungen mit Sinus- und Kosinusfunktionen funktionieren dennoch.

Im Folgenden wird das verallgemeinerte Format für den Einbettungsvektor an der Koordinate v dargestellt,
auch Fourier-Merkmale der Koordinate v genannt. Es hat sich gezeigt, dass Fourier-Merkmale
dass sie die Leistung von Modellen für Aufgaben verbessern, die Koordinaten (oder Positionen) als
Eingaben. Wenn Sie daran interessiert sind, können Sie mehr darüber lesen in "Fourier Features Let
Networks Learn High Frequency Functions in Low Dimensional Domains" (Tancik et
al. 2020).

γv = a 1 cos 2 πb 1 Tv,a 1 sin 2 πb 1 Tv, ...,amcos 2 πbmTv,amsin 2 πbmTv
T
Datenleckage
Im Juli 2021 erschien in der MIT Technology Review ein provokanter Artikel mit dem Titel "Hundreds of AI
Tools Have Been Built to Covid Catch. None of Them Helped." Diese Modelle wurden
trainiert, um COVID-19-Risiken aus medizinischen Scans vorherzusagen. Der Artikel listet mehrere
Beispiele, in denen ML-Modelle, die während der Evaluierung gut abschnitten, in der
in tatsächlichen Produktionsumgebungen.

In einem Beispiel trainierten die Forscher ihr Modell mit einer Mischung aus Scans, die
Patienten im Liegen und im Stehen aufgenommen wurden. "Da Patienten, die im Liegen gescannt wurden
gescannt wurden, war die Wahrscheinlichkeit größer, dass sie schwer krank waren, und das Modell lernte, das
Risiko anhand der Position einer Person vorherzusagen."

Datenlecks | 135
12 Will Douglas Heaven, "Hundreds of AI Tools Have Been Built to Covid Catch. Keines von ihnen hat geholfen", MIT
Technology Review, 30. Juli 2021, https://oreil.ly/Ig1b1.
13 Zidmie, "Das Leck erklärt!" Kaggle, https://oreil.ly/1JgLj.
14 Addison Howard, "Competition Recap-Congratulations to our Winners!" Kaggle, https://oreil.ly/wVUU4.

In einigen anderen Fällen wurde festgestellt, dass die Modelle "die Schriftart, die bestimmte
Krankenhäuser für die Beschriftung der Scans verwendeten. Infolgedessen wurden die Schriftarten von Krankenhäusern mit schwereren
Fallzahlen zu Prädiktoren für das Covid-Risiko wurden."^12
Dies sind beides Beispiele für Datenlecks. Datenlecks bezeichnen das Phänomen
wenn eine Form der Kennzeichnung in den Satz von Merkmalen "durchsickert", der für die Erstellung von Vorhersagen verwendet wird,
und dieselbe Information während der Inferenz nicht verfügbar ist.
Datenlecks sind eine Herausforderung, da sie oft nicht offensichtlich sind. Es ist gefährlich
weil sie dazu führen können, dass Ihre Modelle auf unerwartete und spektakuläre Weise versagen, selbst
selbst nach umfangreichen Auswertungen und Tests. Lassen Sie uns ein weiteres Beispiel durchgehen, um zu zeigen
was Datenlecks sind.
Nehmen wir an, Sie möchten ein ML-Modell erstellen, um vorherzusagen, ob ein CT-Scan einer Lunge
Anzeichen von Krebs zeigt. Sie haben die Daten von Krankenhaus A erhalten, die Diagnosen der Ärzte aus den Daten entfernt
Diagnose aus den Daten entfernt und Ihr Modell trainiert. Es schnitt bei den Testdaten von Krankenhaus A sehr gut
von Krankenhaus A, aber schlecht bei den Daten von Krankenhaus B.
Nach eingehender Untersuchung haben Sie herausgefunden, dass im Krankenhaus A die Ärzte, wenn sie glauben, dass
wenn die Ärzte glauben, dass ein Patient Lungenkrebs hat, diesen Patienten zu einem moderneren Gerät schicken,
das etwas andere CT-Bilder liefert. Ihr Modell hat gelernt, sich auf
Vorhersagen darüber zu treffen, ob ein Scanbild Anzeichen von Lungenkrebs
Bild Anzeichen von Lungenkrebs zeigt. Krankenhaus B schickt die Patienten nach dem Zufallsprinzip zu verschiedenen CT-Geräten.
Geräte, so dass Ihr Modell keine Informationen hat, auf die es sich verlassen kann. Wir sagen, dass Labels
während des Trainings zu den Merkmalen durchgesickert sind.
Ein Datenleck kann nicht nur bei Neulingen auf dem Gebiet auftreten, sondern auch bei
mehreren erfahrenen Forschern, deren Arbeit ich bewundere, und in einem meiner eigenen
Projekten. Obwohl Datenlecks weit verbreitet sind, werden sie in den Lehrplänen für ML nur selten behandelt.
Ein abschreckendes Beispiel: Datenleckage beim Kaggle-Wettbewerb
Im Jahr 2020 startete die Universität Liverpool einen Ion Switching-Wettbewerb auf
Kaggle. Die Aufgabe bestand darin, die Anzahl der zu jedem Zeitpunkt geöffneten Ionenkanäle zu ermitteln.
Punkt. Es wurden Testdaten aus Trainingsdaten synthetisiert, und einige Leute waren in der Lage
Testlabels aus dem Leck zu gewinnen.^13 Die beiden siegreichen Teams in diesem
Wettbewerb sind die beiden Teams, denen es gelang, das Leck auszunutzen, obwohl sie
obwohl sie auch ohne Ausnutzung des Lecks hätten gewinnen können.^14
136 | Kapitel 5: Merkmalstechnik
Häufige Ursachen für Datenlecks
In diesem Abschnitt gehen wir auf einige häufige Ursachen für Datenlecks ein und wie man sie vermeidet.
sie vermeiden.

Aufteilung zeitkorrelierter Daten nach dem Zufallsprinzip statt nach der Zeit

Als ich ML im College lernte, wurde mir beigebracht, meine Daten zufällig in Train,
Validierung und Test aufzuteilen. Dies ist auch die Art und Weise, wie die Daten oft in ML
Forschungspapieren. Dies ist jedoch auch eine häufige Ursache für Datenlecks.

In vielen Fällen sind die Daten zeitkorreliert, was bedeutet, dass der Zeitpunkt der Datenerstellung
Erstellungszeitpunkt die Verteilung der Kennzeichnung beeinflusst. Manchmal ist die Korrelation offensichtlich, wie im Fall
von Aktienkursen. Vereinfacht ausgedrückt, tendieren die Kurse ähnlicher Aktien dazu, sich gemeinsam zu bewegen.
Wenn 90 % der Tech-Aktien heute fallen, ist es sehr wahrscheinlich, dass auch die anderen 10 % der Tech
Aktien ebenfalls sinken. Wenn Sie Modelle zur Vorhersage künftiger Aktienkurse erstellen, sollten Sie
Sie Ihre Trainingsdaten nach Zeit aufteilen, z. B. indem Sie Ihr Modell mit Daten aus den ersten sechs Tagen trainieren
den ersten sechs Tagen trainieren und es anhand der Daten des siebten Tages auswerten. Wenn Sie die Daten zufällig aufteilen
Daten nach dem Zufallsprinzip aufteilen, werden die Kurse des siebten Tages in die Trainingsaufteilung einbezogen und
in Ihr Modell den Zustand des Marktes an diesem Tag. Wir sagen, dass die Informationen
aus der Zukunft in den Trainingsprozess eingesickert ist.

In vielen Fällen ist die Korrelation jedoch nicht offensichtlich. Nehmen wir die Aufgabe, vorherzusagen
vorhersagen, ob jemand auf eine Song-Empfehlung klicken wird. Ob jemand
Lied anhört, hängt nicht nur von seinem Musikgeschmack ab, sondern auch vom allgemeinen
Trend des Tages ab. Wenn ein Künstler eines Tages verstirbt, werden die Leute mit größerer Wahrscheinlichkeit
diesen Künstler zu hören. Durch die Aufnahme von Samples von einem bestimmten Tag in den Zugsplit,
Informationen über den Musiktrend an diesem Tag in das Modell einfließen, wodurch es
Vorhersagen für andere Samples desselben Tages zu treffen.

Um zu verhindern, dass zukünftige Informationen in den Trainingsprozess einfließen und die
Modelle während der Auswertung zu betrügen, sollten Sie Ihre Daten nach Zeit aufteilen, anstatt sie laufend
aufzuteilen, wann immer dies möglich ist. Wenn Sie zum Beispiel Daten aus fünf Wochen haben, verwenden Sie die ersten
vier Wochen für den Trainingssplit und teilen Sie dann Woche 5 nach dem Zufallsprinzip in Validierung und Test
wie in Abbildung 5-7 gezeigt.

Datenleckage | 137
Abbildung 5-7. Daten nach Zeit aufteilen, um zu verhindern, dass zukünftige Informationen in den
Trainingsprozess

Skalierung vor der Aufteilung

Wie im Abschnitt "Skalierung" auf Seite 126 beschrieben, ist es wichtig, Ihre Features zu skalieren.
Die Skalierung erfordert globale Statistiken - z. B. Mittelwert, Varianz - Ihrer Daten. Ein häufiger
Fehler ist es, die gesamten Trainingsdaten zu verwenden, um globale Statistiken zu erstellen, bevor sie in verschiedene Splits aufgeteilt werden.
aufzuteilen, wodurch der Mittelwert und die Varianz der Testproben in den
Trainingsprozess einfließen, so dass ein Modell seine Vorhersagen für die Testproben anpassen kann.
Diese Informationen sind in der Produktion nicht verfügbar, so dass die Leistung des Modells wahrscheinlich
verschlechtern.

Um diese Art von Leckage zu vermeiden, sollten Sie Ihre Daten vor der Skalierung immer erst aufteilen und dann die
Statistiken aus dem Zugsplit zur Skalierung aller Splits. Einige schlagen sogar vor, dass wir unsere Daten
Daten vor jeder explorativen Datenanalyse und Datenverarbeitung aufzuteilen, damit wir nicht
versehentlich Informationen über den Test-Split erhalten.

Auffüllen der fehlenden Daten mit Statistiken aus dem Test-Split

Eine gängige Methode, mit fehlenden Werten eines Merkmals umzugehen, ist das Auffüllen (Eingeben) dieser Werte mit
dem Mittelwert oder Median aller vorhandenen Werte. Ein Leck kann auftreten, wenn der Mittelwert oder Median
unter Verwendung der gesamten Daten und nicht nur des Zugsplits berechnet wird. Diese Art von Leckage ist
Diese Art von Leckage ähnelt der Art von Leckage, die durch Skalierung verursacht wird, und sie kann verhindert werden, indem nur
Statistiken aus dem Zugsplit verwendet werden, um fehlende Werte in allen Splits aufzufüllen.

138 | Kapitel 5: Feature Engineering

15 Björn Barz und Joachim Denzler, "Do We Train on Test Data? Purging CIFAR of Near-Duplicates", Journal of
Imaging 6, Nr. 6 (2020): 41.
16 Michael Roberts, Derek Driggs, Matthew Thorpe, Julian Gilbey, Michael Yeung, Stephan Ursprung, Angelica
I. Aviles-Rivero, et al. "Common Pitfalls and Recommendations for Using Machine Learning to Detect and
Prognosticate for COVID-19 Using Chest Radiographs and CT Scans," Nature Machine Intelligence 3 (2021):
199-217, https://oreil.ly/TzbKJ.

Schlechte Behandlung von Datenduplikaten vor der Aufteilung
Wenn Sie Duplikate oder Beinahe-Duplikate in Ihren Daten haben und diese nicht vor dem
vor der Aufteilung der Daten zu entfernen, können dieselben Stichproben sowohl in der Zug- als auch in der Validierungs-/Testaufteilung erscheinen.
tion/Test-Splits erscheinen. Datenduplikate sind in der Industrie recht häufig und wurden auch in
auch in populären Forschungsdatensätzen gefunden. CIFAR-10 und CIFAR-100 sind zum Beispiel zwei
zwei beliebte Datensätze, die für die Forschung im Bereich Computer Vision verwendet werden. Sie wurden im Jahr 2009 veröffentlicht, doch
Erst 2019 entdeckten Barz und Denzler, dass 3,3 % bzw. 10 % der Bilder aus den
aus den Testsätzen der CIFAR-10- und CIFAR-100-Datensätze Duplikate im
training set.^15
Datenduplikate können aus der Sammlung oder Zusammenführung verschiedener Datenquellen resultieren.
ces. In einem Nature-Artikel aus dem Jahr 2021 wird die Datenduplizierung als eine häufige Falle bei der Verwendung von
ML zur Erkennung von COVID-19, weil "ein Datensatz mehrere andere
Datensatz mehrere andere Datensätze kombinierte, ohne zu bemerken, dass einer der Komponentendatensätze bereits eine
^16 Eine Datenduplizierung kann auch aufgrund der Datenverarbeitung erfolgen.
So kann z. B. ein Oversampling zur Duplizierung bestimmter Beispiele führen.
Um dies zu vermeiden, sollten Sie vor der Aufteilung und auch nach der Aufteilung immer nach Duplikaten suchen, um
um sicher zu gehen. Wenn Sie ein Oversampling durchführen, tun Sie dies nach dem Splitting.
Gruppenleckage
Eine Gruppe von Beispielen hat stark korrelierte Bezeichnungen, ist aber in verschiedene
Splits aufgeteilt. Zum Beispiel könnte ein Patient zwei Lungen-CT-Scans haben, die eine Woche auseinander liegen,
die wahrscheinlich die gleichen Markierungen haben, ob sie Anzeichen von Lungenkrebs enthalten, aber
eine der beiden Aufnahmen gehört zum Zug-Split und die zweite zum Test-Split. Diese Art von Leckage
ist üblich bei objektiven Erkennungsaufgaben, die Fotos desselben Objekts enthalten, die
Millisekunden voneinander entfernt aufgenommen wurden - einige davon landeten im Zug-Split, andere im
Test-Split. Es ist schwierig, diese Art von Datenlecks zu vermeiden, ohne zu verstehen, wie Ihre
Daten generiert wurden.
Datenlecks | 139
Leckagen bei der Datengenerierung

Das Beispiel von vorhin, wie Informationen darüber, ob ein CT-Scan Anzeichen von
Lungenkrebs zeigt, ist ein Beispiel für diese Art von Datenlecks.
Um diese Art von Datenlecks aufzuspüren, ist ein tiefes Verständnis der Art und Weise erforderlich, wie Daten
gesammelt werden. Es wäre zum Beispiel sehr schwer herauszufinden, dass die schlechte Leistung des Modells
dass die schlechte Leistung des Modells in Krankenhaus B auf das unterschiedliche Scanverfahren zurückzuführen ist, wenn man nicht
wissen, dass es unterschiedliche Scangeräte gibt oder dass die Verfahren in den beiden Krankenhäusern
unterschiedlich sind.

Es gibt keine narrensichere Methode, diese Art von Datenlecks zu vermeiden, aber Sie können das Risiko mindern
indem Sie die Quellen Ihrer Daten im Auge behalten und verstehen, wie sie gesammelt
und verarbeitet werden. Normalisieren Sie Ihre Daten, damit die Daten aus verschiedenen Quellen
die gleichen Mittelwerte und Varianzen haben. Wenn verschiedene CT-Geräte Bilder mit unterschiedlichen Auflösungen ausgeben
unterschiedlichen Auflösungen ausgeben, würde eine Normalisierung aller Bilder auf die gleiche Auflösung
würde es für die Modelle schwieriger machen, zu erkennen, welches Bild von welchem Gerät stammt. Und
Vergessen Sie nicht, Fachexperten einzubeziehen, die möglicherweise mehr über die Art und Weise
wie Daten gesammelt und verwendet werden, in den ML-Designprozess einzubeziehen!

Erkennen von Datenlecks
Datenlecks können während vieler Schritte auftreten, von der Erzeugung, Sammlung, Probenahme,
Aufteilung und Verarbeitung von Daten bis hin zum Feature Engineering. Es ist wichtig, während des gesamten
Datenlecks während des gesamten Lebenszyklus eines ML-Projekts zu überwachen.

Messung der Vorhersagekraft jedes Merkmals oder einer Gruppe von Merkmalen in Bezug auf die
Zielvariable (Bezeichnung). Wenn ein Merkmal eine ungewöhnlich hohe Korrelation aufweist, untersuchen Sie, wie dieses
Merkmal erzeugt wird und ob die Korrelation sinnvoll ist. Es ist möglich, dass zwei
Merkmale unabhängig voneinander kein Leck enthalten, aber zwei Merkmale zusammen können ein
Leckage enthalten. Wenn Sie beispielsweise ein Modell zur Vorhersage der Verweildauer eines Mitarbeiters in einem Unternehmen erstellen
wie lange ein Mitarbeiter in einem Unternehmen bleiben wird, sagen das Anfangs- und das Enddatum einzeln nicht viel über die
über die Dauer der Betriebszugehörigkeit, aber beide zusammen können uns diese Information geben.

140 | Kapitel 5: Feature Engineering

Führen Sie Ablationsstudien durch, um zu messen, wie wichtig ein Merkmal oder eine Gruppe von Merkmalen für Ihr Modell ist.
Modell ist. Wenn das Entfernen eines Merkmals die Leistung des Modells deutlich verschlechtert
deutlich verschlechtert, untersuchen Sie, warum dieses Merkmal so wichtig ist. Wenn Sie eine große Anzahl von
Merkmalen, z. B. tausend Merkmalen, kann es undurchführbar sein, Ablationsstudien für jede
Kombinationen von Merkmalen durchzuführen, aber es kann dennoch nützlich sein, gelegentlich Ablationsstudien
Ablationsstudien mit einer Untergruppe von Merkmalen durchzuführen, die Sie am meisten verdächtigen. Dies ist ein weiteres Beispiel
wie sich Fachwissen bei der Entwicklung von Merkmalen als nützlich erweisen kann. Ablation
Studien können nach Ihrem eigenen Zeitplan offline durchgeführt werden, so dass Sie Ihre Maschinen
während der Ausfallzeit für diesen Zweck nutzen.

Halten Sie Ausschau nach neuen Funktionen, die zu Ihrem Modell hinzugefügt werden. Wenn das Hinzufügen eines neuen Merkmals
die Leistung Ihres Modells erheblich verbessert, ist diese Funktion entweder wirklich gut oder
das Merkmal enthält nur durchgesickerte Informationen über Etiketten.

Seien Sie jedes Mal sehr vorsichtig, wenn Sie sich den Test-Split ansehen. Wenn Sie den Test-Split in irgendeiner Weise verwenden
als die endgültige Leistung eines Modells, sei es, um Ideen für neue Funktionen zu entwickeln
Ideen für neue Funktionen zu entwickeln oder Hyperparameter abzustimmen, riskieren Sie, dass Informationen aus der
Zukunft in Ihren Trainingsprozess einfließen.

Gute Merkmale entwickeln
Im Allgemeinen führt das Hinzufügen von mehr Funktionen zu einer besseren Modellleistung. Nach meiner Erfahrung
wird die Liste der Merkmale, die für ein Modell in der Produktion verwendet werden, mit der Zeit immer länger.
Mehr Funktionen bedeuten jedoch nicht immer eine bessere Modellleistung. Zu viele
zu viele Merkmale können sowohl beim Training als auch beim Betrieb des Modells aus folgenden Gründen schlecht sein
folgenden Gründen:

-Je mehr Merkmale Sie haben, desto mehr Möglichkeiten gibt es für Datenverluste.
-Zu viele Merkmale können zu einer Überanpassung führen.
-Zu viele Merkmale können den Speicherbedarf für ein Modell erhöhen, was wiederum
was wiederum dazu führen kann, dass Sie eine teurere Maschine/Instanz für Ihr Modell verwenden müssen.
Modell zu bedienen.
Zu viele Merkmale können bei der Online-Vorhersage die Inferenzlatenz erhöhen,
vor allem, wenn Sie diese Merkmale für die Vorhersagen aus den Rohdaten extrahieren müssen
online. Auf die Online-Vorhersage gehen wir in Kapitel 7 näher ein.
-Unnötige Merkmale werden zu technischen Schulden. Wann immer sich Ihre Datenpipeline ändert,
müssen alle betroffenen Merkmale entsprechend angepasst werden. Wenn zum Beispiel eines Tages
Ihre Anwendung eines Tages beschließt, keine Informationen über das Alter der Benutzer mehr zu erfassen, müssen alle
Funktionen, die das Alter der Benutzer verwenden, aktualisiert werden.
Gute Funktionen entwickeln | 141
17 Mit der XGBoost-Funktion get_score.
18 Ein großartiges Open-Source-Python-Paket zur Berechnung von SHAP finden Sie auf GitHub.

Wenn ein Merkmal einem Modell nicht zu guten Vorhersagen verhilft, sollten Regularisierungstechniken wie die L1-Regularisierung das Gewicht dieses Merkmals theoretisch auf 0 reduzieren.
Regularisierungstechniken wie die L1-Regularisierung das Gewicht dieses Merkmals auf 0 reduzieren,
In der Praxis kann es jedoch dazu beitragen, dass Modelle schneller lernen, wenn die nicht mehr nützlichen
(und möglicherweise sogar schädlich sind) entfernt werden und die guten Merkmale Vorrang haben.
Sie können entfernte Merkmale speichern, um sie später wieder hinzuzufügen. Sie können auch einfach allgemeine
Feature-Definitionen zur Wiederverwendung und gemeinsamen Nutzung durch verschiedene Teams in einem Unternehmen. Wenn man über
Verwaltung von Feature-Definitionen sprechen, denken manche vielleicht an Feature-Stores als
die Lösung. Allerdings verwalten nicht alle Feature-Stores Feature-Definitionen. Wir werden die
werden in Kapitel 10 näher erläutert.
Es gibt zwei Faktoren, die Sie bei der Beurteilung, ob ein Feature
für ein Modell geeignet ist: die Bedeutung für das Modell und die Verallgemeinerung auf ungesehene Daten.
Wichtigkeit eines Merkmals
Es gibt viele verschiedene Methoden, um die Wichtigkeit eines Merkmals zu messen. Wenn Sie
einen klassischen ML-Algorithmus wie Boosted Gradient Trees verwenden, ist es am einfachsten, die
die Wichtigkeit Ihrer Merkmale zu messen, ist die Verwendung der eingebauten Merkmalswichtigkeitsfunktionen, die
von XGBoost.^17 Für modellunabhängigere Methoden sollten Sie sich mit
SHAP (SHapley Additive exPlanations).^18 InterpretML ist ein großartiges Open-Source
Paket, das die Bedeutung von Merkmalen nutzt, um zu verstehen, wie Ihr Modell
Vorhersagen macht.
Der genaue Algorithmus zur Messung der Merkmalsbedeutung ist komplex, aber intuitiv,
wird die Bedeutung eines Merkmals für ein Modell daran gemessen, wie sehr sich die Leistung des Modells
sich die Leistung des Modells verschlechtert, wenn dieses Merkmal oder eine Gruppe von Merkmalen, die dieses Merkmal enthalten, aus dem Modell entfernt wird.
dem Modell entfernt wird. SHAP ist großartig, denn es misst nicht nur die Bedeutung eines Merkmals für ein
gesamten Modells misst, sondern auch den Beitrag jedes Merkmals zur spezifischen Vorhersage eines Modells.
tion. Die Abbildungen 5-8 und 5-9 zeigen, wie SHAP Ihnen helfen kann, den Beitrag
der einzelnen Features zu den Vorhersagen eines Modells.
142 | Kapitel 5: Feature Engineering
19 Scott Lundberg, SHAP (SHapley Additive exPlanations), GitHub-Repository, letzter Zugriff 2021,
https://oreil.ly/c8qqE.

Abbildung 5-8. Wie viel jedes Merkmal zur Einzelvorhersage eines Modells beiträgt, gemessen
durch SHAP. Der Wert LSTAT = 4,98 trägt am meisten zu dieser spezifischen Vorhersage bei.
Quelle: Scott Lundberg^19
Abbildung 5-9. Wie viel jedes Merkmal zu einem Modell beiträgt, gemessen durch SHAP. Das
Merkmal LSTAT hat die höchste Bedeutung. Quelle: Scott Lundberg
Gute Merkmale entwickeln | 143
20 Xinran He, Junfeng Pan, Ou Jin, Tianbing Xu, Bo Liu, Tao Xu, Yanxin Shi, et al., "Practical Lessons from
Predicting Clicks on Ads at Facebook," in ADKDD '14: Proceedings of the Eighth International Workshop on
Data Mining for Online Advertising (August 2014): 1-9, https://oreil.ly/dHXeC.

Oft macht eine kleine Anzahl von Merkmalen einen großen Teil der Bedeutung der Merkmale Ihres Modells aus.
Bedeutung aus. Bei der Messung der Merkmalsbedeutung für ein Modell zur Vorhersage der Click-Through-Rate
Vorhersagemodells fand das Anzeigenteam bei Facebook heraus, dass die 10 wichtigsten Merkmale
für etwa die Hälfte der gesamten Merkmalsbedeutung des Modells verantwortlich sind, während die letzten 300 Merkmale
weniger als 1 % der Merkmalsbedeutung beitragen, wie in Abbildung 5-10 dargestellt.^20
Abbildung 5-10. Bedeutung der Boosting-Merkmale. Die X-Achse entspricht der Anzahl der Merkmale.
Die Merkmalsbedeutung ist logarithmisch skaliert. Quelle: He et al.
Techniken zur Merkmalsbedeutung sind nicht nur gut für die Auswahl der richtigen Merkmale, sie sind auch
für die Interpretierbarkeit, da sie Ihnen helfen zu verstehen, wie Ihre Modelle unter der Haube
der Haube.
Verallgemeinerung von Merkmalen
Da das Ziel eines ML-Modells darin besteht, korrekte Vorhersagen für ungesehene Daten zu treffen, sollten die für das Modell
sollten die für das Modell verwendeten Merkmale auch für unbekannte Daten verallgemeinert werden können. Nicht alle Merkmale verallgemeinern
gleich. Bei der Vorhersage, ob es sich bei einem Kommentar um Spam handelt, ist zum Beispiel die
Kennung eines jeden Kommentars überhaupt nicht verallgemeinerbar und sollte nicht als
Merkmal für das Modell verwendet werden. Die Kennung des Benutzers, der den Kommentar abgibt,
wie z. B. der Benutzername, kann für ein Modell dennoch nützlich sein, um Vorhersagen zu treffen.
144 | Kapitel 5: Merkmalstechnik
Die Messung der Merkmalsgeneralisierung ist weit weniger wissenschaftlich als die Messung der Merkmalsbedeutung.
Bedeutung und erfordert neben statistischem Wissen auch Intuition und Fachwissen.
Wissen. Insgesamt gibt es zwei Aspekte, die Sie in Bezug auf die Generalisierung berücksichtigen sollten
Generalisierung zu berücksichtigen: die Merkmalsabdeckung und die Verteilung der Merkmalswerte.

Der Erfassungsgrad ist der Prozentsatz der Stichproben, die Werte für dieses Merkmal in den Daten enthalten.
Je weniger Werte also fehlen, desto höher ist der Erfassungsgrad. Eine grobe Faustregel
Faustregel lautet: Wenn dieses Merkmal in einem sehr kleinen Prozentsatz der Daten vorkommt, ist es nicht
nicht sehr verallgemeinerbar sein. Wenn Sie zum Beispiel ein Modell erstellen wollen, um vorherzusagen, ob
ob jemand in den nächsten 12 Monaten ein Haus kaufen wird, und Sie denken, dass die Anzahl der
Kinder ein gutes Merkmal wäre, aber Sie können diese Information nur für
1 % Ihrer Daten erhalten können, ist dieses Merkmal möglicherweise nicht sehr nützlich.

Diese Faustregel ist grob, denn einige Merkmale können auch dann noch nützlich sein, wenn sie
in den meisten Ihrer Daten fehlen. Dies gilt insbesondere dann, wenn die fehlenden Werte
nicht zufällig sind, was bedeutet, dass das Vorhandensein oder Nichtvorhandensein des Merkmals ein starker Hinweis auf seinen Wert sein kann.
für seinen Wert sein kann. Wenn zum Beispiel ein Merkmal nur in 1 % Ihrer Daten vorkommt, aber 99 % der
der Beispiele mit diesem Merkmal POSITIVE Beschriftungen haben, ist dieses Merkmal nützlich und Sie
sollten es verwenden.

Der Erfassungsgrad eines Merkmals kann sich zwischen verschiedenen Datenabschnitten und sogar innerhalb eines
derselben Datenscheibe im Laufe der Zeit stark unterscheiden. Wenn sich die Abdeckung eines Merkmals zwischen dem Train
und dem Test-Split stark abweicht (z. B. weil es in 90 % der Beispiele im Train-Split auftaucht, aber nur
20 % der Beispiele im Test-Split auftritt), ist dies ein Hinweis darauf, dass Ihre Trainings- und Test-Splits
Splits nicht der gleichen Verteilung entstammen. Sie sollten untersuchen, ob
Sie untersuchen sollten, ob die Art und Weise, wie Sie Ihre Daten aufteilen, sinnvoll ist und ob dieses Merkmal eine Ursache für Datenlecks ist.
Leckage ist.

Für die vorhandenen Merkmalswerte sollten Sie sich deren Verteilung ansehen.
Wenn der Satz von Werten, der in den gesehenen Daten (z. B. dem Train-Split) erscheint, keine Überschneidung
mit der Menge der Werte, die in den ungesehenen Daten (z. B. dem Test-Split) vorkommen, kann dieses
Merkmal sogar die Leistung Ihres Modells beeinträchtigen.

Ein konkretes Beispiel: Stellen Sie sich vor, Sie möchten ein Modell erstellen, um die Zeit zu schätzen, die
für eine bestimmte Taxifahrt zu schätzen. Sie trainieren dieses Modell jede Woche neu und möchten
die Daten der letzten sechs Tage verwenden, um die voraussichtliche Ankunftszeit (ETA)
für heute vorherzusagen. Eines der Merkmale ist DAY_OF_THE_WEEK, das Sie für nützlich halten
weil der Verkehr an Wochentagen normalerweise schlechter ist als am Wochenende. Dieses Merkmal
hat einen Abdeckungsgrad von 100 %, da es in jedem Merkmal vorhanden ist. Im Zugsplit jedoch
sind die Werte für dieses Merkmal jedoch Montag bis Samstag, während im Test-Split der Wert
für dieses Merkmal der Sonntag ist. Wenn Sie dieses Merkmal in Ihr Modell aufnehmen, ohne ein cleveres
Schema zur Kodierung der Tage in Ihr Modell aufnehmen, lässt es sich nicht auf den Test-Split verallgemeinern und könnte die Leistung Ihres
Leistung des Modells beeinträchtigen.

Gute technische Eigenschaften | 145
Andererseits ist HOUR_OF_THE_DAY ein großartiges Merkmal, weil die Tageszeit
Tageszeit auch den Verkehr beeinflusst, und der Wertebereich für dieses Merkmal im Zugsplit
überschneidet sich zu 100 % mit dem Test-Split.

Bei der Betrachtung der Verallgemeinerung eines Merkmals gibt es einen Kompromiss zwischen Verallgemeinerung
tion und Spezifität. Sie könnten feststellen, dass sich das Verkehrsaufkommen während einer Stunde nur
davon abhängt, ob diese Stunde die Hauptverkehrszeit ist. Sie erzeugen also das Merkmal
IS_RUSH_HOUR und setzen es auf 1, wenn die Stunde zwischen 7.00 und 9.00 Uhr oder zwischen
4 p.m. und 6 p.m. IS_RUSH_HOUR ist allgemeiner, aber weniger spezifisch als
STUNDE_DES_TAGES. Die Verwendung von IS_RUSH_HOUR ohne HOUR_OF_THE_DAY
kann dazu führen, dass Modelle wichtige Informationen über die Uhrzeit verlieren.

Zusammenfassung
Da der Erfolg der heutigen ML-Systeme immer noch von ihren Funktionen abhängt, ist es wichtig, dass Unternehmen, die ML
für Unternehmen, die ML in der Produktion einsetzen wollen, Zeit und Mühe in die
in die Funktionsentwicklung zu investieren.

Wie man gute Funktionen entwickelt, ist eine komplexe Frage, auf die es keine Patentrezepte gibt. Die
beste Weg, dies zu lernen, ist die Erfahrung: Probieren Sie verschiedene Merkmale aus und beobachten Sie
wie sie die Leistung Ihrer Modelle beeinflussen. Es ist auch möglich, von Experten zu lernen. I
finde es äußerst nützlich, darüber zu lesen, wie die Siegerteams der Kaggle-Wettbewerbe
ihre Funktionen entwickelt haben, um mehr über ihre Techniken und die Überlegungen
die sie angestellt haben.

Die Entwicklung von Funktionen erfordert häufig Fachwissen, und Fachleute
Experten sind nicht immer Ingenieure, daher ist es wichtig, den Arbeitsablauf so zu gestalten
so zu gestalten, dass auch Nicht-Ingenieure zum Prozess beitragen können.

Hier finden Sie eine Zusammenfassung der besten Praktiken für die Entwicklung von Funktionen:

-Teilen Sie die Daten nach Zeit in train/valid/test splits auf, anstatt dies zufällig zu tun.
-Wenn Sie Ihre Daten überproben, tun Sie dies nach der Aufteilung.
-Skalieren und normalisieren Sie Ihre Daten nach der Aufteilung, um Datenverluste zu vermeiden.
-Statistiken nur aus dem Train-Split und nicht aus den gesamten Daten verwenden, um Ihre
Features zu skalieren und fehlende Werte zu behandeln.
146 | Kapitel 5: Merkmalstechnik

-Verstehen Sie, wie Ihre Daten erzeugt, gesammelt und verarbeitet werden. Beziehen Sie
Fachleute ein, wenn möglich.
-Behalten Sie den Überblick über die Herkunft Ihrer Daten.
-Verstehen Sie die Bedeutung von Merkmalen für Ihr Modell.
-Verwenden Sie Merkmale, die sich gut verallgemeinern lassen.
-Entfernen Sie nicht mehr nützliche Merkmale aus Ihren Modellen.
Mit einer Reihe guter Merkmale können wir zum nächsten Teil des Arbeitsablaufs übergehen: dem Training von ML
Modelle. Bevor wir fortfahren, möchte ich noch einmal betonen, dass der Übergang zur Modellierung nicht bedeutet
nicht bedeutet, dass wir mit der Datenverarbeitung oder dem Feature Engineering fertig sind. Wir sind nie fertig mit
Daten und Merkmalen. In den meisten realen ML-Projekten geht der Prozess der Datenerfassung und
Datenerfassung und Feature-Engineering so lange weiter, wie die Modelle in Produktion sind. Wir müssen
Wir müssen neue, eingehende Daten verwenden, um die Modelle kontinuierlich zu verbessern, was wir in Kapitel 9 behandeln werden.

Zusammenfassung | 147
KAPITEL 6

Modellentwicklung und Offline-Auswertung
In Kapitel 4 haben wir besprochen, wie Sie Trainingsdaten für Ihr Modell erstellen, und in
Kapitel 5 haben wir erörtert, wie Sie aus diesen Trainingsdaten Merkmale entwickeln. Mit dem
ersten Satz von Merkmalen gehen wir zum Teil der ML-Algorithmen von ML-Systemen über. Für mich
macht mir dieser Schritt immer am meisten Spaß, da ich hier mit verschiedenen Algorithmen und
Algorithmen und Techniken herumzuspielen, auch mit den neuesten. Dies ist auch der erste Schritt, bei dem ich
Daten- und Feature-Engineering investiert habe, in ein System verwandeln kann, dessen
System umgewandelt wird, dessen Ergebnisse (Vorhersagen) ich nutzen kann, um den Erfolg meiner Bemühungen zu bewerten.

Um ein ML-Modell zu erstellen, müssen wir zunächst das zu erstellende ML-Modell auswählen. Es gibt so
viele ML-Algorithmen, und immer mehr werden aktiv entwickelt. Dieses Kapitel
beginnt mit sechs Tipps zur Auswahl der besten Algorithmen für Ihre Aufgabe.

Im folgenden Abschnitt werden verschiedene Aspekte der Modellentwicklung erörtert, wie z. B.
Debugging, Experimentverfolgung und Versionierung, verteiltes Training und AutoML.

Die Modellentwicklung ist ein iterativer Prozess. Nach jeder Iteration sollten Sie die
die Leistung Ihres Modells mit der Leistung früherer Iterationen vergleichen und
beurteilen, wie geeignet diese Iteration für die Produktion ist. Der letzte Abschnitt dieses Kapitels
Der letzte Abschnitt dieses Kapitels widmet sich der Bewertung Ihres Modells, bevor Sie es in der Produktion einsetzen.
Dabei wird eine Reihe von Evaluierungstechniken behandelt, darunter Störungstests, Invarianztests,
Modellkalibrierung und rutschenbasierte Bewertung.

Ich gehe davon aus, dass die meisten Leser bereits ein Verständnis für gängige ML-Algorithmen haben
wie lineare Modelle, Entscheidungsbäume, k-nearest neighbors und verschiedene Arten von
neuronaler Netze. In diesem Kapitel werden Techniken rund um diese Algorithmen besprochen
geht aber nicht im Detail auf ihre Funktionsweise ein. Da sich dieses Kapitel mit ML
Algorithmen beschäftigt, erfordert es viel mehr ML-Kenntnisse als andere Kapitel. Wenn Sie nicht
nicht vertraut sind, empfehle ich, einen Online-Kurs zu besuchen oder ein Buch über ML
Algorithmen zu lesen, bevor Sie dieses Kapitel lesen. Leser, die eine schnelle Auffrischung der ML-Grundkenntnisse wünschen

149
Konzepte könnte der Abschnitt "Basic ML Reviews" im GitHub-Repository des Buches hilfreich sein.
Repository.

Modellentwicklung und -schulung
In diesem Abschnitt besprechen wir die notwendigen Aspekte, die Ihnen bei der Entwicklung und dem Training Ihres
Modells, einschließlich der Bewertung verschiedener ML-Modelle für Ihr Problem, der Erstellung von
Ensembles von Modellen, die Verfolgung und Versionierung von Experimenten und das verteilte Training,
was für die Größenordnung, in der Modelle heute üblicherweise trainiert werden, notwendig ist. Wir werden
diesen Abschnitt mit dem fortgeschrittenen Thema AutoML abschließen - der Verwendung von ML zur automatischen
ein Modell auszuwählen, das für Ihr Problem am besten geeignet ist.

Bewertung von ML-Modellen
Es gibt viele mögliche Lösungen für ein bestimmtes Problem. Angesichts einer Aufgabe, die
Lösung von ML nutzen kann, fragen Sie sich vielleicht, welchen ML-Algorithmus
Sie dafür verwenden sollten. Sollten Sie zum Beispiel mit der logistischen Regression beginnen, einem Algorithmus, mit dem Sie
bereits vertraut sind? Oder sollten Sie ein neues, ausgefallenes Modell ausprobieren, das angeblich
der neue Stand der Technik für Ihr Problem sein soll? Ein erfahrener Kollege erwähnte, dass
gradient-boosted trees in der Vergangenheit bei dieser Aufgabe immer gut funktioniert haben - sollten
Sie auf ihren Rat hören?

Hätten Sie unbegrenzte Zeit und Rechenleistung, wäre es vernünftig, alle möglichen Lösungen
alle möglichen Lösungen auszuprobieren und herauszufinden, was für Sie am besten ist. Aber Zeit und Rechenleistung
Rechenleistung sind jedoch begrenzte Ressourcen, und Sie müssen strategisch entscheiden, welche Modelle Sie
auswählen.

Wenn von ML-Algorithmen die Rede ist, denken viele Menschen an klassische ML
Algorithmen gegen neuronale Netze. Das Interesse an neuronalen Netzen und die Medienberichterstattung über
neuronale Netze, insbesondere Deep Learning, was verständlich ist, da die meisten
die meisten Fortschritte in der Künstlichen Intelligenz im letzten Jahrzehnt auf immer größere und tiefere neuronale
und tiefer wurden.

Dieses Interesse und diese Reichweite könnten den Eindruck erwecken, dass Deep Learning
die klassischen ML-Algorithmen ersetzt. Doch auch wenn Deep Learning immer mehr
mehr Anwendungsfälle in der Produktion findet, werden die klassischen ML-Algorithmen nicht verschwinden. Viele
Empfehlungssysteme basieren immer noch auf kollaborativer Filterung und Matrixfaktorisierung.
Baumbasierte Algorithmen, einschließlich gradient-boosted Bäumen, werden immer noch für viele Klassifizierungsaufgaben
Klassifizierungsaufgaben mit strengen Latenzanforderungen.

Selbst bei Anwendungen, in denen neuronale Netze zum Einsatz kommen, werden klassische ML-Algorithmen
immer noch im Tandem verwendet. Zum Beispiel können neuronale Netze und Entscheidungsbäume
zusammen in einem Ensemble verwendet werden. Ein k-means-Clustermodell könnte verwendet werden, um
Merkmale für die Eingabe in ein neuronales Netz. Umgekehrt kann ein vortrainiertes neuronales Netz

150 | Kapitel 6: Modellentwicklung und Offline-Auswertung

(wie BERT oder GPT-3) können zur Erzeugung von Einbettungen verwendet werden, die in ein logistisches
Regressionsmodell einzugeben.

Wenn Sie ein Modell für Ihr Problem auswählen, wählen Sie nicht aus allen möglichen
Modell aus, sondern konzentrieren sich in der Regel auf eine Reihe von Modellen, die für Ihr Problem geeignet sind. Für
Wenn Ihr Chef Ihnen beispielsweise sagt, Sie sollen ein System zur Erkennung giftiger Tweets entwickeln, wissen Sie
Sie wissen, dass es sich um ein Textklassifizierungsproblem handelt - klassifizieren Sie anhand eines Textes, ob er giftig ist oder nicht.
giftig ist oder nicht, und zu den gängigen Modellen für die Textklassifizierung gehören Naive Bayes, logistische
Regression, rekurrente neuronale Netze und transformatorbasierte Modelle wie BERT,
GPT und ihre Varianten.

Wenn Ihr Kunde möchte, dass Sie ein System zur Erkennung betrügerischer Transaktionen entwickeln, wissen Sie
Sie wissen, dass dies das klassische Problem der Anomalienerkennung ist - betrügerische Transaktionen
sind Anomalien, die Sie erkennen wollen, und es gibt viele gängige Algorithmen für dieses Problem.
Algorithmen für dieses Problem gibt es viele, darunter K-Nächste Nachbarn, Isolation Forest, Clustering und neuronale
Netze.

Die Kenntnis gängiger ML-Aufgaben und der typischen Lösungsansätze ist dabei
in diesem Prozess.

Verschiedene Arten von Algorithmen erfordern eine unterschiedliche Anzahl von Etiketten und eine unterschiedliche
Mengen an Rechenleistung. Einige brauchen länger zum Trainieren als andere, während andere
länger brauchen, um Vorhersagen zu treffen. Algorithmen, die keine neuronalen Netze verwenden, sind eher
erklärbar (z. B. welche Merkmale am meisten zur Einstufung einer E-Mail als
Spam) als neuronale Netze.

Bei der Wahl des Modells ist es wichtig, nicht nur die Leistung des Modells zu berücksichtigen
Leistung des Modells zu berücksichtigen, die anhand von Metriken wie Genauigkeit, F1-Score und Log Loss gemessen wird, sondern
sondern auch seine anderen Eigenschaften, z. B. wie viel Daten, Rechenleistung und Zeit es zum Trainieren benötigt
trainiert werden muss, wie hoch die Inferenzlatenz und die Interpretierbarkeit ist. Zum Beispiel kann ein einfaches logistisches
logistisches Regressionsmodell vielleicht eine geringere Genauigkeit als ein komplexes neuronales Netz, aber es
aber es benötigt zu Beginn weniger markierte Daten, lässt sich viel schneller trainieren und ist viel einfacher zu implementieren,
und es ist auch viel einfacher zu erklären, warum es bestimmte Vorhersagen macht.

Der Vergleich von ML-Algorithmen gehört nicht in den Rahmen dieses Buches. Egal wie gut
ein Vergleich ist, er wird veraltet sein, sobald neue Algorithmen auf den Markt kommen. Damals im
2016 waren LSTM-RNNs der letzte Schrei und das Rückgrat der Architektur seq2seq
(Sequence-to-Sequence), die viele NLP-Aufgaben von maschineller Übersetzung über
Textzusammenfassung bis hin zur Textklassifizierung. Doch nur zwei Jahre später wurden rekurrente
Architekturen weitgehend durch Transformer-Architekturen für NLP-Aufgaben ersetzt.

Um verschiedene Algorithmen zu verstehen, rüsten Sie sich am besten mit grundlegenden
ML-Kenntnissen auszustatten und Experimente mit den Algorithmen durchzuführen, an denen Sie interessiert sind. Um
über die vielen neuen ML-Techniken und -Modelle auf dem Laufenden zu bleiben, finde ich es hilfreich, die
Trends auf großen ML-Konferenzen wie NeurIPS, ICLR und ICML zu verfolgen sowie
Forscher zu verfolgen, deren Arbeit ein hohes Signal-Rausch-Verhältnis auf Twitter aufweist.

Modellentwicklung und Schulung | 151
Sechs Tipps für die Modellauswahl

Ohne auf die Besonderheiten der verschiedenen Algorithmen einzugehen, hier sechs Tipps, die Ihnen helfen könnten
die Ihnen bei der Entscheidung helfen können, an welchen ML-Algorithmen Sie als nächstes arbeiten sollten.

Vermeiden Sie die State-of-the-Art-Falle. Wenn ich Unternehmen und Hochschulabsolventen dabei helfe
Absolventen den Einstieg in ML zu erleichtern, muss ich in der Regel einen nicht unerheblichen Teil der Zeit damit verbringen
sie davon abhalten, direkt in die State-of-the-Art-Modelle zu springen. Ich kann verstehen, warum die Leute
State-of-the-Art-Modelle wollen. Viele glauben, dass diese Modelle die besten
warum eine alte Lösung ausprobieren, wenn man glaubt, dass es eine neuere und bessere
und überlegene Lösung existiert? Viele Unternehmensleiter wollen auch modernste Modelle verwenden
Modelle verwenden, weil sie ihr Unternehmen auf dem neuesten Stand der Technik erscheinen lassen wollen. Entwickler
sind vielleicht auch mehr daran interessiert, neue Modelle in die Hände zu bekommen, als sich immer wieder mit
als sich immer wieder mit denselben alten Dingen zu beschäftigen.

Forscher evaluieren Modelle oft nur im akademischen Umfeld, was bedeutet, dass ein
dass ein Modell, das auf dem neuesten Stand der Technik ist, oft besser abschneidet als bestehende Modelle
auf einigen statischen Datensätzen. Es bedeutet nicht, dass dieses Modell schnell genug oder billig genug ist
schnell oder billig genug ist, um es zu implementieren. Es bedeutet nicht einmal, dass dieses Modell
besser ist als andere Modelle für Ihre Daten.

Es ist zwar wichtig, sich über neue Technologien auf dem Laufenden zu halten und sie für Ihr Unternehmen zu bewerten.
für Ihr Unternehmen, aber das Wichtigste bei der Lösung eines Problems
ist es, Lösungen zu finden, die dieses Problem lösen können. Wenn es eine Lösung gibt, die Ihr Problem lösen kann
Problem lösen kann, die viel billiger und einfacher ist als die modernsten Modelle, sollten Sie die
einfachere Lösung.

Beginnen Sie mit den einfachsten Modellen. Das Zen von Python besagt, dass "einfach besser ist als kompliziert".
plex", und dieser Grundsatz ist auch auf ML anwendbar. Die Einfachheit dient drei Zwecken.
Erstens lassen sich einfachere Modelle leichter einsetzen, und der frühe Einsatz Ihres Modells ermöglicht
können Sie überprüfen, ob Ihre Vorhersage-Pipeline mit Ihrer Trainings-Pipeline übereinstimmt.
Zweitens ist es einfacher, mit einem einfachen Modell zu beginnen und Schritt für Schritt komplexere Komponenten hinzuzufügen.
Schritt für Schritt hinzuzufügen, macht es einfacher, Ihr Modell zu verstehen und zu debuggen. Drittens: Das einfachste
Modell als Ausgangsbasis, mit der Sie Ihre komplexeren Modelle vergleichen können.

Die einfachsten Modelle sind nicht immer gleichbedeutend mit den Modellen mit dem geringsten Aufwand. Ein Beispiel,
vortrainierte BERT-Modelle sind komplex, aber sie erfordern wenig Aufwand für den Einstieg
wenig Aufwand, vor allem wenn Sie eine fertige Implementierung wie die in Hugging Face's Transformer verwenden.
Face's Transformer. In diesem Fall ist es keine schlechte Idee, die komplexe Lösung zu verwenden,
da die Gemeinschaft rund um diese Lösung gut genug entwickelt ist, um Ihnen zu helfen
Probleme zu lösen, auf die Sie stoßen könnten. Vielleicht möchten Sie aber trotzdem
mit einfacheren Lösungen experimentieren, um sicherzustellen, dass das vorgebildete BERT tatsächlich besser
als diese einfacheren Lösungen für Ihr Problem. Vorgeschultes BERT mag anfangs wenig Aufwand bedeuten
sein, aber es kann ziemlich aufwändig sein, es zu verbessern. Wenn Sie hingegen
mit einem einfacheren Modell beginnen, haben Sie viel Spielraum, um Ihr Modell zu verbessern.

152 | Kapitel 6: Modellentwicklung und Offline-Auswertung

1 Andrew Ng hat einen großartigen Vortrag gehalten, in dem er erklärt, dass, wenn ein Lernalgorithmus unter einer starken Verzerrung leidet, die Beschaffung von
mehr Trainingsdaten allein nicht viel helfen. Wenn hingegen ein Lernalgorithmus unter einer hohen Varianz leidet,
hilft es wahrscheinlich, mehr Trainingsdaten zu erhalten.
Vermeiden Sie menschliche Voreingenommenheit bei der Auswahl von Modellen. Stellen Sie sich vor, ein Ingenieur in Ihrem Team wird beauftragt
die Aufgabe, zu bewerten, welches Modell für Ihr Problem besser geeignet ist: ein gradient-boosted tree
oder ein vortrainiertes BERT-Modell. Nach zwei Wochen verkündet dieser Ingenieur, dass das beste
BERT-Modell den besten gradient-boosted tree um 5 % übertrifft. Ihr Team entscheidet sich für
sich für das vortrainierte BERT-Modell zu entscheiden.

Ein paar Monate später stößt jedoch eine erfahrene Ingenieurin zu Ihrem Team. Sie beschließt
sich erneut mit gradient-boosted Bäumen zu beschäftigen und findet heraus, dass diesmal der beste
beste gradient-boosted Baum das vortrainierte BERT-Modell, das Sie derzeit in der Produktion haben, übertrifft.
Produktion. Was ist passiert?

Bei der Bewertung von Modellen gibt es eine Menge menschlicher Vorurteile. Ein Teil des Prozesses der Evaluierung
einer ML-Architektur ist es, mit verschiedenen Merkmalen und verschiedenen Sätzen von
Hyperparametern zu experimentieren, um das beste Modell für diese Architektur zu finden. Wenn ein Ingenieur mehr
Architektur begeistert ist, wird er wahrscheinlich viel mehr Zeit mit dem Experimentieren
experimentieren, was zu leistungsfähigeren Modellen für diese Architektur führen kann.

Wenn man verschiedene Architekturen vergleicht, ist es wichtig, sie unter vergleichbaren Bedingungen zu vergleichen.
vergleichbaren Konfigurationen zu vergleichen. Wenn Sie 100 Experimente für eine Architektur durchführen, ist es nicht fair, wenn Sie nur
nur ein paar Experimente für die Architektur durchzuführen, gegen die Sie sie evaluieren wollen. Sie könnten
müssen Sie möglicherweise auch 100 Experimente für die andere Architektur durchführen.

Da die Leistung einer Modellarchitektur stark von dem Kontext abhängt
abhängt - z.B. von der Aufgabe, den Trainingsdaten, den Testdaten, den Hyperparametern,
usw. abhängt, ist es äußerst schwierig, zu behaupten, dass eine Modellarchitektur besser ist als
eine andere Architektur. Die Behauptung könnte in einem bestimmten Kontext zutreffen, ist aber unwahrscheinlich, dass sie für alle
möglichen Kontexte.

Bewerten Sie gute Leistungen jetzt und gute Leistungen später. Das beste Modell jetzt
bedeutet nicht immer das beste Modell in zwei Monaten. Zum Beispiel kann ein baumbasiertes
baumbasiertes Modell jetzt besser funktionieren, weil Sie noch nicht viele Daten haben, aber in zwei
aber in zwei Monaten können Sie vielleicht die Menge der Trainingsdaten verdoppeln, und
Ihr neuronales Netzwerk könnte dann viel besser funktionieren.^1

Eine einfache Methode, um abzuschätzen, wie sich die Leistung Ihres Modells mit mehr
Daten verändern könnte, ist die Verwendung von Lernkurven. Eine Lernkurve eines Modells ist ein Diagramm seiner Leistung
Leistung - z. B. Trainingsverlust, Trainingsgenauigkeit, Validierungsgenauigkeit - in Abhängigkeit von der Anzahl der
Anzahl der verwendeten Trainingsmuster, wie in Abbildung 6-1 dargestellt. Die Lernkurve hilft nicht
die Lernkurve hilft Ihnen nicht, genau abzuschätzen, wie viel Leistungsgewinn Sie durch mehr

Modellentwicklung und Training | 153
Trainingsdaten, aber es kann Ihnen ein Gefühl dafür geben, ob Sie überhaupt einen Leistungsgewinn
von mehr Trainingsdaten zu erwarten ist.

Abbildung 6-1. Die Lernkurven eines naiven Bayes-Modells und eines SVM-Modells. Quelle:
scikit-learn

Eine Situation, auf die ich gestoßen bin, ist, wenn ein Team ein einfaches neuronales Netz
gegen ein Modell der kollaborativen Filterung zur Erstellung von Empfehlungen. Bei der Offline-Evaluierung
der beiden Modelle offline evaluiert, schnitt das kollaborative Filtermodell besser ab. Allerdings
kann sich das einfache neuronale Netz mit jedem eingehenden Beispiel aktualisieren, während
das kollaborative Filtermodell alle Daten betrachten muss, um seine zugrunde liegende Matrix zu aktualisieren.
Das Team beschloss, sowohl das kollaborative Filtermodell als auch das einfache
neuronales Netzwerk einzusetzen. Mit dem Modell der kollaborativen Filterung wurden Vorhersagen für
Vorhersagen für die Benutzer zu treffen, und trainierte das einfache neuronale Netzwerk in der Produktion kontinuierlich mit neuen Daten,
eingehenden Daten. Nach zwei Wochen war das einfache neuronale Netzwerk in der Lage, die Leistung des
das kollaborative Filtermodell.

Bei der Bewertung von Modellen sollten Sie auch deren Potenzial für
Verbesserungen in naher Zukunft und die Frage, wie einfach/schwierig es ist, diese
Verbesserungen zu erreichen sind.

Abwägen von Kompromissen. Es gibt viele Kompromisse, die Sie bei der Auswahl von Modifikationen eingehen müssen.
els. Wenn Sie verstehen, was für die Leistung Ihres ML-Systems am wichtigsten ist, können Sie
hilft Ihnen, das am besten geeignete Modell zu wählen.

154 | Kapitel 6: Modellentwicklung und Offline-Auswertung

Ein klassisches Beispiel für einen Kompromiss ist die Abwägung zwischen falsch-positiven und falsch-negativen Ergebnissen.
Die Verringerung der Zahl der falsch-positiven Ergebnisse kann die Zahl der falsch-negativen Ergebnisse erhöhen,
und vice versa. Bei einer Aufgabe, bei der falsch-positive Ergebnisse gefährlicher sind als falsch-negative,
wie z. B. bei der Entsperrung von Fingerabdrücken (Unbefugte sollten nicht als autorisiert eingestuft werden
(unbefugte Personen sollten nicht als autorisiert eingestuft werden und Zugang erhalten), könnten Sie ein Modell bevorzugen, das weniger falsch-positive Ergebnisse liefert.
Ähnlich verhält es sich bei einer Aufgabe, bei der falsch-negative Ergebnisse gefährlicher sind als falsch-positive,
wie z. B. beim COVID-19-Screening (Patienten mit COVID-19 sollten nicht als nicht
COVID-19), könnten Sie ein Modell bevorzugen, das weniger falsch-negative Ergebnisse liefert.

Ein weiteres Beispiel für einen Kompromiss sind die Anforderungen an die Rechenleistung und die Genauigkeit - ein komplexeres
komplexere Modell könnte eine höhere Genauigkeit liefern, aber eine leistungsfähigere
einen leistungsfähigeren Rechner, z. B. einen Grafikprozessor anstelle einer CPU, um Vorhersagen mit akzeptabler
Latenzzeit zu erzeugen. Viele Menschen interessieren sich auch für den Kompromiss zwischen Interpretierbarkeit und Leistung.
abwägen. Ein komplexeres Modell kann eine bessere Leistung erbringen, aber seine Ergebnisse sind
weniger interpretierbar.

Verstehen Sie die Annahmen Ihres Modells. Der Statistiker George Box sagte 1976, dass "alle
Modelle sind falsch, aber einige sind nützlich". Die reale Welt ist unlösbar komplex, und
Modelle können sich nur durch Annahmen annähern. Jedes einzelne Modell hat seine
eigenen Annahmen. Wenn Sie verstehen, welche Annahmen ein Modell macht und ob unsere
Daten diese Annahmen erfüllen, kann Ihnen helfen zu beurteilen, welches Modell am besten für
Ihren Anwendungsfall.

Im Folgenden sind einige der häufigsten Annahmen aufgeführt. Es handelt sich nicht um eine erschöpfende
Liste sein, sondern nur als Beispiel dienen:

Vorhersage-Annahme
Jedes Modell, das darauf abzielt, eine Ausgabe Y aus einer Eingabe X vorherzusagen, geht von der
die Annahme, dass es möglich ist, Y auf der Grundlage von X vorherzusagen.

IID
Neuronale Netze gehen davon aus, dass die Beispiele unabhängig und identisch dis- tributiert sind.
gleichverteilt sind, was bedeutet, dass alle Beispiele unabhängig voneinander aus der
gleichen gemeinsamen Verteilung gezogen werden.

Glattheit
Jedes überwachte maschinelle Lernverfahren geht davon aus, dass es eine Reihe von Funktionen gibt
die Eingaben in Ausgaben umwandeln können, so dass ähnliche Eingaben in
in ähnliche Ausgaben umgewandelt werden. Wenn eine Eingabe X eine Ausgabe Y erzeugt, dann würde eine Eingabe, die nahe an X
eine Ausgabe erzeugen, die proportional nahe an Y liegt.

Modellentwicklung und Training | 155
2 Ich bin die Gewinnerlösungen durchgegangen, die auf der "Kaggle Solutions"-Webseite von Farid Rashidi aufgeführt sind. Eine Lösung
tion verwendete 33 Modelle (Giba, "1st Place-Winner Solution-Gilberto Titericz and Stanislav Semenov," Kaggle,
https://oreil.ly/z5od8).
Nachvollziehbarkeit
X sei die Eingabe und Z sei die latente Repräsentation von X. Jedes generative
Modell geht von der Annahme aus, dass die Berechnung der Wahrscheinlichkeit P(Z|X) nachvollziehbar ist.

Grenzen
Ein linearer Klassifikator geht davon aus, dass die Entscheidungsgrenzen linear sind.

Bedingte Unabhängigkeit
Ein naiver Bayes-Klassifikator geht davon aus, dass die Attributwerte unabhängig voneinander sind.
der Klasse unabhängig sind.

Normalverteilt
Viele statistische Methoden gehen davon aus, dass die Daten normalverteilt sind.

Ensembles
Wenn Sie eine ML-Lösung für Ihr Problem in Erwägung ziehen, möchten Sie vielleicht mit folgendem beginnen
einem System beginnen, das nur ein Modell enthält (der Prozess der Auswahl eines Modells für Ihr
Problem wurde weiter oben im Kapitel besprochen). Nachdem Sie ein einziges Modell entwickelt haben,
können Sie darüber nachdenken, wie Sie dessen Leistung weiter verbessern können. Eine Methode, die
eine Methode, die durchweg zu einer Leistungssteigerung geführt hat, ist die Verwendung eines Ensembles aus mehreren Modellen
zu verwenden, anstatt nur ein einzelnes Modell, um Vorhersagen zu treffen. Jedes Modell im Ensemble
wird als Basis-Lernmodell bezeichnet. Wenn Sie zum Beispiel vorhersagen wollen, ob eine E-Mail
SPAM oder NICHT SPAM ist, könnten Sie drei verschiedene Modelle verwenden. Die endgültige Vorhersage
für jede E-Mail ist das Mehrheitsvotum aller drei Modelle. Wenn also mindestens zwei Basislerner
SPAM ausgeben, wird die E-Mail als SPAM klassifiziert.

Zwanzig von 22 Gewinnerlösungen bei Kaggle-Wettbewerben im Jahr 2021 (Stand: August
2021, verwenden Ensembles.^2 Ab Januar 2022 sind die 20 besten Lösungen auf SQuAD 2.0, dem
Stanford Question Answering Dataset, aus Ensembles, wie in Abbildung 6-2 dargestellt.

Ensembling-Methoden werden in der Produktion weniger bevorzugt, da Ensembles
komplexer zu implementieren und schwieriger zu warten sind. Sie sind jedoch immer noch üblich für Aufgaben
wo eine kleine Leistungssteigerung zu einem großen finanziellen Gewinn führen kann, z. B. bei der Vorhersage der
Click-Through-Rate für Anzeigen.

156 | Kapitel 6: Modellentwicklung und Offline-Auswertung

Abbildung 6-2. Ab Januar 2022 sind die 20 besten Lösungen auf SQuAD 2.0 allesamt Ensembles

Anhand eines Beispiels können Sie nachvollziehen, warum die Zusammenlegung funktioniert. Stellen Sie sich vor:
Sie haben drei E-Mail-Spam-Klassifikatoren, jeder mit einer Genauigkeit von 70 %. Unter der Annahme
dass jeder Klassifikator die gleiche Wahrscheinlichkeit hat, eine korrekte Vorhersage für jede E-Mail zu treffen
und dass diese drei Klassifikatoren nicht korreliert sind, werden wir zeigen, dass durch die Ermittlung der
Mehrheitsvotum dieser drei Klassifikatoren eine Genauigkeit von 78,4 % erreicht werden kann.

Für jede E-Mail hat jeder Klassifikator eine 70%ige Chance, richtig zu sein. Das Ensemble wird
korrekt sein, wenn mindestens zwei Klassifikatoren korrekt sind. Tabelle 6-1 zeigt die Wahrscheinlichkeiten der
verschiedenen möglichen Ergebnisse des Ensembles für eine E-Mail. Dieses Ensemble hat
eine Genauigkeit von 0,343 + 0,441 = 0,784, also 78,4 %.

Modellentwicklung und Ausbildung | 157
3 Mikel Galar, Alberto Fernandez, Edurne Barrenechea, Humberto Bustince, und Francisco Herrera, "A Review
on Ensembles for the Class Imbalance Problem: Bagging-, Boosting-, and Hybrid-Based Approaches," IEEE
Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews) 42, no. 4 (July 2012): 463-
84, https://oreil.ly/ZBlgE; G. Rekha, Amit Kumar Tyagi, and V. Krishna Reddy, "Solving Class Imbalance Prob-
lem Using Bagging, Boosting Techniques, With and Without Using Noise Filtering Method," International
Journal of Hybrid Intelligent Systems 15, no. 2 (January 2019): 67-76, https://oreil.ly/hchzU.
4 Trainingsstabilität bedeutet hier eine geringere Schwankung des Trainingsverlustes.
Tabelle 6-1. Mögliche Ergebnisse des Ensembles, das das Mehrheitsvotum
von drei Klassifikatoren

Ergebnisse der drei Modelle Ausgabe des Wahrscheinlichkeitsensembles
Alle drei sind richtig 0,7 * 0,7 * 0,7 = 0,343 Richtig
Nur zwei sind richtig (0,7 * 0,7 * 0,3) * 3 = 0,441 Richtig
Nur einer ist richtig (0,3 * 0,3 * 0,7) * 3 = 0,189 Falsch
Keiner ist richtig 0,3 * 0,3 * 0,3 = 0,027 Falsch
Diese Berechnung gilt nur, wenn die Klassifikatoren in einem Ensemble unkorreliert sind. Wenn alle
Klassifikatoren perfekt korreliert sind - alle drei Klassifikatoren machen die gleiche Vorhersage für
Vorhersage für jede E-Mail machen, hat das Ensemble die gleiche Genauigkeit wie jeder einzelne Klassifikator.
Bei der Erstellung eines Ensembles gilt: Je geringer die Korrelation zwischen den Basislernern ist, desto
desto besser wird das Ensemble sein. Daher ist es üblich, sehr unterschiedliche Typen von
Modelle für ein Ensemble zu wählen. Sie könnten zum Beispiel ein Ensemble erstellen, das besteht aus
einem Transformatormodell, einem rekurrenten neuronalen Netz und einem gradient-boosted Baum besteht.

Es gibt drei Möglichkeiten, ein Ensemble zu erstellen: Bagging, Boosting und Stacking. Unter
mehreren Übersichtsarbeiten zufolge nicht nur die Leistung zu steigern, sondern auch
Methoden wie Boosting und Bagging, zusammen mit Resampling, bei unausgewogenen
unausgewogenen Datensätzen zu helfen.^3 Wir gehen auf jede dieser drei Methoden ein und beginnen mit
Bagging.

Absackung

Bagging, eine Abkürzung der Bootstrap-Aggregation, wurde entwickelt, um sowohl die
Trainingsstabilität und Genauigkeit von ML-Algorithmen zu verbessern.^4 Es reduziert die Varianz und hilft
Überanpassung zu vermeiden.

Anstatt einen Klassifikator auf dem gesamten Datensatz zu trainieren, werden Stichproben
Stichproben mit Ersetzung, um verschiedene Datensätze, so genannte Bootstraps, zu erstellen und ein Klassifikations- oder
Klassifikations- oder Regressionsmodell auf jedem dieser Bootstraps. Stichprobenbildung mit Ersetzung
stellt sicher, dass jeder Bootstrap unabhängig von seinen Kollegen erstellt wird. Abbildung 6-3 zeigt
eine Veranschaulichung des Bagging.

158 | Kapitel 6: Modellentwicklung und Offline-Auswertung

5 Leo Breiman, "Bagging Predictors," Machine Learning 24 (1996): 123-40, https://oreil.ly/adzJu.
Abbildung 6-3. Veranschaulichung von Bagging. Quelle: Angepasst von einem Bild von Sirakorn

Handelt es sich um ein Klassifizierungsproblem, wird die endgültige Vorhersage durch das Mehrheitsvotum aller Modelle entschieden.
aller Modelle. Wenn zum Beispiel 10 Klassifikatoren für SPAM und 6 Modelle für NICHT SPAM stimmen,
lautet die endgültige Vorhersage SPAM.

Wenn es sich um eine Regression handelt, ist die endgültige Vorhersage der Durchschnitt aller Modelle
Vorhersagen.

Bagging verbessert im Allgemeinen instabile Methoden, wie neuronale Netze, Klassifizierungs- und
und Regressionsbäume sowie die Teilmengenauswahl bei der linearen Regression. Allerdings kann es die Leistung
die Leistung von stabilen Methoden wie k-nearest neighbors leicht verschlechtern.^5

Ein Random Forest ist ein Beispiel für Bagging. Ein Random Forest ist eine Sammlung von Entscheidungsbäumen
Entscheidungsbäumen, die sowohl durch Bagging als auch durch Zufälligkeit der Merkmale konstruiert werden, wobei jeder Baum
nur aus einer zufälligen Teilmenge von Merkmalen auswählen kann.

Ankurbelung

Boosting ist eine Familie von iterativen Ensemble-Algorithmen, die schwache Lerner in
starke Lernende umwandeln. Jeder Lerner in diesem Ensemble wird mit demselben Satz von Stichproben trainiert, aber
aber die Stichproben werden in den einzelnen Iterationen unterschiedlich gewichtet. Dadurch konzentrieren sich die zukünftigen schwachen Lerner
Lernenden mehr auf die Beispiele konzentrieren, die die vorherigen schwachen Lernenden falsch klassifiziert haben. Abbildung 6-4
zeigt eine Veranschaulichung des Boosting, das die folgenden Schritte umfasst.

Modellentwicklung und Schulung | 159
Abbildung 6-4. Illustration des Boostings. Quelle: Adaptiert von einem Bild von Sirakorn

1.1. Sie beginnen mit dem Training des ersten schwachen Klassifikators auf dem Originaldatensatz.
2.2. Die Stichproben werden neu gewichtet, je nachdem, wie gut der erste Klassifikator sie klassifiziert, z.B.,
falsch klassifizierte Proben werden höher gewichtet.
3.3. Trainieren Sie den zweiten Klassifikator auf diesem neu gewichteten Datensatz. Ihr Ensemble besteht nun
Ensemble besteht nun aus dem ersten und dem zweiten Klassifikator.
4.4. Die Stichproben werden danach gewichtet, wie gut das Ensemble sie klassifiziert.
5.5. Trainieren Sie den dritten Klassifikator auf diesem neu gewichteten Datensatz. Fügen Sie den dritten Klassifikator zum
Ensemble hinzu.
6.6. Wiederholen Sie dies für so viele Iterationen wie nötig.
7.7. Bilden Sie den endgültigen starken Klassifikator als eine gewichtete Kombination der vorhandenen Klassifikatoren.
Klassifikatoren mit kleineren Trainingsfehlern haben höhere Gewichte.
Ein Beispiel für einen Boosting-Algorithmus ist eine Gradient Boosting Machine (GBM), die
ein Vorhersagemodell erstellt, das in der Regel aus schwachen Entscheidungsbäumen besteht. Er baut das Modell
stufenweise auf, wie andere Boosting-Methoden, und verallgemeinert sie, indem er
die Optimierung einer beliebigen differenzierbaren Verlustfunktion ermöglicht.

160 | Kapitel 6: Modellentwicklung und Offline-Auswertung

6 "Machine Learning Challenge Winning Solutions", https://oreil.ly/YjS8d.
7 Tianqi Chen und Tong He, "Higgs Boson Discovery with Boosted Trees," Proceedings of Machine Learning
Research 42 (2015): 69-80, https://oreil.ly/ysBYO.
XGBoost, eine Variante von GBM, war früher der Algorithmus der Wahl für viele Siegerteams von
Gewinnerteams von ML-Wettbewerben.^6 Er wurde in einem breiten Spektrum von Aufgaben eingesetzt, von der Klassifizierung,
Ranking bis hin zur Entdeckung des Higgs-Bosons.^7 Viele Teams haben sich jedoch
für LightGBM, ein verteiltes Gradient-Boosting-Framework, das paralleles Lernen ermöglicht
paralleles Lernen ermöglicht, was im Allgemeinen ein schnelleres Training auf großen Datensätzen erlaubt.

Stapeln

Stacking bedeutet, dass Sie Basis-Learner aus den Trainingsdaten trainieren und dann einen Meta-Learner erstellen, der die
Lerner, der die Ergebnisse der Basis-Learner kombiniert, um endgültige Vorhersagen auszugeben, wie
in Abbildung 6-5 dargestellt. Der Meta-Learner kann so einfach wie eine Heuristik sein: Sie nehmen die
Mehrheitsvotum (für Klassifizierungsaufgaben) oder das Durchschnittsvotum (für Regressionsaufgaben) von
allen Basislernern. Es kann ein anderes Modell sein, z. B. ein logistisches Regressionsmodell oder ein
lineares Regressionsmodell.

Abbildung 6-5. Eine Visualisierung eines gestapelten Ensembles aus drei Basislernern

Weitere großartige Ratschläge zur Erstellung eines Ensembles finden Sie in dem großartigen Ensemble
Leitfaden von MLWave, einem der legendären Teams von Kaggle.

Modellentwicklung und Training | 161
Versuchsverfolgung und Versionierung
Während des Modellentwicklungsprozesses müssen Sie oft mit vielen
Architekturen und vielen verschiedenen Modellen experimentieren, um das beste Modell für Ihr Problem auszuwählen.
Einige Modelle scheinen einander ähnlich zu sein und unterscheiden sich nur in einem Hyperparameter.
Hyperparametern unterscheiden, z. B. ein Modell mit einer Lernrate von 0,003 und ein anderes Modell mit
mit einer Lernrate von 0,002 - und doch sind ihre Leistungen dramatisch unterschiedlich. Es ist
Es ist wichtig, alle Definitionen im Auge zu behalten, die für die Neuerstellung eines Experiments und
seine relevanten Artefakte. Ein Artefakt ist eine Datei, die während eines Experiments erzeugt wird - Beispiele für
Beispiele für Artefakte können Dateien sein, die die Verlustkurve, das Auswertungsverlustdiagramm, Protokolle oder
Zwischenergebnisse eines Modells während eines Trainingsprozesses. Dies ermöglicht Ihnen
verschiedene Experimente zu vergleichen und das für Ihre Bedürfnisse am besten geeignete auszuwählen.
Der Vergleich verschiedener Experimente kann Ihnen auch helfen zu verstehen, wie kleine Änderungen
wie sich kleine Änderungen auf die Leistung Ihres Modells auswirken, was Ihnen wiederum einen besseren Einblick in die
Ihr Modell funktioniert.

Der Prozess der Verfolgung des Fortschritts und der Ergebnisse eines Experiments wird als Experi-
Versuchsverfolgung. Der Prozess der Aufzeichnung aller Details eines Experiments zum Zweck
nachzustellen oder mit anderen Experimenten zu vergleichen, wird als Ver- sionierung
sionierung. Diese beiden Prozesse gehen Hand in Hand miteinander. Viele Werkzeuge waren ursprünglich dazu gedacht
wie MLflow und Weights & Biases, haben sich zu Werkzeugen entwickelt, die
Versionierung integriert. Viele Werkzeuge, die ursprünglich als Versionierungswerkzeuge gedacht waren, wie z. B.
DVC, haben auch die Versuchsverfolgung integriert.

Verfolgung von Experimenten

Ein großer Teil des Trainings eines ML-Modells besteht darin, auf die Lernprozesse aufzupassen. Viele
Probleme können während des Trainingsprozesses auftreten, z. B. nicht abnehmende Verluste, Über- und
Überanpassung, Unteranpassung, schwankende Gewichtungswerte, tote Neuronen und zu wenig
Speicher. Es ist wichtig, die Vorgänge während des Trainings zu verfolgen, nicht nur um
um diese Probleme zu erkennen und zu beheben, sondern auch um zu bewerten, ob Ihr Modell etwas
nützlich ist.

Als ich anfing, mich mit ML zu beschäftigen, wurde mir nur gesagt, ich solle Verlust und Geschwindigkeit verfolgen. Schnell -
mehrere Jahre vorwärts, und die Leute verfolgen so viele Dinge, dass ihre Experiment
Experimentiertafeln schön und erschreckend zugleich aussehen. Im Folgenden finden Sie nur
eine kurze Liste von Dingen, die Sie während des Trainingsprozesses für jedes Experiment verfolgen sollten
Ausbildungsprozess:

Die Verlustkurve, die dem Train-Split und jedem der Eval-Splits entspricht.
Die Leistungskennzahlen des Modells, die Ihnen für alle Nicht-Test-Splits wichtig sind, wie z. B.
Genauigkeit, F1, Perplexität.
Das Protokoll der entsprechenden Probe, der Vorhersage und des Ground-Truth-Labels. Dies ist nützlich
für Ad-hoc-Analysen und zur Überprüfung der Korrektheit nützlich.
162 | Kapitel 6: Modellentwicklung und Offline-Auswertung

8 Wir werden die Beobachtbarkeit in Kapitel 8 behandeln.
9 Ich warte immer noch auf ein Tool zur Nachverfolgung von Experimenten, das sich mit Git- und DVC-Commits integrieren lässt.
-Geschwindigkeit Ihres Modells, gemessen an der Anzahl der Schritte pro Sekunde oder, wenn es sich um
die Anzahl der verarbeiteten Token pro Sekunde.
Systemleistungsmetriken wie Speichernutzung und CPU/GPU-Auslastung.
Sie sind wichtig, um Engpässe zu erkennen und die Verschwendung von Systemressourcen zu vermeiden.
-Die zeitlichen Werte aller Parameter und Hyperparameter, deren Änderungen die Leistung Ihres Modells beeinflussen können
die Leistung Ihres Modells beeinflussen können, wie z. B. die Lernrate, wenn Sie einen Lernratenplan verwenden
Lernrate, Gradientennormen (sowohl global als auch pro Schicht), insbesondere wenn Sie
Gradientennormen; und Gewichtsnorm, insbesondere wenn Sie Gewichtsabnahme
abklingen.
Theoretisch ist es keine schlechte Idee, alles zu verfolgen, was Sie können. Die meiste Zeit über
brauchen Sie sich die meisten davon wahrscheinlich nicht anzusehen. Aber wenn doch etwas passiert, könnten eine
eine oder mehrere von ihnen Hinweise zum Verständnis und/oder zur Fehlerbehebung Ihres Modells geben. In
Im Allgemeinen gibt Ihnen das Tracking Einblick in den Zustand Ihres Modells.^8 In der Praxis ist es jedoch so, dass
In der Praxis kann es jedoch aufgrund der begrenzten Möglichkeiten der heutigen Werkzeuge überwältigend sein, zu viele Dinge zu verfolgen.
zu viele Dinge zu verfolgen, und die Verfolgung weniger wichtiger Dinge kann Sie davon ablenken, das zu verfolgen
wirklich wichtig ist.

Die Verfolgung von Experimenten ermöglicht den Vergleich zwischen verschiedenen Experimenten. Indem Sie beobachten, wie eine
einer Komponente auf die Leistung des Modells auswirkt, gewinnen Sie ein
Verständnis dafür, was diese Komponente tut.

Eine einfache Möglichkeit, Ihre Experimente zu verfolgen, ist die automatische Erstellung von Kopien aller
Code-Dateien, die für ein Experiment benötigt werden, zu erstellen und alle Ausgaben mit ihren Zeitstempeln zu protokollieren.^9 Mit
Tools von Drittanbietern zur Verfolgung von Experimenten können Sie jedoch schöne Dashboards erstellen und
können Sie Ihre Experimente mit Ihren Kollegen teilen.

Versionierung

Stellen Sie sich folgendes Szenario vor. Sie und Ihr Team haben in den letzten Wochen an Ihrem Modell gefeilt
Modell zu optimieren, und einer der Durchläufe zeigte schließlich vielversprechende Ergebnisse. Sie wollten das Modell
also versuchten Sie, das Modell mit den Hyperparametern zu replizieren, die Sie irgendwo notiert hatten.
die Sie irgendwo notiert hatten, zu replizieren, nur um festzustellen, dass die Ergebnisse nicht ganz dieselben waren.
Sie erinnerten sich daran, dass Sie zwischen diesem und dem nächsten Durchlauf einige Änderungen am Code vorgenommen hatten.
Sie versuchten also, die Änderungen aus dem Gedächtnis rückgängig zu machen, weil Ihr rücksichtsloses
Vergangenheit entschieden hatte, dass die Änderung zu geringfügig war, um übernommen zu werden. Aber Sie konnten trotzdem
das vielversprechende Ergebnis nicht wiederholen, weil es einfach zu viele Möglichkeiten gibt
um Änderungen vorzunehmen.

Modellentwicklung und Training | 163
Dieses Problem hätte vermieden werden können, wenn Sie Ihre ML-Experimente versioniert hätten. ML
Systeme bestehen zum Teil aus Code und zum Teil aus Daten, daher müssen Sie nicht nur Ihren Code, sondern auch Ihre
sondern auch die Daten. Die Codeversionierung ist in der Branche mehr oder weniger zum Standard geworden.
Die Versionierung von Daten ist derzeit jedoch wie Zahnseide. Alle sind sich einig, dass es eine gute
zu tun, aber nur wenige tun es.

Es gibt einige Gründe, warum die Versionierung von Daten eine Herausforderung darstellt. Ein Grund ist, dass
da Daten oft viel größer sind als Code, können wir nicht dieselbe Strategie anwenden, die
die man normalerweise für die Versionierung von Code verwendet, nicht auf Daten anwenden.

Bei der Codeversionierung werden zum Beispiel alle Änderungen an einer
Code-Basis. Eine Änderung wird als Diff bezeichnet, kurz für Differenz. Jede Änderung wird gemessen
durch einen zeilenweisen Vergleich. Eine Codezeile ist in der Regel so kurz, dass ein zeilenweiser
Vergleich sinnvoll ist. Eine Zeile Ihrer Daten, insbesondere wenn sie in einem binären Format gespeichert sind, kann jedoch
Binärformat gespeichert sind, können unendlich lang sein. Die Aussage, dass diese Zeile mit 1.000.000 Zeichen
sich von der anderen Zeile mit 1.000.000 Zeichen unterscheidet, ist nicht sehr hilfreich.

Werkzeuge zur Codeversionierung ermöglichen es den Benutzern, zu einer früheren Version der Codebasis zurückzukehren, indem sie
Kopien aller alten Dateien aufbewahren. Ein verwendeter Datensatz kann jedoch so groß sein, dass
dass eine mehrfache Duplizierung nicht praktikabel ist.

Tools zur Codeversionierung ermöglichen es mehreren Personen, gleichzeitig an derselben Codebasis zu arbeiten.
zur gleichen Zeit arbeiten, indem sie die Codebasis auf dem lokalen Rechner jeder Person duplizieren. Allerdings kann ein
Datensatz passt jedoch möglicherweise nicht auf einen lokalen Rechner.

Zweitens herrscht immer noch Verwirrung darüber, was genau ein Diff ist, wenn wir Daten versionieren.
Bedeutet ein Diff eine Änderung des Inhalts einer beliebigen Datei in Ihrem Datenspeicher, nur
wenn eine Datei entfernt oder hinzugefügt wird, oder wenn sich die Prüfsumme des gesamten Repositorys
geändert hat?

Ab 2021 registrieren Datenversionierungstools wie DVC ein Diff nur, wenn sich die Prüfsumme des
Gesamtverzeichnisses geändert hat und wenn eine Datei entfernt oder hinzugefügt wurde.

Eine weitere Unklarheit besteht darin, wie Konflikte bei der Zusammenführung von Daten gelöst werden können: Wenn Entwickler 1 die Daten
Version X verwendet, um Modell A zu trainieren, und Entwickler 2 die Datenversion Y verwendet, um Modell B zu trainieren, macht es
macht es keinen Sinn, die Datenversionen X und Y zusammenzuführen, um Z zu erstellen, da es kein Modell gibt
das mit Z korrespondiert.

Drittens, wenn Sie Benutzerdaten zum Trainieren Ihres Modells verwenden, können Vorschriften wie die General Data Protec-
tionsverordnung (GDPR) die Versionierung dieser Daten kompliziert machen können. Zum Beispiel,
können die Vorschriften vorschreiben, dass Sie Nutzerdaten auf Anfrage löschen müssen, was es rechtlich
rechtlich unmöglich machen, ältere Versionen Ihrer Daten wiederherzustellen.

Eine konsequente Verfolgung und Versionierung von Experimenten trägt zur Reproduzierbarkeit bei, aber
aber sie garantiert keine Reproduzierbarkeit. Die von Ihnen verwendeten Frameworks und Hardware können

164 | Kapitel 6: Modellentwicklung und Offline-Auswertung

10 Zu den bemerkenswerten Beispielen gehören atomare Operationen in CUDA, bei denen nicht-deterministische Reihenfolgen von Operationen zu
zu unterschiedlichen Rundungsfehlern in Gleitkommazahlen zwischen den Läufen führen.

Nichtdeterminismus in Ihre Versuchsergebnisse einführen,^10 was es unmöglich macht, das
das Ergebnis eines Experiments wiederzugeben, ohne alles über die Umgebung zu wissen
in der das Experiment abläuft.
Die Art und Weise, wie wir derzeit so viele Experimente durchführen müssen, um das bestmögliche Modell zu finden
Modell zu finden, ist das Ergebnis davon, dass wir ML als Blackbox behandeln. Da wir nicht vorhersagen können, welche
Konfiguration am besten funktionieren wird, müssen wir mit mehreren Konfigurationen experimentieren.
Ich hoffe jedoch, dass wir mit dem Fortschreiten des Feldes mehr Verständnis für
verschiedene Modelle und können Schlussfolgerungen darüber ziehen, welches Modell am besten funktioniert
Hunderte oder Tausende von Experimenten durchzuführen.
Fehlersuche bei ML-Modellen
Die Fehlersuche ist ein fester Bestandteil der Entwicklung jeder Software. ML-Modelle sind keine
eine Ausnahme. Das Debuggen macht nie Spaß, und das Debuggen von ML-Modellen kann besonders
aus den folgenden drei Gründen besonders frustrierend sein.
Erstens: ML-Modelle versagen stillschweigend, ein Thema, das wir in Kapitel 8 ausführlich behandeln werden. Der Code
kompiliert. Der Verlust nimmt ab, wie er sollte. Die richtigen Funktionen werden aufgerufen. Die
Vorhersagen werden gemacht, aber die Vorhersagen sind falsch. Die Entwickler bemerken die
Fehler. Und schlimmer noch, die Benutzer merken es auch nicht und verwenden die Vorhersagen, als ob die Anwendung
als würde die Anwendung so funktionieren, wie sie sollte.
Zweitens, selbst wenn man glaubt, den Fehler gefunden zu haben, kann es frustrierend langsam sein
um zu überprüfen, ob der Fehler behoben wurde. Bei der Fehlersuche in einer herkömmlichen Software
Programm können Sie vielleicht Änderungen am fehlerhaften Code vornehmen und das Ergebnis sofort sehen.
sofort sehen. Wenn Sie jedoch Änderungen an einem ML-Modell vornehmen, müssen Sie möglicherweise
das Modell neu trainieren und warten, bis es konvergiert, um zu sehen, ob der Fehler behoben ist, was
Stunden dauern kann. In manchen Fällen können Sie nicht einmal sicher sein, ob die Fehler behoben sind, bis
das Modell den Benutzern zur Verfügung gestellt wird.
Drittens ist das Debugging von ML-Modellen aufgrund ihrer funktionsübergreifenden Komplexität schwierig.
Ein ML-System besteht aus vielen Komponenten: Daten, Labels, Merkmale, ML-Algorithmen,
Code, Infrastruktur, usw. Diese verschiedenen Komponenten können in den Händen verschiedener
Teams gehören. So werden beispielsweise Daten von Dateningenieuren verwaltet, Beschriftungen von Fachexperten
Experten, ML-Algorithmen von Datenwissenschaftlern und die Infrastruktur von ML-Ingenieuren oder
dem ML-Plattform-Team. Wenn ein Fehler auftritt, kann er durch eine dieser Komponenten
oder einer Kombination dieser Komponenten liegen, so dass es schwierig ist, zu erkennen, wo man
wer sich damit befassen sollte.
Modellentwicklung und Schulung | 165
Hier sind einige der Gründe, die zum Scheitern eines ML-Modells führen können:
Theoretische Beschränkungen
Wie bereits erwähnt, hat jedes Modell seine eigenen Annahmen über die
Daten und die verwendeten Merkmale. Ein Modell kann fehlschlagen, weil die Daten, aus denen es lernt
nicht mit seinen Annahmen übereinstimmen. Sie verwenden zum Beispiel ein lineares Modell für die
Daten, deren Entscheidungsgrenzen nicht linear sind.
Schlechte Implementierung des Modells
Das Modell könnte gut zu den Daten passen, aber die Fehler liegen in der Implementierung des Modells.
tion des Modells. Wenn Sie zum Beispiel PyTorch verwenden, haben Sie vielleicht vergessen
Gradientenaktualisierungen während der Auswertung zu stoppen, obwohl Sie das sollten. Je mehr Kom- ponenten ein
Komponenten ein Modell hat, desto mehr Dinge können schief gehen und desto schwieriger ist es
desto schwieriger ist es, herauszufinden, was schief läuft. Da die Modelle jedoch zunehmend
werden und immer mehr Unternehmen Modelle von der Stange verwenden, wird dieses Problem
immer weniger ein Problem.
Schlechte Wahl von Hyperparametern
Bei ein und demselben Modell kann ein Satz von Hyperparametern das beste Ergebnis liefern.
Ergebnis liefern, aber ein anderer Satz von Hyperparametern kann dazu führen, dass das Modell nie
konvergiert. Das Modell passt sehr gut zu Ihren Daten, und seine Implementierung ist
korrekt, aber ein schlechter Satz von Hyperparametern kann Ihr Modell unbrauchbar machen.
Probleme mit den Daten
Bei der Datenerhebung und -vorverarbeitung können viele Dinge schief gehen
die dazu führen können, dass Ihre Modelle schlecht funktionieren, z. B. wenn
Beschriftungen, verrauschte Beschriftungen, mit veralteten Statistiken normalisierte Merkmale
Statistiken normalisiert wurden, und mehr.
Schlechte Auswahl von Merkmalen
Es kann viele mögliche Merkmale geben, aus denen Ihre Modelle lernen können. Zu viele
Features können dazu führen, dass sich Ihre Modelle zu sehr an die Trainingsdaten anpassen oder Daten
Datenverluste. Bei zu wenigen Merkmalen könnte es an Vorhersagekraft fehlen, damit Ihre Modelle
gute Vorhersagen zu treffen.
Das Debugging sollte sowohl präventiv als auch kurativ sein. Sie sollten gesunde Praktiken haben
um die Möglichkeiten zur Verbreitung von Fehlern zu minimieren, sowie ein Verfahren zur
Erkennung, Lokalisierung und Behebung von Fehlern. Die Disziplin, sowohl die besten Praktiken
Praktiken und das Debugging-Verfahren zu befolgen, ist bei der Entwicklung, Implementierung und
Einsatz von ML-Modellen.
166 | Kapitel 6: Modellentwicklung und Offline-Auswertung

Leider gibt es immer noch keinen wissenschaftlichen Ansatz für die Fehlersuche in der ML. Allerdings
gibt es eine Reihe von bewährten Debugging-Techniken, die von erfahrenen
erfahrenen ML-Ingenieuren und -Forschern veröffentlicht. Im Folgenden werden drei davon vorgestellt. Leserinnen und Leser
die mehr erfahren möchten, sollten sich Andrej Karpathys großartigen Beitrag
"Ein Rezept für das Training neuronaler Netze".

Einfach anfangen und nach und nach weitere Komponenten hinzufügen
Beginnen Sie mit dem einfachsten Modell und fügen Sie dann langsam weitere Komponenten hinzu, um zu sehen, ob
ob dies die Leistung verbessert oder beeinträchtigt. Wenn Sie zum Beispiel ein rekurrentes
rekurrentes neuronales Netz (RNN) aufbauen möchten, beginnen Sie mit nur einer Ebene von RNN-Zellen, bevor Sie mehrere übereinander
bevor Sie mehrere übereinander stapeln oder mehr Regularisierung hinzufügen. Wenn Sie ein BERT-ähnliches Modell verwenden möchten
Modell (Devlin et al. 2018), das sowohl ein maskiertes Sprachmodell (MLM)
und einen Verlust für die Vorhersage des nächsten Satzes (NSP) verwendet, sollten Sie vielleicht nur den MLM
Verlust zu verwenden, bevor der NSP-Verlust hinzugefügt wird.
Derzeit beginnen viele Menschen mit dem Klonen einer Open-Source-Implementierung eines
modernsten Modells und fügen ihre eigenen Daten ein. Wenn die Chance besteht, dass es
funktioniert, ist das großartig. Aber wenn es nicht funktioniert, ist es sehr schwierig, das System zu debuggen, weil das
Denn das Problem könnte durch jede der vielen Komponenten des Modells verursacht worden sein.

Überanpassung einer einzelnen Charge
Nachdem Sie eine einfache Implementierung Ihres Modells erstellt haben, versuchen Sie, eine kleine
eine kleine Menge von Trainingsdaten und führen die Auswertung mit denselben Daten durch, um sicherzustellen, dass
um sicherzustellen, dass es den kleinstmöglichen Verlust erreicht. Wenn es sich um Bilderkennung handelt, führen Sie eine Überanpassung an 10
Bilder und prüfen Sie, ob Sie eine Genauigkeit von 100 % erreichen können, oder bei maschineller
Übersetzung, überarbeiten Sie 100 Satzpaare und prüfen Sie, ob Sie einen BLEU-Wert
von nahezu 100 erreichen. Wenn es nicht möglich ist, eine kleine Datenmenge zu überarbeiten, könnte etwas
mit Ihrer Implementierung falsch.

Einen Zufallswert festlegen
Es gibt so viele Faktoren, die zur Zufälligkeit Ihres Modells beitragen:
Gewichtsinitialisierung, Dropout, Datenmischung usw. Zufälligkeit macht es schwer
Ergebnisse zwischen verschiedenen Experimenten zu vergleichen - Sie wissen nicht, ob die Veränderung der
Leistung auf eine Änderung des Modells oder einen anderen Zufallswert zurückzuführen ist. Die Einstellung
eines zufälligen Seeds sorgt für Konsistenz zwischen verschiedenen Läufen. Außerdem können Sie damit
Fehler zu reproduzieren und anderen Personen die Möglichkeit zu geben, Ihre Ergebnisse zu reproduzieren.

Modellentwicklung und Schulung | 167
11 Bei Produkten, die eine große Anzahl von Nutzern bedienen, müssen Sie auch auf die Skalierbarkeit eines Modells achten,
was außerhalb des Rahmens eines ML-Projekts liegt und daher in diesem Buch nicht behandelt wird.
12 Laut Wikipedia sind "Out-of-core-Algorithmen Algorithmen, die für die Verarbeitung von Daten konzipiert sind, die zu
die zu groß sind, um auf einmal in den Hauptspeicher eines Computers zu passen" (s.v. "External memory algorithm", https://oreil.ly/apv5m).
13 Tim Salimans, Yaroslav Bulatov, und Mitwirkende, gradient-checkpointing repository, 2017,
https://oreil.ly/GTUgC.

Verteilte Ausbildung
Da die Modelle immer größer und ressourcenintensiver werden, legen Unternehmen viel mehr Wert auf
Skalierbarkeit.^11 Fachwissen über Skalierbarkeit ist schwer zu erwerben, denn es erfordert
einen regelmäßigen Zugang zu massiven Rechenressourcen erfordert. Skalierbarkeit ist ein Thema, das eine
eine Reihe von Büchern verdient. Dieser Abschnitt behandelt einige bemerkenswerte Themen, um die Herausforderungen
Herausforderungen bei der skalierten Durchführung von ML hervorzuheben und ein Gerüst bereitzustellen, das Sie bei der Planung der Ressourcen für Ihr
Projekt entsprechend zu planen.
Es kommt häufig vor, dass ein Modell mit Daten trainiert wird, die nicht in den Speicher passen. Das ist besonders
besonders häufig bei medizinischen Daten wie CT-Scans oder Genomsequenzen. Es
kann auch bei Textdaten vorkommen, wenn Sie für Teams arbeiten, die große Sprachmodelle trainieren
(Stichwort OpenAI, Google, NVIDIA, Cohere).
Wenn Ihre Daten nicht in den Speicher passen, müssen Ihre Algorithmen für die Vorverarbeitung (z. B.,
Zero-Centering, Normalisierung, Whitening), Shuffling und Stapelverarbeitung von Daten
parallel laufen.^12 Wenn eine Stichprobe Ihrer Daten groß ist, z. B. wenn eine Maschine
nur wenige Proben gleichzeitig verarbeiten kann, können Sie möglicherweise nur mit einer kleinen Stapelgröße arbeiten
Größe arbeiten, was zu Instabilität bei der auf Gradientenabstieg basierenden Optimierung führt.
In einigen Fällen ist eine Datenprobe so groß, dass sie nicht einmal in den Speicher passt, und Sie müssen
dann muss man so etwas wie Gradient Checkpointing verwenden, eine Technik, die den
Kompromiss zwischen Speicherplatzbedarf und Rechenleistung ausnutzt, damit Ihr System mehr Berechnungen
mit weniger Speicher. Laut den Autoren des Open-Source-Pakets gradient-
Checkpointing: "Bei Feed-Forward-Modellen waren wir in der Lage, mehr als 10x größere Modelle
Modelle auf unserem Grafikprozessor unterbringen, bei nur 20 % mehr Rechenzeit."^13 Selbst wenn eine
Probe in den Speicher passt, können Sie mit Checkpointing mehr Proben in einen
Stapel passen, wodurch Sie Ihr Modell möglicherweise schneller trainieren können.
Datenparallelität
Es ist heute die Norm, ML-Modelle auf mehreren Rechnern zu trainieren. Die häufigste
Parallelisierungsmethode, die von modernen ML-Frameworks unterstützt wird, ist die Datenparallelisierung:
Sie teilen Ihre Daten auf mehrere Rechner auf, trainieren Ihr Modell auf allen Rechnern und
akkumulieren Gradienten. Dies führt zu einer Reihe von Problemen.
168 | Kapitel 6: Modellentwicklung und Offline-Auswertung
14 Dipankar Das, Sasikanth Avancha, Dheevatsa Mudigere, Karthikeyan Vaidynathan, Srinivas Sridharan, Dhiraj
Kalamkar, Bharat Kaul, and Pradeep Dubey, "Distributed Deep Learning Using Synchronous Stochastic
Gradient Descent," arXiv, February 22, 2016, https://oreil.ly/ma8Y6.
15 Jianmin Chen, Xinghao Pan, Rajat Monga, Samy Bengio, and Rafal Jozefowicz, "Revisiting Distributed
Synchronous SGD", ICLR 2017, https://oreil.ly/dzVZ5; Matei Zaharia, Andy Konwinski, Anthony D. Joseph,
Randy Katz, and Ion Stoica, "Improving MapReduce Performance in Heterogeneous Environments," 8th
USENIX Symposium on Operating Systems Design and Implementation, https://oreil.ly/FWswd; Aaron Har-
lap, Henggang Cui, Wei Dai, Jinliang Wei, Gregory R. Ganger, Phillip B. Gibbons, Garth A. Gibson, und Eric
P. Xing, "Addressing the Straggler Problem for Iterative Convergent Parallel ML" (SoCC '16, Santa Clara, CA,
October 5-7, 2016), https://oreil.ly/wZgOO.
16 Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Marc'aurelio Ranzato, et al,
"Large Scale Distributed Deep Networks", NIPS 2012, https://oreil.ly/EWPun.
17 Jim Dowling, "Distributed TensorFlow," O'Reilly Media, December 19, 2017, https://oreil.ly/VYlOP.

Ein schwieriges Problem ist die genaue und effektive Akkumulation von Gradienten aus
verschiedenen Maschinen. Da jede Maschine ihren eigenen Gradienten erzeugt, kann Ihr Modell, wenn es wartet
wenn Ihr Modell darauf wartet, dass alle Maschinen einen Lauf beenden - synchroner stochastischer Gradientenabstieg (SGD) -, führen
führen Nachzügler dazu, dass das gesamte System langsamer wird, wodurch Zeit und Ressourcen verschwendet werden.^14
Das Problem der Nachzügler wächst mit der Anzahl der Maschinen, denn je mehr Arbeiter, desto
desto wahrscheinlicher ist es, dass mindestens ein Arbeiter in einer bestimmten Iteration ungewöhnlich langsam arbeitet.
Es gibt jedoch viele Algorithmen, die dieses Problem wirksam angehen.^15
Wenn Ihr Modell die Gewichtung unter Verwendung des Gradienten von jeder Maschine separat aktualisiert -
asynchrone SGD -, kann Gradientenstalinität zu einem Problem werden, weil die Gradienten von einer Maschine
Gradienten von einer Maschine die Gewichte verändert haben, bevor die Gradienten von einer
einer anderen Maschine eingetroffen sind.^16
Der Unterschied zwischen synchroner SGD und asynchroner SGD wird in
Abbildung 6-6.
Abbildung 6-6. Synchrone SGD gegenüber asynchroner SGD für Datenparallelität. Quelle:
Adaptiert von einem Bild von Jim Dowling^17
Modellentwicklung und Training | 169
18 Feng Niu, Benjamin Recht, Christopher Ré, und Stephen J. Wright, "Hogwild! A Lock-Free Approach to
Parallelizing Stochastic Gradient Descent", 2011, https://oreil.ly/sAEbv.
19 Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, et al. "Language Models Are Few-Shot Learners," arXiv, May 28, 2020, https://oreil.ly/qjg2S.
20 Sam McCandlish, Jared Kaplan, Dario Amodei, und OpenAI Dota Team, "An Empirical Model of Large-
Batch Training," arXiv, December 14, 2018, https://oreil.ly/mcjbV; Christopher J. Shallue, Jaehoon Lee,
Joseph Antognini, Jascha Sohl-Dickstein, Roy Frostig, and George E. Dahl, "Measuring the Effects of
Data Parallelism on Neural Network Training," Journal of Machine Learning Research 20 (2019): 1-49,
https://oreil.ly/YAEOM.

Theoretisch konvergiert die asynchrone SGD, erfordert aber mehr Schritte als die synchrone
SGD. In der Praxis jedoch, wenn die Anzahl der Gewichte groß ist, sind Gradientenaktualisierungen
spärlich, d. h. die meisten Gradientenaktualisierungen ändern nur kleine Bruchteile der Parameter
der Parameter, und es ist weniger wahrscheinlich, dass zwei Gradientenaktualisierungen von verschiedenen Maschinen
die gleichen Gewichte ändern. Wenn Gradientenaktualisierungen spärlich sind, ist die Staleness der Gradienten
weniger ein Problem und das Modell konvergiert in ähnlicher Weise sowohl für synchrone
und asynchronen SGD.^18
Ein weiteres Problem besteht darin, dass die Verteilung des Modells auf mehrere Rechner dazu führen kann, dass die
Stapelgröße sehr groß sein kann. Wenn eine Maschine eine Losgröße von 1.000 verarbeitet, dann verarbeiten 1.000
Maschinen eine Stapelgröße von 1 Mio. (OpenAIs GPT-3 175B verwendet eine Stapelgröße von
3,2M im Jahr 2020).^19 Um die Berechnung zu vereinfachen: Wenn das Training einer Epoche auf einer Maschine
1M Schritte dauert, könnte das Training auf 1.000 Maschinen nur 1.000 Schritte erfordern. Ein intuitiver
Ansatz ist es, die Lernrate zu erhöhen, um bei jedem Schritt mehr zu lernen, aber
aber wir können die Lernrate auch nicht zu groß wählen, da dies zu einer instabilen Konvergenz führt. In der
In der Praxis führt eine Erhöhung der Stapelgröße ab einem bestimmten Punkt zu abnehmenden Erträgen.^20
Zu guter Letzt verbraucht die Hauptarbeiterin bei der gleichen Modellkonfiguration manchmal viel mehr
mehr Ressourcen als andere Arbeiter. Wenn das der Fall ist, muss man, um alle Maschinen optimal zu nutzen
Maschinen zu nutzen, müssen Sie einen Weg finden, die Arbeitslast auf die einzelnen Maschinen aufzuteilen.
Der einfachste, aber nicht der effektivste Weg ist die Verwendung einer kleineren Stapelgröße auf dem
Hauptarbeiter und eine größere Stapelgröße auf den anderen Arbeitern zu verwenden.
Modellparallelität
Bei der Datenparallelität hat jeder Arbeiter seine eigene Kopie des gesamten Modells und führt
alle für seine Kopie des Modells erforderlichen Berechnungen aus. Modellparallelität bedeutet, dass
verschiedene Komponenten Ihres Modells auf verschiedenen Rechnern trainiert werden, wie in
in Abbildung 6-7. So übernimmt beispielsweise Rechner 0 die Berechnungen für die ersten beiden
Schichten, während Maschine 1 die nächsten beiden Schichten bearbeitet, oder einige Maschinen können den
Vorwärtspass, während mehrere andere den Rückwärtspass bearbeiten.
170 | Kapitel 6: Modellentwicklung und Offline-Auswertung
21 Jure Leskovec, Mining Massive Datasets course, Stanford, lecture 13, 2020, https://oreil.ly/gZcja.

Abbildung 6-7. Datenparallelität und Modellparallelität. Quelle: Angepasst an ein Bild von
Jure Leskovec^21
Modellparallelität kann irreführend sein, denn in einigen Fällen bedeutet Parallelität nicht, dass
dass verschiedene Teile des Modells auf verschiedenen Rechnern parallel ausgeführt werden. Unter
Wenn es sich bei Ihrem Modell beispielsweise um eine riesige Matrix handelt und die Matrix auf zwei Rechnern in zwei Hälften aufgeteilt wird
zwei Maschinen aufgeteilt ist, dann werden diese beiden Hälften möglicherweise parallel ausgeführt. Wenn jedoch Ihr
Modell ein neuronales Netz ist und Sie die erste Schicht auf Maschine 1 und die zweite
Schicht auf Maschine 2 legen und die Schicht 2 die Ausgaben von Schicht 1 benötigt, um ausgeführt zu werden, dann muss Maschine 2
warten, bis Maschine 1 fertig ist, um ausgeführt zu werden.
Pipeline-Parallelität ist eine clevere Technik, mit der verschiedene Komponenten eines Modells
auf verschiedenen Rechnern paralleler ablaufen zu lassen. Hierfür gibt es mehrere Varianten,
Der Grundgedanke besteht jedoch darin, die Berechnung auf jeder Maschine in mehrere Teile aufzuteilen.
Wenn Maschine 1 den ersten Teil ihrer Berechnung abschließt, übergibt sie das Ergebnis an
Maschine 2 weiter, die dann mit dem zweiten Teil fortfährt, und so weiter. Maschine 2 kann nun
ersten Teil durchführen, während Maschine 1 ihre Berechnung im zweiten Teil ausführt.
zweiten Teil ausführt.
Modellentwicklung und Training | 171
22 Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Mia Xu Chen, Dehao Chen, HyoukJoong Lee, et
al. "GPipe: Easy Scaling with Micro-Batch Pipeline Parallelism," arXiv, 25. Juli 2019, https://oreil.ly/wehkx.

Um dies zu verdeutlichen, stellen Sie sich vor, Sie haben vier verschiedene Maschinen und die erste,
zweite, dritte und vierte Lage befinden sich auf Maschine 1, 2, 3 bzw. 4. Mit
Pipeline-Parallelität wird jedes Minilos in vier Mikro-Lose aufgeteilt. Maschine
1 berechnet die erste Schicht auf dem ersten Mikrostapel, dann berechnet Maschine 2 die
Maschine 2 berechnet die zweite Schicht auf der Grundlage der Ergebnisse von Maschine 1, während Maschine 1 die erste Schicht der zweiten Mikrocharge berechnet.
zweiten Mikrostapel berechnet, und so weiter. Abbildung 6-8 zeigt, wie die Pipeline-Parallelität aussieht
auf vier Rechnern aussieht; jeder Rechner führt sowohl den Vorwärtsdurchlauf als auch den Rückwärtsdurchlauf
für eine Komponente eines neuronalen Netzes aus.
Abbildung 6-8. Pipeline-Parallelität für ein neuronales Netzwerk auf vier Rechnern; jeder Rechner
jeder Rechner führt sowohl den Vorwärtsdurchlauf (F) als auch den Rückwärtsdurchlauf (B) für eine Komponente des
neuronalen Netzes. Quelle: Adaptiert von einem Bild von Huang et al.^22
Modellparallelität und Datenparallelität schließen sich nicht gegenseitig aus. Viele Unternehmen
nutzen beide Methoden zur besseren Ausnutzung ihrer Hardware, auch wenn die Einrichtung für beide
beide Methoden zu verwenden, einen erheblichen technischen Aufwand erfordern kann.
AutoML
Es gibt einen Witz, dass ein guter ML-Forscher jemand ist, der sich selbst automatisiert
und einen KI-Algorithmus entwickelt, der intelligent genug ist, sich selbst zu entwickeln. Das war lustig
bis zum TensorFlow Dev Summit 2018, wo Jeff Dean auf der Bühne erklärte
dass Google beabsichtigt, ML-Expertise durch 100-mal mehr Rechenleistung zu ersetzen.
Rechenleistung zu ersetzen und AutoML zur Begeisterung und zum Entsetzen der Community einzuführen. Stattdessen
eine Gruppe von 100 ML-Forschern/Ingenieuren zu bezahlen, die mit verschiedenen Modellen herumprobieren und
schließlich ein suboptimales Modell auszuwählen, warum nicht das Geld für die Suche nach dem
nach dem optimalen Modell zu suchen? Ein Bildschirmfoto aus der Aufzeichnung der Veranstaltung ist in
Abbildung 6-9.
172 | Kapitel 6: Modellentwicklung und Offline-Evaluierung
23 Wir werden die Quantisierung in Kapitel 7 behandeln.

Abbildung 6-9. Jeff Dean bei der Vorstellung von Googles AutoML auf dem TensorFlow Dev Summit 2018
Soft AutoML: Hyperparameter-Abstimmung
AutoML bezieht sich auf die Automatisierung des Prozesses der Suche nach ML-Algorithmen zur Lösung realer Probleme.
Welt Probleme zu lösen. Eine milde Form und die beliebteste Form von AutoML in der Produktion
tion ist die Abstimmung von Hyperparametern. Ein Hyperparameter ist ein Parameter, der von den Benutzern
dessen Wert zur Steuerung des Lernprozesses verwendet wird, z. B. Lernrate, Stapelgröße,
Anzahl der versteckten Schichten, Anzahl der versteckten Einheiten, Dropout-Wahrscheinlichkeit, β 1 und β 2 im
Adam-Optimierer, usw. Auch die Quantisierung - z.B. ob 32 Bit, 16 Bit oder 8 Bit verwendet werden sollen
Bits zur Darstellung einer Zahl oder einer Mischung dieser Darstellungen - kann als ein
abzustimmenden Hyperparameter betrachtet werden.^23
Mit verschiedenen Sätzen von Hyperparametern kann ein und dasselbe Modell bei ein und demselben Datensatz drastisch unterschiedliche Leistungen erbringen.
Leistungen auf demselben Datensatz ergeben. Melis et al. zeigten in ihrem 2018 erschienenen Papier "On the
State of the Art of Evaluation in Neural Language Models", dass schwächere Modelle mit
gut abgestimmten Hyperparametern stärkere, ausgefallenere Modelle übertreffen können. Das Ziel der
Hyperparameter-Tuning ist es, den optimalen Satz von Hyperparametern für ein bestimmtes
Modell innerhalb eines Suchraums zu finden - die Leistung jedes Satzes wird anhand einer Validierungsmenge
Satz.
Modellentwicklung und Training | 173
24 GSD ist eine gut dokumentierte Technik. Siehe "How Do People Come Up With All These Crazy Deep Learning
Architectures?", Reddit, https://oreil.ly/5vEsH; "Debate About Science at Organizations like Google Brain/
FAIR/DeepMind," Reddit, https://oreil.ly/2K77r; "Grad Student Descent," Science Dryad, January 25, 2014,
https://oreil.ly/dIR9r; und Guy Zyskind (@GuyZys), "Grad Student Descent: the preferred #nonlinear #optimi-
zation technique #machinelearning," Twitter, April 27, 2015, https://oreil.ly/SW1or.
25 auto-sklearn 2.0 bietet auch grundlegende Modellauswahlkapazitäten.
26 Unser Team bei NVIDIA hat Milano entwickelt, ein Framework-unabhängiges Tool für die automatische Abstimmung von Hyperparametern
unter Verwendung der Zufallssuche.
27 Eine gängige Praxis, die ich beobachtet habe, besteht darin, mit einer groben bis feinen Zufallssuche zu beginnen und dann mit
Bayes'sche oder Rastersuche zu experimentieren, sobald der Suchraum erheblich reduziert wurde.

Trotz des Wissens um ihre Bedeutung ignorieren viele immer noch systematische Ansätze zur Hyper-
Parameterabstimmung zugunsten eines manuellen, gefühlsbetonten Ansatzes. Die beliebteste Methode ist
wohl das Graduate Student Descent (GSD), eine Technik, bei der ein Doktorand
mit den Hyperparametern herumspielt, bis das Modell funktioniert.^24
Immer mehr Leute übernehmen jedoch die Abstimmung der Hyperparameter als Teil ihrer
Standard-Pipelines. Beliebte ML-Frameworks verfügen entweder über integrierte Dienstprogramme oder
Dienstprogramme von Drittanbietern für die Abstimmung von Hyperparametern - zum Beispiel scikit-learn mit
auto-sklearn,^25 TensorFlow mit Keras Tuner, und Ray mit Tune. Beliebte Methoden
zur Abstimmung von Hyperparametern sind die zufällige Suche,^26 die Rastersuche und die Bayes'sche Opti-
mization.^27 Das Buch AutoML: Methods, Systems, Challenges von der AutoML-Gruppe an der
der Universität Freiburg widmet sein erstes Kapitel (das Sie online kostenlos lesen können) der
kostenlos lesen können) der Hyperparameter-Optimierung.
Bei der Abstimmung von Hyperparametern ist zu beachten, dass die Leistung eines Modells möglicherweise
die Leistung eines Modells empfindlicher auf die Änderung eines Hyperparameters reagieren kann als auf die eines anderen, so dass
empfindliche Hyperparameter sollten daher sorgfältiger abgestimmt werden.
Es ist von entscheidender Bedeutung, dass Sie niemals Ihren Test-Split zur Abstimmung von Hyperparametern verwenden.
Wählen Sie den besten Satz von Hyperparametern für ein Modell auf der Grundlage seiner
Leistung auf einem Validierungssplit, und berichten Sie dann die endgültige
Leistung des Modells für den Test-Split. Wenn Sie Ihren Test-Split zum Abstimmen von
Hyperparametern verwenden, riskieren Sie eine Überanpassung Ihres Modells an den Test-Split.
Hartes AutoML: Architektursuche und gelernter Optimierer
Einige Teams gehen bei der Abstimmung von Hyperparametern noch einen Schritt weiter: Was wäre, wenn wir andere Komponenten eines Modells oder das gesamte Modell behandeln?
Komponenten eines Modells oder das gesamte Modell als Hyperparameter behandeln. Die Größe einer Faltungs
Schicht oder das Vorhandensein oder Nichtvorhandensein einer Überspringungsschicht kann als Hyperparameter betrachtet werden.
Anstatt manuell eine Pooling-Schicht nach einer Faltungsschicht oder ReLu
(rectified linear unit) nach einer linearen Schicht zu setzen, geben Sie Ihrem Algorithmus diese Bausteine und
und überlassen es ihm, herauszufinden, wie er sie kombinieren kann. Dieser Bereich der Forschung ist als Architektur bekannt
174 | Kapitel 6: Modellentwicklung und Offline-Bewertung
28 Barret Zoph und Quoc V. Le, "Neural Architecture Search with Reinforcement Learning," arXiv, November
5, 2016, https://oreil.ly/FhsuQ; Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V. Le, "Regularized
Evolution for Image Classifier Architecture Search," AAAI 2019, https://oreil.ly/FWYjn.
29 Man kann den Suchraum kontinuierlich machen, um eine Differenzierung zu ermöglichen, aber die resultierende Architektur muss
in eine diskrete Architektur umgewandelt werden. Siehe "DARTS: Differentiable Architecture Search" (Liu et al. 2018).
30 Wir behandeln Lernverfahren und Optimierer ausführlicher im Abschnitt "Basic ML Reviews" im Buch auf dem
GitHub Repository.

Suche oder neuronale Architektursuche (NAS) für neuronale Netze, bei der nach der optimalen
optimalen Modellarchitektur.
Ein NAS-Setup besteht aus drei Komponenten:
Einem Suchraum
Definiert mögliche Modellarchitekturen, d. h. Bausteine, aus denen gewählt werden kann, und
Beschränkungen, wie sie kombiniert werden können.
Eine Strategie zur Leistungsabschätzung
Zur Bewertung der Leistung einer Kandidatenarchitektur, ohne dass jede
ohne jede Kandidatenarchitektur von Grund auf neu zu trainieren, bis Konvergenz erreicht ist. Wenn wir eine
großen Anzahl von Architekturkandidaten, z. B. 1.000, kann es kostspielig sein, alle von ihnen bis zur
Konvergenz kostspielig sein.
Eine Suchstrategie
Erkunden des Suchraums. Ein einfacher Ansatz ist die Zufallssuche - die zufällige
die zufällige Auswahl aus allen möglichen Konfigurationen, was unbeliebt ist, weil es
hibitisch teuer ist, selbst für NAS. Gängige Ansätze sind Verstärkungs
Lernen (Belohnung von Entscheidungen, die die Leistungsabschätzung verbessern) und
Evolution (Hinzufügen von Mutationen zu einer Architektur, Auswahl der leistungsstärksten
Architektur, Auswahl der leistungsstärksten, Hinzufügen von Mutationen zu dieser usw.).^28
Bei NAS ist der Suchraum diskret - die endgültige Architektur verwendet nur eine der
der verfügbaren Optionen für jede Schicht/Operation,^29 und Sie müssen den Satz von Bausteinen bereitstellen.
Bausteine bereitstellen. Die üblichen Bausteine sind verschiedene Faltungen unterschiedlicher Größe,
linear, verschiedene Aktivierungen, Pooling, Identität, Null, usw. Die Menge der Bausteine variiert
je nach Basisarchitektur, z. B. Faltungsneuronale Netze oder Transformatoren.
In einem typischen ML-Trainingsprozess haben Sie ein Modell und dann ein Lernverfahren,
ein Algorithmus, der dem Modell hilft, den Parametersatz zu finden, der eine bestimmte
Zielfunktion für einen gegebenen Datensatz minimiert. Das am häufigsten verwendete Lernverfahren für
neuronale Netze ist heute der Gradientenabstieg, der einen Optimierer nutzt, um festzulegen
wie die Gewichte eines Modells angesichts von Gradientenaktualisierungen aktualisiert werden sollen.^30 Beliebte Optimierer sind,
wie Sie wahrscheinlich bereits wissen, Adam, Momentum, SGD, usw. Theoretisch können Sie
Optimierer als Bausteine in NAS einbinden und nach einem suchen, der am besten funktioniert.
In der Praxis ist dies schwierig, da Optimierer empfindlich auf die Einstellung von
Modellentwicklung und Training | 175
31 Luke Metz, Niru Maheswaranathan, C. Daniel Freeman, Ben Poole, und Jascha Sohl-Dickstein, "Tasks,
Stabilität, Architektur und Rechenleistung: Training effektiverer gelernter Optimierer und deren Verwendung zum Training
Themselves," arXiv, September 23, 2020, https://oreil.ly/IH7eT.
32 Mingxing Tan und Quoc V. Le, "EfficientNet: Verbesserung der Genauigkeit und Effizienz durch AutoML und
Model Scaling," Google AI Blog, May 29, 2019, https://oreil.ly/gonEn.

ihre Hyperparameter, und die Standard-Hyperparameter funktionieren oft nicht gut über
Architekturen.
Dies führt zu einer spannenden Forschungsrichtung: Was wäre, wenn wir die Funktionen, die
die die Aktualisierungsregel spezifizieren, durch ein neuronales Netz ersetzen? Wie stark die Gewichte des Modells zu aktualisieren sind
Gewichte des Modells zu aktualisieren sind, wird von diesem neuronalen Netz berechnet. Dieser Ansatz führt zu gelernten
Optimierer, im Gegensatz zu manuell entworfenen Optimierern.
Da erlernte Optimierer neuronale Netze sind, müssen sie trainiert werden. Sie können
Sie können Ihren gelernten Optimierer mit demselben Datensatz trainieren, mit dem Sie auch den Rest Ihres neuronalen Netzes trainieren.
Netzwerks trainieren, aber das erfordert, dass Sie jedes Mal, wenn Sie eine Aufgabe haben, einen Optimierer trainieren.
Ein anderer Ansatz besteht darin, einen gelernten Optimierer einmal auf einem Satz vorhandener Aufgaben zu trainieren
-wobei der aggregierte Verlust für diese Aufgaben als Verlustfunktion und die bereits entwickelten
Optimierer als Lernregel zu verwenden und ihn danach für jede neue Aufgabe zu verwenden. Zum Beispiel,
Metz et al. konstruierten einen Satz von Tausenden von Aufgaben, um gelernte Optimierer zu trainieren. Ihr
erlernte Optimierer war in der Lage, sich sowohl auf neue Datensätze und Domänen als auch auf
neue Architekturen zu verallgemeinern.^31 Und das Schöne an diesem Ansatz ist, dass der gelernte Optimierer
Optimierer verwendet werden kann, um einen besser erlernten Optimierer zu trainieren, einen Algorithmus, der sich selbst verbessert.
Ob es sich nun um die Suche nach Architekturen oder um das Erlernen von Meta-Lernregeln handelt, die Kosten für das Training
Kosten sind so hoch, dass sich nur eine Handvoll Unternehmen auf der Welt
sie zu verfolgen. Dennoch ist es für alle, die an ML in der Produktion interessiert sind, wichtig
die Fortschritte bei AutoML zu kennen, und zwar aus zwei Gründen. Erstens können die resultierenden Architekturen
und gelernte Optimierer können ML-Algorithmen bei mehreren realen Aufgaben sofort
realen Aufgaben arbeiten, was sowohl beim Training als auch bei der Inferenz Zeit und Kosten für die Produktion spart.
inferenzieren. Beispielsweise übertreffen EfficientNets, eine Familie von Modellen, die vom AutoML-Team von Google
Team entwickelt wurde, übertrifft den Stand der Technik mit einer bis zu 10-fach besseren Effizienz.^32 Zweitens können sie
können sie viele reale Aufgaben lösen, die mit bestehenden Architekturen und
Architekturen und Optimierern unmöglich war.
176 | Kapitel 6: Modellentwicklung und Offline-Bewertung
33 Samantha Murphy, "The Evolution of Facebook News Feed", Mashable, 12. März 2013,
https://oreil.ly/1HMXh.
34 Iveta Ryšavá, "What Mark Zuckerberg's News Feed Looked Like in 2006," Newsfeed.org, January 14, 2016,
https://oreil.ly/XZT6Q.

Vier Phasen der ML-Modellentwicklung
Bevor wir zum Modelltraining übergehen, werfen wir einen Blick auf die vier Phasen der ML
Modellentwicklung. Wenn Sie sich entschieden haben, ML zu erforschen, hängt Ihre Strategie davon ab
in welcher Phase der ML-Einführung Sie sich befinden. Es gibt vier Phasen der Einführung von ML. Die
Lösungen aus einer Phase können als Grundlage für die Bewertung der Lösungen aus der
nächsten Phase:
Phase 1. Vor dem maschinellen Lernen
Wenn Sie zum ersten Mal versuchen, diese Art von Vorhersage mit dieser Art von Daten zu treffen
Daten zu machen, sollten Sie mit Nicht-ML-Lösungen beginnen. Ihr erster Versuch kann die einfachste
einfachste Heuristik sein. Um zum Beispiel vorherzusagen, welchen Buchstaben der Benutzer als nächstes tippen wird
tippen werden, können Sie die drei häufigsten englischen Buchstaben, "e", "t" und "a", anzeigen
und "a", wodurch die Genauigkeit bei 30 % liegen könnte.
Der Facebook-Newsfeed wurde 2006 ohne intelligente Algorithmen eingeführt.
Beiträge wurden in chronologischer Reihenfolge angezeigt, wie in Abbildung 6-10 dargestellt.^33 Erst
2011 begann Facebook damit, die Nachrichten, die Sie am meisten interessierten, ganz oben
oben im Feed angezeigt.
Abbildung 6-10. Facebook-Newsfeed um 2006. Quelle: Iveta Ryšavá^34
Modellentwicklung und Training | 177
35 Martin Zinkevich, "Rules of Machine Learning: Best Practices for ML Engineering", Google, 2019,
https://oreil.ly/YtEsN.
36 Wie oft Sie Ihre Modelle aktualisieren sollten, wird in Kapitel 9 eingehend erläutert.
37 Siehe den Abschnitt "Geschäfts- und ML-Ziele" auf Seite 26.

Laut Martin Zinkevich in seinem großartigen Buch "Rules of Machine Learning:
Best Practices for ML Engineering": "Wenn Sie glauben, dass maschinelles Lernen Ihnen einen
100 % bringen wird, dann wird eine Heuristik Sie zu 50 % ans Ziel bringen."^35 Sie könnten
Sie könnten sogar feststellen, dass Nicht-ML-Lösungen gut funktionieren und Sie ML noch nicht brauchen.
Phase 2. Die einfachsten Modelle für maschinelles Lernen
Für Ihr erstes ML-Modell sollten Sie mit einem einfachen Algorithmus beginnen, etwas
Algorithmus beginnen, der Ihnen einen Einblick in seine Funktionsweise gibt, damit Sie die Nützlichkeit Ihrer Problemstellung und Ihrer Daten überprüfen können.
Ihrer Problemstellung und Ihrer Daten zu überprüfen. Logistische Regression, gradient-boosted trees,
k-nearest neighbors können dafür hervorragend geeignet sein. Sie sind auch einfacher zu implementieren und
Implementierung und Bereitstellung, so dass Sie schnell einen Rahmen für die Datentechnik
über die Entwicklung bis hin zur Bereitstellung ein Framework aufzubauen, das Sie testen und mit dem Sie Vertrauen gewinnen können.
Phase 3. Optimieren einfacher Modelle
Sobald Sie Ihr ML-Framework eingerichtet haben, können Sie sich auf die Optimierung der
einfache ML-Modelle mit verschiedenen Zielfunktionen, Hyperparametersuche,
Feature-Engineering, mehr Daten und Ensembles.
Phase 4. Komplexe Modelle
Wenn Sie die Grenzen Ihrer einfachen Modelle erreicht haben und Ihr Anwendungsfall eine
eine deutliche Verbesserung des Modells erfordert, experimentieren Sie mit komplexeren Modellen.
Sie werden auch experimentieren wollen, um herauszufinden, wie schnell Ihr Modell in der
(z. B. wie oft es neu trainiert werden muss), damit Sie Ihre Infrastruktur so aufbauen können
Infrastruktur aufbauen können, um diesen Umschulungsbedarf zu unterstützen.^36
Modell-Offline-Bewertung
Eine häufige, aber recht schwierige Frage, auf die ich bei der Unterstützung von Unternehmen
Unternehmen bei ihren ML-Strategien helfe, ist: "Woher weiß ich, dass unsere ML-Modelle überhaupt gut sind?" In
einem Fall setzte ein Unternehmen ML ein, um Eindringlinge in 100 Überwachungsdrohnen zu erkennen, aber
aber es gab keine Möglichkeit zu messen, wie viele Eindringlinge das System nicht erkennen konnte, und
Sie konnten nicht entscheiden, ob ein ML-Algorithmus für ihre Bedürfnisse besser war als ein anderer.
Das Fehlen einer klaren Vorstellung davon, wie Sie Ihre ML-Systeme bewerten können, ist nicht unbedingt
ist nicht unbedingt ein Grund für das Scheitern Ihres ML-Projekts, aber es könnte es unmöglich machen, die beste Lösung für Ihren Bedarf zu finden
beste Lösung für Ihren Bedarf zu finden, und es wird schwieriger, Ihre Manager von der Einführung von
ML ZU ÜBERZEUGEN. Vielleicht sollten Sie mit dem Geschäftsteam zusammenarbeiten, um Metriken für die Modellbewertung zu entwickeln
zu entwickeln, die für das Geschäft Ihres Unternehmens relevanter sind.^37
178 | Kapitel 6: Modellentwicklung und Offline-Evaluierung
38 Fréchet-Inception-Distanz, eine gängige Metrik zur Messung der Qualität von synthetisierten Bildern. Je kleiner
der Wert ist, desto höher ist die vermeintliche Qualität.
39 Die Genauigkeit würde in diesem Fall etwa 0,80 betragen.
40 Schauen Sie sich den Abschnitt "Verwendung der richtigen Bewertungsmetrik" auf Seite 106 an, um sich über die Asymmetrie von F1 zu informieren.

Idealerweise sollten die Bewertungsmethoden sowohl in der Entwicklung als auch in der
Produktion gleich sein. In vielen Fällen ist dies jedoch nicht möglich, da während der Entwicklung
haben Sie die wahren Bezeichnungen, aber in der Produktion nicht.
Bei bestimmten Aufgaben ist es möglich, die Bezeichnungen in der Produktion auf der Grundlage von Benutzerfeedback abzuleiten oder zu approximieren.
wie im Abschnitt "Natürliche Bezeichnungen" auf Seite 91 beschrieben. Ein Beispiel,
für die Empfehlungsaufgabe ist es möglich, daraus zu schließen, ob eine Empfehlung gut ist
ob Benutzer darauf klicken. Dies ist jedoch mit vielen Verzerrungen verbunden.
Bei anderen Aufgaben kann es sein, dass Sie die Leistung Ihres Modells in der Produktion nicht direkt bewerten können
und müssen sich möglicherweise auf eine umfassende Überwachung verlassen, um Änderungen
und Ausfälle in der Leistung Ihres ML-Systems zu erkennen. Wir werden die Überwachung in Kapitel 8 behandeln.
Nach der Bereitstellung Ihres Modells müssen Sie Ihr Modell weiterhin überwachen und testen.
Modell in der Produktion. In diesem Abschnitt werden wir Methoden zur Bewertung der Leistung Ihres Modells
Leistung Ihres Modells zu bewerten, bevor es zum Einsatz kommt. Wir beginnen mit den Basislinien, gegen die wir unsere Modelle
unsere Modelle bewerten. Dann werden wir einige der üblichen Methoden zur Bewertung Ihres
Modells über die allgemeine Genauigkeit hinaus.
Grundlinien
Jemand erzählte mir einmal, dass sein neues generatives Modell einen FID-Wert von 10,3
auf ImageNet erreicht hat.^38 Ich hatte keine Ahnung, was diese Zahl bedeutet oder ob ihr Modell
für mein Problem nützlich sein würde.
Ein anderes Mal half ich einem Unternehmen bei der Implementierung eines Klassifikationsmodells, bei dem die
die positive Klasse in 90 % der Fälle auftritt. Ein ML-Ingenieur aus dem Team erzählte mir ganz
aufgeregt, dass sein ursprüngliches Modell einen F1-Wert von 0,90 erreichte. Ich fragte ihn, wie das
im Vergleich zum Zufall. Er hatte keine Ahnung. Es stellte sich heraus, dass bei seiner Aufgabe die
die Klasse POSITIV 90 % der Beschriftungen ausmacht, wenn sein Modell in 90 % der Fälle zufällig die
Wenn sein Modell in 90 % der Fälle die positive Klasse ausgibt, würde sein F1-Ergebnis ebenfalls bei 0,90 liegen.^39 Sein Modell
könnte genauso gut zufällige Vorhersagen machen.^40
Modell-Offline-Bewertung | 179
Bewertungskennzahlen allein sind wenig aussagekräftig. Wenn Sie Ihr Modell bewerten, ist es
ist es unerlässlich, die Basislinie zu kennen, anhand derer Sie das Modell bewerten. Die genaue Basislinie sollte
von einem Anwendungsfall zum anderen variieren, aber hier sind die fünf Grundlinien, die nützlich sein können
über Anwendungsfälle hinweg:

Zufällige Basislinie
Wenn unser Modell nur zufällige Vorhersagen macht, wie hoch ist dann die erwartete Leistung? Die
Vorhersagen werden nach dem Zufallsprinzip erzeugt und folgen einer bestimmten Verteilung, die
die Gleichverteilung oder die Etikettenverteilung der Aufgabe sein kann.
Betrachten wir zum Beispiel eine Aufgabe mit zwei Labels, NEGATIVE, das in 90 % der Fälle erscheint, und POSITIVE, das in 90 % der Fälle erscheint.
der Zeit und POSITIV, das 10 % der Zeit auftritt. Tabelle 6-2 zeigt die F1
und die Genauigkeitswerte der Basismodelle, die Vorhersagen nach dem Zufallsprinzip treffen. Wie auch immer,
um zu sehen, wie schwierig es für die meisten Menschen ist, ein Gespür für diese Werte zu haben
für diese Werte zu haben, versuchen Sie, diese Zahlen im Kopf zu berechnen, bevor Sie sich die Tabelle
der Tabelle.

Tabelle 6-2. F1- und Genauigkeitswerte eines Basismodells, das nach dem Zufallsprinzip vorhersagt
Zufallsverteilung Bedeutung F1 Genauigkeit
Gleichmäßige Zufallsverteilung Vorhersage jedes Labels mit gleicher Wahrscheinlichkeit (50%) 0,167 0,5
Verteilung des Labels der Aufgabe Vorhersage von NEGATIV in 90% der Fälle und POSITIV in 10% der Fälle 0,1 0,82
Einfache Heuristik
Vergessen Sie ML. Wenn Sie nur Vorhersagen auf der Grundlage einfacher Heuristiken machen, welche Leistung
Leistung würden Sie erwarten? Wenn Sie zum Beispiel ein Ranking-System erstellen wollen, um
um Elemente im Newsfeed eines Nutzers zu bewerten, mit dem Ziel, dass der Nutzer mehr Zeit im Newsfeed verbringt.
Zeit im Newsfeed verbringt, wie viel Zeit würde ein Nutzer verbringen, wenn Sie einfach alle
Artikel in umgekehrter chronologischer Reihenfolge ordnen und den neuesten Artikel zuerst anzeigen?

Null-Regel-Basislinie
Die Null-Regel-Baseline ist ein Sonderfall der einfachen heuristischen Baseline, wenn Ihr
Basismodell immer die am häufigsten vorkommende Klasse vorhersagt.
Wenn es zum Beispiel darum geht, die App zu empfehlen, die ein Nutzer mit hoher Wahrscheinlichkeit als nächstes auf seinem Handy nutzen wird
wird, wäre das einfachste Modell die Empfehlung der am häufigsten
häufig verwendete App empfehlen. Wenn diese einfache Heuristik die nächste App genau vorhersagen kann
70 % der Zeit die nächste App vorhersagen kann, muss ein von Ihnen entwickeltes Modell diese Leistung deutlich übertreffen, um die zusätzliche Komplexität zu rechtfertigen.
die zusätzliche Komplexität zu rechtfertigen.

Menschliche Ausgangssituation
In vielen Fällen besteht das Ziel von ML darin, Aufgaben zu automatisieren, die sonst von Menschen erledigt worden wären.
sonst von Menschen erledigt worden wäre. Daher ist es nützlich zu wissen, wie Ihr Modell im Vergleich zu
menschlichen Experten. Wenn Sie zum Beispiel an einem selbstfahrenden System arbeiten, ist es wichtig
den Fortschritt Ihres Systems im Vergleich zu menschlichen Fahrern zu messen, denn sonst

180 | Kapitel 6: Modellentwicklung und Offline-Auswertung

können Sie Ihre Nutzer vielleicht nie davon überzeugen, diesem System zu vertrauen. Selbst wenn Ihr
System nicht dazu gedacht ist, menschliche Experten zu ersetzen, sondern sie nur bei der Verbesserung
ihre Produktivität zu verbessern, ist es dennoch wichtig zu wissen, in welchen Szenarien dieses System
für den Menschen nützlich wäre.
Bestehende Lösungen
In vielen Fällen sind ML-Systeme darauf ausgelegt, bestehende Lösungen zu ersetzen, die
Dabei kann es sich um Geschäftslogik mit vielen if/else-Anweisungen oder um Lösungen von Drittanbietern handeln.
Es ist wichtig, dass Sie Ihr neues Modell mit diesen bestehenden Lösungen vergleichen. Ihr ML
Modell muss nicht immer besser sein als bestehende Lösungen, um nützlich zu sein. A
Modell, dessen Leistung ein wenig schlechter ist, kann trotzdem nützlich sein, wenn es viel
einfacher oder billiger zu verwenden ist.

Bei der Bewertung eines Modells ist es wichtig, zwischen "einem guten System" und
"einem nützlichen System". Ein gutes System ist nicht unbedingt nützlich, und ein schlechtes System ist nicht notwendigerweise
nicht unbedingt nutzlos. Ein selbstfahrendes Fahrzeug kann gut sein, wenn es eine deutliche Verbesserung
gegenüber früheren selbstfahrenden Systemen ist, aber es ist vielleicht nicht nützlich, wenn es nicht
mindestens so gut wie ein menschlicher Fahrer. In manchen Fällen, selbst wenn ein ML-System besser fährt als ein
besser fährt als ein durchschnittlicher Mensch, kann es sein, dass die Menschen ihm trotzdem nicht trauen, so dass es nicht nützlich ist. Andererseits
Andererseits kann ein System, das vorhersagt, welches Wort ein Nutzer als nächstes in sein Telefon tippen wird
tippen wird, könnte als schlecht angesehen werden, wenn es viel schlechter ist als ein Muttersprachler. Es könnte jedoch
dennoch nützlich sein, wenn die Vorhersagen den Nutzern dabei helfen können, manchmal schneller zu tippen.

Bewertungsmethoden
Im akademischen Umfeld neigt man bei der Bewertung von ML-Modellen dazu, sich auf ihre
Leistungsmetriken. In der Produktion wollen wir jedoch auch, dass unsere Modelle robust sind,
fair, kalibriert und insgesamt sinnvoll sind. Wir werden einige Bewertungsmethoden vorstellen, die
die bei der Messung dieser Eigenschaften eines Modells helfen.

Störaussendungstests

Eine Gruppe meiner Studenten wollte eine App entwickeln, die anhand des Hustens einer Person vorhersagt, ob sie
COVID-19 an ihrem Husten erkennt. Ihr bestes Modell funktionierte hervorragend mit den Trainingsdaten,
die aus zwei Sekunden langen Hustensegmenten bestanden, die von Krankenhäusern gesammelt wurden. Allerdings,
Als sie es jedoch bei tatsächlichen Benutzern einsetzten, waren die Vorhersagen dieses Modells nahezu zufällig.

Einer der Gründe dafür ist, dass der Husten der tatsächlichen Nutzer im Vergleich zum
den in Krankenhäusern gesammelten Husten. Die Aufnahmen der Nutzer können Hintergrundmusik
oder Gespräche in der Nähe. Die Mikrofone, die sie verwenden, sind von unterschiedlicher Qualität. Sie beginnen möglicherweise
mit der Aufzeichnung ihres Hustens beginnen, sobald die Aufzeichnung aktiviert ist, oder einen Bruchteil
Sekunde.

Idealerweise sollten die Eingaben, die Sie für die Entwicklung Ihres Modells verwenden, den Eingaben ähnlich sein, mit denen Ihr
mit denen Ihr Modell in der Produktion arbeiten muss, aber das ist in vielen Fällen nicht möglich. Diese

Modell Offline-Auswertung | 181
41 Andere Beispiele für verrauschte Daten sind Bilder mit unterschiedlicher Beleuchtung oder Texte mit versehentlichen Tippfehlern oder absichtlichen
oder absichtliche Textänderungen, wie z. B. das Tippen von "long" als "loooooong".
42 Khristopher J. Brooks, "Disparity in Home Lending Costs Minorities Millions, Researchers Find," CBS News,
November 15, 2019, https://oreil.ly/TMPVl.
43 Es könnte auch gesetzlich vorgeschrieben sein, sensible Informationen aus dem Modelltrainingsprozess auszuschließen.

Dies gilt vor allem dann, wenn die Datenerfassung teuer oder schwierig ist und die besten verfügbaren
Daten, auf die Sie für das Training zugreifen können, immer noch sehr unterschiedlich zu Ihren realen Daten sind.
Die Eingaben, mit denen Ihre Modelle in der Produktion arbeiten müssen, sind oft verrauscht im Vergleich
^41 Das Modell, das bei den Trainingsdaten am besten abschneidet, ist nicht
nicht unbedingt das Modell, das bei verrauschten Daten am besten abschneidet.
Um ein Gefühl dafür zu bekommen, wie gut Ihr Modell bei verrauschten Daten abschneiden könnte, können Sie
können Sie kleine Änderungen an Ihren Test-Splits vornehmen, um zu sehen, wie sich diese Änderungen auf die Leistung Ihres Modells
Leistung auswirken. Für die Aufgabe, anhand des Hustens einer Person vorherzusagen, ob sie COVID-19 hat
Husten hat, könnten Sie zufällig Hintergrundgeräusche hinzufügen oder die Testclips zufällig
Clips zufällig abschneiden, um die Varianz in den Aufnahmen Ihrer Benutzer zu simulieren. Sie sollten das Modell wählen
Modell wählen, das mit den gestörten Daten am besten funktioniert, anstatt das Modell, das mit den
sauberen Daten.
Je empfindlicher Ihr Modell auf Rauschen reagiert, desto schwieriger wird es, es aufrechtzuerhalten,
denn wenn sich das Verhalten Ihrer Nutzer nur geringfügig ändert, z. B. wenn sie ihr Handy wechseln,
kann sich die Leistung Ihres Modells verschlechtern. Außerdem macht es Ihr Modell anfällig für
gegnerische Angriffe.
Invarianz-Tests
Eine Berkeley-Studie ergab, dass zwischen 2008 und 2015 1,3 Millionen kreditwürdige Schwarze
und Latino-Antragsteller ihre Hypothekenanträge aufgrund ihrer Rasse abgelehnt
Rasse abgelehnt wurden.^42 Als die Forscher die Einkommens- und Kreditwürdigkeitswerte der abgelehnten
der abgelehnten Anträge verwendeten, aber die Merkmale zur Identifizierung der Rasse löschten, wurden die Anträge angenommen.
Bestimmte Änderungen an den Inputs sollten nicht zu Änderungen am Output führen. Im Fall der Vor
Im Fall der Vorzession sollten sich Änderungen der Rasseninformationen nicht auf das Ergebnis der Hypothek auswirken.
Ebenso sollten sich Änderungen an den Namen der Bewerber nicht auf die Ergebnisse der Lebenslaufprüfung auswirken
noch sollte das Geschlecht eines Bewerbers Auswirkungen auf die Höhe der Vergütung haben. Wenn dies der Fall ist,
gibt es Verzerrungen in Ihrem Modell, die es unbrauchbar machen könnten, egal wie gut
seine Leistung ist.
Um diese Verzerrungen zu vermeiden, besteht eine Lösung darin, das gleiche Verfahren anzuwenden, mit dem die Berkeley
Berkeley-Forscher geholfen hat, die Verzerrungen zu entdecken: Man lässt die Eingaben gleich, ändert aber die empfindlichen
Informationen, um zu sehen, ob sich die Ergebnisse ändern. Besser noch, man schließt die sensiblen
Informationen von vornherein aus den Merkmalen ausschließen, die zum Trainieren des Modells verwendet werden.^43
182 | Kapitel 6: Modellentwicklung und Offline-Evaluierung
44 Weitere Informationen zu kalibrierten Empfehlungen finden Sie in der Veröffentlichung "Calibrated Recommendations"
von Harald Steck aus dem Jahr 2018, basierend auf seiner Arbeit bei Netflix.

Direktionale Erwartungstests
Bestimmte Änderungen an den Inputs sollten jedoch vorhersehbare Änderungen an den Outputs bewirken.
Bei der Entwicklung eines Modells zur Vorhersage von Immobilienpreisen sollten zum Beispiel alle
Merkmale gleich bleiben, aber die Grundstücksgröße erhöht wird, sollte der vorhergesagte Preis nicht sinken,
und die Verringerung der Quadratmeterzahl sollte ihn nicht erhöhen. Wenn sich die Ergebnisse in die
in die entgegengesetzte Richtung, lernt Ihr Modell möglicherweise nicht das Richtige, und
Sie müssen es weiter untersuchen, bevor Sie es einsetzen.
Modellkalibrierung
Die Modellkalibrierung ist ein subtiles, aber wichtiges Konzept, das es zu verstehen gilt. Stellen Sie sich vor, dass jemand
eine Vorhersage macht, dass etwas mit einer Wahrscheinlichkeit von 70 % eintreten wird. Was diese
Vorhersage bedeutet, dass das vorhergesagte Ergebnis in 70 % der Fälle, in denen diese Vorhersage getroffen wurde
Ergebnis in 70 % der Fälle mit dem tatsächlichen Ergebnis übereinstimmt. Wenn ein Modell vorhersagt, dass Mannschaft
A die Mannschaft B mit einer Wahrscheinlichkeit von 70 % schlagen wird, und von den 1.000 Spielen dieser beiden
Mannschaft A nur in 60 % der Fälle gewinnt, dann ist dieses Modell nicht
nicht kalibriert ist. Ein kalibriertes Modell sollte vorhersagen, dass Mannschaft A mit einer Wahrscheinlichkeit von 60 % gewinnt.
Wahrscheinlichkeit gewinnt.
Die Modellkalibrierung wird von ML-Praktikern oft übersehen, aber sie ist eine der wichtigsten
wichtigsten Eigenschaften eines jeden Vorhersagesystems. Um es mit den Worten von Nate Silver in seinem Buch The
Signal and the Noise" zu zitieren, ist die Kalibrierung "einer der wichtigsten Tests für eine Vorhersage.
Ich würde sogar behaupten, dass sie die wichtigste ist".
Wir werden anhand von zwei Beispielen zeigen, warum die Modellkalibrierung wichtig ist. Erstens,
die Aufgabe, ein Empfehlungssystem zu entwickeln, das den Benutzern empfiehlt, welche Filme
Benutzer wahrscheinlich als nächstes sehen werden. Angenommen, Benutzer A sieht sich zu 80 % Liebesfilme
und 20 % der Zeit Komödien. Wenn Ihr Empfehlungssystem genau die Filme anzeigt
A am ehesten sehen wird, werden die Empfehlungen nur aus Liebesfilmen bestehen
denn die Wahrscheinlichkeit, dass A sich Liebesfilme ansieht, ist viel größer als bei allen anderen Filmtypen. Sie
System, dessen Empfehlungen repräsentativ für die tatsächlichen Sehgewohnheiten
tatsächlichen Sehgewohnheiten der Benutzer repräsentativ sind. In diesem Fall sollten sie zu 80 % aus Liebesfilmen und
20% Komödien bestehen.^44
Betrachten wir zweitens die Aufgabe, ein Modell zu erstellen, das vorhersagt, wie wahrscheinlich es ist, dass ein Nutzer
auf eine Anzeige klicken wird. Der Einfachheit halber stellen wir uns vor, dass es nur zwei Anzeigen gibt,
Anzeige A und Anzeige B. Ihr Modell sagt voraus, dass dieser Nutzer mit einer Wahrscheinlichkeit von 10 % auf Anzeige A
Wahrscheinlichkeit auf Anzeige A und mit einer Wahrscheinlichkeit von 8 % auf Anzeige B klicken wird. Sie brauchen Ihr Modell nicht zu
kalibriert sein, um Anzeige A vor Anzeige B zu platzieren. Wenn Sie jedoch vorhersagen möchten, wie viele Klicks
Ihre Anzeigen erhalten werden, muss Ihr Modell kalibriert sein. Wenn Ihr Modell vorhersagt, dass
ein Nutzer mit einer Wahrscheinlichkeit von 10 % auf Anzeige A klickt, aber in Wirklichkeit wird die Anzeige nur angeklickt
Modell Offline-Bewertung | 183
5 % der Zeit angeklickt wird, liegt die geschätzte Anzahl der Klicks weit daneben. Wenn Sie ein anderes
Modell zur Verfügung, das dieselbe Bewertung liefert, aber besser kalibriert ist, sollten Sie
das besser kalibrierte Modell.

Um die Kalibrierung eines Modells zu messen, ist eine einfache Methode das Zählen: Sie zählen die Anzahl
Sie zählen, wie oft Ihr Modell die Wahrscheinlichkeit X und die Häufigkeit Y des Eintreffens dieser Vorhersage ausgibt.
und tragen X gegen Y auf. Das Diagramm für ein perfekt kalibriertes Modell
X gleich Y an allen Datenpunkten. In scikit-learn können Sie die Kalibrierungskurve
eines binären Klassifikators mit der Methode sklearn.calibration.calibration_curve zeichnen, wie
in Abbildung 6-11 gezeigt.

Abbildung 6-11. Die Kalibrierungskurven der verschiedenen Modelle für eine Spielzeugaufgabe. Das logistische
Regressionsmodell ist das am besten kalibrierte Modell, da es den logistischen Verlust direkt optimiert.
Quelle: scikit-learn

Eine gängige Methode zur Kalibrierung Ihrer Modelle ist die Platt-Skalierung, die
in scikit-learn mit sklearn.calibration.CalibratedClassifierCV. Eine weitere gute
Open-Source-Implementierung von Geoff Pleiss kann auf GitHub gefunden werden. Für Leser
die mehr über die Bedeutung der Modellkalibrierung und die Kalibrierung
neuronale Netze kalibrieren, haben Lee Richardson und Taylor Pospisil einen ausgezeichneten Blog
Beitrag, der auf ihrer Arbeit bei Google basiert.

184 | Kapitel 6: Modellentwicklung und Offline-Auswertung

Vertrauensmessung

Die Konfidenzmessung kann als eine Möglichkeit betrachtet werden, über die Nützlichkeits
Schwellenwert für jede einzelne Vorhersage. Die unterschiedslose Anzeige aller Vorhersagen eines Modells
Vorhersagen eines Modells zu zeigen, selbst die Vorhersagen, bei denen das Modell unsicher ist, kann bestenfalls
bestenfalls ärgerlich sein und dazu führen, dass die Nutzer das Vertrauen in das System verlieren, wie bei einem
Aktivitätserkennungssystem auf Ihrer Smartwatch, das denkt, Sie würden laufen, obwohl Sie
obwohl man nur ein bisschen schnell geht. Im schlimmsten Fall kann es zu katastrophalen Folgen führen, wie z. B. ein
Algorithmus der vorausschauenden Polizeiarbeit, der eine unschuldige Person als potenziellen Kriminellen einstuft.

Wenn Sie nur die Vorhersagen zeigen wollen, bei denen Ihr Modell sicher ist, wie können Sie
Sie diese Gewissheit messen? Wie hoch ist die Sicherheitsschwelle, ab der die Vorhersagen
angezeigt werden sollten? Was wollen Sie mit Vorhersagen unterhalb dieser Schwelle machen?
Verwerfen Sie sie, schalten Sie Menschen ein oder bitten Sie die Benutzer um weitere Informationen?

Während die meisten anderen Metriken die Leistung des Systems im Durchschnitt messen, ist die Vertrauensmessung
Messung ist eine Kennzahl für jede einzelne Probe. Messungen auf Systemebene sind
nützlich, um ein Gefühl für die Gesamtleistung zu bekommen, aber Metriken auf Stichprobenebene sind entscheidend, wenn
Sie sich für die Leistung Ihres Systems bei jeder einzelnen Probe interessieren.

Scheibenbasierte Auswertung

Slicing bedeutet, dass Sie Ihre Daten in Teilmengen aufteilen und die Leistung Ihres Modells
Leistung Ihres Modells für jede Teilmenge separat zu betrachten. Ein häufiger Fehler, den ich bei vielen Unternehmen gesehen habe
Unternehmen gesehen habe, ist, dass sie sich zu sehr auf grobkörnige Metriken wie die Gesamt-F1 oder
Genauigkeit für die gesamten Daten und nicht genug auf Metriken, die auf Untergruppen basieren. Dies kann zu
zwei Problemen führen.

Eine davon ist, dass ihr Modell bei verschiedenen Datenabschnitten unterschiedlich abschneidet, obwohl das
Modell dasselbe leisten sollte. Zum Beispiel haben ihre Daten zwei Untergruppen, eine
eine Mehrheit und eine Minderheit, und die Mehrheitsuntergruppe macht 90 % der Daten aus:

-Modell A erreicht eine Genauigkeit von 98 % in der Mehrheitsgruppe, aber nur 80 % in der
Minderheitengruppe, d. h. seine Gesamtgenauigkeit beträgt 96,2 %.
-Modell B erreicht 95% Genauigkeit bei der Mehrheit und 95% bei der Minderheit.
was bedeutet, dass seine Gesamtgenauigkeit 95% beträgt.
Diese beiden Modelle werden in Tabelle 6-3 verglichen. Welches Modell würden Sie wählen?

Tabelle 6-3. Leistung der beiden Modelle bei den Untergruppen Mehrheit und Minderheit

Genauigkeit der Mehrheit Genauigkeit der Minderheit Gesamtgenauigkeit
Modell A 98% 80% 96,2%
Modell B 95% 95% 95%
Modell Offline-Auswertung | 185
45 Maggie Zhang, "Google Photos Tags Two African-Americans As Gorillas Through Facial Recognition Soft-
ware," Forbes, July 1, 2015, https://oreil.ly/VYG2j.

Wenn sich ein Unternehmen nur auf die Gesamtkennzahlen konzentriert, könnte es sich für Modell A entscheiden.
mit der hohen Genauigkeit dieses Modells sehr zufrieden sein, bis die Endnutzer eines Tages
entdecken, dass dieses Modell gegen die Untergruppe der Minderheiten voreingenommen ist, weil die Untergruppe der
Minderheitenuntergruppe zufällig einer unterrepräsentierten demografischen Gruppe entspricht.^45
Die Konzentration auf die Gesamtleistung ist nicht nur wegen der möglichen
Rückwirkung in der Öffentlichkeit, sondern auch, weil sie das Unternehmen blind macht für große potenzielle
Verbesserungen. Wenn das Unternehmen die scheibenbasierte Leistung der beiden Modelle sieht, könnte es
könnte es andere Strategien verfolgen. Es könnte zum Beispiel beschließen, die Leistung von Modell
die Leistung von Modell A in der Untergruppe der Minderheiten zu verbessern, was zu einer Verbesserung der
Gesamtleistung dieses Modells führt. Oder sie könnten beide Modelle unverändert lassen, verfügen aber nun über mehr
Informationen, um eine fundiertere Entscheidung darüber zu treffen, welches Modell eingesetzt werden soll.
Ein weiteres Problem besteht darin, dass ihr Modell in verschiedenen Datenabschnitten die gleiche Leistung erbringt
obwohl das Modell anders funktionieren sollte. Einige Teilmengen von Daten sind kritischer.
Wenn Sie zum Beispiel ein Modell für die Vorhersage der Nutzerabwanderung erstellen (Vorhersage, wann ein
Vorhersage, wann ein Nutzer ein Abonnement oder einen Dienst kündigt), sind bezahlte Nutzer kritischer als unbezahlte
Nutzer. Wenn Sie sich auf die Gesamtleistung eines Modells konzentrieren, könnte die Leistung bei
diesen kritischen Slices.
Ein faszinierender und scheinbar kontraintuitiver Grund, warum eine Slice-basierte Bewertung
ist das Simpson-Paradoxon, ein Phänomen, bei dem ein Trend in mehreren Datengruppen auftaucht
Gruppen von Daten auftritt, aber verschwindet oder sich umkehrt, wenn die Gruppen kombiniert werden. Das bedeutet
dass Modell B bei allen Daten zusammen besser abschneiden kann als Modell A, aber Modell A
besser abschneidet als Modell B für jede einzelne Untergruppe. Betrachten Sie die Leistung von Modell A und
von Modell B für Gruppe A und Gruppe B, wie in Tabelle 6-4 dargestellt. Modell
A übertrifft Modell B sowohl für die Gruppe A als auch für die Gruppe B, aber wenn es kombiniert wird, übertrifft Modell B
besser ab als Modell A.
Tabelle 6-4. Ein Beispiel für Simpsons Paradoxa
Gruppe A Gruppe B Insgesamt
Modell A 93% (81/87) 73% (192/263) 78% (273/350)
Modell B 87% (234/270) 69% (55/80) 83% (289/350)
a Zahlen aus der Studie von Charig et al. zur Nierensteinbehandlung aus dem Jahr 1986: C. R. Charig, D. R. Webb, S. R. Payne, und J. E. Wickham,
"Vergleich der Behandlung von Nierensteinen durch offene Chirurgie, perkutane Nephrolithotomie und extrakorporale Stoßwellenlithotripsie
Lithotripsie", British Medical Journal (Clinical Research Edition) 292, Nr. 6524 (März 1986): 879-82, https://oreil.ly/X8oWr.
186 | Kapitel 6: Modellentwicklung und Offline-Auswertung
46 P. J. Bickel, E. A. Hammel, und J. W. O'Connell, "Sex Bias in Graduate Admissions: Daten aus Berkeley,"
Science 187 (1975): 398-404, https://oreil.ly/TeR7E.

Das Simpson-Paradoxon kommt häufiger vor, als man denkt. 1973 zeigte die Berkeley-Absolventenstatistik
Statistiken zeigten, dass die Zulassungsrate für Männer viel höher war als für Frauen,
was den Verdacht der Voreingenommenheit gegenüber Frauen aufkommen ließ. Ein genauerer Blick in die
einzelnen Fachbereichen zeigte jedoch, dass die Zulassungsquoten für Frauen tatsächlich
in vier von sechs Fachbereichen höher waren als bei den Männern,^46 wie Tabelle 6-5 zeigt.
Tabelle 6-5. Berkeley's Zulassungsdaten für Absolventen 1973a
Alle Männer Frauen
Fachbereich Zugelassene Bewerber Zugelassene Bewerber Zugelassene Bewerber Zugelassene Bewerber
A 933 64% 825 62% 108 82%
B 585 63% 560 63% 25 68 %
C 918 35% 325 37 % 593 34%
D 792 34% 417 33% 375 35 %
E 584 25% 191 28 % 393 24%
F 714 6% 373 6% 341 7 %
Insgesamt 12.763 41% 8.442 44% 4.321 35%
a Daten von Bickel et al. (1975)
Unabhängig davon, ob man diesem Paradoxon tatsächlich begegnet, geht es hier darum, dass
Aggregation kann tatsächliche Situationen verschleiern und widerlegen. Um fundierte Entscheidungen zu treffen
zu treffen, müssen wir die Leistung des Modells nicht nur für die gesamten Daten
nicht nur für die Gesamtheit der Daten, sondern auch für einzelne Slices. Eine scheibenbasierte Auswertung kann Ihnen
Erkenntnisse zur Verbesserung der Leistung Ihres Modells sowohl insgesamt als auch für kritische Daten
und helfen, potenzielle Verzerrungen aufzudecken. Sie kann auch dazu beitragen, Nicht-ML-Probleme aufzudecken. Einmal,
entdeckte unser Team einmal, dass unser Modell zwar insgesamt gut abschnitt, aber sehr schlecht bei
Verkehr von mobilen Nutzern. Nach eingehender Untersuchung stellten wir fest, dass dies daran lag, dass eine Schaltfläche
auf kleinen Bildschirmen (z. B. Telefonbildschirmen) halb versteckt war.
Selbst wenn Sie nicht glauben, dass Slices eine Rolle spielen, können Sie die Leistung Ihres Modells auf
kann Ihnen Vertrauen in Ihr Modell geben, um andere Beteiligte, wie Ihren
Stakeholder, wie z. B. Ihren Chef oder Ihre Kunden, davon zu überzeugen, dass sie Ihren ML-Modellen vertrauen.
Um die Leistung Ihres Modells bei kritischen Slices zu verfolgen, müssen Sie zunächst wissen, was
Ihre kritischen Slices sind. Sie fragen sich vielleicht, wie Sie kritische Slices in Ihren Daten entdecken können.
Slicing ist leider immer noch eher eine Kunst als eine Wissenschaft und erfordert eine intensive Daten
Datenexploration und -analyse. Hier sind die drei wichtigsten Ansätze:
Modell-Offline-Auswertung | 187
47 Für Leser, die mehr über UX-Design in verschiedenen Kulturen erfahren möchten, hat Jenny Shen einen großartigen Beitrag verfasst.

Heuristik-basiert
Schneiden Sie Ihre Daten auf der Grundlage Ihres Wissens über die Daten und die jeweilige Aufgabe
Hand. Wenn Sie z. B. mit Webdaten arbeiten, können Sie Ihre Daten nach
Daten nach Dimensionen wie Mobiltelefon und Desktop, Browsertyp und Standort aufteilen.
Mobile Benutzer verhalten sich möglicherweise ganz anders als Desktop-Benutzer. In ähnlicher Weise können Inter-
Netznutzer an verschiedenen geografischen Standorten haben möglicherweise unterschiedliche Erwartungen an das Aussehen einer Website.
wie eine Website aussehen sollte.^47
Fehleranalyse
Manuelles Durchgehen falsch klassifizierter Beispiele und Auffinden von Mustern unter ihnen. Wir
entdeckten das Problem unseres Modells mit mobilen Nutzern, als wir sahen, dass die meisten
falsch klassifizierten Beispiele von mobilen Nutzern stammten.
Slice-Finder
Es gibt Forschungsarbeiten zur Systematisierung des Verfahrens zum Auffinden von Slices, darunter
Chung et al.'s "Slice Finder: Automated Data Slicing for Model Validation" in
2019 und in Sumyea Helal's "Subgroup Discovery Algorithms: A Survey
and Empirical Evaluation" (2016). Der Prozess beginnt im Allgemeinen mit der Generierung von
Slice-Kandidaten mit Algorithmen wie Balkensuche, Clustering oder Entscheidung,
Dann werden eindeutig schlechte Kandidaten für Slices herausgefiltert und die verbleibenden Candi-
Daten, die übrig bleiben.
Denken Sie daran, dass Sie, sobald Sie diese kritischen Slices entdeckt haben, eine
ausreichend viele, korrekt beschriftete Daten für jede dieser Slices zur Auswertung. Die Qualität der
Ihrer Auswertung ist nur so gut wie die Qualität Ihrer Auswertungsdaten.
Zusammenfassung
In diesem Kapitel haben wir den ML-Algorithmus-Teil von ML-Systemen behandelt, den viele
ML-Praktiker den unterhaltsamsten Teil des Lebenszyklus eines ML-Projekts betrachten. Mit
den ersten Modellen können wir all unsere harte Arbeit (in Form von Vorhersagen) zum Leben erwecken
Daten- und Feature-Engineering zum Leben erwecken (in Form von Vorhersagen) und können schließlich unsere Hypothese bewerten (d. h. wir können
Vorhersage der Ergebnisse anhand der Eingaben).
Wir haben damit begonnen, die für unsere Aufgaben am besten geeigneten ML-Modelle auszuwählen. Anstatt die
Vor- und Nachteile der einzelnen Modellarchitekturen zu erörtern - was angesichts der
angesichts der wachsenden Zahl existierender Modelle -, umreißt das Kapitel die Aspekte, die Sie
Aspekte, die Sie berücksichtigen müssen, um eine fundierte Entscheidung darüber zu treffen, welches Modell für Ihre
Ziele, Beschränkungen und Anforderungen am besten geeignet ist.
188 | Kapitel 6: Modellentwicklung und Offline-Auswertung
Anschließend haben wir verschiedene Aspekte der Modellentwicklung behandelt. Wir behandelten nicht
nicht nur einzelne Modelle, sondern auch Ensembles von Modellen, eine Technik, die in
Wettbewerben und in der Forschung im Stil eines Leaderboards.

In der Phase der Modellentwicklung experimentieren Sie möglicherweise mit vielen verschiedenen
Modellen experimentieren. Eine intensive Verfolgung und Versionierung Ihrer vielen Experimente ist allgemein
wichtig, aber viele ML-Ingenieure verzichten darauf, weil sie es als lästige
eine lästige Pflicht ist. Daher sind Werkzeuge und eine geeignete Infrastruktur zur Automatisierung des
Nachverfolgung und Versionierung zu automatisieren, unerlässlich. Wir behandeln Werkzeuge und Infrastruktur
für die ML-Produktion in Kapitel 10.

Da die Modelle heute immer größer werden und mehr Daten benötigen, wird das verteilte Training
eine wesentliche Fähigkeit für ML-Modellentwickler, und wir diskutierten Techniken
für Parallelität, einschließlich Datenparallelität, Modellparallelität und Pipelineparallelität
ismus. Damit Ihre Modelle auf einem großen verteilten System funktionieren, wie dem, auf dem
Modelle mit Hunderten von Millionen, wenn nicht Milliarden von Parametern ausführt, kann eine Herausforderung sein
und erfordert spezielle systemtechnische Kenntnisse.

Am Ende des Kapitels erfahren Sie, wie Sie Ihre Modelle bewerten, um das beste Modell für den Einsatz auszuwählen.
Einsatz. Evaluierungsmetriken sind nur von Bedeutung, wenn Sie eine Basislinie zum Vergleich haben.
und wir haben verschiedene Arten von Basisdaten behandelt, die Sie für die Bewertung
für die Bewertung. Wir haben auch eine Reihe von Evaluierungstechniken behandelt, die notwendig sind, um
um Ihre Modelle auf ihre Tauglichkeit zu prüfen, bevor Sie sie in einer Produktionsumgebung weiter evaluieren
Umgebung.

Egal, wie gut Ihre Offline-Evaluierung eines Modells ist, Sie können sich oft nicht sicher sein
über die Leistung Ihres Modells in der Produktion erst dann sicher sein, wenn das Modell eingesetzt wurde. Unter
nächsten Kapitel werden wir uns mit der Bereitstellung eines Modells befassen.

Zusammenfassung | 189
KAPITEL 7

Modellimplementierung und Vorhersagedienst
In den Kapiteln 4 bis 6 haben wir die Überlegungen zur Entwicklung eines ML
Modells, von der Erstellung von Trainingsdaten, der Extraktion von Merkmalen und der Entwicklung des Modells bis hin zur
Ausarbeitung von Metriken zur Bewertung des Modells. Diese Überlegungen bilden die Logik des
des Modells - Anleitungen, wie man von Rohdaten zu einem ML-Modell kommt, wie in
in Abbildung 7-1. Die Entwicklung dieser Logik erfordert sowohl ML-Kenntnisse als auch fachliche
Fachwissen. In vielen Unternehmen ist dies der Teil des Prozesses, der von den ML- oder
Datenwissenschaftsteams.

Abbildung 7-1. Verschiedene Aspekte, die die Logik des ML-Modells ausmachen

In diesem Kapitel werden wir einen weiteren Teil des iterativen Prozesses besprechen: den Einsatz Ihres
Modells. "Bereitstellen" ist ein weiter Begriff, der im Allgemeinen bedeutet, dass Ihr Modell lauffähig
lauffähig und zugänglich zu machen. Während der Modellentwicklung läuft Ihr Modell normalerweise in einem

191
1 Wir werden Entwicklungsumgebungen in Kapitel 10 ausführlich behandeln.
2 Auf Container gehen wir in Kapitel 9 näher ein.
3 CS 329S: Machine Learning Systems Design in Stanford; Sie können die Projektdemos auf YouTube sehen.
Entwicklungsumgebung.^1 Um deployed zu werden, muss Ihr Modell die Entwicklungsumgebung verlassen.
Entwicklungsumgebung verlassen. Ihr Modell kann in einer Staging-Umgebung zum Testen
oder in eine Produktionsumgebung, die von Ihren Endbenutzern verwendet wird. In diesem Kapitel,
konzentrieren wir uns auf die Bereitstellung von Modellen in Produktionsumgebungen.

Bevor wir fortfahren, möchte ich betonen, dass die Produktion ein Spektrum ist. Für
bedeutet Produktion für einige Teams, schöne Diagramme in Notebooks zu erstellen, um sie dem
Geschäftsteam zu zeigen. Für andere Teams bedeutet Produktion, dass sie ihre Modelle für
für Millionen von Nutzern pro Tag zu betreiben. Wenn Ihre Arbeit in das erste Szenario fällt, ist Ihre
Produktionsumgebung ähnlich wie die Entwicklungsumgebung, und dieses Kapitel
ist für Sie weniger relevant. Wenn Ihre Arbeit eher dem zweiten Szenario entspricht, lesen Sie weiter.

Ich habe einmal irgendwo im Internet gelesen, dass die Bereitstellung einfach ist, wenn man alle schwierigen Teile ignoriert.
Teile ignoriert. Wenn Sie ein Modell für Ihre Freunde zum Spielen bereitstellen wollen, müssen Sie nur
Ihre Vorhersagefunktion in einen POST-Anfrageendpunkt mit Flask oder FastAPI verpacken,
die Abhängigkeiten, die diese Vorhersagefunktion zur Ausführung benötigt, in einen Container zu packen,^2 und
Ihr Modell und den zugehörigen Container zu einem Cloud-Service wie AWS oder GCP, um den Endpunkt freizugeben
den Endpunkt:

# Beispiel für die Verwendung von FastAPI zur Umwandlung Ihrer Predict-Funktion
# in einen POST-Endpunkt verwandelt
@app.route('/predict', methods=['POST'])
def predict():
X = request.get_json()['X']
y = MODEL.predict(X).tolist()
return json.dumps({'y': y}), 200
Sie können diesen exponierten Endpunkt für nachgelagerte Anwendungen verwenden: z.B. wenn eine Anwendung eine
Anwendung eine Vorhersageanforderung von einem Benutzer erhält, wird diese Anforderung an den exponierten
Endpunkt gesendet, der eine Vorhersage zurückgibt. Wenn Sie mit den erforderlichen Tools vertraut sind, können Sie
kann man innerhalb einer Stunde eine funktionsfähige Anwendung erstellen. Meine Studenten waren nach einem 10-wöchigen Kurs
waren alle in der Lage, eine ML-Anwendung in ihren Abschlussprojekten einzusetzen, obwohl nur wenige
Einsatzerfahrung hatten.^3

Der schwierige Teil besteht darin, Ihr Modell für Millionen von Nutzern mit einer
Latenzzeit von Millisekunden und einer Betriebszeit von 99 % zur Verfügung zu stellen, die Infrastruktur so einzurichten, dass die richtige
die richtige Person sofort benachrichtigt werden kann, wenn etwas schief läuft, herauszufinden, was
was schief gelaufen ist, und die nahtlose Bereitstellung von Updates zur Behebung des Fehlers.

192 | Kapitel 7: Modellbereitstellung und Vorhersagedienst

4 Siehe die Diskussion über "Datenserialisierung" im Abschnitt "Datenformate" auf Seite 53.
In vielen Unternehmen liegt die Verantwortung für den Einsatz von Modellen in den Händen von
denselben Personen, die diese Modelle entwickelt haben. In vielen anderen Unternehmen wird, sobald ein
Sobald ein Modell einsatzbereit ist, wird es exportiert und an ein anderes Team weitergegeben
um es einzusetzen. Diese Trennung der Zuständigkeiten kann jedoch einen hohen Arbeitsaufwand verursachen
Kommunikation zwischen den Teams führen und die Aktualisierung Ihres Modells verlangsamen. Außerdem kann es
schwer zu beheben sein, wenn etwas schief geht. Wir werden mehr über Team
Strukturen in Kapitel 11.

Ein Modell zu exportieren bedeutet, dieses Modell in ein Format zu konvertieren, das
von einer anderen Anwendung verwendet werden kann. Manche Leute nennen diesen Vorgang
"Serialisierung"^4 Es gibt zwei Teile eines Modells, die Sie exportieren können
die Modelldefinition und die Parameterwerte des Modells. Die Modell
Definition definiert die Struktur Ihres Modells, z. B. wie viele
Schichten es hat und wie viele Einheiten sich in jeder Schicht befinden. Die Param-
Die Parameterwerte geben die Werte für diese Einheiten und Schichten an. Normalerweise werden
werden diese beiden Teile zusammen exportiert.
In TensorFlow 2 können Sie tf.keras.Model.save() verwenden, um
Ihr Modell in TensorFlow's SavedModel Format zu exportieren. In PyTorch, können Sie
torch.onnx.export() verwenden, um das Modell in das ONNX
Format zu exportieren.
Unabhängig davon, ob Ihre Arbeit den Einsatz von ML-Modellen beinhaltet, ist es wichtig zu wissen
wie Ihre Modelle verwendet werden, können Sie deren Einschränkungen verstehen und
und helfen Ihnen, sie an den jeweiligen Zweck anzupassen.

In diesem Kapitel beginnen wir mit einigen verbreiteten Mythen über den ML-Einsatz, die
die ich oft von Leuten gehört habe, die keine ML-Modelle eingesetzt haben. Dann besprechen wir
die beiden Hauptwege, auf denen ein Modell seine Vorhersagen generiert und den Nutzern zur Verfügung stellt: Online-Vorhersage
diction und Batch-Vorhersage. Der Prozess der Erstellung von Vorhersagen wird Inferenz genannt.

Wir fahren damit fort, wo die Berechnungen für die Erstellung von Vorhersagen durchgeführt werden sollten
erfolgen sollte: auf dem Gerät (auch als Edge bezeichnet) und in der Cloud. Wie ein Modell die Vorhersagen
und die Berechnung der Vorhersagen hat Einfluss darauf, wie es gestaltet werden sollte, welche Infrastruktur
Infrastruktur, die es benötigt, und die Verhaltensweisen, die die Nutzer antreffen.

Modellbereitstellung und Vorhersagedienst | 193
Wenn Sie aus dem akademischen Bereich kommen, liegen einige der in diesem Kapitel behandelten Themen
Kapitel besprochen werden, liegen vielleicht außerhalb Ihrer Komfortzone. Wenn ein unbekannter Begriff auftaucht, nehmen Sie sich einen
Moment, um ihn nachzuschlagen. Wenn ein Abschnitt zu umfangreich wird, können Sie ihn überspringen. Dieses Kapitel
ist modular aufgebaut, so dass das Überspringen eines Abschnitts keine Auswirkungen auf das Verständnis eines anderen
Abschnitt beeinträchtigen.

Mythen über den Einsatz von maschinellem Lernen
Wie in Kapitel 1 erörtert, kann sich die Bereitstellung eines ML-Modells stark von der
Einsatz eines herkömmlichen Softwareprogramms. Dieser Unterschied kann dazu führen, dass Personen, die
die noch nie ein Modell implementiert haben, entweder Angst vor dem Prozess haben oder unterschätzen, wie
wie viel Zeit und Mühe er erfordert. In diesem Abschnitt räumen wir mit einigen der gängigen
Mythen über den Bereitstellungsprozess, die Sie hoffentlich in eine gute Verfassung versetzen
für den Beginn des Prozesses. Dieser Abschnitt ist vor allem für Personen hilfreich, die wenig oder gar keine
Erfahrung mit der Bereitstellung.

Mythos 1: Man setzt nur ein oder zwei ML-Modelle auf einmal ein
Bei akademischen Projekten wurde mir geraten, ein kleines Problem zu wählen, auf das ich mich konzentrieren sollte,
was normalerweise zu einem einzigen Modell führte. Viele Leute mit akademischem Hintergrund, mit denen ich
mit denen ich gesprochen habe, neigen ebenfalls dazu, die ML-Produktion im Kontext eines einzigen Modells zu sehen. Unter-
In der Folge funktioniert die Infrastruktur, die sie im Kopf haben, nicht für tatsächliche Anwendungen,
weil sie nur ein oder zwei Modelle unterstützen kann.

In Wirklichkeit haben Unternehmen viele, viele ML-Modelle. Eine Anwendung kann
viele verschiedene Funktionen haben, und jede Funktion kann ihr eigenes Modell erfordern. Nehmen wir eine
Mitfahr-App wie Uber. Sie benötigt ein Modell, um jedes der folgenden Elemente vorherzusagen:
Nachfrage nach Fahrten, Verfügbarkeit der Fahrer, geschätzte Ankunftszeit, dynamische Preisgestaltung, betrügerische
lente Transaktion, Kundenabwanderung und mehr. Wenn diese App außerdem in 20 Ländern operiert
Ländern betrieben wird, bis Sie Modelle haben, die über verschiedene Nutzerprofile verallgemeinert werden können,
Kulturen und Sprachen verallgemeinern lassen, bräuchte jedes Land seinen eigenen Satz von Modellen. Bei 20
Ländern und 10 Modellen für jedes Land haben Sie also bereits 200 Modelle. Abbildung 7-2
zeigt ein breites Spektrum von Aufgaben, bei denen ML bei Netflix zum Einsatz kommt.

194 | Kapitel 7: Modellbereitstellung und Vorhersagedienst

5 Ville Tuulos, "Human-Centric Machine Learning Infrastructure @Netflix", InfoQ, 2018, Video, 49:11,
https://oreil.ly/j4Hfx.
6 Wayne Cunningham, "Science at Uber: Powering Machine Learning at Uber," Uber Engineering Blog, Septem-
ber 10, 2019, https://oreil.ly/WfaCF.
7 Daniel Papasian und Todd Underwood, "OpML '20-How ML Breaks: A Decade of Outages for One Large
ML Pipeline", Google, 2020, Video, 19:06, https://oreil.ly/HjQm0.
8 Lucas Bernardi, Themistoklis Mavridis, und Pablo Estevez, "150 Successful Machine Learning Models: 6
Lessons Learned at Booking.com," KDD '19: Proceedings of the 25th ACM SIGKDD International Conference
on Knowledge Discovery & Data Mining (Juli 2019): 1743-51, https://oreil.ly/Ea1Ke.
9 "2021 Enterprise Trends in Machine Learning", Algorithmia, https://oreil.ly/9kdcw.
Abbildung 7-2. Verschiedene Aufgaben, bei denen ML bei Netflix zum Einsatz kommt. Quelle: Ville Tuulos^5

Tatsächlich hat Uber Tausende von Modellen in Produktion.^6 Google hat zu jedem Zeitpunkt
Tausende von Modellen mit Hunderten von Milliarden Parametern gleichzeitig trainieren
Größe.^7 Booking.com hat 150+ Modelle.^8 Eine Studie von Algorithmia aus dem Jahr 2021 zeigt, dass
41% der Unternehmen mit mehr als 25.000 Mitarbeitern mehr als 100 Modelle in
Produktion haben.^9

Mythos 2: Wenn wir nichts tun, bleibt die Modellleistung gleich
Software altert nicht wie guter Wein. Sie altert schlecht. Das Phänomen, bei dem ein
Softwareprogramm im Laufe der Zeit verschlechtert, auch wenn sich scheinbar nichts geändert hat, ist
als "Softwarefäule" oder "Bitfäule" bekannt.

Mythen über den Einsatz von maschinellem Lernen | 195
10 Wir werden die Verschiebung der Datenverteilung in Kapitel 8 weiter erörtern.
11 Christopher Null, "10 Companies Killing It at DevOps," TechBeacon, 2015, https://oreil.ly/JvNwu.
12 Qian Yu, "Machine Learning with Flink in Weibo", QCon 2019, Video, 17:57, https://oreil.ly/RcTMv.
13 Josh Wills, "Instrumentation, Observability and Monitoring of Machine Learning Models," InfoQ 2019,
https://oreil.ly/5Ot5m.

ML-Systeme sind dagegen nicht immun. Darüber hinaus leiden ML-Systeme unter so genannten
Datenverteilungsverschiebungen, wenn die Datenverteilung, auf die Ihr Modell
von der Datenverteilung abweicht, auf die es trainiert wurde.^10 Daher neigt ein
ML-Modell in der Regel direkt nach dem Training die beste Leistung und wird im Laufe der Zeit schlechter.
Mythos 3: Sie werden Ihre Modelle nicht so oft aktualisieren müssen
Häufig werde ich gefragt: "Wie oft sollte ich meine Modelle aktualisieren?" Das ist die falsche
Frage, die man stellen sollte. Die richtige Frage sollte lauten: "Wie oft kann ich meine Modelle aktualisieren?"
Da die Leistung eines Modells mit der Zeit abnimmt, sollten wir es so schnell wie möglich aktualisieren.
Dies ist ein Bereich von ML, in dem wir von bestehenden DevOps-Best-Practices lernen sollten.
Schon im Jahr 2015 wurden ständig Updates für die Systeme veröffentlicht.
Systeme. Etsy stellte 50 Mal pro Tag bereit, Netflix tausende Male pro Tag, AWS alle
11,7 Sekunden.^11
Während viele Unternehmen ihre Modelle immer noch nur einmal im Monat oder sogar einmal im
Quartal aktualisieren, beträgt der Iterationszyklus von Weibo für die Aktualisierung einiger ihrer ML-Modelle 10 Minuten.^12
Ich habe ähnliche Zahlen bei Unternehmen wie Alibaba und ByteDance (das Unternehmen
hinter TikTok).
In den Worten von Josh Wills, einem ehemaligen Mitarbeiter von Google und Director of Data
bei Slack: "Wir versuchen immer, neue Modelle so schnell wie möglich in die Produktion
so schnell wie möglich in Produktion zu bringen."^13
Wir werden in Kapitel 9 mehr über die Häufigkeit des Retrainings Ihrer Modelle diskutieren.
Mythos 4: Die meisten ML-Ingenieure müssen sich nicht um die Skalierung kümmern
Was "Skalierung" bedeutet, variiert von Anwendung zu Anwendung, aber Beispiele sind ein
System, das Hunderte von Abfragen pro Sekunde oder Millionen von Benutzern pro Monat bedient.
Man könnte argumentieren, dass sich in diesem Fall nur eine kleine Anzahl von Unternehmen
machen müssen. Es gibt nur ein Google, ein Facebook, ein Amazon. Das ist wahr, aber eine
aber eine kleine Anzahl großer Unternehmen beschäftigt die Mehrheit der Softwareentwickler
Arbeitskräfte. Laut der Stack Overflow Developer Survey 2019 arbeitet mehr als die Hälfte
der Befragten für ein Unternehmen mit mindestens 100 Mitarbeitern tätig (siehe Abbildung 7-3).
Dies ist keine perfekte Korrelation, aber ein Unternehmen mit 100 Mitarbeitern hat eine gute Chance, eine
eine angemessene Anzahl von Benutzern zu bedienen.
196 | Kapitel 7: Modellimplementierung und Vorhersagedienst
14 "Developer Survey Results," Stack Overflow, 2019, https://oreil.ly/guYIq.

Abbildung 7-3. Verteilung der Größe von Unternehmen, in denen Softwareingenieure arbeiten.
Quelle: Angepasst von einem Bild von Stack Overflow^14
Ich konnte keine Umfrage für ML-spezifische Rollen finden, also habe ich auf Twitter nachgefragt und fand ähnliche
Ergebnisse. Das bedeutet, dass Sie, wenn Sie eine ML-bezogene Stelle in der Branche suchen, wahrscheinlich
wahrscheinlich für ein Unternehmen mit mindestens 100 Mitarbeitern arbeiten, dessen ML-Anwendungen wahrscheinlich
skalierbar sein müssen. Statistisch gesehen sollte sich ein ML-Ingenieur um die Skalierbarkeit kümmern.
Batch-Vorhersage versus Online-Vorhersage
Eine grundlegende Entscheidung, die Sie treffen müssen und die sich sowohl auf Ihre Endbenutzer
Endnutzer als auch die an Ihrem System arbeitenden Entwickler betrifft, ist die Art und Weise, wie die Vorhersagen generiert und
Endnutzern zur Verfügung stellt: online oder Batch. Die Terminologie rund um Batch- und Online
Vorhersage sind immer noch recht verwirrend, da es in der Branche keine standardisierten Praktiken
Branche. Ich werde mein Bestes tun, um die Nuancen der einzelnen Begriffe in diesem Abschnitt zu erklären. Wenn Sie
einen der hier genannten Begriffe zu verwirrend finden, können Sie ihn vorerst ignorieren.
Wenn Sie alles andere vergessen haben, gibt es drei Hauptarten der Vorhersage, die Sie hoffentlich
Sie sich merken werden:
Batch-Vorhersage, bei der nur Batch-Merkmale verwendet werden.
Online-Vorhersage, die nur Batch-Features verwendet (z. B. vorberechnete Einbettungen).
Online-Vorhersage, die sowohl Batch-Features als auch Streaming-Features verwendet. Dies wird
auch als Streaming-Vorhersage bekannt.
Batch-Vorhersage versus Online-Vorhersage | 197
Bei der Online-Vorhersage werden die Vorhersagen generiert und zurückgegeben, sobald Anfragen
für diese Vorhersagen eintreffen. Sie geben zum Beispiel einen englischen Satz in Goo-
gle Translate ein und erhalten sofort die französische Übersetzung zurück. Die Online-Vorhersage wird
auch als On-Demand-Vorhersage bezeichnet. Traditionell werden bei der Online-Vorhersage
werden Anfragen an den Vorhersagedienst über RESTful APIs (z. B. HTTP-Anfragen) gesendet.
siehe "Datenübergabe über Dienste" auf Seite 73 ). Wenn die Vorhersageanforderungen
über HTTP-Anforderungen gesendet werden, wird die Online-Vorhersage auch als synchrone Vorhersage bezeichnet:
Die Vorhersagen werden synchron mit den Anforderungen erstellt.

Bei der Batch-Vorhersage werden Vorhersagen in regelmäßigen Abständen oder bei Auslösung erstellt.
Die Vorhersagen werden irgendwo gespeichert, z. B. in SQL-Tabellen oder einer In-Memory-Datenbank.
Datenbank, und werden bei Bedarf abgerufen. Netflix könnte zum Beispiel alle zwei Wochen Filmempfehlungen
Netflix könnte beispielsweise alle vier Stunden Filmempfehlungen für alle seine Nutzer erstellen, und die vorberechneten Empfehlungen
werden abgerufen und den Benutzern angezeigt, wenn sie sich bei Netflix anmelden. Die Batch-Vorhersage ist auch
auch als asynchrone Vorhersage bezeichnet: Vorhersagen werden asynchron mit
Anfragen.

Terminologie-Verwirrung
Die Begriffe "Online-Vorhersage" und "Batch-Vorhersage" können miteinander
verschmelzen. Beide können Vorhersagen für mehrere Proben (im Batch)
oder eine Probe auf einmal. Um diese Verwechslung zu vermeiden, werden manchmal
die Begriffe "synchrone Vorhersage" und "asynchrone Vorhersage" bevorzugt.
diction". Aber auch diese Unterscheidung ist nicht perfekt, denn wenn
Online-Vorhersage einen Echtzeit-Transport nutzt, um Vorhersage
Vorhersageanfragen an Ihr Modell sendet, sind die Anfragen und Vorhersagen technisch
asynchron.
Abbildung 7-4 zeigt eine vereinfachte Architektur für die Batch-Vorhersage, und Abbildung 7-5 zeigt
eine vereinfachte Version der Online-Vorhersage, die nur Batch-Funktionen verwendet. Wir gehen darauf ein, was
was es bedeutet, nur Batch-Features zu verwenden.

Abbildung 7-4. Eine vereinfachte Architektur für die Batch-Vorhersage

198 | Kapitel 7: Modellbereitstellung und Vorhersagedienst

Abbildung 7-5. Eine vereinfachte Architektur für die Online-Vorhersage, die nur Batch-Merkmale verwendet

Wie in Kapitel 3 erörtert, handelt es sich bei Merkmalen, die aus historischen Daten berechnet werden, wie z. B. Daten in
Datenbanken und Data Warehouses, sind Stapelmerkmale. Aus Streaming-Daten berechnete Merkmale
Daten - Daten in Echtzeit-Transporten - berechnet werden, sind Streaming-Features. Bei der Batch-Vorhersage werden nur
Batch-Merkmale verwendet. Bei der Online-Vorhersage ist es jedoch möglich, sowohl Batch
und Streaming-Merkmale verwendet werden. Nachdem ein Benutzer beispielsweise eine Bestellung bei Door-
Dash eine Bestellung aufgegeben hat, benötigt er möglicherweise die folgenden Merkmale, um die Lieferfrist abzuschätzen:

Merkmale der Charge
Die durchschnittliche Zubereitungszeit dieses Restaurants in der Vergangenheit

Streaming-Funktionen
in den letzten 10 Minuten, wie viele andere Aufträge sie haben und wie viele Zusteller verfügbar sind
Personen verfügbar sind

Streaming-Funktionen vs. Online-Funktionen
Ich habe gehört, dass die Begriffe "Streaming-Funktionen" und "Online-Funktionen"
austauschbar verwendet. Tatsächlich sind sie unterschiedlich. Online-Funktionen sind
allgemeiner, da sie sich auf jedes Merkmal beziehen, das für die Online-Vorhersage
Vorhersage verwendet werden, einschließlich im Speicher abgelegter Batch-Features.
Eine sehr häufige Art von Stapelmerkmalen, die für Online-Vorhersagen verwendet werden,
insbesondere bei sitzungsbasierten Empfehlungen, sind Item Embeddings.
Elementeinbettungen werden normalerweise im Batch vorberechnet und immer dann abgerufen
wenn sie für die Online-Vorhersage benötigt werden. In diesem Fall
Einbettungen als Online-Merkmale betrachtet werden, aber nicht als Streaming
Merkmale.
Streaming Features beziehen sich ausschließlich auf Features, die aus
Streaming-Daten berechnet werden.
Batch-Vorhersage versus Online-Vorhersage | 199
Eine vereinfachte Architektur für die Online-Vorhersage, die sowohl Streaming-Features als auch
Batch-Features verwendet, ist in Abbildung 7-6 dargestellt. Einige Unternehmen nennen diese Art der Vorhersage
"Streaming-Vorhersage", um sie von der Art der Online-Vorhersage zu unterscheiden, die
die keine Streaming-Features verwendet.

Abbildung 7-6. Eine vereinfachte Architektur für die Online-Vorhersage, die sowohl Batch-Features
und Streaming-Merkmale

Online-Vorhersage und Batch-Vorhersage müssen sich jedoch nicht gegenseitig ausschließen.
Eine Hybridlösung besteht darin, dass Sie Vorhersagen für beliebte Abfragen vorberechnen und dann
Vorhersagen für weniger beliebte Abfragen online generieren. Tabelle 7-1 fasst die wichtigsten
Punkte zusammen, die bei der Online-Vorhersage und der Batch-Vorhersage zu beachten sind.

Tabelle 7-1. Einige wesentliche Unterschiede zwischen Batch-Vorhersage und Online-Vorhersage

Batch-Vorhersage (asynchron) Online-Vorhersage (synchron)
Häufigkeit Periodisch, z. B. alle vier Stunden Sobald Anfragen kommen
Nützlich für die Verarbeitung akkumulierter Daten, wenn Sie keine
sofortige Ergebnisse benötigen (z. B. Empfehlungssysteme)
Wenn Vorhersagen benötigt werden, sobald eine Datenprobe
generiert wird (z. B. Betrugserkennung)
Optimiert für hohen Durchsatz Niedrige Latenzzeit
200 | Kapitel 7: Modellbereitstellung und Vorhersagedienst

15 David Curry, "Grubhub Revenue and Usage Statistics (2022)", Business of Apps, Januar 11, 2022,
https://oreil.ly/jX43M; "Average Number of Grubhub Orders per Day Worldwide from 2011 to 2020", Statista,
https://oreil.ly/Tu9fm.
16 Die URL des Einstiegspunkts für einen Dienst, der in diesem Fall der Vorhersagedienst Ihres ML-Modells ist.

In vielen Anwendungen werden Online-Vorhersage und Batch-Vorhersage nebeneinander für
verschiedene Anwendungsfälle verwendet. So verwenden beispielsweise Apps für Essensbestellungen wie DoorDash und UberEats
Batch-Vorhersage, um Restaurantempfehlungen zu generieren - es würde zu lange dauern, diese
Diese Empfehlungen online zu generieren, würde zu lange dauern, da es viele Restaurants gibt. Wie auch immer,
Sobald Sie jedoch auf ein Restaurant klicken, werden mithilfe der Online-Vorhersage Empfehlungen für
Online-Vorhersage generiert.
Viele glauben, dass die Online-Vorhersage sowohl in Bezug auf die Kosten als auch auf die Leistung weniger effizient ist als die
und Leistung weniger effizient ist als die Batch-Vorhersage, weil man möglicherweise nicht in der Lage ist, die
Eingaben zusammenzufassen und die Vektorisierung oder andere Optimierungstechniken zu nutzen. Dies ist
nicht unbedingt richtig, wie wir bereits im Abschnitt "Stapelverarbeitung versus
Stream-Verarbeitung" auf Seite 78 beschrieben.
Außerdem müssen Sie bei der Online-Vorhersage keine Vorhersagen für Benutzer erstellen, die
die Ihre Website nicht besuchen. Stellen Sie sich vor, Sie betreiben eine Anwendung, bei der sich nur 2 % Ihrer Benutzer anmelden
täglich anmelden - z. B. hatte Grubhub im Jahr 2020 31 Millionen Nutzer und 622.000 tägliche Bestellungen.^15 Wenn Sie
jeden Tag Vorhersagen für jeden Nutzer erstellen, wird die Rechenleistung, die für die Erstellung von 98 % der
Ihrer Vorhersagen verschwendet werden.
Von der Batch-Vorhersage zur Online-Vorhersage
Für Menschen, die sich mit ML aus dem akademischen Bereich beschäftigen, ist der natürlichste Weg, um
Vorhersagen wahrscheinlich online zu erstellen. Sie geben Ihrem Modell eine Eingabe und es erstellt
eine Vorhersage, sobald es diese Eingabe erhält. So interagieren wahrscheinlich die meisten Menschen
mit ihren Modellen während des Prototypings. Für die meisten Unternehmen ist dies auch einfacher
Unternehmen beim ersten Einsatz eines Modells einfacher. Sie exportieren Ihr Modell, laden das exportierte
Modell zu Amazon SageMaker oder Google App Engine und erhalten einen exponierten
Endpunkt zurück.^16 Wenn Sie nun eine Anfrage mit einer Eingabe an diesen Endpunkt senden, wird dieser
eine Vorhersage zurück, die auf der Grundlage dieser Eingabe erstellt wurde.
Ein Problem bei der Online-Vorhersage ist, dass Ihr Modell möglicherweise zu lange braucht, um
Vorhersagen. Anstatt Vorhersagen zu erstellen, sobald sie eintreffen, könnten Sie
Vorhersagen im Voraus zu berechnen und in Ihrer Datenbank zu speichern und sie abzurufen
wenn Anfragen eintreffen? Genau das macht die Batch-Vorhersage. Mit diesem Ansatz
können Sie Vorhersagen für mehrere Eingaben gleichzeitig generieren, indem Sie verteilte
Techniken zur effizienten Verarbeitung einer großen Menge von Stichproben.
Batch-Vorhersage gegenüber Online-Vorhersage | 201
17 Wenn sich ein neuer Benutzer anmeldet, können Sie ihm einige allgemeine Empfehlungen geben.

Da die Vorhersagen vorberechnet werden, müssen Sie sich keine Gedanken darüber machen, wie lange
Ihre Modelle brauchen, um Vorhersagen zu erstellen. Aus diesem Grund kann die Stapelvorhersage
auch als Trick zur Verringerung der Inferenzlatenz bei komplexeren Modellen angesehen werden - die
Die Zeit, die zum Abrufen einer Vorhersage benötigt wird, ist in der Regel kürzer als die Zeit, die für die Erstellung der Vorhersage benötigt wird.
Die Stapelvorhersage eignet sich gut, wenn Sie viele Vorhersagen erstellen möchten und die Ergebnisse nicht
die Ergebnisse nicht sofort benötigen. Sie müssen nicht alle erstellten Vorhersagen verwenden. Für
Sie können z. B. Vorhersagen für alle Kunden erstellen, wie wahrscheinlich es ist, dass sie ein neues Produkt kaufen
neues Produkt kaufen werden, und die besten 10 % ansprechen.
Das Problem bei der Batch-Vorhersage ist jedoch, dass Ihr Modell dadurch weniger
auf die veränderten Präferenzen der Nutzer reagiert. Diese Einschränkung zeigt sich sogar bei
technologisch fortschrittlicheren Unternehmen wie Netflix. Angenommen, Sie haben sich in letzter Zeit viele
in letzter Zeit viele Horrorfilme angesehen. Wenn Sie sich also zum ersten Mal bei Netflix einloggen, dominieren Horrorfilme die
Empfehlungen. Aber heute sind Sie gut drauf, also suchen Sie nach "Komödie" und beginnen
die Kategorie "Komödie" zu durchsuchen. Netflix sollte lernen und Ihnen mehr Komödien in Ihrer
Ihrer Empfehlungsliste anzeigen, oder? Zum Zeitpunkt des Verfassens dieses Buches kann die Liste nicht aktualisiert werden
Liste nicht aktualisieren, bis der nächste Stapel von Empfehlungen generiert wird, aber ich habe keinen Zweifel, dass
dass diese Einschränkung in naher Zukunft behoben wird.
Ein weiteres Problem bei der Batch-Vorhersage besteht darin, dass Sie im Voraus wissen müssen, für welche Anfragen Sie
Vorhersagen im Voraus generiert werden sollen. Im Falle der Empfehlung von Filmen für Benutzer,
wissen Sie im Voraus, für wie viele Benutzer Sie Empfehlungen erstellen müssen.^17
für Fälle, in denen Sie unvorhersehbare Anfragen haben - wenn Sie ein System zur Übersetzung
aus dem Englischen ins Französische zu übersetzen, ist es möglicherweise unmöglich, jeden möglichen englischen
zu übersetzenden englischen Text vorhersehen - müssen Sie die Online-Vorhersage verwenden, um Vorhersagen zu
Anfragen eintreffen.
Im Beispiel von Netflix verursacht die Batch-Vorhersage leichte Unannehmlichkeiten (die eng mit der
eng mit der Nutzerbindung und -bindung gekoppelt ist), aber keine katastrophalen Ausfälle. Es gibt
Es gibt viele Anwendungen, bei denen die Stapelvorhersage zu katastrophalen Ausfällen führen würde oder einfach
nicht funktionieren würde. Beispiele, bei denen Online-Vorhersagen entscheidend sind, sind der Hochfrequenzhandel
Hochfrequenzhandel, autonome Fahrzeuge, Sprachassistenten, Entsperren des Telefons per Gesicht oder
Fingerabdrücke, Sturzerkennung in der Altenpflege und Betrugserkennung. Die Fähigkeit, eine betrügerische
eine betrügerische Transaktion zu erkennen, die vor drei Stunden stattgefunden hat, ist immer noch besser als
zu erkennen, aber in der Lage zu sein, sie in Echtzeit zu erkennen, kann verhindern, dass die betrügerische Transaktion
zu verhindern.
202 | Kapitel 7: Modellbereitstellung und Vorhersagedienst
Die Stapelvorhersage ist eine Abhilfe für den Fall, dass die Online-Vorhersage nicht billig genug ist oder
nicht schnell genug ist. Warum eine Million Vorhersagen im Voraus generieren und sich Gedanken über
speichern und abrufen, wenn Sie jede Vorhersage bei Bedarf zu genau den gleichen Kosten
gleichen Kosten und mit der gleichen Geschwindigkeit erstellen können?

Da die Hardware immer individueller und leistungsfähiger wird und bessere Techniken
entwickelt werden, um schnellere und billigere Online-Prognosen zu ermöglichen, könnte die Online-Prognose
der Standard werden.

In den letzten Jahren haben die Unternehmen erhebliche Investitionen getätigt, um von der Batch
Vorhersage zur Online-Vorhersage zu wechseln. Um das Problem der Latenzzeit bei der Online-Vorhersage zu
tion zu überwinden, sind zwei Komponenten erforderlich:

-Eine (nahezu) Echtzeit-Pipeline, die mit eingehenden Daten arbeiten kann, Streaming
Streaming-Merkmale extrahiert, sie in ein Modell eingibt und eine Vorhersage in nahezu Echtzeit
Zeit. Eine Streaming-Pipeline mit Echtzeit-Transport und einer Stream-Computation
Engine kann dabei helfen.
-Ein Modell, das Vorhersagen mit einer für die Endnutzer akzeptablen Geschwindigkeit erstellen kann. Für
die meisten Verbraucheranwendungen bedeutet dies Millisekunden.
Wir haben die Stream-Verarbeitung in Kapitel 3 besprochen. Wir werden nun die Vereinheitlichung der
der Stream-Pipeline mit der Batch-Pipeline im nächsten Abschnitt. Dann werden wir
im Abschnitt "Modelloptimierung" auf Seite 216, wie man die Inferenz beschleunigen kann.

Vereinheitlichung von Batch-Pipeline und Streaming-Pipeline
Die Batch-Vorhersage ist weitgehend ein Produkt von Altsystemen. Im letzten Jahrzehnt wurde die Big Data
Verarbeitung von Big Data von Batch-Systemen wie MapReduce und Spark dominiert, die
die es uns ermöglichen, eine große Datenmenge regelmäßig und sehr effizient zu verarbeiten. Als die Unternehmen
Als Unternehmen mit ML begannen, nutzten sie ihre bestehenden Batch-Systeme, um Vorhersagen zu treffen.
Wenn diese Unternehmen Streaming-Funktionen für ihre Online-Vorhersagen nutzen wollen, müssen sie eine separate
müssen sie eine separate Streaming-Pipeline aufbauen. Lassen Sie uns ein Beispiel durchgehen, um
um dies zu verdeutlichen.

Batch-Vorhersage versus Online-Vorhersage | 203
Stellen Sie sich vor, Sie möchten ein Modell zur Vorhersage der Ankunftszeit für eine Anwendung wie
Google Maps. Die Vorhersage wird im Verlauf der Reise eines Nutzers ständig aktualisiert. A
Funktion, die Sie verwenden möchten, ist die Durchschnittsgeschwindigkeit aller Autos auf Ihrem Weg in den
letzten fünf Minuten. Für das Training könnten Sie die Daten des letzten Monats verwenden. Zum Extrahieren
dieses Merkmal aus Ihren Trainingsdaten zu extrahieren, sollten Sie alle Daten in einen
Datenrahmen packen, um dieses Merkmal für mehrere Trainingsmuster gleichzeitig zu berechnen.
Während der Inferenz wird dieses Merkmal kontinuierlich in einem gleitenden Fenster berechnet. Dies
bedeutet, dass dieses Merkmal beim Training im Batch berechnet wird, während es bei der Inferenz
dieses Merkmal in einem Streaming-Prozess berechnet wird.

Zwei verschiedene Pipelines zur Verarbeitung Ihrer Daten sind eine häufige Ursache für Fehler
in der ML-Produktion. Eine Ursache für Fehler ist, dass die Änderungen in einer Pipeline nicht
Pipeline nicht korrekt in der anderen repliziert werden, was dazu führt, dass zwei Pipelines zwei unterschiedliche
Sätze von Merkmalen extrahieren. Dies ist besonders häufig der Fall, wenn die beiden Pipelines von
zwei verschiedenen Teams verwaltet werden, z. B. wenn das ML-Team die Batch-Pipeline für das Training verwaltet
verwaltet, während das Deployment-Team die Stream-Pipeline für die Inferenz verwaltet, wie in
Abbildung 7-7.

Abbildung 7-7. Zwei verschiedene Pipelines für Training und Inferenz sind eine häufige Quelle für
für Fehler bei ML in der Produktion

Abbildung 7-8 zeigt ein detaillierteres, aber auch komplexeres Merkmal der Datenpipeline
für ML-Systeme, die Online-Vorhersagen machen. Das mit Forschung bezeichnete Element in dem Kasten ist
ist das, womit die Menschen in einem akademischen Umfeld häufig konfrontiert werden.

204 | Kapitel 7: Modellbereitstellung und Vorhersagedienst

18 Shuyi Chean und Fabian Hueske, "Streaming SQL to Unify Batch & Stream Processing w/ Apache Flink
@Uber", InfoQ, https://oreil.ly/XoaNu; Yu, "Machine Learning with Flink in Weibo".

Abbildung 7-8. Eine Datenpipeline für ML-Systeme, die Online-Vorhersagen machen
Der Aufbau einer Infrastruktur zur Vereinheitlichung von Stream- und Batch-Verarbeitung ist in den letzten Jahren zu einem
den letzten Jahren ein beliebtes Thema in der ML-Gemeinschaft. Unternehmen wie Uber und
Weibo haben ihre Infrastruktur grundlegend überarbeitet, um ihre Batch- und Stream-Verarbeitungspipelines zu vereinheitlichen.
Pipelines zu vereinheitlichen, indem sie einen Stream-Prozessor wie Apache Flink verwenden.^18 Einige Unternehmen
Unternehmen verwenden Feature-Stores, um die Konsistenz zwischen den Batch-Features, die beim
Training und den bei der Vorhersage verwendeten Streaming-Features sicherzustellen. Wir werden Feature-Stores in
Kapitel 10.
Batch-Vorhersage gegenüber Online-Vorhersage | 205
19 Yu Cheng, Duo Wang, Pan Zhou, und Tao Zhang, "A Survey of Model Compression and Acceleration for
Deep Neural Networks," arXiv, June 14, 2020, https://oreil.ly/1eMho.
20 Max Jaderberg, Andrea Vedaldi, und Andrew Zisserman, "Speeding up Convolutional Neural Networks with
Low Rank Expansions," arXiv, May 15, 2014, https://oreil.ly/4Vf4s.

Modell-Komprimierung
Wir haben über eine Streaming-Pipeline gesprochen, die es einem ML-System ermöglicht, Stream-
aus den eingehenden Daten zu extrahieren und sie (fast) in Echtzeit in ein ML-Modell einzugeben.
Echtzeit. Eine Pipeline, die nahezu in Echtzeit arbeitet, reicht jedoch für Online-Vorhersagen nicht aus.
Im nächsten Abschnitt werden wir Techniken zur schnellen Inferenz für ML-Modelle erörtern.
Wenn das Modell, das Sie einsetzen möchten, zu lange braucht, um Vorhersagen zu erstellen, gibt es drei
gibt es drei Hauptansätze zur Verringerung der Inferenzlatenz: Beschleunigung der Inferenz, Verkleinerung des
Modell zu verkleinern oder die Hardware, auf der es eingesetzt wird, schneller laufen zu lassen.
Die Verkleinerung eines Modells wird als Modellkomprimierung bezeichnet, und der Prozess
zur Beschleunigung der Inferenz wird als Inferenzoptimierung bezeichnet. Ursprünglich diente die Modell
Ursprünglich diente die Modellkomprimierung dazu, dass Modelle auf Randgeräte passen. Durch die Verkleinerung von Modellen
Modelle zu verkleinern, macht sie oft schneller.
Wir werden die Inferenzoptimierung im Abschnitt "Modelloptimierung" auf Seite 216 besprechen,
und wir diskutieren die Landschaft der Hardware-Backends, die speziell für die schnellere Ausführung von ML-Modellen
speziell für die schnellere Ausführung von ML-Modellen entwickelt werden, im Abschnitt "ML in der Cloud und am Rande" auf
Seite 212. Hier werden wir die Modellkomprimierung diskutieren.
Die Zahl der Forschungsarbeiten zur Modellkomprimierung nimmt zu. Standardmäßige
Hilfsprogramme werden immer zahlreicher. Ab April 2022 hat Awesome Open Source eine Liste der "The
Top 168 Model Compression Open Source Projects", und diese Liste wird immer länger. Während
viele neue Techniken entwickelt werden, sind die vier Arten von Techniken, auf die Sie
am häufigsten anzutreffen sind: Low-Rank-Optimierung, Wissensdestillation,
Pruning und Quantisierung. Leser, die an einer umfassenden Übersicht interessiert sind, sollten
Cheng et al.'s "Survey of Model Compression and Acceleration for Deep
Neural Networks", das im Jahr 2020 aktualisiert wurde.^19
Faktorisierung mit niedrigem Rang (Low-Rank Factorization)
Die Schlüsselidee hinter der Low-Rank-Faktorisierung ist die Ersetzung hochdimensionaler Tensoren
durch niederdimensionale Tensoren zu ersetzen.^20 Ein Typ der Low-Rank-Faktorisierung sind kompakte
Faltungsfilter, bei denen die überparametrisierten (mit zu vielen Parametern)
überparametrisierten (zu viele Parameter aufweisenden) Faltungsfilter durch kompakte Blöcke ersetzt werden, um sowohl die Anzahl der Parameter zu
Parameter zu reduzieren und die Geschwindigkeit zu erhöhen.
206 | Kapitel 7: Modellbereitstellung und Vorhersagedienst
21 Forrest N. Iandola, Song Han, Matthew W. Moskewicz, Khalid Ashraf, William J. Dally, und Kurt Keutzer,
"SqueezeNet: AlexNet-Level Accuracy with 50x Fewer Parameters and <0.5MB Model Size," arXiv, November
4, 2016, https://oreil.ly/xs3mi.
22 Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco
Andreetto, und Hartwig Adam, "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision
Applications," arXiv, April 17, 2017, https://oreil.ly/T84fD.

Beispielsweise erreicht SqueezeNets durch die Verwendung einer Reihe von Strategien, darunter das Ersetzen der 3 × 3-Faltung
durch 1 × 1-Faltung, erreicht SqueezeNets bei ImageNet eine Genauigkeit auf AlexNet-Niveau
mit 50 Mal weniger Parametern.^21
In ähnlicher Weise zerlegt MobileNets die Standardfaltung der Größe K × K × C in eine
tiefenweise Faltung (K × K × 1) und eine punktweise Faltung (1 × 1 × C), wobei K
für die Kernelgröße und C für die Anzahl der Kanäle steht. Dies bedeutet, dass jede neue
Faltung nur K^2 + C anstelle von K^2 C Parameter verwendet. Wenn K = 3 ist, bedeutet dies eine acht
bis neunfache Reduzierung der Anzahl der Parameter (siehe Abbildung 7-9).^22
Abbildung 7-9. Kompakte Faltungsfilter in MobileNets. Die Standard-Faltungsfilter
Filter in (a) werden durch die tiefenweise Faltung in (b) und die punktweise Faltung
in (c) ersetzt, um einen tiefenmäßig separierbaren Filter zu bilden. Quelle: Angepasst an ein Bild von Howard
et al.
Diese Methode wurde verwendet, um kleinere Modelle mit einer erheblichen Beschleunigung
im Vergleich zu Standardmodellen. Allerdings ist sie in der Regel spezifisch für bestimmte Arten
Modelle (z. B. sind kompakte Faltungsfilter spezifisch für neuronale Faltungsfilter).
Modellkomprimierung | 207
23 Geoffrey Hinton, Oriol Vinyals, und Jeff Dean, "Distilling the Knowledge in a Neural Network", arXiv, March
9, 2015, https://oreil.ly/OJEPW.
24 Victor Sanh, Lysandre Debut, Julien Chaumond, und Thomas Wolf, "DistilBERT, a Distilled Version of BERT:
Smaller, Faster, Cheaper and Lighter," arXiv, October 2, 2019, https://oreil.ly/mQWBv.
25 Daher auch der Name "Pruning".

Netze) und erfordert viel architektonisches Wissen für die Entwicklung, so dass es noch nicht
noch nicht auf viele Anwendungsfälle anwendbar.
Wissensdestillation
Wissensdestillation ist eine Methode, bei der ein kleines Modell (Schüler) trainiert wird, um
ein größeres Modell oder ein Ensemble von Modellen (Lehrer) nachzubilden. Das kleinere Modell ist das, was
das Sie einsetzen werden. Auch wenn der Student oft nach einem vorher trainierten Lehrer trainiert wird, können beide
können auch gleichzeitig trainiert werden.^23 Ein Beispiel für ein destilliertes Netz, das in der
Beispiel für ein destilliertes Netz, das in der Produktion eingesetzt wird, ist DistilBERT, das die Größe eines BERT-Modells um 40 % reduziert, während
97% seiner Sprachverständnisfähigkeiten beibehält und 60% schneller ist.^24
Der Vorteil dieses Ansatzes ist, dass er unabhängig von den architektonischen
Unterschiede zwischen dem Lehrernetz und dem Schülernetz. Zum Beispiel kann man
ein Random Forest als Schüler und ein Transformator als Lehrer. Der Nachteil
dieses Ansatzes ist, dass er in hohem Maße von der Verfügbarkeit eines Lehrernetzwerks abhängig ist.
Wenn Sie ein vorab trainiertes Modell als Lehrermodell verwenden, erfordert das Training des Schülernetzwerks
weniger Daten und ist wahrscheinlich auch schneller. Wenn Sie jedoch keinen Lehrer zur Verfügung haben
nicht zur Verfügung steht, müssen Sie zunächst ein Lehrernetzwerk trainieren, bevor Sie ein Schülernetzwerk trainieren können,
und das Trainieren eines Lehrernetzwerks erfordert viel mehr Daten und nimmt mehr Zeit
zu trainieren. Diese Methode ist auch empfindlich gegenüber Anwendungen und Modellarchitekturen, und
hat sich daher in der Produktion noch nicht durchgesetzt.
Pruning
Pruning ist eine Methode, die ursprünglich für Entscheidungsbäume verwendet wurde und bei der Abschnitte eines Baums entfernt werden, die
eines Baums entfernt werden, die für die Klassifizierung unkritisch und redundant sind.^25 Als neuronale Netze eine
verbreitete sich die Erkenntnis, dass neuronale Netze zu stark parametrisiert sind
und begannen, Wege zu finden, um die durch die zusätzlichen Parameter verursachte Arbeitslast zu verringern.
Pruning hat im Zusammenhang mit neuronalen Netzen zwei Bedeutungen. Die eine ist das Entfernen
die Entfernung ganzer Knoten eines neuronalen Netzes, was eine Änderung seiner Architektur und eine Verringerung der
Anzahl von Parametern zu reduzieren. Die häufigere Bedeutung besteht darin, Parameter zu finden
die für die Vorhersage am wenigsten nützlich sind, und setzt sie auf 0. In diesem Fall wird durch Pruning nicht die
Anzahl der Parameter, sondern nur die Anzahl der Parameter, die nicht Null sind. Die Architektur des
des neuronalen Netzes bleibt gleich. Dies hilft bei der Reduzierung der Größe eines Modells
denn durch Pruning wird ein neuronales Netz spärlicher, und eine spärliche Architektur benötigt in der Regel
weniger Speicherplatz benötigt als eine dichte Struktur. Experimente zeigen, dass Pruning
208 | Kapitel 7: Modellbereitstellung und Vorhersagedienst
26 Jonathan Frankle und Michael Carbin, "Die Lotterielos-Hypothese: Finding Sparse, Trainable Neural
Networks", ICLR 2019, https://oreil.ly/ychdl.
27 Davis Blalock, Jose Javier Gonzalez Ortiz, Jonathan Frankle, und John Guttag, "What Is the State of Neural
Network Pruning?", arXiv, March 6, 2020, https://oreil.ly/VQsC3.
28 Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, und Trevor Darrell, "Rethinking the Value of Network
Pruning", arXiv, 5. März 2019, https://oreil.ly/mB4IZ.
29 Michael Zhu und Suyog Gupta, "To Prune, or Not to Prune: Exploring the Efficacy of Pruning for Model
Compression," arXiv, November 13, 2017, https://oreil.ly/KBRjy.
30 Matthieu Courbariaux, Yoshua Bengio, und Jean-Pierre David, "BinaryConnect: Training Deep Neural Net-
works with Binary Weights During Propagations," arXiv, November 2, 2015, https://oreil.ly/Fwp2G; Moham-
mad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi, "XNOR-Net: ImageNet Classification
Using Binary Convolutional Neural Networks," arXiv, August 2, 2016, https://oreil.ly/gr3Ay.
31 Alan Boyle, Taylor Soper, und Todd Bishop, "Exclusive: Apple Acquires Xnor.ai, Edge AI Spin-out from Paul
Allen's AI2, for Price in $200M Range," GeekWire, January 15, 2020, https://oreil.ly/HgaxC.

Techniken können die Anzahl der von Null verschiedenen Parameter der trainierten Netze um über
90% reduzieren, was den Speicherbedarf senkt und die Rechenleistung der
Inferenz ohne Beeinträchtigung der Gesamtgenauigkeit.^26 In Kapitel 11 werden wir erörtern, wie
Pruning zu Verzerrungen in Ihrem Modell führen kann.
Es ist zwar allgemein anerkannt, dass Pruning funktioniert,^27 aber es gab viele Diskussionen
über den tatsächlichen Wert des Prunings. Liu et al. argumentierten, dass der Hauptwert des Pruning nicht
in den vererbten "wichtigen Gewichten" liegt, sondern in der beschnittenen Architektur selbst.^28 In einigen
Fällen kann Pruning als Paradigma für die Architektursuche nützlich sein, und die beschnittene
Architektur sollte von Grund auf als dichtes Modell neu trainiert werden. Allerdings haben Zhu et al.
zeigten jedoch, dass das große spärliche Modell nach dem Pruning besser abschneidet als das neu trainierte dichte
Gegenstück übertraf.^29
Quantisierung
Die Quantisierung ist die allgemeinste und am häufigsten verwendete Methode zur Modellkomprimierung.
Sie ist einfach durchzuführen und lässt sich für alle Aufgaben und Architekturen anwenden.
Durch Quantisierung wird die Größe eines Modells reduziert, indem weniger Bits zur Darstellung seiner Parameter verwendet werden.
Die meisten Softwarepakete verwenden standardmäßig 32 Bits zur Darstellung einer Fließkommazahl (einfache
Gleitkommazahl). Wenn ein Modell 100M Parameter hat und jeder 32 Bits benötigt
benötigt, nimmt es 400 MB in Anspruch. Wenn wir 16 Bits zur Darstellung einer Zahl verwenden, können wir den
Speicherplatzbedarf um die Hälfte. Die Verwendung von 16 Bits zur Darstellung einer Fließkommazahl wird als halbe Präzision bezeichnet.
Anstelle von Gleitkommazahlen können Sie ein Modell vollständig in Ganzzahlen darstellen; jede Ganzzahl
benötigt nur 8 Bits zur Darstellung. Diese Methode ist auch als "Festkomma" bekannt. Im Extremfall
Extremfall wurde versucht, jedes Gewicht mit 1 Bit darzustellen (binäre
gewichtete neuronale Netze), z. B. BinaryConnect und XNOR-Net.^30 Die Autoren des
XNOR-Net haben Xnor.ai ausgegründet, ein Startup, das sich auf die Modellkompression konzentriert. Unter
Anfang 2020 wurde es von Apple für 200 Millionen Dollar übernommen.^31
Modellkomprimierung | 209
32 Ab Oktober 2020 trainiert das quantisierungsbewusste Training von TensorFlow nicht mehr Modelle mit Gewichten in
Gewichten in unteren Bits, sondern sammelt Statistiken, die für die Quantisierung nach dem Training verwendet werden.
33 Chip Huyen, Igor Gitman, Oleksii Kuchaiev, Boris Ginsburg, Vitaly Lavrukhin, Jason Li, Vahid Noroozi,
und Ravi Gadde, "Mixed Precision Training for NLP and Speech Recognition with OpenSeq2Seq," NVIDIA
Devblogs, October 9, 2018, https://oreil.ly/WDT1l. Das ist mein Beitrag!
34 Shibo Wang und Pankaj Kanwar, "BFloat16: The Secret to High Performance on Cloud TPUs," Google Cloud
Blog, 23. August 2019, https://oreil.ly/ZG5p0.
35 Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, und Yoshua Bengio, "Quantized Neural
Networks: Training Neural Networks with Low Precision Weights and Activations," Journal of Machine
Learning Research 18 (2018): 1-30; Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang,
Andrew Howard, Hartwig Adam, und Dmitry Kalenichenko, "Quantization and Training of Neural Networks
for Efficient Integer-Arithmetic-Only Inference," arXiv, December 15, 2017, https://oreil.ly/sUuMT.

Die Quantisierung verringert nicht nur den Speicherbedarf, sondern verbessert auch die Rechengeschwindigkeit
Geschwindigkeit. Erstens können wir so unsere Stapelgröße erhöhen. Zweitens beschleunigt eine geringere Genauigkeit die
Berechnung, was die Trainingszeit und die Inferenzlatenz weiter reduziert. Betrachten Sie
die Addition von zwei Zahlen. Wenn wir die Addition Bit für Bit durchführen, und jedes Bit benötigt
x Nanosekunden benötigt, dauert die Addition bei 32-Bit-Zahlen 32x Nanosekunden, bei 16-Bit-Zahlen jedoch nur 16x Nanosekunden.
onds für 16-Bit-Zahlen.
Die Quantisierung hat auch Nachteile. Die Verringerung der Anzahl von Bits zur Darstellung
der Zahlen bedeutet, dass Sie einen kleineren Wertebereich darstellen können. Für Werte
die außerhalb dieses Bereichs liegen, müssen Sie aufrunden und/oder skalieren, um sie in den Bereich zu bekommen.
Die Rundung von Zahlen führt zu Rundungsfehlern, und kleine Rundungsfehler können zu
großen Leistungsänderungen führen. Außerdem besteht die Gefahr, dass Sie Ihre Zahlen so runden/skalieren, dass sie
zu runden/zu skalieren und auf 0 zu setzen. Effizientes Runden und Skalieren ist nicht trivial
nicht einfach zu implementieren, aber zum Glück haben die meisten Frameworks dies bereits integriert.
Die Quantisierung kann entweder während des Trainings erfolgen (quantisierungsbewusstes Training),^32
wobei die Modelle in niedrigerer Genauigkeit trainiert werden, oder nach dem Training, wobei die Modelle
mit einfacher Genauigkeit trainiert und dann für die Inferenz quantisiert werden. Die Verwendung von
Quantisierung während des Trainings bedeutet, dass Sie weniger Speicher für jeden Parameter verwenden können,
Dadurch können Sie größere Modelle auf der gleichen Hardware trainieren.
In letzter Zeit wird das Training mit niedriger Genauigkeit immer beliebter, da es von den meisten modernen Trainingsgeräten unterstützt wird.
modernster Trainingshardware. NVIDIA hat Tensor Cores eingeführt, Verarbeitungseinheiten
die Training mit gemischter Genauigkeit unterstützen.^33 Google TPUs (Tensor Processing Units) unterstützen auch
unterstützen auch das Training mit Bfloat16 (16-Bit Brain Floating Point Format), das das
Unternehmen als "das Geheimnis für hohe Leistung auf Cloud TPUs" bezeichnete.^34 Das Training in
Festkomma ist noch nicht so populär, hat aber vielversprechende Ergebnisse gebracht.^35
Die Festkomma-Inferenz hat sich zu einem Standard in der Branche entwickelt. Einige Edge Devi-
ces unterstützen nur Festkomma-Inferenz. Die beliebtesten Frameworks für On-Device
ML-Inferenz - Googles TensorFlow Lite, Facebooks PyTorch Mobile, NVIDIAs
TensorRT bieten eine kostenlose Quantisierung nach dem Training mit ein paar Zeilen Code.
210 | Kapitel 7: Modellbereitstellung und Vorhersagedienst
36 Quoc Le und Kip Kaehler, "How We Scaled Bert To Serve 1+ Billion Daily Requests on CPUs," Roblox, May
27, 2020, https://oreil.ly/U01Uj.

Fallstudie
Um ein besseres Verständnis dafür zu bekommen, wie Modelle in der Produktion optimiert werden können, sehen Sie sich
eine faszinierende Fallstudie von Roblox über die Skalierung von BERT auf über 1 Milliarde
täglicher Anfragen auf CPUs.^36 Für viele ihrer NLP-Dienste mussten sie über
25.000 Inferenzen pro Sekunde bei einer Latenzzeit von unter 20 ms verarbeiten, wie in Abbildung 7-10 dargestellt.
Sie begannen mit einem großen BERT-Modell mit fester Eingabeform, ersetzten dann BERT
durch DistilBERT und die feste Formeingabe durch eine dynamische Formeingabe ersetzt und schließlich quan-
quan- tisiert.
Abbildung 7-10. Latenzverbesserung durch verschiedene Modellkompressionsmethoden. Quelle:
Adaptiert von einem Bild von Le und Kaehler
Die größte Leistungssteigerung wurde durch die Quantisierung erzielt. Die Umwandlung von 32-Bit
Fließkommazahlen in 8-Bit-Ganzzahlen wird die Latenzzeit um das 7-fache verringert und der Durchsatz
um das 8-fache.
Die Ergebnisse scheinen sehr vielversprechend zu sein, um die Latenzzeit zu verbessern; sie sollten jedoch
sollten jedoch mit Vorsicht genossen werden, da nach jeder Leistungsverbesserung keine Änderungen der
jeder Leistungsverbesserung.
Modellkomprimierung | 211
37 Amir Efrati und Kevin McLaughlin, "As AWS Use Soars, Companies Surprised by Cloud Bills", The Infor-
mation, February 25, 2019, https://oreil.ly/H9ans; Mats Bauer, "How Much Does Netflix Pay Amazon Web
Services Each Month?" Quora, 2020, https://oreil.ly/HtrBk.
38 "2021 State of Cloud Cost Report", Anodot, https://oreil.ly/5ZIJK.
39 "Burnt $72K Testing Firebase and Cloud Run and Almost Went Bankrupt," Hacker News, Dezember 10, 2020,
https://oreil.ly/vsHHC; "How to Burn the Most Money with a Single Click in Azure", Hacker News, März
29, 2020, https://oreil.ly/QvCiI. Wie Unternehmen auf hohe Cloud-Rechnungen reagieren, erörtern wir ausführlicher im
Abschnitt "Public Cloud vs. private Rechenzentren" auf Seite 300.

ML in der Cloud und am Rande der Welt
Eine weitere Entscheidung, die Sie treffen sollten, ist der Ort, an dem die Berechnungen Ihres Modells
stattfinden soll: in der Cloud oder am Rande. In der Cloud bedeutet, dass ein großer Teil der Berechnungen
der Berechnung in der Cloud erfolgt, entweder in öffentlichen oder privaten Clouds. Am Rande
bedeutet, dass ein großer Teil der Berechnungen auf Endgeräten - wie Browsern, Telefonen
Telefonen, Laptops, Smartwatches, Autos, Überwachungskameras, Robotern und eingebetteten Geräten,
FPGAs (Field Programmable Gate Arrays) und ASICs (anwendungsspezifische integrierte
Schaltungen), die auch als Edge Devices bezeichnet werden.
Am einfachsten ist es, Ihr Modell zu verpacken und es über einen verwalteten Cloud
Cloud-Dienst wie AWS oder GCP bereitzustellen, und so gehen viele Unternehmen vor, wenn sie
in ML einsteigen. Cloud-Dienste haben einen unglaublichen Beitrag dazu geleistet, dass
Unternehmen, ML-Modelle in die Produktion zu bringen.
Allerdings hat die Cloud-Bereitstellung auch viele Nachteile. Der erste sind die Kosten. ML
Modelle können sehr rechenintensiv sein, und Rechenleistung ist teuer. Selbst im Jahr 2018,
gaben große Unternehmen wie Pinterest, Infor und Intuit bereits Hunderte von
Millionen von Dollar für Cloud-Rechnungen aus.^37 Diese Zahl für kleine und mittlere
Unternehmen zwischen 50.000 und 2 Millionen Dollar pro Jahr liegen.^38 Ein Fehler im Umgang mit Cloud
Dienste kann Startups in den Bankrott treiben.^39
Da die Cloud-Rechnungen steigen, suchen immer mehr Unternehmen nach Möglichkeiten, ihre
ihre Berechnungen auf Edge-Geräte zu verlagern. Je mehr Berechnungen am Rand durchgeführt werden, desto
desto weniger wird die Cloud benötigt, und desto weniger müssen sie für Server bezahlen.
Abgesehen von der Kostenkontrolle gibt es viele Eigenschaften, die das Edge-Computing
Computing attraktiv machen. Die erste ist, dass Ihre Anwendungen dort ausgeführt werden können, wo Cloud
Computing nicht möglich ist. Wenn Ihre Modelle in öffentlichen Clouds laufen, sind sie auf stabile
Internetverbindungen, um Daten in die Cloud und zurück zu senden. Edge Computing ermöglicht Ihren
Modelle in Situationen funktionieren, in denen es keine Internetverbindungen gibt oder in denen die
Verbindungen unzuverlässig sind, wie etwa in ländlichen Gebieten oder Entwicklungsländern. Ich habe mit
Ich habe mit mehreren Unternehmen und Organisationen zusammengearbeitet, die striktes Internetverbot praktizieren.
was bedeutet, dass die Anwendungen, die wir ihnen verkaufen wollten, nicht auf Internet
Verbindungen angewiesen sind.
212 | Kapitel 7: Modellbereitstellung und Vorhersagedienst
40 "Nearly 80% of Companies Experienced a Cloud Data Breach in Past 18 Months", Security, June 5, 2020,
https://oreil.ly/gA1am.
41 Siehe Folie #53, Vorlesung 8 von CS 329S: Deployment - Prediction Service, 2022, https://oreil.ly/cXTou.
42 "Internet of Things (IoT) and Non-IoT Active Device Connections Worldwide from 2010 to 2025", Statista,
https://oreil.ly/BChLN.

Zweitens: Wenn sich Ihre Modelle bereits auf den Geräten der Verbraucher befinden, können Sie sich weniger Sorgen
über die Netzwerklatenz. Die erforderliche Datenübertragung über das Netz (Senden von Daten an
(Senden von Daten an das Modell in der Cloud zur Erstellung von Vorhersagen und
Nutzer) könnte einige Anwendungsfälle unmöglich machen. In vielen Fällen ist die Netzwerklatenz ein
ein größerer Engpass als die Latenzzeit bei den Schlussfolgerungen. So könnte man zum Beispiel die
Inferenzlatenz von ResNet-50 von 30 ms auf 20 ms reduzieren, aber die Netzwerklatenz kann
aber die Netzwerklatenz kann bis zu Sekunden betragen, je nachdem, wo Sie sich befinden und welche Dienste Sie nutzen wollen.
Die Verlagerung der Modelle an den Rand des Netzes ist auch beim Umgang mit sensiblen Nutzerdaten interessant.
Daten. ML in der Cloud bedeutet, dass Ihre Systeme möglicherweise Benutzerdaten über Netzwerke senden müssen.
Netze übertragen müssen, was sie anfällig für Abhörmaßnahmen macht. Cloud Computing bedeutet auch oft
bedeutet auch, dass die Daten vieler Nutzer am selben Ort gespeichert werden, was bedeutet, dass eine Verletzung
viele Menschen betreffen kann. "Fast 80 % der Unternehmen hatten in den letzten 18 Monaten einen Verstoß gegen Cloud-Daten.
18 Monaten", so die Zeitschrift Security.^40
Edge Computing erleichtert die Einhaltung von Vorschriften wie der GDPR, wie Nutzerdaten
Nutzerdaten übertragen oder gespeichert werden können. Edge Computing kann zwar die Bedenken hinsichtlich des
Bedenken hinsichtlich des Datenschutzes verringern, sie aber nicht völlig ausräumen. In einigen Fällen kann Edge Computing
Angreifern den Diebstahl von Nutzerdaten erleichtern, da sie das Gerät einfach mitnehmen können.
mitnehmen.
Um Berechnungen an den Rand zu verlagern, müssen die Randgeräte leistungsfähig genug sein, um
Berechnungen zu bewältigen, über genügend Speicher verfügen, um ML-Modelle zu speichern und
in den Speicher zu laden, und sie müssen über eine ausreichende Batterie verfügen oder an eine Energiequelle angeschlossen sein, um die
die Anwendung über einen angemessenen Zeitraum zu betreiben. Die Ausführung eines BERT in voller Größe auf
Telefon, sofern Ihr Telefon BERT-fähig ist, ist ein sehr schneller Weg, um seine
Akku.
Aufgrund der vielen Vorteile, die das Edge-Computing gegenüber dem Cloud-Computing bietet, sind die Unternehmen
Aufgrund der vielen Vorteile, die Edge Computing gegenüber Cloud Computing bietet, befinden sich die Unternehmen in einem Wettlauf um die Entwicklung von Edge-Geräten, die für verschiedene ML-Anwendungsfälle optimiert sind.
Etablierte Unternehmen wie Google, Apple und Tesla haben angekündigt, dass sie
angekündigt, ihre eigenen Chips herzustellen. Inzwischen haben ML-Hardware-Startups Milliarden von Dollar
Milliarden Dollar aufgebracht, um bessere KI-Chips zu entwickeln.^41 Es wird prognostiziert, dass bis 2025 die Zahl der aktiven
Edge-Geräte weltweit über 30 Milliarden sein wird.^42
Bei so vielen neuen Angeboten für Hardware, auf der ML-Modelle ausgeführt werden können, stellt sich eine Frage:
Wie bringen wir unser Modell auf beliebiger Hardware effizient zum Laufen? Im folgenden
Abschnitt wird erörtert, wie ein Modell kompiliert und optimiert werden kann, um es auf einer bestimmten Hardware laufen zu lassen.
ML in der Cloud und am Rande der Welt | 213
43 Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Meghan Cowan, Haichen Shen, et
al., "TVM: An Automated End-to-End Optimizing Compiler for Deep Learning," arXiv, February 12, 2018,
https://oreil.ly/vGnkW.

Hardware-Backend. Dabei werden wir wichtige Konzepte vorstellen, die Ihnen bei der
die Ihnen bei der Handhabung von Modellen am Rande begegnen können, einschließlich Zwischenrepräsentationen
(IRs) und Compiler.
Kompilieren und Optimieren von Modellen für Edge-Geräte
Damit ein Modell, das mit einem bestimmten Framework, wie TensorFlow oder PyTorch, erstellt wurde, auf einem
auf einem Hardware-Backend laufen, muss dieses Framework vom Hardware-Anbieter unterstützt
Anbieter unterstützt werden. Obwohl die TPUs beispielsweise im Februar 2018 veröffentlicht wurden, war es
wurde PyTorch erst im September 2020 auf TPUs unterstützt. Wenn man vorher
wenn man eine TPU verwenden wollte, ein Framework verwenden, das von TPUs unterstützt wurde.
Die Bereitstellung von Unterstützung für ein Framework auf einem Hardware-Backend ist zeitaufwändig und
technikintensiv. Das Mapping von ML-Workloads auf ein Hardware-Backend erfordert
das Design dieser Hardware zu verstehen und auszunutzen, und verschiedene Hardware
Hardware-Backends haben unterschiedliche Speicherlayouts und Rechenprimitive, wie in
Abbildung 7-11.
Abbildung 7-11. Verschiedene Berechnungsprimitive und Speicherlayouts für CPU, GPU und TPU.
Quelle: Angepasst an eine Abbildung von Chen et al.^43
214 | Kapitel 7: Modellimplementierung und Vorhersagedienst
44 Heutzutage verfügen viele CPUs über Vektorbefehle und einige GPUs über Tensorkerne, die
zweidimensional sind.

Zum Beispiel war das Rechenprimitiv von CPUs früher eine Zahl (skalar) und
das Rechenprimitiv von GPUs ein eindimensionaler Vektor, während das
Berechnungsprimitiv von TPUs ein zweidimensionaler Vektor (Tensor) ist.^44 Die Durchführung eines
Faltungsoperators ist bei eindimensionalen Vektoren ganz anders als bei
als bei zweidimensionalen Vektoren. In ähnlicher Weise müssen Sie die unterschiedlichen L1,
L2- und L3-Layouts und Puffergrößen berücksichtigen, um sie effizient zu nutzen.
Aufgrund dieser Herausforderung konzentrieren sich die Entwickler von Frameworks in der Regel auf die Unterstützung
nur eine Handvoll Hardware der Server-Klasse zu unterstützen, und die Hardware-Anbieter bieten ihre
eigene Kernelbibliotheken für eine begrenzte Anzahl von Frameworks anzubieten. Der Einsatz von ML-Modellen auf neuer
Hardware erfordert erheblichen manuellen Aufwand.
Anstatt neue Compiler und Bibliotheken für jedes neue Hardware-Backend zu entwickeln,
wie wäre es, wenn wir einen Mittelsmann schaffen, der Frameworks und Plattformen miteinander verbindet? Framework
Framework-Entwickler müssen nicht mehr jede Art von Hardware unterstützen; sie müssen lediglich
ihren Framework-Code in diesen Vermittler übersetzen. Hardware-Anbieter können dann
einen Vermittler statt mehrerer Frameworks unterstützen.
Diese Art von "Vermittler" wird als Zwischenrepräsentation (IR) bezeichnet. IRs sind das Herzstück
Kern der Arbeitsweise von Compilern. Aus dem ursprünglichen Code eines Modells erzeugen die Compiler
eine Reihe von High-Level-IRs und Low-Level-IRs, bevor sie den Code nativ für ein Hardware-Backend erzeugen
Backend generiert, so dass er auf diesem Hardware-Backend ausgeführt werden kann (siehe Abbildung 7-12).
Abbildung 7-12. Eine Reihe von High-Level-IRs und Low-Level-IRs zwischen dem ursprünglichen Modellcode und
Maschinencode, der auf einem bestimmten Hardware-Backend ausgeführt werden kann
ML in der Cloud und am Rande der Welt | 215
45 Shoumik Palkar, James Thomas, Deepak Narayanan, Pratiksha Thaker, Rahul Palamuttam, Parimajan Negi,
Anil Shanbhag, et al., "Evaluating End-to-End Optimization for Data Analytics Applications in Weld," Pro-
ceedings of the VLDB Endowment 11, no. 9 (2018): 1002-15, https://oreil.ly/ErUIo.

Dieser Prozess wird auch als "lowering" bezeichnet, d. h. Sie "senken" Ihren High-Level-Rahmencode
in Hardware-nativen Code auf niedriger Ebene. Es ist keine Übersetzung, weil es keine Eins-zu-Eins
Abbildung zwischen ihnen gibt.
High-Level-IRs sind normalerweise Berechnungsgraphen Ihrer ML-Modelle. Ein Berechnungs
Graph ist ein Graph, der die Reihenfolge beschreibt, in der Ihre Berechnungen ausgeführt werden.
Interessierte Leser können über Berechnungsgraphen in PyTorch und TensorFlow lesen.
Modell-Optimierung
Nachdem Sie Ihren Code "heruntergeschraubt" haben, um Ihre Modelle auf der Hardware Ihrer Wahl auszuführen
Hardware Ihrer Wahl auszuführen, ist ein Problem, auf das Sie stoßen könnten, die Leistung. Der generierte Maschinencode
kann zwar auf einem Hardware-Backend ausgeführt werden, aber möglicherweise nicht
effizient. Der generierte Code nutzt möglicherweise nicht die Vorteile der Datenlokalität und der Hardware
Caches nutzen, oder er kann erweiterte Funktionen wie Vektor- oder Paralleloperationen nicht nutzen
die den Code beschleunigen könnten.
Ein typischer ML-Workflow besteht aus vielen Frameworks und Bibliotheken. Ein Beispiel,
Sie könnten pandas/dask/ray verwenden, um Merkmale aus Ihren Daten zu extrahieren. Sie verwenden vielleicht
NumPy für die Vektorisierung. Sie könnten ein vortrainiertes Modell wie Hugging
Face's Transformers verwenden, um Merkmale zu erzeugen, und dann Vorhersagen mit einem Ensemble von
Modellen, die mit verschiedenen Frameworks wie Sklearn, TensorFlow oder LightGBM erstellt wurden.
Auch wenn einzelne Funktionen in diesen Frameworks optimiert werden können, gibt es
gibt es wenig bis gar keine Optimierung zwischen den Frameworks. Eine naive Art und Weise des Verschiebens von Daten über diese
Funktionen zu übertragen, kann den gesamten Workflow um eine Größenordnung verlangsamen.
Arbeitsablauf. Eine Studie von Forschern des DAWN-Labors in Stanford ergab, dass typische ML
Workloads mit NumPy, Pandas und TensorFlow in einem Thread 23 Mal langsamer laufen
im Vergleich zu handoptimiertem Code.^45
In vielen Unternehmen ist es üblich, dass Datenwissenschaftler und ML-Ingenieure
Modelle entwickeln, die in der Entwicklung gut zu funktionieren scheinen. Wenn diese Modelle jedoch
Wenn diese Modelle jedoch eingesetzt werden, erweisen sie sich als zu langsam, sodass die Unternehmen Optimierungs
Optimierungsingenieure ein, um ihre Modelle für die Hardware, auf der sie laufen, zu optimieren. Ein
Beispiel für eine Stellenbeschreibung für Optimierungsingenieure bei Mythic folgt:
216 | Kapitel 7: Modellbereitstellung und Vorhersagedienst
Diese Vision kommt im AI-Engineering-Team zusammen, wo unser Fachwissen genutzt wird, um
KI-Algorithmen und -Modelle zu entwickeln, die für unsere Hardware optimiert sind, aber auch um
Mythic's Hardware- und Compiler-Teams zu beraten.
Das AI Engineering Team trägt maßgeblich zu Mythic bei durch:
Entwicklung von Quantisierungs- und Robustheits-KI-Umschulungstools
Erforschung neuer Funktionen für unseren Compiler, die die Anpassungsfähigkeit neuronaler
Netzwerke nutzen
Entwicklung neuer neuronaler Netze, die für unsere Hardware-Produkte optimiert sind
Zusammenarbeit mit internen und externen Kunden, um deren Entwicklungsanforderungen zu erfüllen
Optimierungsingenieure sind schwer zu finden und teuer einzustellen, da sie
da sie über Fachwissen sowohl in ML als auch in Hardware-Architekturen verfügen müssen. Optimierung von Compilern
(Compiler, die auch Ihren Code optimieren) sind eine alternative Lösung, denn sie können
den Prozess der Optimierung von Modellen automatisieren können. Bei der Umwandlung von ML-Modell
Code in Maschinencode umzuwandeln, können die Compiler den Berechnungsgraphen Ihres ML
Modells und die Operatoren, aus denen es besteht - Faltung, Schleifen, Kreuzentropie - und finden
einen Weg finden, es zu beschleunigen.

Es gibt zwei Möglichkeiten, Ihre ML-Modelle zu optimieren: lokal und global. Lokal ist, wenn
Sie einen Operator oder einen Satz von Operatoren Ihres Modells optimieren. Global ist, wenn Sie
den gesamten Berechnungsgraphen von Ende zu Ende optimieren.

Es gibt standardmäßige lokale Optimierungstechniken, die Ihr Modell bekanntermaßen beschleunigen
Modell zu beschleunigen. Die meisten davon sorgen dafür, dass Dinge parallel laufen oder der Speicherzugriff auf
Chips. Hier sind vier der gängigen Techniken:

Vektorisierung
Eine Schleife oder eine geschachtelte Schleife wird nicht einzeln ausgeführt, sondern es werden
mehrere im Speicher zusammenhängende Elemente gleichzeitig aus, um die durch
die durch Daten-E/A verursacht wird.

Parallelisierung
Bei einem Eingabe-Array (oder n-dimensionalen Array) wird dieses in verschiedene, unabhängige
chunks auf und führen die Operation an jedem chunk einzeln durch.

ML in der Cloud und am Rande der Welt | 217
46 Eine hilfreiche Visualisierung von Schleifenkacheln finden Sie auf Folie 33 der Präsentation "Access to Caches" von Colfax Research
und Speicher", Sitzung 10 des Vortrags Programming and Optimization for Intel Architecture: Hands-on Work-
Workshop-Reihe. Die gesamte Reihe ist verfügbar unter https://oreil.ly/hT1g4.
47 Matthias Boehm, "Architecture of ML Systems 04 Operator Fusion and Runtime Adaptation", Technische Universität Graz
of Technology, April 5, 2019, https://oreil.ly/py43J.

Schleifen-Kacheln^46
Ändern Sie die Reihenfolge des Datenzugriffs in einer Schleife, um das Speicherlayout der Hardware zu nutzen
und den Cache zu nutzen. Diese Art der Optimierung ist hardwareabhängig. Ein gutes Zugriffsmuster
Zugriffsmuster auf CPUs ist kein gutes Zugriffsmuster auf GPUs.
Operator-Fusion
Verschmelzung mehrerer Operatoren zu einem einzigen, um redundante Speicherzugriffe zu vermeiden. Zum Beispiel
Beispiel: Zwei Operationen mit demselben Array erfordern zwei Schleifen über dieses Array. In einem
verschmolzenen Fall ist es nur eine Schleife. Abbildung 7-13 zeigt ein Beispiel für eine Operatorfusion.
Abbildung 7-13. Ein Beispiel für eine Operatorfusion. Quelle: Angepasst an ein Bild von
Matthias Böhm^47
Um eine viel größere Beschleunigung zu erreichen, müssen Sie Strukturen auf höherer Ebene nutzen
des Berechnungsgraphen nutzen. Zum Beispiel kann ein neuronales Faltungsnetz mit dem
Berechnungsgraphen vertikal oder horizontal fusioniert werden, um den Speicherzugriff zu
und das Modell zu beschleunigen, wie in Abbildung 7-14 gezeigt.
218 | Kapitel 7: Modellimplementierung und Vorhersagedienst
48 Shashank Prasanna, Prethvi Kashinkunti, und Fausto Milletari, "TensorRT 3: Schnellere TensorFlow-Inferenz und
Volta Support," NVIDIA Developer, December 4, 2017, https://oreil.ly/d9h98. CBR steht für "convolution,
bias, and ReLU".

Abbildung 7-14. Vertikale und horizontale Verschmelzung des Berechnungsgraphen eines Faltungs
neuronalen Netzes. Quelle: Adaptiert von einem Bild des TensorRT-Teams^48
ML in der Cloud und am Rande der Welt | 219
49 Das ist auch der Grund, warum Sie nicht zu viel in Benchmarking-Ergebnisse wie die von MLPerf hineininterpretieren sollten. Ein beliebtes
Modell, das auf einer bestimmten Art von Hardware sehr schnell läuft, bedeutet nicht, dass ein beliebiges Modell auf dieser
Hardware läuft. Es könnte einfach sein, dass dieses Modell überoptimiert ist.

Verwendung von ML zur Optimierung von ML-Modellen
Wie im vorherigen Abschnitt über die vertikale und horizontale Fusion für ein konvo- lutionelles neuronales Netz angedeutet, gibt es viele
lutionale neuronale Netz angedeutet hat, gibt es viele Möglichkeiten, einen gegebenen Berechnungs
Graphen. Wenn beispielsweise drei Operatoren A, B und C gegeben sind, kann man A mit B fusionieren, B mit C fusionieren
mit C verschmelzen, oder A, B und C zusammen verschmelzen.
Traditionell stellen Framework- und Hardware-Anbieter Optimierungsingenieure ein, die
die aufgrund ihrer Erfahrung Heuristiken entwickeln, wie man den Berechnungsgraphen eines Modells am besten
Berechnungsgraphen eines Modells am besten ausführt. NVIDIA könnte zum Beispiel einen Ingenieur oder ein
Team von Ingenieuren, die sich ausschließlich damit beschäftigen, wie ResNet-50 auf dem
ihrem DGX A100-Server laufen zu lassen.^49
Von Hand entwickelte Heuristiken haben eine Reihe von Nachteilen. Erstens sind sie nicht opti-
mal. Es gibt keine Garantie dafür, dass die Heuristik, die ein Ingenieur entwickelt hat, die bestmögliche Lösung ist.
mögliche Lösung sind. Zweitens sind sie nicht anpassungsfähig. Die Wiederholung des Prozesses für ein neues
oder einer neuen Hardware-Architektur zu wiederholen, ist mit einem enormen Aufwand verbunden.
Erschwerend kommt hinzu, dass die Modelloptimierung von den Operatoren abhängig ist, aus denen
aus denen sein Berechnungsgraph besteht. Die Optimierung eines neuronalen Faltungsnetzes ist anders
sich von der Optimierung eines rekurrenten neuronalen Netzes, die sich von der Optimierung eines
eines Transformators. Hardware-Anbieter wie NVIDIA und Google konzentrieren sich auf die Optimierung
beliebte Modelle wie ResNet-50 und BERT für ihre Hardware zu optimieren. Was aber, wenn Sie als
ML-Forscher, eine neue Modellarchitektur entwickeln? Möglicherweise müssen Sie
selbst optimieren, um zu zeigen, dass sie schnell ist, bevor sie von Hardwareanbietern übernommen und
Anbietern übernommen und optimiert wird.
Wenn Sie keine Ideen für gute Heuristiken haben, könnte eine mögliche Lösung darin bestehen, Folgendes auszuprobieren
alle möglichen Wege zur Ausführung eines Berechnungsgraphen auszuprobieren und die dafür benötigte Zeit aufzuzeichnen,
und dann den besten Weg zu wählen. Bei einer kombinatorischen Anzahl von möglichen Wegen
wäre die Erkundung aller Pfade jedoch unpraktikabel. Glücklicherweise ist die Annäherung an die Lösungen von
unlösbare Probleme zu approximieren, was ML gut kann. Was wäre, wenn wir ML verwenden, um den
Suchraum einzugrenzen, damit wir nicht so viele Pfade erforschen müssen, und vorherzusagen, wie lange ein
Vorhersage, wie lange ein Pfad dauern wird, so dass wir nicht warten müssen, bis der gesamte Berechnungsgraph fertig
ausgeführt wird?
Die Abschätzung der Zeit, die ein Pfad durch einen Berechnungsgraphen benötigt, um ausgeführt zu werden
erweist sich als schwierig, da dazu viele Annahmen über diesen Graphen getroffen werden müssen.
Es ist viel einfacher, sich auf einen kleinen Teil des Graphen zu konzentrieren.
220 | Kapitel 7: Modellbereitstellung und Vorhersagedienst
Wenn Sie PyTorch auf GPUs verwenden, haben Sie vielleicht torch.backends.cudnn.bench
mark=True. Wenn dies auf True gesetzt ist, wird cuDNN autotune aktiviert. cuDNN auto-
tune durchsucht einen vorgegebenen Satz von Optionen zur Ausführung eines Faltungsoperators
und wählt dann den schnellsten Weg. cuDNN autotune funktioniert trotz seiner Effektivität nur
funktioniert nur für Faltungsoperatoren. Eine viel allgemeinere Lösung ist autoTVM, die
Teil des Open-Source-Compiler-Stacks TVM ist. autoTVM arbeitet mit Teilgraphen
Untergraphen und nicht nur mit einem Operator, so dass die Suchräume, mit denen es arbeitet, viel
komplexer. Die Funktionsweise von autoTVM ist recht kompliziert, aber in einfachen Worten ausgedrückt:

1.1. Es zerlegt zunächst Ihren Berechnungsgraphen in Teilgraphen.
2.2. Es sagt voraus, wie groß jeder Teilgraph ist.
3.3. Es teilt Zeit für die Suche nach dem bestmöglichen Pfad für jeden Teilgraphen zu.
4.4. Es fügt den bestmöglichen Weg zur Ausführung jedes Teilgraphen zusammen, um den
gesamten Graphen auszuführen.
autoTVM misst die tatsächliche Zeit, die für die Ausführung jedes einzelnen Pfades benötigt wird, was
Daten zur Verfügung, um ein Kostenmodell zu trainieren, das vorhersagt, wie lange ein zukünftiger Pfad
dauern wird. Der Vorteil dieses Ansatzes besteht darin, dass das Modell anhand der zur Laufzeit generierten Daten trainiert wird.
trainiert wird, kann es sich an jede Art von Hardware anpassen, auf der es läuft. Der Nachteil ist
dass es länger dauert, bis sich das Kostenmodell zu verbessern beginnt. Abbildung 7-15 zeigt den
Leistungssteigerung, die autoTVM im Vergleich zu cuDNN für das Modell ResNet-50
auf NVIDIA TITAN X.

Die Ergebnisse von ML-gestützten Compilern sind zwar beeindruckend, haben aber einen Haken:
Sie können langsam sein. Man geht alle möglichen Pfade durch und findet die am besten optimierten
Pfade. Dieser Prozess kann Stunden, bei komplexen ML-Modellen sogar Tage dauern. Allerdings ist es
ein einmaliger Vorgang, und die Ergebnisse Ihrer Optimierungssuche können zwischengespeichert und
können zwischengespeichert und zur Optimierung bestehender Modelle und als Ausgangspunkt für künftige
Sitzungen. Sie optimieren Ihr Modell einmal für ein Hardware-Backend und führen es dann auf
mehreren Geräten desselben Hardwaretyps aus. Diese Art der Optimierung ist ideal, wenn
Sie ein produktionsreifes Modell und eine Zielhardware haben, auf der Sie Inferenzen ausführen können.

ML in der Cloud und am Rande der Welt | 221
50 Chen et al. "TVM: An Automated End-to-End Optimizing Compiler for Deep Learning".

Abbildung 7-15. Von autoTVM gegenüber cuDNN erzielte Geschwindigkeitssteigerung für ResNet-50 auf NVIDIA
TITAN X. Es dauert ~70 Versuche, bis autoTVM die Leistung von cuDNN übertrifft. Quelle: Chen et al.^50
ML in Browsern
Wir haben darüber gesprochen, wie Compiler uns helfen können, maschinennativen Code zu erzeugen
Modelle auf bestimmten Hardware-Backends auszuführen. Es ist jedoch möglich, Code zu erzeugen
zu generieren, der auf beliebigen Hardware-Backends ausgeführt werden kann, indem man diesen Code in Browsern ausführt. Wenn
Sie Ihr Modell in einem Browser ausführen können, können Sie Ihr Modell auf jedem Gerät ausführen, das
Browsern unterstützt: MacBooks, Chromebooks, iPhones, Android-Telefone und mehr.
Dabei ist es egal, welche Chips diese Geräte verwenden. Wenn Apple beschließt, von
von Intel-Chips auf ARM-Chips zu wechseln, ist das nicht Ihr Problem.
Wenn von Browsern die Rede ist, denken viele Leute an JavaScript. Es gibt Tools, die
Ihnen helfen können, Ihre Modelle in JavaScript zu kompilieren, wie z.B. TensorFlow.js, Synaptic,
und brain.js. JavaScript ist jedoch langsam, und seine Kapazität als Programmiersprache
ist für komplexe Logiken wie die Extraktion von Merkmalen aus Daten begrenzt.
222 | Kapitel 7: Modellbereitstellung und Vorhersagedienst
51 Wasmer, https://oreil.ly/dTRxr; Awesome Wasm, https://oreil.ly/hlIFb.
52 Kann ich _____ verwenden?, https://oreil.ly/slI05.
53 Abhinav Jangda, Bobby Powers, Emery D. Berger, und Arjun Guha, "Not So Fast: Analyzing the Performance
of WebAssembly vs. Native Code", USENIX, https://oreil.ly/uVzrX.

Ein vielversprechenderer Ansatz ist WebAssembly (WASM). WASM ist ein offener Standard
der es Ihnen ermöglicht, ausführbare Programme in Browsern auszuführen. Nachdem Sie Ihre
Modelle in Scikit-Learn, PyTorch, TensorFlow oder anderen von Ihnen verwendeten Frameworks erstellt haben,
können Sie Ihr Modell in WASM kompilieren, anstatt es für die Ausführung auf bestimmter Hardware zu kompilieren.
Modell nach WASM kompilieren. Sie erhalten eine ausführbare Datei zurück, die Sie einfach mit JavaScript verwenden können.
WASM ist einer der aufregendsten technologischen Trends, die ich in den letzten paar Jahren gesehen habe.
Jahren gesehen habe. Es ist leistungsfähig, einfach zu verwenden und hat ein Ökosystem, das wie ein Lauffeuer wächst.^51
Ab September 2021 wird es von 93 % der Geräte weltweit unterstützt.^52
Der größte Nachteil von WASM ist, dass es langsam ist, weil es in Browsern läuft.
Auch wenn WASM bereits viel schneller ist als JavaScript, ist es immer noch langsam im Vergleich zu
der nativen Ausführung von Code auf Geräten (wie iOS- oder Android-Apps). Eine Studie von Jangda et
al. zeigte, dass mit WASM kompilierte Anwendungen langsamer laufen als native Anwendungen
um durchschnittlich 45 % (auf Firefox) bis 55 % (auf Chrome) langsamer sind.^53
Zusammenfassung
Herzlichen Glückwunsch, Sie haben eines der wahrscheinlich technischsten Kapitel in diesem Buch
Buches! Das Kapitel ist technisch, weil der Einsatz von ML-Modellen eine technische
Herausforderung ist, nicht eine ML-Herausforderung.
Wir haben verschiedene Möglichkeiten für den Einsatz eines Modells erörtert und dabei die Online-Vorhersage mit der
Batch-Prognose und ML on the edge mit ML on the cloud. Jede Methode hat ihre eigenen
Herausforderungen. Durch die Online-Vorhersage kann Ihr Modell besser auf die sich ändernden Präferenzen der Benutzer reagieren.
Präferenzen der Nutzer, aber Sie müssen sich um die Latenzzeit bei der Inferenz kümmern. Batch-Vorhersage ist eine
für den Fall, dass Ihre Modelle zu lange brauchen, um Vorhersagen zu generieren, aber sie
Ihr Modell weniger flexibel.
Ähnlich ist die Inferenz in der Cloud einfach einzurichten, aber sie wird unpraktisch
mit Netzwerklatenz und Cloud-Kosten. Für Schlussfolgerungen am Rande des Netzwerks sind erforderlich
Edge-Geräte mit ausreichender Rechenleistung, Speicher und Batterie.
Ich glaube jedoch, dass die meisten dieser Herausforderungen auf die Einschränkungen der Hardware zurückzuführen sind
Hardware, auf der ML-Modelle laufen. Da die Hardware immer leistungsfähiger und opti-
für ML optimiert wird, glaube ich, dass ML-Systeme dazu übergehen werden, Online-Vorhersagen
auf dem Gerät, wie in Abbildung 7-16 dargestellt.
Zusammenfassung | 223
Abbildung 7-16. Wenn die Hardware leistungsfähiger wird, werden ML-Modelle zu Online- und
am Rande

Ich dachte immer, dass ein ML-Projekt abgeschlossen ist, nachdem das Modell bereitgestellt wurde, und ich hoffe
ich hoffe, dass ich in diesem Kapitel deutlich gemacht habe, dass ich mich ernsthaft geirrt habe. Das Verschieben des Modells
von der Entwicklungsumgebung in die Produktionsumgebung schafft eine ganze Reihe
eine ganze Reihe von neuen Problemen. Das erste ist die Frage, wie das Modell in der Produktion gehalten werden kann. Im
nächsten Kapitel werden wir erörtern, wie unsere Modelle in der Produktion versagen können und wie man
Modelle kontinuierlich überwachen, um Probleme zu erkennen und so schnell wie möglich zu beheben.

224 | Kapitel 7: Modellbereitstellung und Vorhersagedienst

1 Dies scheint ein recht gängiges Muster für Bestandsvorhersagen zu sein. Eugene Yan schrieb über eine ähnliche Geschichte, um
um das Problem degenerierter Rückkopplungsschleifen zu veranschaulichen, in seinem Artikel "6 Little-Known Challenges After Deploying
Maschinelles Lernen" (2021).
KAPITEL 8

Verschiebung der Datenverteilung und Überwachung
Beginnen wir das Kapitel mit einer Geschichte, die mir von einer Führungskraft erzählt wurde und die viele Leser vielleicht
nachempfinden können. Vor etwa zwei Jahren beauftragte sein Unternehmen eine Beratungsfirma mit der
Beratungsunternehmen mit der Entwicklung eines ML-Modells, mit dem es vorhersagen konnte, wie viele Lebensmittel
nächste Woche benötigt werden, damit die Vorräte entsprechend aufgefüllt werden können. Die Beratungsfirma benötigte
sechs Monate für die Entwicklung des Modells. Nach der Übergabe des Modells durch das Beratungsunternehmen
setzte sein Unternehmen es ein und war mit seiner Leistung sehr zufrieden. Endlich konnten sie
Investoren damit prahlen, dass sie ein KI-gestütztes Unternehmen waren.

Ein Jahr später ging ihre Zahl jedoch zurück. Die Nachfrage nach einigen Artikeln wurde
überschätzt, was dazu führte, dass die zusätzlichen Artikel ausliefen. Gleichzeitig
Gleichzeitig wurde die Nachfrage nach einigen Artikeln durchweg unterschätzt, was zu Umsatzeinbußen führte.
^1 Anfänglich änderte sein Inventurteam die Vorhersagen des Modells manuell, um die
um die festgestellten Muster zu korrigieren, aber schließlich waren die Vorhersagen des Modells
so schlecht geworden, dass sie es nicht mehr verwenden konnten. Sie hatten drei Möglichkeiten: die gleiche
Beratungsfirma eine unverschämt hohe Summe für die Aktualisierung des Modells zu bezahlen, eine andere
Beratungsfirma noch mehr Geld zu zahlen, weil diese Firma Zeit brauchen würde, um sich
auf den neuesten Stand zu bringen, oder ein internes Team einzustellen, das das Modell weiter pflegt.

225
Sein Unternehmen hat auf die harte Tour eine wichtige Lektion gelernt, die auch der Rest der Branche
entdeckt: Mit dem Einsatz eines Modells ist der Prozess noch nicht abgeschlossen. Die Leistung eines Modells
eines Modells verschlechtert sich im Laufe der Zeit in der Produktion. Sobald ein Modell eingesetzt wurde, müssen wir
müssen wir die Leistung des Modells kontinuierlich überwachen, um Probleme zu erkennen und Updates
um diese Probleme zu beheben.

In diesem und dem nächsten Kapitel werden wir die notwendigen Themen behandeln, die Ihnen helfen, ein
Modell in Produktion zu halten. Zunächst werden wir uns mit den Gründen befassen, warum ML-Modelle, die
während der Entwicklung gut funktionieren, in der Produktion versagen. Dann werden wir einen tiefen Einblick in eine
besonders weit verbreitetes und heikles Problem, das fast alle ML-Modelle in der Produktion betrifft:
Verschiebung der Datenverteilung. Dies tritt auf, wenn die Datenverteilung in der Produktion von der
und von der Datenverteilung abweicht, der das Modell während des Trainings ausgesetzt war.
Wir fahren fort mit der Überwachung von Verteilungsverschiebungen. Im nächsten Kapitel werden wir
wie Sie Ihre Modelle in der Produktion laufend aktualisieren können, um sich an Verschiebungen in der Datenverteilung
Verteilungen anpasst.

Ursachen von ML-Systemfehlern
Bevor wir die Ursache für ML-Systemausfälle ermitteln, sollten wir kurz erörtern, was ein ML
Systemfehler ist. Ein Fehler tritt auf, wenn eine oder mehrere Erwartungen an das System
verletzt werden. Bei herkömmlicher Software kümmern wir uns hauptsächlich um die Betriebserwartungen eines Systems
tionen eines Systems: ob das System seine Logik innerhalb der erwarteten Betriebskennzahlen ausführt,
z. B. Latenzzeit und Durchsatz.

Bei einem ML-System interessieren uns sowohl seine Betriebskennzahlen als auch seine ML-Leistungskennzahlen.
Leistungsmetriken. Nehmen wir zum Beispiel ein System für die maschinelle Übersetzung Englisch-Französisch.
Die betriebliche Erwartung könnte sein, dass das System bei einem englischen Satz
eine französische Übersetzung innerhalb einer Latenzzeit von einer Sekunde liefert. Seine ML-Leistungserwartung
Die Erwartung an die ML-Leistung ist, dass die zurückgegebene Übersetzung in 99 % der Fälle eine genaue Übersetzung des englischen
Satzes in 99 % der Fälle ist.

Wenn Sie einen englischen Satz in das System eingeben und keine Übersetzung zurückbekommen, ist die
ist die erste Erwartung verletzt, und dies ist ein Systemfehler.

226 | Kapitel 8: Verschiebung der Datenverteilung und Überwachung

2 Dies ist einer der Gründe, warum viele Unternehmen zögern, Produkte von Start-ups zu verwenden, und warum viele
Unternehmen es vorziehen, Open-Source-Software zu verwenden. Wenn ein Produkt, das Sie verwenden, nicht mehr von seinen
Wenn ein Produkt, das Sie verwenden, nicht mehr von seinen Entwicklern gepflegt wird, können Sie bei Open-Source-Software zumindest auf die Codebasis zugreifen und es selbst pflegen.
Wenn Sie eine Übersetzung zurückbekommen, die nicht korrekt ist, ist das nicht unbedingt ein Systemfehler
denn die Genauigkeitserwartung lässt einen gewissen Spielraum zu. Wenn Sie jedoch weiterhin
verschiedene englische Sätze in das System eingeben und immer wieder falsche Übersetzungen zurückbekommen
zurückbekommen, ist die zweite Erwartung verletzt, was einen Systemfehler darstellt.

Operative Erwartungsverletzungen sind leichter zu erkennen, da sie in der Regel mit einem
einem Timeout, einem 404-Fehler auf einer Webseite, einem Out-of-Memory-Fehler oder einem
Out-of-Memory-Fehler oder ein Segmentierungsfehler. Allerdings sind ML-Leistungserwartungs
Verletzungen der ML-Leistungserwartungen sind jedoch schwieriger zu erkennen, da dies die Messung und Überwachung der
Leistung von ML-Modellen in der Produktion. Im vorangegangenen Beispiel des maschinellen Übersetzungssystems Englisch-Französisch
französischen maschinellen Übersetzungssystems ist die Feststellung, ob die zurückgegebenen Übersetzungen
99 % der Zeit korrekt sind, ist schwierig, wenn wir nicht wissen, wie die korrekten Übersetzungen
sein sollen. Es gibt zahllose Beispiele dafür, dass die schmerzhaft falschen Übersetzungen von Google Translate
falschen Übersetzungen von Google Translate, die von Nutzern verwendet werden, weil sie nicht wissen, dass es sich um falsche
Übersetzungen sind. Aus diesem Grund sagen wir, dass ML-Systeme oft im Stillen versagen.

Um ML-Systemfehler in der Produktion effektiv zu erkennen und zu beheben, ist es nützlich, zu verstehen
zu verstehen, warum ein Modell, das sich während der Entwicklung bewährt hat, in der Produktion versagt.
ausfällt. Wir werden zwei Arten von Fehlern untersuchen: Software-Systemfehler und ML-spezifische
Ausfälle.

Ausfälle von Softwaresystemen
Software-Systemfehler sind Fehler, die auch bei Nicht-ML-Systemen aufgetreten wären.
Hier sind einige Beispiele für Software-Systemausfälle:

Ausfall von Abhängigkeiten
Ein Softwarepaket oder eine Codebasis, von der Ihr System abhängt, geht kaputt, was
zu einem Ausfall Ihres Systems führt. Dieser Fehlermodus tritt häufig auf, wenn die Abhängigkeit
von einer dritten Partei gepflegt wird, und besonders häufig, wenn die dritte Partei, die
der die Abhängigkeit pflegt, nicht mehr existiert.^2

Fehler bei der Bereitstellung
Fehler aufgrund von Bereitstellungsfehlern, z. B. wenn Sie versehentlich die
versehentlich die Binärdateien einer älteren Version Ihres Modells anstelle der aktuellen Version bereitstellen, oder wenn
Ihre Systeme nicht über die richtigen Berechtigungen zum Lesen oder Schreiben bestimmter Dateien verfügen.

Ursachen für Ausfälle von ML-Systemen | 227
3 Kosmische Strahlung kann dazu führen, dass Ihre Hardware ausfällt (Wikipedia, s.v. "Soft error", https://oreil.ly/4cvNg).
4 Daniel Papasian und Todd Underwood, "How ML Breaks: A Decade of Outages for One Large ML Pipeline,"
Google, 17. Juli 2020, Video, 19:06, https://oreil.ly/WGabN. Ein Nicht-ML-Ausfall kann dennoch indirekt auf
auf ML zurückzuführen sein. So kann beispielsweise ein Server für Nicht-ML-Systeme ausfallen, aber da ML-Systeme in der Regel mehr
Rechenleistung benötigen, kann dies dazu führen, dass dieser Server häufiger abstürzt.
5 Der Höhepunkt meiner Karriere: Elon Musk hat mir zugestimmt.
Hardware-Ausfälle
Wenn die Hardware, die Sie für den Einsatz Ihres Modells verwenden, z. B. CPUs oder GPUs,
sich nicht so verhält, wie sie sollte. Die von Ihnen verwendeten CPUs könnten beispielsweise überhitzen
und kaputt gehen.^3

Ausfallzeiten oder Abstürze
Wenn eine Komponente Ihres Systems von einem Server irgendwo läuft, wie AWS oder einem
gehosteter Dienst läuft und dieser Server ausfällt, wird auch Ihr System ausfallen.

Nur weil einige Fehler nicht spezifisch für ML sind, heißt das nicht, dass sie nicht wichtig sind
dass ML-Ingenieure sie nicht verstehen müssen. Im Jahr 2020 untersuchten Daniel Papasian und Todd Underwood,
zwei ML-Ingenieure bei Google, 96 Fälle untersucht, in denen eine große ML-Pipeline bei Google
brach. Sie überprüften Daten aus den vergangenen 15 Jahren, um die Ursachen zu ermitteln
und fanden heraus, dass 60 dieser 96 Ausfälle auf Ursachen zurückzuführen waren, die nicht direkt mit
nicht direkt mit ML zusammenhängen.^4 Die meisten Probleme stehen im Zusammenhang mit verteilten Systemen, z. B. wenn der
Workflow-Scheduler oder -Orchestrator einen Fehler macht, oder mit der Datenpipeline zusammenhängen,
z. B. wenn Daten aus mehreren Quellen falsch zusammengeführt werden oder die falschen Datenstrukturen verwendet werden.
turen verwendet werden.

Die Behebung von Fehlern in Softwaresystemen erfordert keine ML-Fähigkeiten, sondern traditionelle Software
Fähigkeiten, und diese zu behandeln, würde den Rahmen dieses Buches sprengen. Wegen der
der Bedeutung der traditionellen Softwaretechnik für den Einsatz von ML-Systemen,
ist ML-Engineering vor allem Technik, nicht ML.^5 Leser, die lernen möchten
wie man ML-Systeme aus der Perspektive der Softwaretechnik zuverlässig macht, empfehle ich
empfehle ich das Buch Reliable Machine Learning, das von O'Reilly mit Todd
Underwood als einer der Autoren.

Ein Grund für die Häufigkeit von Software-Systemfehlern ist die Tatsache, dass die Einführung von ML
in der Industrie noch im Entstehen begriffen ist, die Werkzeuge für die ML-Produktion begrenzt sind und die
Praktiken noch nicht gut entwickelt oder standardisiert sind. Da jedoch die Werkzeuge und bewährten
Methoden für die ML-Produktion ausgereift sind, gibt es Grund zu der Annahme, dass der Anteil
von Software-Systemfehlern abnehmen und der Anteil der ML-spezifischen Fehler
zunehmen wird.

228 | Kapitel 8: Verschiebung der Datenverteilung und Überwachung

6 Als es noch persönliche akademische Konferenzen gab, habe ich oft gehört, wie sich Forscher darüber stritten, wessen
Modelle besser verallgemeinern können. "Mein Modell verallgemeinert besser als dein Modell" ist die ultimative Flex.
7 Masashi Sugiyama und Motoaki Kawanabe, Machine Learning in Non-stationary Environments: Einführung in die
Covariate Shift Adaptation (Cambridge, MA: MIT Press, 2012).
ML-spezifische Ausfälle
ML-spezifische Ausfälle sind Ausfälle, die für ML-Systeme spezifisch sind. Beispiele sind Probleme bei der Datenerfassung
Probleme bei der Datensammlung und -verarbeitung, schlechte Hyperparameter, Änderungen in der Trainingsleitung
Trainingsleitung, die nicht korrekt in der Inferenzleitung repliziert werden und umgekehrt, Datenverteilungs
Datenverteilung, die dazu führen, dass sich die Leistung eines Modells mit der Zeit verschlechtert, Randfälle und
degenerierte Rückkopplungsschleifen.

In diesem Kapitel konzentrieren wir uns auf die Behebung ML-spezifischer Fehler. Auch wenn sie
einen kleinen Teil der Ausfälle ausmachen, können sie gefährlicher sein als nicht-ML
als nicht-ML-Fehler, da sie schwer zu erkennen und zu beheben sind und ML-Systeme
gänzlich verhindern. Wir haben Datenprobleme in Kapitel 4 sehr ausführlich behandelt,
die Abstimmung der Hyperparameter in Kapitel 6 und die Gefahr, zwei getrennte Pipelines
für Training und Inferenz in Kapitel 7. In diesem Kapitel werden wir drei neue, aber
sehr häufige Probleme, die nach dem Einsatz eines Modells auftreten: Produktionsdaten
die sich von den Trainingsdaten unterscheiden, Randfälle und degenerierte Rückkopplungsschleifen.

Produktionsdaten unterscheiden sich von Trainingsdaten

Wenn wir sagen, dass ein ML-Modell aus den Trainingsdaten lernt, bedeutet dies, dass das
Modell die zugrunde liegende Verteilung der Trainingsdaten mit dem Ziel lernt, diese
diese gelernte Verteilung zu nutzen, um genaue Vorhersagen für ungesehene Daten zu treffen - Daten
Daten, die es beim Training nicht gesehen hat. Was dies mathematisch bedeutet, wird im Abschnitt
Abschnitt "Verschiebung der Datenverteilung" auf Seite 237. Wenn das Modell in der Lage ist
Vorhersagen für ungesehene Daten zu treffen, wird dieses Modell "auf ungesehene Daten verallgemeinert".
^6 Die Testdaten, die wir zur Bewertung eines Modells während der Entwicklung verwenden, sollen
Testdaten, die wir zur Bewertung eines Modells während der Entwicklung verwenden, sollen ungesehene Daten repräsentieren, und die Leistung des Modells bei den Testdaten soll
einen Eindruck davon vermitteln, wie gut das Modell verallgemeinert werden kann.

Eines der ersten Dinge, die ich in ML-Kursen gelernt habe, ist, dass es wichtig ist, dass die Trainingsdaten
und die ungesehenen Daten aus einer ähnlichen Verteilung stammen müssen. Die Annahme ist, dass die
ungesehenen Daten einer stationären Verteilung entstammen, die mit der Verteilung der Trainingsdaten identisch ist.
verteilung. Wenn die ungesehenen Daten aus einer anderen Verteilung stammen, kann das Modell
nicht gut verallgemeinern.^7

Ursachen von ML-Systemfehlern | 229
8 John Mcquaid, "Die Grenzen des Wachstums: Can AI's Voracious Appetite for Data Be Tamed?" Undark, 18. Oktober,
2021, https://oreil.ly/LSjVD.
9 Der Chief Technology Officer (CTO) eines Überwachungsdienstleisters sagte mir, dass seiner Schätzung nach 80 % der
der von seinem Dienst erfassten Abweichungen auf menschliche Fehler zurückzuführen sind.
Diese Annahme ist in den meisten Fällen aus zwei Gründen falsch. Erstens: Die zugrunde liegende
Verteilung der realen Daten wahrscheinlich nicht mit der Verteilung der Trainingsdaten übereinstimmen.
Verteilung der Trainingsdaten. Das Kuratieren eines Trainingsdatensatzes, der genau die Daten
Daten repräsentieren kann, die ein Modell in der Produktion vorfindet, erweist sich als sehr schwierig.^8
Daten aus der realen Welt sind vielschichtig und in vielen Fällen praktisch unendlich, während
Trainingsdaten endlich sind und durch die verfügbaren Zeit-, Rechen- und Personalressourcen
bei der Erstellung und Verarbeitung von Datensätzen. Es gibt viele verschiedene Auswahl- und
Auswahl- und Stichprobenfehler, die, wie in Kapitel 4 erörtert, dazu führen können, dass reale Daten
von den Trainingsdaten abweichen. Die Abweichung kann so geringfügig sein, dass die realen
Daten eine andere Art der Kodierung von Emojis verwenden. Diese Art von Abweichung führt zu einem
häufigen Fehlermodus, der als "train-serving skew" bekannt ist: ein Modell, das in der Entwicklung gut
Entwicklung gut funktioniert, aber beim Einsatz schlecht abschneidet.

Zweitens ist die reale Welt nicht unbeweglich. Die Dinge ändern sich. Datenverteilungen verschieben sich. Auf
2019, als die Leute nach Wuhan suchten, wollten sie wahrscheinlich Reiseinformationen erhalten,
aber seit COVID-19 wollen die Leute, wenn sie nach Wuhan suchen, wahrscheinlich wissen
über den Ort, an dem COVID-19 entstanden ist. Eine weitere häufige Fehlerart ist, dass ein
Modell beim ersten Einsatz gut funktioniert, seine Leistung aber im Laufe der Zeit abnimmt, wenn sich die
Datenverteilung ändert. Dieser Fehlermodus muss kontinuierlich überwacht und
und erkannt werden, solange ein Modell in der Produktion eingesetzt wird.

Wenn ich COVID-19 als Beispiel für eine Datenverschiebung anführe, haben manche Leute den Eindruck
den Eindruck, dass Datenverschiebungen nur bei ungewöhnlichen Ereignissen auftreten, was impliziert
dass sie nicht oft vorkommen. Datenverschiebungen treten ständig auf, plötzlich, allmählich oder
saisonal. Sie können plötzlich aufgrund eines bestimmten Ereignisses auftreten, z. B. wenn Ihre
Konkurrenten ihre Preispolitik ändern und Sie daraufhin Ihre Preisprognosen aktualisieren müssen
Preisprognosen aktualisieren müssen, oder wenn Sie Ihr Produkt in einer neuen Region einführen, oder wenn ein
Prominenter Ihr Produkt erwähnt, was zu einem sprunghaften Anstieg der Nutzerzahlen führt, und so weiter. Sie
können schrittweise erfolgen, weil sich soziale Normen, Kulturen, Sprachen, Trends, Branchen usw,
usw. sich mit der Zeit ändern. Sie können auch durch saisonale Schwankungen ausgelöst werden, z. B.
Im Winter, wenn es kalt und verschneit ist, werden Mitfahrgelegenheiten vielleicht eher
verschneit ist als im Frühling.

Aufgrund der Komplexität von ML-Systemen und der schlechten Praktiken bei ihrer Einführung
ein großer Prozentsatz der Datenverschiebungen, die auf den Überwachungs-Dashboards aussehen könnten
werden durch interne Fehler verursacht,^9 wie Fehler in der Datenpipeline, fehlende Werte
falsch eingegebene Werte, Inkonsistenzen zwischen den beim Training extrahierten Merkmalen
und Inferenz, Merkmale, die anhand von Statistiken aus der falschen Teilmenge von Daten standardisiert wurden,

230 | Kapitel 8: Verschiebung der Datenverteilung und Überwachung

10 Das bedeutet, dass das selbstfahrende Auto ein bisschen sicherer ist als ein durchschnittlicher menschlicher Fahrer. Im Jahr 2019 lag die Quote der
verkehrsbedingten Todesfälle pro 100.000 zugelassene Fahrer bei 15,8 oder 0,0158 % ("Fatality Rate per 100.000 Licensed
Drivers in the U.S. from 1990 to 2019", Statista, 2021, https://oreil.ly/w3wYh).
11 Rodney Brooks, "Edge Cases for Self Driving Cars", Robots, AI, and Other Stuff, 17. Juni 2017, https://.
oreil.ly/Nyp4F; Lance Eliot, "Whether Those Endless Edge or Corner Cases Are the Long-Tail Doom for AI
Self-Driving Cars", Forbes, Juli 13, 2021, https://oreil.ly/L2Sbp; Kevin McAllister, "Self-Driving Cars Will Be
Shaped by Simulated, Location Data," Protocol, 25. März 2021, https://oreil.ly/tu8hs.
12 E-Discovery oder elektronische Offenlegung bezieht sich auf die Offenlegung in Gerichtsverfahren, wie Rechtsstreitigkeiten, staatliche Untersuchungen
behördliche Untersuchungen oder Anfragen nach dem Freedom of Information Act, bei denen die gesuchten Informationen in elektronischer Form vorliegen.

falsche Modellversion oder Fehler in der App-Oberfläche, die den Benutzer zwingen, sein Verhalten zu ändern
Verhaltensweisen.
Da es sich hierbei um einen Fehlermodus handelt, der fast alle ML-Modelle betrifft, werden wir dies im Abschnitt
im Abschnitt "Verschiebung der Datenverteilung" auf Seite 237.
Randfälle
Stellen Sie sich vor, es gäbe ein selbstfahrendes Auto, das Sie in 99,99 % der Fälle sicher fahren kann,
aber in den anderen 0,01 % der Zeit könnte es in einen katastrophalen Unfall verwickelt werden, bei dem Sie dauerhaft verletzt oder sogar getötet werden.
Sie dauerhaft verletzen oder sogar töten kann.^10 Würden Sie dieses Auto benutzen?
Wenn Sie versucht sind, nein zu sagen, sind Sie nicht allein. Ein ML-Modell, das in den meisten Fällen gut funktioniert
in den meisten Fällen gut funktioniert, aber in einer kleinen Anzahl von Fällen versagt, ist möglicherweise unbrauchbar, wenn diese Versäumnisse
katastrophale Folgen haben. Aus diesem Grund konzentrieren sich die großen Unternehmen für selbstfahrende Autos
selbstfahrende Autos darauf konzentriert, ihre Systeme auch in Grenzfällen funktionieren zu lassen.^11
Randfälle sind Datenmuster, die so extrem sind, dass sie das Modell zu
katastrophale Fehler macht. Auch wenn sich Randfälle im Allgemeinen auf Datenproben
aus derselben Verteilung gezogen werden, kann es zu einem plötzlichen Anstieg der Anzahl von
Datenproben, bei denen Ihr Modell nicht gut abschneidet, könnte dies ein Hinweis darauf sein, dass sich die
zugrundeliegende Datenverteilung verschoben hat.
Autonome Fahrzeuge werden oft zur Veranschaulichung dafür verwendet, wie Grenzfälle den Einsatz eines
ML-System vom Einsatz abhalten können. Dies gilt aber auch für alle sicherheitskritischen Anwendungen
tion wie medizinische Diagnose, Verkehrssteuerung, e-Discovery12 usw. Es kann auch für
für nicht sicherheitskritische Anwendungen. Stellen Sie sich einen Chatbot für den Kundendienst vor, der
auf die meisten Anfragen vernünftige Antworten gibt, aber manchmal unverschämte
rassistische oder sexistische Inhalte aus. Dieser Chatbot stellt für jedes Unternehmen, das ihn einsetzen will, ein Markenrisiko dar
das ihn einsetzen will, und macht ihn damit unbrauchbar.
Ursachen von ML-Systemfehlern | 231
Grenzfälle und Ausreißer
Sie werden sich vielleicht fragen, was der Unterschied zwischen einem Ausreißer und einem Grenzfall ist. Die
Definition dessen, was einen Grenzfall ausmacht, variiert je nach Disziplin. In ML werden aufgrund seiner
Einführung in die Produktion, werden Randfälle immer noch entdeckt, was die
ihre Definition umstritten.
In diesem Buch bezieht sich der Begriff "Ausreißer" auf Daten: ein Beispiel, das sich deutlich von anderen
Beispielen. Randfälle beziehen sich auf die Leistung: ein Beispiel, bei dem ein Modell
deutlich schlechter abschneidet als andere Beispiele. Ein Ausreißer kann dazu führen, dass ein Modell
ungewöhnlich schlecht abschneidet, was es zu einem Grenzfall macht. Allerdings sind nicht alle Ausreißer Randfälle
Fälle. Eine Person, die bei Rot über die Straße geht, ist zum Beispiel ein Ausreißer, aber kein Grenzfall.
aber es ist kein Sonderfall, wenn Ihr selbstfahrendes Auto diese Person genau erkennen und eine
angemessen reagieren kann.
Während der Modellentwicklung können sich Ausreißer negativ auf die Leistung Ihres Modells auswirken,
wie in Abbildung 8-1 dargestellt. In vielen Fällen kann es von Vorteil sein, Ausreißer zu entfernen
Ausreißer zu entfernen, da Ihr Modell dadurch bessere Entscheidungsgrenzen lernt und besser auf
ungesehene Daten. Während der Inferenz haben Sie jedoch in der Regel nicht die Möglichkeit, die Abfragen zu entfernen
oder die Abfragen zu ignorieren, die sich signifikant von anderen Abfragen unterscheiden. Sie können wählen
umzuwandeln - zum Beispiel, wenn Sie "mechin learnin" in die Google-Suche eingeben,
könnte Google fragen, ob Sie "maschinelles Lernen" meinen. Aber höchstwahrscheinlich wollen Sie
ein Modell so entwickeln, dass es auch bei unerwarteten Eingaben eine gute Leistung erbringen kann.
Abbildung 8-1. Das linke Bild zeigt die Entscheidungsgrenze, wenn es keinen Ausreißer gibt.
Das rechte Bild zeigt die Entscheidungsgrenze, wenn es einen Ausreißer gibt, was sich
Diese unterscheidet sich stark von der Entscheidungsgrenze im ersten Fall und ist wahrscheinlich weniger genau.
232 | Kapitel 8: Verschiebung der Datenverteilung und Überwachung

13 Ray Jiang, Silvia Chiappa, Tor Lattimore, András György, und Pushmeet Kohli, "Degenerate Feedback Loops
in Recommender Systems", arXiv, 27. Februar 2019, https://oreil.ly/b9G7o.
14 Dies ist verwandt mit dem "Survivorship Bias".

Entartete Rückkopplungsschleifen
Im Abschnitt "Natürliche Etiketten" auf Seite 91 haben wir eine Rückkopplungsschleife als die Zeit
zwischen der Anzeige einer Vorhersage und der Rückmeldung zu dieser Vorhersage
gegeben wird. Das Feedback kann dazu verwendet werden, natürliche Labels zu extrahieren, um die Leistung des Modells zu bewerten
Leistung des Modells zu bewerten und die nächste Iteration des Modells zu trainieren.
Eine degenerierte Feedbackschleife kann entstehen, wenn die Vorhersagen selbst das
Rückmeldung beeinflussen, die wiederum die nächste Iteration des Modells beeinflusst. Formal ausgedrückt, eine
degenerierte Rückkopplungsschleife entsteht, wenn die Ausgaben eines Systems dazu verwendet werden, die
zukünftigen Eingaben des Systems verwendet werden, die wiederum die zukünftigen Ausgaben des Systems beeinflussen. Bei ML,
können die Vorhersagen eines Systems beeinflussen, wie Benutzer mit dem System interagieren, und da
da die Interaktionen der Benutzer mit dem System manchmal als Trainingsdaten für dasselbe System verwendet werden
System verwendet werden, können degenerierte Rückkopplungsschleifen auftreten und unbeabsichtigte Folgen haben.
Degenerierte Rückkopplungsschleifen sind besonders häufig bei Aufgaben mit natürlichen Labels von
wie z. B. Empfehlungssystemen und der Vorhersage der Klickrate von Anzeigen.
Um dies zu verdeutlichen, stellen Sie sich vor, Sie bauen ein System, das den Benutzern Lieder empfiehlt
die ihnen gefallen könnten. Die Lieder, die vom System hoch eingestuft werden, werden den Nutzern zuerst angezeigt.
Benutzer angezeigt. Da sie zuerst angezeigt werden, klicken die Nutzer häufiger auf sie, wodurch das
System davon überzeugt, dass diese Empfehlungen gut sind. Zu Beginn werden die
die Bewertungen von zwei Liedern, A und B, nur geringfügig voneinander abweichen, aber da A
ursprünglich etwas höher eingestuft wurde, taucht es in der Empfehlungsliste weiter oben auf,
Dies führte dazu, dass die Nutzer häufiger auf A klickten, wodurch das System A noch höher einstufte. Nach einer
Nach einer Weile war der Rang von A viel höher als der von B.^13 Degenerierte Rückkopplungsschleifen sind ein
Grund, warum beliebte Filme, Bücher oder Lieder immer beliebter werden, was es
was es für neue Artikel schwer macht, in populäre Listen einzudringen. Diese Art von Szenario ist unglaublich
in der Produktion sehr verbreitet und wird intensiv erforscht. Es hat viele verschiedene Namen,
darunter "Exposure Bias", "Popularitätsverzerrung", "Filterblasen" und manchmal "Echokammern".
Kammern".
Hier ein weiteres Beispiel, um die Gefahr degenerativer Rückkopplungsschleifen zu verdeutlichen.
Stellen Sie sich vor, Sie bauen ein Modell zur Lebenslaufprüfung, das vorhersagt, ob jemand mit einem
bestimmten Lebenslauf für die Stelle qualifiziert ist. Das Modell stellt fest, dass Merkmal X genau
die Qualifikation einer Person genau vorhersagt, also empfiehlt es Lebensläufe mit Merkmal X. Sie können
können X durch Merkmale wie "ging nach Stanford", "arbeitete bei Google" oder "identifiziert
als männlich". Die Personalverantwortlichen führen nur Gespräche mit Personen, deren Lebensläufe von dem
Modell empfohlen werden, was bedeutet, dass sie nur Bewerber mit Merkmal X interviewen, was bedeutet
das Unternehmen stellt nur Bewerber mit Merkmal X ein. Dies wiederum führt dazu, dass das Modell
noch mehr Gewicht auf das Merkmal X legt.^14 Wenn Sie wissen, wie Ihr Modell die
Ursachen von ML-Systemfehlern | 233
15 Erik Brynjolfsson, Yu (Jeffrey) Hu, und Duncan Simester, "Goodbye Pareto Principle, Hello Long Tail:
The Effect of Search Costs on the Concentration of Product Sales", Management Science 57, no. 8 (2011):
1373-86, https://oreil.ly/tGhHi; Daniel Fleder und Kartik Hosanagar, "Blockbuster Culture's Next Rise or
Fall: The Impact of Recommender Systems on Sales Diversity", Management Science 55, no. 5 (2009), https://
oreil.ly/Zwkh8; Himan Abdollahpouri, Robin Burke, und Bamshad Mobasher, "Managing Popularity Bias in
Recommender Systems with Personalized Re-ranking," arXiv, 22. Januar 2019, https://oreil.ly/jgYLr.
16 Patrick John Chia, Jacopo Tagliabue, Federico Bianchi, Chloe He, and Brian Ko, "Beyond NDCG: Behavioral
Testing of Recommender Systems with RecList," arXiv, November 18, 2021, https://oreil.ly/7GfHk.

Vorhersagen - wie etwa die Messung der Bedeutung jedes Merkmals für das Modell, wie
wie in Kapitel 5 erörtert - kann dazu beitragen, die Vorliebe für Merkmal X in diesem Fall zu erkennen.
Unbeaufsichtigte degenerierte Rückkopplungsschleifen können dazu führen, dass Ihr Modell bestenfalls suboptimal arbeitet.
bestenfalls unzureichend. Schlimmstenfalls können sie die in den Daten eingebetteten Verzerrungen aufrechterhalten und verstärken,
wie z. B. die Voreingenommenheit gegenüber Kandidaten ohne Merkmal X.
Erkennen degenerierter Rückkopplungsschleifen. Wenn entartete Rückkopplungsschleifen so schlecht sind, wie können
wissen wir, ob eine Rückkopplungsschleife in einem System degeneriert ist? Wenn ein System offline ist,
sind entartete Rückkopplungsschleifen schwer zu erkennen. Entartete Schleifen entstehen durch Benutzer
Feedback, und ein System hat erst dann Nutzer, wenn es online ist (d. h. für Nutzer bereitgestellt wird).
Bei Empfehlungssystemen ist es möglich, degenerierte Feedbackschleifen zu erkennen
Feedbackschleifen zu erkennen, indem man die Popularitätsvielfalt der Ausgaben eines Systems misst, auch wenn das
System offline ist. Die Popularität eines Objekts kann anhand der Anzahl der Interaktionen mit ihm gemessen werden
in der Vergangenheit interagiert wurde (z.B. gesehen, gemocht, gekauft, etc.). Die Popularität
aller Artikel wird wahrscheinlich einer Long-Tail-Verteilung folgen: Eine kleine Anzahl von Artikeln
wird viel interagiert, während die meisten Artikel kaum interagiert werden. Verschiedene
Metriken wie die aggregierte Vielfalt und die durchschnittliche Abdeckung von Long-Tail-Artikeln, die
von Brynjolfsson et al. (2011), Fleder und Hosanagar (2009) und Abdollahpouri et al.
(2019) vorgeschlagenen Methoden können Ihnen helfen, die Vielfalt der Ergebnisse eines Empfehlungssystems zu messen.^15
Eine niedrige Punktzahl bedeutet, dass die Ergebnisse Ihres Systems homogen sind, was möglicherweise
durch Popularitätsverzerrungen verursacht werden.
Im Jahr 2021 gingen Chia et al. noch einen Schritt weiter und schlugen die Messung der Trefferquote
gegen die Popularität. Sie unterteilten die Artikel zunächst nach ihrer Popularität in Bereiche.
Eimer 1 besteht z. B. aus Elementen, mit denen weniger als 100 Mal interagiert wurde,
Bereich 2 besteht aus Elementen, mit denen mehr als 100 Mal interagiert wurde, aber
aber weniger als 1.000 Mal interagiert wurde, usw. Anschließend wurde die Vorhersagegenauigkeit eines Empfehlungssystems
Empfehlungssystems für jede dieser Kategorien. Wenn ein Empfehlungssystem viel besser darin ist
beliebte Artikel besser empfiehlt als weniger beliebte Artikel, leidet es wahrscheinlich
^16 Sobald Ihr System in Produktion ist und Sie feststellen, dass seine
Vorhersagen im Laufe der Zeit immer homogener werden, leidet es wahrscheinlich unter degenerierten
Rückkopplungsschleifen.
234 | Kapitel 8: Verschiebung der Datenverteilung und Überwachung
17 Catherine Wang, "Why TikTok Made Its User So Obsessive? The AI Algorithm That Got You Hooked,"
Towards Data Science, 7. Juni 2020, https://oreil.ly/J7nJ9.
18 Gediminas Adomavicius und YoungOk Kwon, "Improving Aggregate Recommendation Diversity Using
Ranking-Based Techniques," IEEE Transactions on Knowledge and Data Engineering 24, no. 5 (May 2012):
896-911, https://oreil.ly/0JjUV.
19 Tobias Schnabel, Adith Swaminathan, Ashudeep Singh, Navin Chandak, und Thorsten Joachims,
"Recommendations as Treatments: Debiasing Learning and Evaluation," arXiv, February 17, 2016,
https://oreil.ly/oDPSK.

Korrektur von entarteten Rückkopplungsschleifen. Da entartete Rückkopplungsschleifen ein weitverbreitetes Problem sind
Da entartete Rückkopplungsschleifen ein weit verbreitetes Problem sind, gibt es viele Vorschläge, wie man sie korrigieren kann. In diesem
Kapitel werden wir zwei Methoden diskutieren. Die erste ist die Verwendung von Randomisierung, die
die zweite ist die Verwendung von Positionsmerkmalen.
Wir haben erörtert, dass degenerierte Rückkopplungsschleifen dazu führen können, dass die Ausgaben eines Systems
im Laufe der Zeit homogener werden. Die Einführung der Randomisierung in die Vorhersagen kann
ihre Homogenität verringern. Im Fall von Empfehlungssystemen kann man, anstatt
den Nutzern nur die Artikel zu zeigen, die das System für sie hoch einstuft, zeigen wir den Nutzern zufällige
und nutzen ihr Feedback, um die tatsächliche Qualität dieser Artikel zu bestimmen. Dies ist der
Ansatz, den TikTok verfolgt. Jedem neuen Video wird zufällig ein anfänglicher Pool von
Traffic zugewiesen (der bis zu Hunderten von Impressionen betragen kann). Diese Menge an Datenverkehr wird verwendet, um
die unvoreingenommene Qualität jedes Videos zu bewerten, um festzustellen, ob es in einen
größeren Traffic-Pool verschoben oder als irrelevant markiert werden soll.^17
Es hat sich gezeigt, dass die Zufallsauswahl die Vielfalt verbessert, allerdings auf Kosten der Nutzererfahrung.
^18 Wenn wir unseren Nutzern völlig zufällige Elemente zeigen, könnten sie das Interesse an unserem Produkt verlieren.
an unserem Produkt verlieren. Eine intelligente Erkundungsstrategie, wie sie im Abschnitt
Abschnitt "Contextual Bandits als Explorationsstrategie" auf Seite 289 beschrieben werden, können dazu beitragen, die
Artikelvielfalt mit akzeptablem Verlust an Vorhersagegenauigkeit zu erhöhen. Schnabel et al. verwenden eine kleine
Randomisierungs- und Kausalschlussverfahren, um den unvoreingenommenen
Wert jedes Liedes zu schätzen.^19 Sie konnten zeigen, dass dieser Algorithmus in der Lage war, ein
Empfehlungssystem so zu korrigieren, dass die Empfehlungen den Urhebern gegenüber gerecht werden.
Wir haben auch erörtert, dass degenerierte Rückkopplungsschleifen durch das Feedback der Nutzer
Vorhersagen verursacht werden, und dass das Feedback der Nutzer zu einer Vorhersage je nach dem, wo sie
angezeigt wird. Betrachten Sie das vorangegangene Beispiel eines Empfehlungssystems, bei dem Sie jedes Mal
Nutzern fünf Lieder empfehlen. Sie stellen fest, dass das am häufigsten empfohlene Lied viel
wahrscheinlicher angeklickt wird als die anderen vier Lieder. Sie sind sich nicht sicher
ob Ihr Modell besonders gut darin ist, den Top-Song auszuwählen, oder ob die Nutzer
auf jedes beliebige Lied klicken, solange es an erster Stelle empfohlen wird.
Ursachen von ML-Systemfehlern | 235
Wenn die Position, an der eine Vorhersage angezeigt wird, das Feedback in irgendeiner Weise beeinflusst, sollten Sie
können Sie die Positionsinformationen mit Positionsmerkmalen kodieren. Positionsbezogene
Merkmale können numerisch (z. B. die Positionen 1, 2, 3,...) oder boolesch sein (z. B. ob eine
Vorhersage an der ersten Position angezeigt wird oder nicht). Beachten Sie, dass "Positionsmerkmale" sich
sich von den in Kapitel 5 erwähnten "positionellen Einbettungen" unterscheiden.

Hier ist ein naives Beispiel, um zu zeigen, wie man Positionsmerkmale verwendet. Während des Trainings,
fügen Sie Ihren Trainingsdaten das Merkmal "ob ein Lied zuerst empfohlen wird" hinzu, wie
in Tabelle 8-1 gezeigt. Mit diesem Merkmal kann Ihr Modell lernen, inwieweit eine Top-Empfehlung
Empfehlung einen Einfluss darauf hat, wie wahrscheinlich ein Lied angeklickt wird.

Tabelle 8-1. Hinzufügen von Positionsmerkmalen zu Ihren Trainingsdaten zur Abschwächung
degenerierte Rückkopplungsschleifen

ID Song Genre Jahr Künstler User 1. Position Klick
1 Shallow Pop 2020 Lady Gaga listenr32 Falsch Nein
2 Good Vibe Funk 2019 Funk Overlord listenr32 Falsch Nein
3 Beat It Rock 1989 Michael Jackson fancypants Falsch Nein
4 In Bloom Rock 1991 Nirvana fancypants Wahr Ja
5 Shallow Pop 2020 Lady Gaga listenr32 Wahr Ja
Während der Inferenz wollen Sie vorhersagen, ob ein Nutzer auf einen Song klicken wird, unabhängig davon
unabhängig davon, wo der Song empfohlen wird. Daher sollten Sie das Merkmal 1st Position
auf False setzen. Dann sehen Sie sich die Vorhersagen des Modells für verschiedene Lieder für jeden Benutzer an
und können die Reihenfolge wählen, in der die einzelnen Titel angezeigt werden sollen.

Dies ist ein naives Beispiel, denn dies allein reicht möglicherweise nicht aus, um
degenerierte Rückkopplungsschleifen zu bekämpfen. Ein ausgefeilterer Ansatz wäre die Verwendung von zwei
verschiedene Modelle. Das erste Modell sagt die Wahrscheinlichkeit voraus, dass der Nutzer eine Empfehlung sieht und
eine Empfehlung sehen und berücksichtigen wird, wobei die Position, an der die Empfehlung angezeigt wird, berücksichtigt wird.
Empfehlung angezeigt wird. Das zweite Modell sagt dann die Wahrscheinlichkeit voraus, dass der
Wahrscheinlichkeit, dass der Nutzer den Artikel anklickt, wenn er ihn gesehen und in Betracht gezogen hat. Das zweite Modell
beschäftigt sich überhaupt nicht mit den Positionen.

236 | Kapitel 8: Verschiebung der Datenverteilung und Überwachung

20 Jeffrey C. Schlimmer und Richard H. Granger, Jr., "Inkrementelles Lernen aus verrauschten Daten", Machine Learning
1 (1986): 317-54, https://oreil.ly/FxFQi.

Verschiebung der Datenverteilung
Im vorangegangenen Abschnitt haben wir die häufigsten Ursachen für Ausfälle von ML-Systemen erörtert. In diesem
Abschnitt befassen wir uns mit einer besonders hartnäckigen Ursache für Ausfälle: Datenverteilungs
Verschiebungen oder kurz Datenverschiebungen. Datenverschiebung bezieht sich auf das Phänomen beim
überwachtem Lernen, wenn sich die Daten, mit denen ein Modell arbeitet, im Laufe der Zeit ändern, was
was dazu führt, dass die Vorhersagen des Modells im Laufe der Zeit ungenauer werden. Die Verteilung
Die Verteilung der Daten, mit denen das Modell trainiert wird, wird als Quellverteilung bezeichnet. Die
Die Verteilung der Daten, auf denen das Modell seine Inferenzen ausführt, wird als Zielverteilung bezeichnet.
Auch wenn Diskussionen über die Verschiebung der Datenverteilung erst in den letzten Jahren
in den letzten Jahren mit der zunehmenden Verbreitung von ML in der Industrie üblich geworden ist, hat sich die Datenverteilung
Datenverteilungsänderungen in Systemen, die aus Daten lernen, wurden bereits 1986 untersucht.^20 Es gibt
auch ein Buch über die Verschiebung von Datensätzen, Dataset Shift in Machine Learning von
Quiñonero-Candela u.a., veröffentlicht von MIT Press im Jahr 2008.
Arten von Datenverteilungsverschiebungen
Während Datenverteilungs-Shift oft austauschbar mit Konzeptdrift und
Kovariaten-Shift und gelegentlich Label-Shift verwendet wird, handelt es sich dabei um drei verschiedene Untertypen von
Verschiebung. Beachten Sie, dass diese Diskussion über die verschiedenen Arten von Datenverschiebungen sehr mathematiklastig ist und
vor allem aus Sicht der Forschung nützlich ist: Die Entwicklung effizienter Algorithmen zur Erkennung
Algorithmen zur Erkennung und Behandlung von Datenverschiebungen zu entwickeln, muss man die Ursachen dieser Verschiebungen verstehen. In der Produktion
Wenn Datenwissenschaftler in der Produktion auf eine Verteilungsverschiebung stoßen, machen sie sich normalerweise nicht
um welche Art von Verschiebung es sich handelt. Sie interessieren sich vor allem dafür, was sie tun können, um mit
diese Verschiebung zu bewältigen. Wenn Ihnen diese Diskussion zu langatmig ist, können Sie gerne zum Abschnitt "Allgemeine
Datenverteilungsverschiebungen" auf Seite 241.
Um zu verstehen, was Konzeptdrift, Kovariatenverschiebung und Etikettenverschiebung bedeuten, müssen wir zunächst
müssen wir zunächst ein paar mathematische Begriffe definieren. Nennen wir die Eingaben eines Modells X und seine Ausgaben Y.
Wir wissen, dass beim überwachten Lernen die Trainingsdaten als eine Menge von
als eine Reihe von Stichproben aus der gemeinsamen Verteilung P(X, Y) betrachtet werden können, und ML modelliert dann normalerweise
P(Y|X). Diese gemeinsame Verteilung P(X, Y) kann auf zwei Arten zerlegt werden:
-P(X, Y) = P(Y|X)P(X)
-P(X, Y) = P(X|Y)P(Y)
Datenverteilung verschiebt sich | 237
21 Man könnte sich fragen, was mit dem Fall ist, dass sich P(X|Y) ändert, P(Y) aber gleich bleibt, wie in der zweiten
Zerlegung. Ich bin noch nie auf Forschungen in diesem Bereich gestoßen. Ich habe ein paar Forscher, die
Ich habe einige Forscher, die sich auf Datenverschiebungen spezialisiert haben, danach gefragt, und auch sie sagten mir, dass es zu schwierig sei, diese Einstellung zu untersuchen.
22 Wouter M. Kouw und Marco Loog, "An Introduction to Domain Adaptation and Transfer Learning," arXiv,
December 31, 2018, https://oreil.ly/VKSVP.
23 "Breast Cancer Risk in American Women", National Cancer Institute, https://oreil.ly/BFP3U.

P(Y|X) bezeichnet die bedingte Wahrscheinlichkeit einer Ausgabe in Abhängigkeit von einer Eingabe - zum Beispiel,
die Wahrscheinlichkeit, dass es sich bei einer E-Mail um Spam handelt, wenn der Inhalt der E-Mail gegeben ist. P(X) bezeichnet
die Wahrscheinlichkeitsdichte der Eingabe. P(Y) bezeichnet die Wahrscheinlichkeitsdichte der
Ausgabe. Label-Shift, Kovariaten-Shift und Konzeptdrift sind wie folgt definiert:
Kovariatenverschiebung
Wenn sich P(X) ändert, aber P(Y|X) gleich bleibt. Dies bezieht sich auf die erste Dekom-
Position der gemeinsamen Verteilung.
Label-Verschiebung
Wenn sich P(Y) ändert, aber P(X|Y) gleich bleibt. Dies bezieht sich auf die zweite
Zerlegung der gemeinsamen Verteilung.
Konzeptdrift
Wenn sich P(Y|X) ändert, aber P(X) gleich bleibt. Dies bezieht sich auf die erste Dekom-
position der gemeinsamen Verteilung.^21
Wenn Sie das verwirrend finden, geraten Sie nicht in Panik. Wir werden im folgenden Abschnitt Beispiele durchgehen
um die Unterschiede zu verdeutlichen.
Kovariatenverschiebung
Die Kovariatenverschiebung ist eine der am meisten untersuchten Formen der Datenverschiebung.^22 In der
Statistik ist eine Kovariate eine unabhängige Variable, die das Ergebnis einer bestimmten
statistischen Versuchs beeinflussen kann, die aber nicht von direktem Interesse ist. Nehmen wir an, Sie führen
ein Experiment durchführen, um festzustellen, wie sich der Standort auf die Immobilienpreise auswirkt. Die Wohnungs
Preisvariable ist von direktem Interesse, aber Sie wissen, dass die Quadratmeterzahl den Preis beeinflusst.
Preis beeinflusst, also ist die Fläche eine Kovariate. Beim überwachten ML ist das Label die Variable
von direktem Interesse, und die Eingangsmerkmale sind Kovariatenvariablen.
Mathematisch gesehen liegt eine Kovariatenverschiebung vor, wenn sich P(X) ändert, aber P(Y|X) gleich bleibt,
Das bedeutet, dass sich die Verteilung der Eingabe ändert, aber die bedingte Wahrscheinlichkeit
Wahrscheinlichkeit einer Ausgabe bei einer Eingabe gleich bleibt.
Um dies zu verdeutlichen, betrachten wir die Aufgabe, Brustkrebs zu erkennen. Sie wissen, dass das
Sie wissen, dass das Risiko, an Brustkrebs zu erkranken, bei Frauen über 40 Jahren höher ist,^23 also haben Sie eine Variable
"Alter" als Eingabe. Möglicherweise haben Sie in Ihren Trainingsdaten mehr Frauen über 40 Jahre
Trainingsdaten mehr Frauen über 40 Jahre als in Ihren Inferenzdaten, so dass sich die Verteilungen der Eingaben für Ihre Trainings- und
und Inferenzdaten. Für ein Beispiel mit einem bestimmten Alter, wie z. B. über 40, wird jedoch die
238 | Kapitel 8: Verschiebung der Datenverteilung und Überwachung
24 Arthur Gretton, Alex Smola, Jiayuan Huang, Marcel Schmittfull, Karsten Borgwardt, und Bernard
Schölkopf, "Covariate Shift by Kernel Mean Matching", Journal of Machine Learning Research (2009),
https://oreil.ly/s49MI.
25 Sugiyama und Kawanabe, Maschinelles Lernen in nicht-stationären Umgebungen.
26 Tongtong Fang, Nan Lu, Gang Niu, und Masashi Sugiyama, "Rethinking Importance Weighting for Deep
Learning under Distribution Shift," NeurIPS Proceedings 2020, https://oreil.ly/GzJ1r; Gretton et al., "Covariate
Shift by Kernel Mean Matching".

Die Wahrscheinlichkeit, dass dieses Beispiel Brustkrebs hat, ist konstant. Daher ist P(Y|X), die Wahrscheinlichkeit
an Brustkrebs zu erkranken, bei einem Alter von über 40 Jahren, ist gleich.
Während der Modellentwicklung kann es zu Kovariatenverschiebungen aufgrund von Verzerrungen bei der Datenauswahl
Datenauswahl entstehen, die sich aus Schwierigkeiten bei der Sammlung von Beispielen für bestimmte
Klassen. Nehmen wir zum Beispiel an, dass Sie zur Untersuchung von Brustkrebs Daten aus einer Klinik erhalten
in der sich Frauen auf Brustkrebs untersuchen lassen. Da Menschen über 40 von ihren Ärzten ermutigt werden
zur Vorsorgeuntersuchung zu gehen, werden Ihre Daten von Frauen über 40 dominiert. Aus diesem
Aus diesem Grund ist die Kovariatenverschiebung eng mit dem Problem der Verzerrung der Stichprobenauswahl verbunden.^24
Kovariatenverschiebungen können auch auftreten, weil die Trainingsdaten künstlich verändert werden, um
um dem Modell das Lernen zu erleichtern. Wie in Kapitel 4 erörtert, ist es schwierig für ML
Modellen schwer, aus unausgewogenen Datensätzen zu lernen, weshalb Sie vielleicht mehr Stichproben
der seltenen Klassen sammeln oder eine Überstichprobe der Daten für die seltenen Klassen
um Ihrem Modell das Lernen der seltenen Klassen zu erleichtern.
Kovariatenverschiebungen können auch durch den Lernprozess des Modells verursacht werden, insbesondere durch
aktives Lernen. In Kapitel 4 haben wir aktives Lernen wie folgt definiert: Anstatt laufend
Auswahl von Stichproben zum Trainieren eines Modells, verwenden wir die Stichproben, die für das
Modell gemäß einiger Heuristiken. Das bedeutet, dass die Verteilung der Trainingsdaten
durch den Lernprozess so verändert wird, dass sie sich von der realen Eingangsverteilung unterscheidet, und
Kovariatenverschiebungen sind ein Nebenprodukt.^25
In der Produktion treten Kovariatenverschiebungen in der Regel aufgrund größerer Veränderungen in der
Umgebung oder in der Art und Weise, wie Ihre Anwendung genutzt wird. Stellen Sie sich vor, Sie haben ein Modell zur Vorhersage
wie wahrscheinlich es ist, dass ein kostenloser Nutzer zu einem zahlenden Nutzer wird. Das Einkommensniveau des Nutzers
ist ein Merkmal. Die Marketingabteilung Ihres Unternehmens hat kürzlich eine Kampagne gestartet, die
die Nutzer aus einer Bevölkerungsgruppe anzieht, die wohlhabender ist als Ihre derzeitige Zielgruppe. Die
Input-Verteilung in Ihrem Modell hat sich geändert, aber die Wahrscheinlichkeit, dass ein Nutzer mit einem
Einkommensniveau konvertieren wird, bleibt gleich.
Wenn Sie im Voraus wissen, wie sich die reale Eingabeverteilung von Ihrer
Trainingsverteilung unterscheidet, können Sie Techniken wie die Wichtigkeitsgewichtung nutzen
nutzen, um Ihr Modell so zu trainieren, dass es mit den realen Daten funktioniert. Die Wichtigkeitsgewichtung besteht
aus zwei Schritten: Schätzen des Dichteverhältnisses zwischen der realen Eingabeverteilung und der
der Trainingseingangsverteilung, dann Gewichtung der Trainingsdaten nach diesem Verhältnis
und trainieren ein ML-Modell auf diesen gewichteten Daten.^26
Datenverteilung verschiebt sich | 239
27 Han Zhao, Remi Tachet Des Combes, Kun Zhang, und Geoffrey Gordon, "On Learning Invariant Rep-
resentations for Domain Adaptation," Proceedings of Machine Learning Research 97 (2019): 7523-32,
https://oreil.ly/ZxYWD.

Da wir jedoch nicht im Voraus wissen, wie sich die Verteilung in der
der realen Welt ändern wird, ist es sehr schwierig, Ihre Modelle im Voraus so zu trainieren, dass sie robust
auf neue, unbekannte Verteilungen zu trainieren. Es gibt Forschungsarbeiten, die versuchen, Modelle dabei zu unterstützen
zu helfen, Repräsentationen latenter Variablen zu erlernen, die über Datenverteilungen hinweg invariant sind,^27
aber mir ist nicht bekannt, dass sie in der Industrie eingesetzt werden.
Etikettenverschiebung
Label Shift, auch bekannt als Prior Shift, Prior Probability Shift oder Target Shift, liegt vor, wenn
P(Y) sich ändert, aber P(X|Y) gleich bleibt. Man kann sich dies als den Fall vorstellen, dass sich die
Ausgabeverteilung ändert, aber die Eingabeverteilung für eine bestimmte Ausgabe gleich bleibt
gleich bleibt.
Denken Sie daran, dass eine Kovariatenverschiebung vorliegt, wenn sich die Eingangsverteilung ändert. Wenn sich die
Wenn sich die Eingangsverteilung ändert, ändert sich auch die Ausgangsverteilung, was sowohl zu
Kovariatenverschiebung und Etikettenverschiebung gleichzeitig auftreten. Betrachten Sie das vorangegangene
Brustkrebs als Beispiel für eine Kovariatenverschiebung. Da es in unseren Trainingsdaten mehr Frauen über 40 gibt
Trainingsdaten mehr Frauen über 40 sind als in unseren Inferenzdaten, ist der Prozentsatz der POSITIVEN
während des Trainings höher. Wenn Sie jedoch zufällig eine Person A mit Brustkrebs
aus Ihren Trainingsdaten und Person B mit Brustkrebs aus Ihren Testdaten auswählen, haben A und B
die gleiche Wahrscheinlichkeit, über 40 zu sein. Das bedeutet, dass P(X|Y), oder die Wahrscheinlichkeit
über 40 Jahre alt ist, wenn man Brustkrebs hat, gleich ist. Es handelt sich also auch hier um einen Fall von Label-Shift.
Allerdings führen nicht alle Kovariatenverschiebungen zu Labelverschiebungen. Das ist ein heikler Punkt, deshalb werden wir
ein anderes Beispiel betrachten. Stellen Sie sich vor, es gäbe jetzt ein präventives Medikament, das jede
Frau einnimmt, das ihr Risiko, an Brustkrebs zu erkranken, verringert. Die Wahrscheinlichkeit
P(Y|X) sinkt für Frauen jeden Alters, es handelt sich also nicht mehr um eine Kovariatenverschiebung.
Bei einer Person, die an Brustkrebs erkrankt ist, bleibt die Altersverteilung jedoch dieselbe, so dass
ist dies immer noch ein Fall von Label-Shift.
Da der Label-Shift eng mit dem Kovariaten-Shift verwandt ist, sind die Methoden zur Erkennung und
Methoden zur Erkennung und Anpassung von Modellen an Etikettenverschiebungen ähnlich wie Methoden zur Anpassung von Kovariatenverschiebungen. Wir werden
Wir werden sie später in diesem Kapitel näher erläutern.
240 | Kapitel 8: Datenverteilungsverschiebungen und Überwachung
28 Man kann sich dies als den Fall vorstellen, dass sich sowohl P(X) als auch P(Y|X) ändern.

Konzeptdrift
Konzeptdrift, auch bekannt als Posterior Shift, liegt vor, wenn die Eingangsverteilung gleich bleibt
gleich bleibt, sich aber die bedingte Verteilung der Ausgabe bei einer Eingabe ändert. Sie
kann man sich dies als "gleiche Eingabe, andere Ausgabe" vorstellen. Nehmen wir an, Sie sind verantwortlich für ein
Modell, das den Preis eines Hauses auf der Grundlage seiner Eigenschaften vorhersagt. Vor COVID-19 war eine
Drei-Zimmer-Wohnung in San Francisco 2.000.000 Dollar kosten. Doch zu Beginn
COVID-19 verließen jedoch viele Menschen San Francisco, so dass die gleiche Wohnung
nur noch $1.500.000 kosten würde. Auch wenn also die Verteilung der Wohnungsmerkmale
gleich bleibt, hat sich die bedingte Verteilung des Preises eines Hauses angesichts seiner
Eigenschaften geändert.
In vielen Fällen sind die Konzeptverschiebungen zyklisch oder saisonal. Zum Beispiel schwanken die Preise für Mitfahrgelegenheiten
schwanken an Wochentagen im Vergleich zu Wochenenden, und die Preise für Flugtickets steigen während der
Jahreszeiten. Unternehmen können unterschiedliche Modelle haben, um mit zyklischen und saisonalen
Schwankungen. Sie könnten zum Beispiel ein Modell zur Vorhersage von Mitfahrpreisen an Wochentagen und ein anderes Modell für Wochenenden verwenden.
Tagen und ein anderes Modell für Wochenenden.
Allgemeine Verschiebungen in der Datenverteilung
Es gibt noch andere Arten von Veränderungen in der realen Welt, die zwar in der Forschung nicht gut untersucht
in der Forschung zwar nicht gut untersucht sind, aber dennoch die Leistung Ihrer Modelle beeinträchtigen können.
Eine davon ist die Änderung von Merkmalen, z. B. wenn neue Merkmale hinzukommen, ältere Merkmale entfernt werden
entfernt werden oder sich die Menge aller möglichen Werte eines Merkmals ändert.^28 Zum Beispiel hat Ihr
Modell verwendete beispielsweise Jahre für das Merkmal "Alter", jetzt aber Monate, so dass der Wertebereich
der Werte dieses Merkmals hat sich verändert. Einmal hat unser Team festgestellt, dass die Leistung unseres Modells
Leistung unseres Modells einbrach, weil ein Fehler in unserer Pipeline dazu führte, dass ein Merkmal zu
NaNs (kurz für "not a number").
Eine Änderung des Bezeichnungsschemas liegt vor, wenn sich die Menge der möglichen Werte für Y ändert. Mit Label-Verschiebung,
P(Y) ändert sich, aber P(X|Y) bleibt gleich. Bei einer Änderung des Etikettenschemas ändern sich sowohl P(Y) als auch
P(X|Y) ändern. Ein Schema beschreibt die Struktur der Daten, also beschreibt das Labelschema einer
einer Aufgabe die Struktur der Bezeichnungen dieser Aufgabe. Zum Beispiel ein Wörterbuch, das
von einer Klasse auf einen ganzzahligen Wert abbildet, wie z. B. {"POSITIVE": 0, "NEGATIV": 1}, ist ein
Schema.
Datenverteilung verschiebt sich | 241
29 Wenn Sie ein neuronales Netz mit Softmax als letzte Schicht für Ihre Klassifikationssteuer verwenden, ist die Dimension dieser
softmax-Schicht [Anzahl_der_versteckten_Einheiten × Anzahl_der_Klassen]. Wenn sich die Anzahl der Klassen ändert, wird die
Anzahl der Parameter in Ihrer Softmax-Schicht ändern.
30 Sie brauchen keine Ground-Truth-Labels, wenn Sie eine unüberwachte Lernmethode verwenden, aber die große Mehrheit der
Anwendungen sind heute überwacht.

Bei Regressionsaufgaben kann es zu einer Änderung des Etikettenschemas aufgrund von Änderungen im
Bereich der möglichen Label-Werte. Stellen Sie sich vor, Sie erstellen ein Modell zur Vorhersage des
Kreditwürdigkeit. Ursprünglich verwendeten Sie ein Kreditsystem, das von 300 bis 850 reichte,
Sie wechselten jedoch zu einem neuen System, das zwischen 250 und 900 liegt.
Bei Klassifizierungsaufgaben kann es zu einer Änderung des Beschriftungsschemas kommen, weil Sie neue
Klassen. Nehmen wir zum Beispiel an, Sie erstellen ein Modell zur Diagnose von Krankheiten und
es gibt eine neue Krankheit zu diagnostizieren. Klassen können auch veraltet sein oder feiner
körniger werden. Stellen Sie sich vor, Sie sind für ein Stimmungsanalysemodell für Tweets zuständig
die Ihre Marke erwähnen. Ursprünglich hat Ihr Modell nur drei Klassen vorhergesagt: POSI-
TIVE, NEGATIVE und NEUTRAL. Ihre Marketingabteilung hat jedoch erkannt
dass die schädlichsten Tweets die wütenden sind, also wollte man die NEG-
ATIVE Klasse in zwei Klassen unterteilen: Traurig und Wütend. Anstatt drei Klassen zu haben,
hat Ihre Aufgabe jetzt vier Klassen. Wenn sich die Anzahl der Klassen ändert, kann sich die Struktur Ihres Moduls
Struktur Ihres Modells ändern,^29 und Sie müssen möglicherweise sowohl Ihre Daten neu etikettieren als auch
Ihr Modell von Grund auf neu trainieren. Eine Änderung des Etikettenschemas ist besonders häufig bei
Aufgaben mit hoher Kardinalität - Aufgaben mit einer großen Anzahl von Klassen - wie z. B. die
Kategorisierung von Dokumentation.
Es gibt keine Regel, die besagt, dass nur eine Art von Verschiebung zur gleichen Zeit stattfinden sollte. A
Modell kann von mehreren Arten der Verschiebung betroffen sein, was den Umgang mit ihnen sehr
schwieriger macht.
Erkennung von Verschiebungen in der Datenverteilung
Datenverteilungsverschiebungen sind nur dann ein Problem, wenn sie die Leistung Ihres Modells
verschlechtern. Die erste Idee könnte also sein, die genauigkeitsbezogenen Metriken Ihres Modells zu überwachen
Genauigkeit, F1-Score, Recall, AUC-ROC usw. in der Produktion zu überwachen, um zu sehen, ob sie
verändert haben. "Veränderung" bedeutet hier normalerweise "Abnahme", aber wenn die Genauigkeit meines Modells
aber wenn die Genauigkeit meines Modells plötzlich ansteigt oder ohne ersichtlichen Grund erheblich schwankt, würde ich das
nachforschen.
Genauigkeitsbezogene Metriken funktionieren durch den Vergleich der Modellvorhersagen mit der Grundwahrheit
Kennzeichnungen.^30 Während der Modellentwicklung haben Sie Zugang zu Kennzeichnungen, aber in der Produktion,
aber in der Produktion hat man nicht immer Zugang zu Kennzeichnungen, und selbst wenn, werden die Kennzeichnungen verzögert, wie
im Abschnitt "Natürliche Bezeichnungen" auf Seite 91 beschrieben. Der Zugriff auf Etiketten innerhalb eines
Zeitfenster zur Verfügung zu haben, hilft Ihnen enorm dabei, die Leistung Ihres Modells zu
Leistung Ihres Modells.
242 | Kapitel 8: Verschiebung der Datenverteilung und Überwachung
31 Hamel Husain hielt einen großartigen Vortrag darüber, warum die Schräglagenerkennung von TensorFlow Extended so schlecht für CS 329S ist:
Machine Learning Systems Design (Stanford, 2022). Sie können das Video auf YouTube finden.

Wenn die Kennzeichnungen der Grundwahrheit nicht verfügbar oder zu verzögert sind, um nützlich zu sein, können wir stattdessen
andere Verteilungen von Interesse überwachen. Die Verteilungen, die von Interesse sind, sind die Eingabe
Verteilung P(X), die Label-Verteilung P(Y) und die bedingten Verteilungen P(X|
Y) und P(Y|X).
Während wir für die Überwachung der Eingabeverteilung nicht die wahren Etiketten Y kennen müssen, ist die Überwachung der
Die Überwachung der Etikettenverteilung und der beiden bedingten Verteilungen erfordert jedoch die Kenntnis von Y.
Y zu kennen. In der Forschung hat man sich bemüht, Etikettenverschiebungen zu verstehen und zu erkennen
Etikettenverschiebungen ohne Etiketten aus der Zielverteilung zu verstehen und zu erkennen. Ein solcher Versuch ist die Black
Box Shift Estimation von Lipton et al. (2018). In der Industrie konzentrieren sich die meisten Drift
Methoden zur Erkennung von Drift auf die Erkennung von Änderungen in der Eingangsverteilung, insbesondere in der
Verteilungen von Merkmalen, wie wir in diesem Kapitel ausführlich diskutieren.
Statistische Methoden
In der Industrie verwenden viele Unternehmen eine einfache Methode, um zu erkennen, ob die beiden Verteilungen gleich sind.
eine einfache Methode, die viele Unternehmen anwenden, um festzustellen, ob die beiden Verteilungen gleich sind, ist der Vergleich ihrer statistischen Werte wie Minimum, Maximum, Mittelwert, Median,
Varianz, verschiedene Quantile (wie das 5., 25., 75. oder 95. Quantil), Schiefe,
Kurtosis, usw. Sie können zum Beispiel den Median und die Varianz der Werte
eines Merkmals während der Inferenz berechnen und mit den beim Training berechneten Metriken
Training. Ab Oktober 2021 verwenden sogar die in TensorFlow Extended eingebauten Datenvalidierungs
Datenvalidierungstools von TensorFlow Extended nur zusammenfassende Statistiken, um die Verzerrung zwischen Trainings- und
Daten und Verschiebungen zwischen verschiedenen Tagen der Trainingsdaten zu erkennen. Dies ist ein guter Anfang, aber diese
aber diese Metriken sind bei weitem nicht ausreichend.^31 Mittelwert, Median und Varianz sind nur nützlich bei
den Verteilungen, für die Mittelwert/Median/Varianz sinnvolle Zusammenfassungen sind. Wenn diese
Metriken signifikant abweichen, hat sich die Inferenzverteilung möglicherweise von der
Trainingsverteilung verschoben. Wenn diese Metriken jedoch ähnlich sind, gibt es keine Garantie, dass
dass es keine Verschiebung gibt.
Eine anspruchsvollere Lösung ist die Verwendung eines Hypothesentests mit zwei Stichproben, abgekürzt als
Zwei-Stichproben-Test. Mit diesem Test lässt sich feststellen, ob der Unterschied zwischen zwei Popu-
lationen (zwei Datensätze) statistisch signifikant ist. Wenn der Unterschied statistisch
statistisch signifikant, dann ist die Wahrscheinlichkeit, dass der Unterschied eine zufällige Schwankung aufgrund von
Stichprobenvariabilität handelt, ist sehr gering, und der Unterschied ist daher darauf zurückzuführen, dass
dass diese beiden Populationen aus zwei unterschiedlichen Verteilungen stammen. Wenn Sie die
Daten von gestern als die Ausgangspopulation und die Daten von heute als die
Zielpopulation betrachten und sie statistisch unterschiedlich sind, ist es wahrscheinlich, dass sich die zugrundeliegende Daten
Verteilung zwischen gestern und heute verschoben hat.
Verschiebung der Datenverteilung | 243
32 I. M. Chakravarti, R. G. Laha und J. Roy, Handbuch der Methoden der angewandten Statistik, Bd. 1, Techniken der
Computation, Descriptive Methods, and Statistical Inference (New York: Wiley, 1967).
33 Eric Feigelson und G. Jogesh Babu, "Beware the Kolmogorov-Smirnov Test!" Zentrum für Astrostatistik, Penn
State University, https://oreil.ly/7AHcT.
34 Eric Breck, Marty Zinkevich, Neoklis Polyzotis, Steven Whang, und Sudip Roy, "Data Validation for Machine
Learning," Proceedings of SysML, 2019, https://oreil.ly/xoneh.
35 Li Bu, Cesare Alippi, and Dongbin Zhao, "A pdf-Free Change Detection Test Based on Density Difference
Estimation," IEEE Transactions on Neural Networks and Learning Systems 29, no. 2 (February 2018): 324-34,
https://oreil.ly/RD8Uy. Die Autoren behaupten, dass die Methode bei mehrdimensionalen Eingaben funktioniert.
36 Stephan Rabanser, Stephan Günnemann, und Zachary C. Lipton, "Failing Loudly: Eine empirische Studie über
Methods for Detecting Dataset Shift," arXiv, October 29, 2018, https://oreil.ly/HxAwV.

Nur weil der Unterschied statistisch signifikant ist, heißt das noch lange nicht, dass er
er praktisch wichtig ist. Eine gute Heuristik ist jedoch, dass, wenn Sie in der Lage sind
wenn man den Unterschied bei einer relativ kleinen Stichprobe feststellen kann, dann handelt es sich wahrscheinlich um einen gravierenden Unterschied.
Wenn eine große Anzahl von Stichproben erforderlich ist, um den Unterschied festzustellen, dann ist er wahrscheinlich nicht
wert, sich darüber Gedanken zu machen.
Ein grundlegender Test für zwei Stichproben ist der Kolmogorov-Smirnov-Test, auch bekannt als K-S
oder KS-Test bekannt ist.^32 Es handelt sich um einen nichtparametrischen statistischen Test, d. h. er erfordert keine
Parameter der zugrundeliegenden Verteilung, um zu funktionieren. Er macht keine Annahmen
über die zugrunde liegende Verteilung, was bedeutet, dass er für jede beliebige Verteilung funktionieren kann.
Ein großer Nachteil des KS-Tests ist jedoch, dass er nur für eindimensionale Daten verwendet werden kann.
dimensionalen Daten verwendet werden kann. Wenn die Vorhersagen und Beschriftungen Ihres Modells eindimensional sind (skalare
Zahlen), dann ist der KS-Test nützlich, um Beschriftungs- oder Vorhersageverschiebungen zu erkennen. Er ist jedoch
funktioniert jedoch nicht bei hochdimensionalen Daten, und Merkmale sind in der Regel hochdimensional.^33
KS-Tests können auch teuer sein und zu viele falsch-positive Alarme erzeugen.^34
Ein weiterer Test ist Least-Squares Density Difference, ein Algorithmus, der auf der
Kleinste-Quadrate-Dichte-Differenz-Schätzmethode basiert.^35 Es gibt auch MMD, Maximum
Mean Discrepancy (Gretton et al. 2012), eine kernelbasierte Technik für multivariate
Zwei-Stichproben-Tests und ihre Variante Learned Kernel MMD (Liu et al. 2020). MMD
ist in der Forschung sehr beliebt, aber zum Zeitpunkt der Erstellung dieses Buches ist mir kein Unternehmen bekannt
das es in der Industrie einsetzt. Alibi Detect ist ein großartiges Open-Source-Paket mit den
Alibi Detect ist ein großartiges Open-Source-Paket mit Implementierungen vieler Algorithmen zur Drifterkennung, wie in Abbildung 8-2 gezeigt.
Da Zweistichprobentests bei niedrigdimensionalen Daten oft besser funktionieren als bei hoch
dimensionalen Daten besser funktionieren als bei hochdimensionalen Daten, ist es sehr empfehlenswert, die Dimensionalität
Ihrer Daten zu reduzieren, bevor Sie einen Zwei-Stichproben-Test durchführen.^36
244 | Kapitel 8: Verschiebung der Datenverteilung und Überwachung
37 Manuel Baena-García, José del Campo-Ávila, Raúl Fidalgo, Albert Bifet, Ricard Gavaldà, und Rafael Morales-
Bueno, "Early Drift Detection Method", 2006, https://oreil.ly/Dnv0s.
38 Nandini Ramanan, Rasool Tahmasbi, Marjorie Sayer, Deokwoo Jung, Shalini Hemachandran, und Clau-
dionor Nunes Coelho Jr., "Real-time Drift Detection on Time-series Data," arXiv, 12. Oktober 2021,
https://oreil.ly/xmdqW.

Abbildung 8-2. Einige von Alibi Detect implementierte Algorithmen zur Drifterkennung. Quelle: Bildschirm-
Shot aus dem GitHub-Repository des Projekts
Zeitskalenfenster für die Erkennung von Verschiebungen
Nicht alle Arten von Verschiebungen sind gleich - einige sind schwieriger zu erkennen als andere. Zum Beispiel,
Verschiebungen mit unterschiedlicher Geschwindigkeit, und abrupte Veränderungen sind leichter zu erkennen als langsame,
^37 Verschiebungen können auch in zwei Dimensionen stattfinden: räumlich oder zeitlich.
Räumliche Verschiebungen sind Verschiebungen, die an verschiedenen Zugangspunkten stattfinden, z. B. wenn Ihre Anwendung eine
eine neue Gruppe von Nutzern oder Ihre Anwendung wird jetzt auf einem anderen Gerätetyp bereitgestellt.
Zeitliche Verschiebungen sind Verschiebungen, die im Laufe der Zeit auftreten. Um zeitliche Verschiebungen zu erkennen, ist ein gängiger
Ansatz darin, die Eingabedaten für ML-Anwendungen als Zeitreihendaten zu behandeln.^38
Verschiebungen in der Datenverteilung | 245
Beim Umgang mit zeitlichen Verschiebungen beeinflusst das Zeitfenster der betrachteten Daten
die Verschiebungen, die wir erkennen können. Wenn Ihre Daten einen wöchentlichen Zyklus haben, dann wird eine Zeitskala von
weniger als eine Woche den Zyklus nicht erkennen. Betrachten Sie die Daten in Abbildung 8-3. Wenn wir
Daten von Tag 9 bis Tag 14 als Quellverteilung verwenden, dann sieht Tag 15 wie eine Verschiebung aus.
Wenn wir jedoch die Daten von Tag 1 bis Tag 14 als Quellverteilung verwenden, dann werden alle Datenpunkte
Punkte von Tag 15 wahrscheinlich durch dieselbe Verteilung erzeugt. Wie dieses Beispiel zeigt
Beispiel zeigt, ist es schwierig, zeitliche Verschiebungen zu erkennen, wenn diese durch
saisonale Schwankungen.

Abbildung 8-3. Ob eine Verteilung im Laufe der Zeit gedriftet ist, hängt von der Zeitskala ab
Fenster ab

Bei der Berechnung laufender Statistiken über die Zeit ist es wichtig, zu unterscheiden zwischen
kumulativen und gleitenden Statistiken zu unterscheiden. Gleitende Statistiken werden innerhalb eines einzelnen Zeitfensters berechnet
Zeitfenster, z. B. eine Stunde, berechnet. Kumulative Statistiken werden ständig mit weiteren Daten aktualisiert.
Daten. Das bedeutet, dass zu Beginn eines jeden Zeitfensters die gleitende Genauigkeit
zurückgesetzt, während die kumulative gleitende Genauigkeit nicht zurückgesetzt wird. Da kumulative Statistiken
Informationen aus früheren Zeitfenstern enthalten, können sie den Blick auf die Vorgänge
was in einem bestimmten Zeitfenster passiert. Abbildung 8-4 zeigt ein Beispiel dafür, wie die kumulative Genauigkeit
den plötzlichen Einbruch der Genauigkeit zwischen den Stunden 16 und 18 verbergen kann.

246 | Kapitel 8: Verschiebung der Datenverteilung und Überwachung

39 Ich arbeite an einer Lösung, die die Granularität im Minutentakt verarbeiten kann.
40 Vielen Dank an Goku Mohandas, der diesen Tipp auf dem MLOps Discord-Server geteilt hat.

Abbildung 8-4. Die kumulative Genauigkeit verdeckt den plötzlichen Einbruch der Genauigkeit zwischen Stunde 16 und
Quelle: Angepasst von einem Bild von MadeWithML
Die Arbeit mit Daten im zeitlichen Raum macht die Dinge sehr viel komplizierter,
und erfordert die Kenntnis von Zeitreihenanalysetechniken wie Zeitreihendekomposition
sitionen, die den Rahmen dieses Buches sprengen würden. Für Leser, die sich für die Zeitserien
Zeitreihendekomposition interessiert sind, bietet Lyft Engineering eine großartige Fallstudie, wie sie ihre
Zeitreihendaten zerlegt, um der Saisonalität des Marktes Rechnung zu tragen.
Heutzutage verwenden viele Unternehmen die Verteilung der Trainingsdaten als Basis
und überwachen die Verteilung der Produktionsdaten in einer bestimmten Granularität
Granularität, z. B. stündlich oder täglich.^39 Je kürzer das Zeitfenster ist, desto schneller können Sie
desto schneller können Sie Änderungen in Ihrer Datenverteilung erkennen. Ein zu kurzes Zeitskalenfenster kann jedoch
Zeitfenster kann jedoch zu Fehlalarmen durch Verschiebungen führen, wie das Beispiel in Abbildung 8-3 zeigt.
Einige Plattformen, insbesondere solche, die sich mit der Analyse von Echtzeitdaten wie der Überwachung befassen, bieten eine
toring, bieten einen Zusammenführungsvorgang an, der es ermöglicht, Statistiken aus kürzeren Zeitfenstern zusammenzuführen
Zeitfenstern zusammenführen, um Statistiken für größere Zeitfenster zu erstellen. Zum Beispiel können Sie
Datenstatistiken, die Sie interessieren, stündlich berechnen und dann diese stündlichen
in Tagesansichten zusammenführen.
Fortschrittlichere Überwachungsplattformen bieten sogar eine Funktion zur Ursachenanalyse (RCA)
die automatisch Statistiken über verschiedene Zeitfenster hinweg analysiert, um genau das Zeitfenster zu erkennen
um genau das Zeitfenster zu ermitteln, in dem eine Datenveränderung stattgefunden hat.^40
Datenverteilung verschiebt sich | 247
41 Wie Han-chung Lee, ein früher Rezensent, feststellte, liegt dies auch daran, dass kleinere Unternehmen nicht genügend Daten für ihre Modelle haben.
Daten für ihre Modelle haben. Wenn man nicht über viele Daten verfügt, ist es besser, ein zeitbasiertes System zu haben, als
als sein System an unzureichende Daten anzupassen.
42 Kun Zhang, Bernhard Schölkopf, Krikamol Muandet, und Zhikun Wang, "Domain Adaptation under Tar-
get and Conditional Shift," Proceedings of the 30th International Conference on Machine Learning (2013),
https://oreil.ly/C123l.
43 Han Zhao, Remi Tachet Des Combes, Kun Zhang, und Geoffrey Gordon, "On Learning Invariant Rep-
resentations for Domain Adaptation," Proceedings of Machine Learning Research 97 (2019): 7523-32,
https://oreil.ly/W78hH.
44 Zachary C. Lipton, Yu-Xiang Wang, and Alex Smola, "Detecting and Correcting for Label Shift with Black
Box Predictors," arXiv, February 12, 2018, https://oreil.ly/zKSlj.

Umgang mit Verschiebungen in der Datenverteilung
Wie Unternehmen mit Datenverschiebungen umgehen, hängt davon ab, wie ausgefeilt ihre ML-Infra-
Struktur ausgereift ist. Am einen Ende des Spektrums haben wir Unternehmen, die gerade erst
mit ML begonnen haben und noch daran arbeiten, ML-Modelle in die Produktion zu bringen, also
Sie sind also vielleicht noch nicht an dem Punkt angelangt, an dem Datenverschiebungen für sie katastrophale Folgen haben.
Irgendwann in der Zukunft - vielleicht in drei, vielleicht in sechs Monaten - könnten sie jedoch feststellen
könnten sie feststellen, dass ihre anfänglich eingesetzten Modelle sich so weit verschlechtert haben, dass sie
mehr Schaden anrichten als Nutzen bringen. Sie müssen dann ihre Modelle an die veränderten Verteilungen anpassen
Verteilungen anpassen oder sie durch andere Lösungen ersetzen.
Gleichzeitig gehen viele Unternehmen davon aus, dass Datenverschiebungen unvermeidlich sind, so dass sie
ihre Modelle in regelmäßigen Abständen - einmal im Monat, einmal in der Woche oder einmal am Tag - neu zu trainieren.
unabhängig vom Ausmaß der Verschiebung. Die Bestimmung der optimalen Häufigkeit des Retrainings
Modelle zu bestimmen, ist eine wichtige Entscheidung, die viele Unternehmen immer noch auf der Grundlage
Bauchgefühl statt auf experimentellen Daten beruhen.^41 Wir werden die Häufigkeit der Nachschulung
Häufigkeit in Kapitel 9.
Damit ein Modell mit einer neuen Verteilung in der Produktion funktioniert, gibt es drei wesentliche
Ansätze. Der erste ist der Ansatz, der derzeit die Forschung dominiert: Modelle trainieren
unter Verwendung großer Datensätze. Die Hoffnung dabei ist, dass das Modell, wenn der Trainingsdatensatz groß genug ist,
wenn der Trainingsdatensatz groß genug ist, das Modell in der Lage sein wird, eine so umfassende Verteilung zu lernen, dass
Datenpunkte, auf die das Modell in der Produktion trifft, wahrscheinlich aus dieser Verteilung stammen werden.
Der zweite Ansatz, der in der Forschung weniger verbreitet ist, besteht darin, ein trainiertes Modell an eine Zielverteilung anzupassen
Verteilung anzupassen, ohne dass neue Kennzeichnungen erforderlich sind. Zhang et al. (2013) verwendeten kausale Interpreta- tionen
Kausalinterpretationen zusammen mit der Kernel-Einbettung von bedingten und marginalen Verteilungen, um
Vorhersagen der Modelle für Kovariatenverschiebungen und Etikettenverschiebungen zu korrigieren, ohne
der Zielverteilung zu korrigieren.42 Ähnlich schlugen Zhao et al. (2020) ein domänen
invariante Repräsentation vor: eine unüberwachte Technik der Domänenanpassung
das Datenrepräsentationen erlernen kann, die invariant zu sich ändernden Verteilungen sind.^43 Allerdings
Dieser Forschungsbereich ist jedoch stark untererforscht und hat noch keine breite Anwendung in der
Industrie.^44
248 | Kapitel 8: Veränderungen der Datenverteilung und Überwachung
45 Einige Anbieter von Überwachungslösungen behaupten, dass ihre Lösungen nicht nur erkennen können, wann Ihr Modell
nicht nur erkennen, wann das Modell neu trainiert werden sollte, sondern auch, auf welchen Daten es neu trainiert werden sollte. Ich habe die Gültigkeit dieser Behauptungen nicht überprüfen können.

Der dritte Ansatz ist das, was in der Branche heute üblicherweise gemacht wird: das Modell neu trainieren
unter Verwendung der markierten Daten aus der Zielverteilung. Die Umschulung Ihres Modells ist jedoch
nicht so einfach. Eine Neutrainierung kann bedeuten, dass Sie Ihr Modell von Grund auf neu trainieren auf
sowohl mit den alten als auch mit den neuen Daten oder die Fortsetzung des Trainings des bestehenden Modells mit neuen Daten. Die
letztere Vorgehensweise wird auch als Feinabstimmung bezeichnet.
Wenn Sie Ihr Modell neu trainieren wollen, stellen sich zwei Fragen. Erstens, ob Sie
Modell von Grund auf neu zu trainieren (zustandslose Umschulung) oder es ab dem letzten
Kontrollpunkt fortsetzen (zustandsorientiertes Training). Zweitens, welche Daten verwendet werden sollen: Daten der letzten 24 Stunden,
der letzten Woche, der letzten 6 Monate oder ab dem Zeitpunkt, an dem die Daten zu driften begonnen haben. Möglicherweise müssen Sie
Sie müssen möglicherweise Experimente durchführen, um herauszufinden, welche Umschulungsstrategie für Sie am besten geeignet ist.^45
In diesem Buch beziehen wir uns mit dem Begriff "Umschulung" sowohl auf das Training von Grund auf als auch auf die Feinabstimmung.
abstimmung. Wir werden im nächsten Kapitel mehr über die Umschulungsstrategie diskutieren.
Leser, die mit der Literatur über Datenverschiebungen vertraut sind, werden diese oft zusammen mit
zusammen mit Domänenanpassung und Transferlernen erwähnt. Wenn Sie eine Verteilung als eine
Domäne betrachtet, dann ist die Frage, wie man sein Modell an neue Verteilungen anpasst, ähnlich
wie die Frage, wie man sein Modell an verschiedene Domänen anpasst.
Ähnlich verhält es sich, wenn man das Lernen einer gemeinsamen Verteilung P(X, Y) als eine Aufgabe betrachtet, dann ist die Anpassung
eines Modells, das für eine gemeinsame Verteilung trainiert wurde, an eine andere gemeinsame Verteilung
als eine Form des Transferlernens bezeichnet werden. Wie in Kapitel 4 erläutert, bezieht sich Transferlernen auf
die Familie der Methoden, bei denen ein für eine Aufgabe entwickeltes Modell als Ausgangspunkt für ein Modell für eine zweite Aufgabe wiederverwendet
Ausgangspunkt für ein Modell für eine zweite Aufgabe verwendet wird. Der Unterschied besteht darin, dass man beim Transferlernen
das Basismodell für die zweite Aufgabe nicht von Grund auf neu trainiert wird. Um jedoch Ihr
Modell an eine neue Verteilung anzupassen, müssen Sie Ihr Modell möglicherweise von Grund auf neu trainieren.
Der Umgang mit Verschiebungen in der Datenverteilung muss nicht erst beginnen, nachdem die Verschiebungen stattgefunden haben.
stattgefunden haben. Es ist möglich, Ihr System so zu gestalten, dass es robuster gegenüber Verschiebungen wird. Ein System
verwendet mehrere Merkmale, und verschiedene Merkmale verschieben sich unterschiedlich schnell. Stellen Sie sich vor, dass
Sie bauen ein Modell, um vorherzusagen, ob ein Nutzer eine App herunterladen wird. Sie könnten
Sie könnten versucht sein, das Ranking dieser App im App-Store als Merkmal zu verwenden, da Apps mit einem höheren Ranking
Apps tendenziell häufiger heruntergeladen werden. Das App-Ranking ändert sich jedoch sehr schnell. Sie
sollten stattdessen das Ranking jeder App in allgemeine Kategorien einteilen, z. B. Top
10, zwischen 11 und 100, zwischen 101 und 1.000, zwischen 1.001 und 10.000, und
und so weiter. Gleichzeitig ändern sich die Kategorien einer App vielleicht viel seltener, aber
aber sie können weniger gut vorhersagen, ob ein Nutzer diese App herunterladen wird. Wenn Sie
Auswahl von Merkmalen für Ihre Modelle sollten Sie die Abwägung zwischen
zwischen der Leistung und der Stabilität eines Merkmals berücksichtigen: Ein Merkmal könnte sehr gut für die
Genauigkeit, verschlechtert sich aber schnell und zwingt Sie, Ihr Modell häufiger zu trainieren.
Datenverteilung verschiebt sich | 249
Vielleicht möchten Sie Ihr System auch so gestalten, dass es sich leichter an
Verschiebungen. Zum Beispiel können sich die Immobilienpreise in Großstädten wie San Francisco viel schneller ändern als im ländlichen Arizona.
San Francisco schneller ändern als im ländlichen Arizona, so dass ein Modell zur Vorhersage von Immobilienpreisen im
Arizona weniger häufig aktualisiert werden muss als ein Modell für San Francisco.
Francisco. Wenn Sie dasselbe Modell für beide Märkte verwenden, müssen Sie die Daten
aus beiden Märkten verwenden, um Ihr Modell in dem von San Francisco geforderten Rhythmus zu aktualisieren.
Wenn Sie jedoch für jeden Markt ein eigenes Modell verwenden, können Sie jedes Modell
nur bei Bedarf aktualisieren.

Bevor wir zum nächsten Abschnitt übergehen, möchte ich noch einmal betonen, dass nicht alle Leistungs
Leistungsverschlechterung von Modellen in der Produktion ML-Lösungen erfordert. Viele ML-Fehler werden heute
werden immer noch durch menschliche Fehler verursacht. Wenn Ihr Modellversagen auf menschliche Fehler zurückzuführen ist,
müssen Sie diese Fehler erst einmal finden, um sie zu beheben. Die Erkennung einer Datenverschiebung ist schwierig, aber
aber noch schwieriger ist es, die Ursachen für eine Verschiebung zu ermitteln.

Überwachung und Beobachtbarkeit
Da die Branche erkannt hat, dass bei einem ML-System viele Dinge schief gehen können, haben viele
Unternehmen begonnen, in die Überwachung und Beobachtbarkeit ihrer ML-Systeme in der
Produktion.

Überwachung und Beobachtbarkeit werden manchmal austauschbar verwendet, aber sie sind unterschiedlich.
ent. Überwachung bezieht sich auf die Verfolgung, Messung und Protokollierung verschiedener Metriken
die uns helfen können, festzustellen, wenn etwas schief läuft. Beobachtbarkeit bedeutet, dass
System so einzurichten, dass wir einen Einblick in unser System erhalten, um herauszufinden
was schief gelaufen ist. Der Prozess, unser System auf diese Weise einzurichten, wird auch als
"Instrumentierung". Beispiele für die Instrumentierung sind das Hinzufügen von Timern zu Ihren Funktionen,
das Zählen von NaNs in Ihren Funktionen, das Verfolgen, wie Eingaben durch Ihre Systeme
Systeme umgewandelt werden, die Protokollierung ungewöhnlicher Ereignisse wie ungewöhnlich lange Eingaben usw. Beobachtbarkeit ist
Teil der Überwachung. Ohne ein gewisses Maß an Beobachtbarkeit ist eine Überwachung unmöglich.

Bei der Überwachung geht es um Metriken. Da ML-Systeme Softwaresysteme sind, sind die ersten
Klasse von Metriken, die Sie überwachen müssen, die Betriebsmetriken. Diese Metriken sind
dienen dazu, den Zustand Ihrer Systeme zu ermitteln. Sie werden im Allgemeinen in drei Ebenen unterteilt
Ebenen unterteilt: das Netzwerk, in dem das System läuft, der Rechner, auf dem das System läuft, und die
Anwendung, die das System ausführt. Beispiele für diese Metriken sind Latenz und Durchsatz;
die Anzahl der Vorhersageanfragen, die Ihr Modell in der letzten Minute, Stunde oder Tag erhält
Tag; der Prozentsatz der Anfragen, die mit einem 2xx-Code zurückkommen; CPU/GPU-Auslastung;
Speicherauslastung; usw. Ganz gleich, wie gut Ihr ML-Modell ist, wenn das System
ausfällt, werden Sie keinen Nutzen daraus ziehen können.

Schauen wir uns ein Beispiel an. Eine der wichtigsten Eigenschaften eines Software
Systems in der Produktion ist die Verfügbarkeit - wie oft ist das System verfügbar, um den
um den Benutzern eine angemessene Leistung zu bieten. Diese Eigenschaft wird durch die Betriebszeit gemessen, die

250 | Kapitel 8: Verschiebung der Datenverteilung und Überwachung

46 "Amazon Compute Service Level Agreement", Amazon Web Services, zuletzt aktualisiert am 24. August 2021,
https://oreil.ly/5bjx9.

Prozentsatz der Zeit, die ein System in Betrieb ist. Die Bedingungen, die bestimmen, ob ein System
ist, sind in den Service Level Objectives (SLOs) oder Service Level Agreements
(SLAs). In einem SLA kann beispielsweise festgelegt sein, dass der Dienst als betriebsbereit gilt, wenn er
eine mittlere Latenzzeit von weniger als 200 ms und ein 99. Perzentil von unter 2 s aufweist.
Ein Dienstanbieter könnte eine SLA anbieten, die seine Betriebszeitgarantie spezifiziert, z. B.
99,99 % der Zeit, und wenn diese Garantie nicht eingehalten wird, gibt er seinen Kunden das Geld zurück.
Geld zurück. Zum Beispiel bietet der AWS EC2-Dienst ab Oktober 2021 eine monatliche Betriebszeit
eine monatliche Betriebszeit von mindestens 99,99 % (vier Neunen), und wenn die monatliche
niedriger ist, erhalten Sie eine Service-Gutschrift für zukünftige EC2-Zahlungen.^46
Eine monatliche Betriebszeit von 99,99 % bedeutet, dass der Dienst nur etwas mehr als 4 Minuten im Monat ausfallen darf.
Minuten im Monat ausfallen, und 99,999 % bedeuten nur 26 Sekunden im Monat!
Bei ML-Systemen geht der Zustand des Systems jedoch über die Betriebszeit hinaus. Wenn
Ihr ML-System läuft, aber seine Vorhersagen sind miserabel, werden Ihre Benutzer nicht
zufrieden sein. Eine weitere Klasse von Metriken, die Sie überwachen sollten, sind ML-spezifische Metriken, die
die Ihnen Auskunft über den Zustand Ihrer ML-Modelle geben.
ML-spezifische Metriken
Innerhalb der ML-spezifischen Metriken gibt es im Allgemeinen vier zu überwachende Artefakte: die
Genauigkeitsmetriken, Vorhersagen, Merkmale und Roheingaben. Dies sind Artefakte
die in vier verschiedenen Stadien einer ML-Systempipeline erzeugt werden, wie in Abbildung 8-5 dargestellt.
Je tiefer ein Artefakt in der Pipeline ist, desto mehr Transformationen hat es durchlaufen.
durchlaufen hat, desto wahrscheinlicher ist es, dass eine Änderung in diesem Artefakt durch Fehler
in einer dieser Transformationen verursacht wird. Aber je mehr Transformationen ein Artefakt durchlaufen hat
Artefakt durchlaufen hat, desto strukturierter ist es geworden und desto näher ist es an den Metriken, die Ihnen
die Ihnen tatsächlich wichtig sind, was seine Überwachung erleichtert. Wir werden uns jedes dieser
Artefakte im Detail in den folgenden Abschnitten.
Abbildung 8-5. Je mehr Transformationen ein Artefakt durchlaufen hat, desto größer ist die Wahrscheinlichkeit, dass seine
Änderungen durch Fehler in einer dieser Transformationen verursacht werden.
Überwachung und Beobachtbarkeit | 251
47 Seien Sie vorsichtig, wenn Sie die Vervollständigungsrate als Metrik zur Optimierung verwenden, da sie Ihr Empfehlungssystem in Richtung kurzer Videos verzerren könnte.
Empfehlungssystem zu kurzen Videos verleiten könnte.

Überwachung genauigkeitsbezogener Metriken
Wenn Ihr System irgendeine Art von Benutzerfeedback für die von ihm getroffenen Vorhersagen erhält -
klicken, ausblenden, kaufen, hoch- oder runterstimmen, favorisieren, bookmarken, teilen usw., sollten Sie
auf jeden Fall protokollieren und verfolgen. Einige Rückmeldungen können dazu verwendet werden, natürliche Labels abzuleiten, die
die dann zur Berechnung der genauigkeitsbezogenen Metriken Ihres Modells verwendet werden können. Genauigkeitsbezogene
Metriken sind die direktesten Metriken, die Ihnen helfen zu entscheiden, ob die Leistung eines Modells
verschlechtert hat.
Auch wenn das Feedback nicht direkt zur Ableitung natürlicher Bezeichnungen verwendet werden kann, so kann es doch verwendet werden, um
Änderungen in der Leistung Ihres ML-Modells zu erkennen. Wenn Sie zum Beispiel ein System entwickeln
Wenn Sie beispielsweise ein System entwickeln, das Benutzern empfiehlt, welche Videos sie sich als nächstes auf YouTube ansehen sollen, möchten Sie
nicht nur verfolgen, ob die Nutzer auf ein empfohlenes Video klicken (Click-Through-Rate),
sondern auch, wie viel Zeit die Nutzer mit dem Video verbringen und ob sie es zu Ende
beenden (Abschlussrate). Wenn im Laufe der Zeit die Klickrate gleich bleibt
aber die Abschlussrate sinkt, könnte dies bedeuten, dass Ihr Empfehlungssystem
schlechter wird.^47
Es ist auch möglich, Ihr System so zu gestalten, dass Sie das Feedback der Benutzer sammeln können.
Google Translate bietet zum Beispiel die Möglichkeit, eine Übersetzung hoch- oder runterzustufen
Übersetzung hoch- oder runterzustufen, wie in Abbildung 8-6 gezeigt. Wenn die Anzahl der Abwertungen, die das System erhält
plötzlich ansteigt, könnte es Probleme geben. Diese Abwertungen können auch zur Steuerung des
um den Beschriftungsprozess zu steuern, z. B. indem menschliche Experten neue Übersetzungen für
neue Übersetzungen für die Beispiele mit Ablehnungen zu erstellen, um die nächste Iteration ihrer Modelle zu trainieren.
Abbildung 8-6. Google Translate ermöglicht es Nutzern, eine Übersetzung hoch- oder herunterzustufen. Diese
Stimmen werden verwendet, um die Qualität des Übersetzungsmodells zu bewerten und um den
Beschriftungsprozess.
Überwachung von Vorhersagen
Die Vorhersage ist das am häufigsten zu überwachende Artefakt. Wenn es sich um eine Regressionsaufgabe handelt, ist jede
Vorhersage ein kontinuierlicher Wert (z. B. der vorhergesagte Preis eines Hauses), und wenn es sich um eine
Klassifizierungsaufgabe, ist jede Vorhersage ein diskreter Wert, der der vorhergesagten Kategorie entspricht.
Kategorie entspricht. Da jede Vorhersage normalerweise nur eine Zahl ist (niedrige Dimension),
252 | Kapitel 8: Datenverteilungsverschiebungen und Überwachung
Vorhersagen leicht zu visualisieren, und ihre zusammenfassenden Statistiken sind einfach zu
berechnen und interpretieren.

Sie können Vorhersagen für Verteilungsverschiebungen überwachen. Da die Vorhersagen niedrig
Vorhersagen niedrigdimensional sind, ist es auch einfacher, Zwei-Stichproben-Tests zu berechnen, um festzustellen, ob die
Vorhersageverteilung verschoben hat. Verteilungsverschiebungen bei Vorhersagen sind auch ein Indikator
für die Verschiebung der Eingangsverteilung. Unter der Annahme, dass sich die Funktion, die von der Eingabe zur
Ausgabe abbildet - die Gewichte und Verzerrungen Ihres Modells haben sich nicht geändert - dann
eine Änderung in der Vorhersageverteilung im Allgemeinen auf eine Änderung in der zugrunde liegenden
Eingabeverteilung.

Sie können die Vorhersagen auch auf merkwürdige Vorkommnisse überwachen, wie z. B. die Vorhersage einer
eine ungewöhnliche Anzahl von Falschmeldungen hintereinander. Es könnte eine lange Verzögerung zwischen Vorhersagen
und den Ground-Truth-Labels, wie im Abschnitt "Natürliche Labels" auf Seite 91 beschrieben.
Änderungen in den genauigkeitsbezogenen Metriken werden möglicherweise erst nach Tagen oder Wochen deutlich,
wohingegen ein Modell, das 10 Minuten lang alle Falschmeldungen vorhersagt, sofort erkannt werden kann.

Überwachungsfunktionen

ML-Überwachungslösungen in der Branche konzentrieren sich auf die Verfolgung von Änderungen der Merkmale, sowohl
sowohl die Merkmale, die ein Modell als Eingaben verwendet, als auch die Zwischenumwandlungen von
Rohdaten in endgültige Merkmale. Die Überwachung von Merkmalen ist attraktiv, weil im Vergleich zu
rohen Eingabedaten sind die Merkmale gut strukturiert und folgen einem vordefinierten Schema. Der erste
Schritt der Merkmalsüberwachung ist die Merkmalsvalidierung: Sie stellt sicher, dass Ihre Merkmale einem
erwarteten Schema folgen. Die erwarteten Schemata werden normalerweise aus Trainingsdaten oder
aus gesundem Menschenverstand. Wenn diese Erwartungen in der Produktion verletzt werden, kann es zu einer
eine Verschiebung in der zugrunde liegenden Verteilung. Hier sind zum Beispiel einige der Dinge, die Sie
für ein bestimmtes Merkmal überprüfen können:

-Wenn die Minimal-, Maximal- oder Medianwerte eines Merkmals innerhalb eines akzeptablen Bereichs liegen
-Wenn die Werte eines Merkmals einem regulären Ausdrucksformat entsprechen
-Wenn alle Werte eines Merkmals zu einer vordefinierten Menge gehören
-Wenn die Werte eines Merkmals immer größer sind als die Werte eines anderen Merkmals
Da Merkmale oft in Tabellen organisiert sind - jede Spalte steht für ein Merkmal
und jede Zeile eine Datenprobe darstellt, wird die Merkmalsvalidierung auch als Tabellenprüfung oder
Test oder Tabellenvalidierung bezeichnet. Manche nennen sie auch Einheitstests für Daten. Es gibt viele Open
Source-Bibliotheken, die Ihnen bei der grundlegenden Feature-Validierung helfen, und die beiden gängigsten
sind Great Expectations und Deequ, das von AWS stammt. Abbildung 8-7 zeigt einige der
eingebauten Funktionen der Feature-Validierung von Great Expectations und ein Beispiel für die
sie verwenden.

Überwachung und Beobachtbarkeit | 253
Abbildung 8-7. Einige der in Great Expectations eingebauten Funktionen zur Merkmalsüberprüfung und
ein Beispiel, wie sie verwendet werden können. Quelle: Angepasst an den Inhalt des Great Expectations
GitHub-Repository

Neben der grundlegenden Merkmalsvalidierung können Sie auch Zweistichprobentests verwenden, um festzustellen, ob
die zugrundeliegende Verteilung eines Merkmals oder einer Gruppe von Merkmalen sich verschoben hat. Da ein
Merkmal oder ein Satz von Merkmalen hochdimensional sein kann, müssen Sie möglicherweise die
Dimension zu reduzieren, bevor Sie den Test durchführen, was den Test weniger effektiv machen kann.

Bei der Überwachung von Merkmalen gibt es vier Hauptprobleme:

Ein Unternehmen kann Hunderte von Modellen in Produktion haben, und jedes Modell verwendet Hunderte
Hunderte, wenn nicht Tausende von Merkmalen.
Selbst etwas so Einfaches wie die stündliche Berechnung von zusammenfassenden Statistiken für alle diese Merkmale
stündlich zu berechnen, kann teuer sein, nicht nur in Bezug auf die benötigte Rechenleistung, sondern auch
Speicherverbrauch. Die Verfolgung, d. h. die ständige Berechnung, zu vieler Metriken kann auch
Ihr System verlangsamen und sowohl die Latenzzeit, die Ihre Nutzer erleben
und die Zeit, die Sie benötigen, um Anomalien in Ihrem System zu erkennen.

Die Nachverfolgung von Merkmalen ist zwar für Debugging-Zwecke nützlich, aber nicht sehr hilfreich, wenn es darum geht
Verschlechterung der Modellleistung.
Theoretisch kann eine kleine Verteilungsverschiebung zu einem katastrophalen Ausfall führen, aber in der Praxis,
können die geringfügigen Änderungen eines einzelnen Merkmals die Leistung des Modells nicht
überhaupt nicht. Die Merkmalsverteilungen verschieben sich ständig, und die meisten dieser Änderungen sind

254 | Kapitel 8: Verschiebung der Datenverteilung und Überwachung

48 Rabanser, Günnemann und Lipton, "Failing Loudly".

gutartig.^48 Wenn Sie immer dann gewarnt werden möchten, wenn ein Merkmal abgewichen zu sein scheint,
könnten Sie bald von Warnungen überschwemmt werden und feststellen, dass die meisten dieser Warnungen
Fehlalarme sind. Dies kann zu einem Phänomen namens "Warnmüdigkeit" führen, bei dem das
Überwachungsteam aufhört, den Warnungen Aufmerksamkeit zu schenken, weil sie so häufig auftreten.
Das Problem der Funktionsüberwachung wird zu dem Problem, zu entscheiden
welche Merkmalsverschiebungen kritisch sind und welche nicht.
Die Merkmalsextraktion erfolgt oft in mehreren Schritten (z. B. Auffüllen fehlender Werte und
Standardisierung), unter Verwendung mehrerer Bibliotheken (z. B. Pandas, Spark), auf mehreren Diensten
(wie BigQuery oder Snowflake).
Sie könnten eine relationale Datenbank als Eingabe für den Merkmalsextraktionsprozess haben
und ein NumPy-Array als Ausgabe. Selbst wenn Sie eine schädliche Veränderung in einem
Merkmal feststellt, ist es möglicherweise unmöglich festzustellen, ob diese Änderung durch eine
der zugrundeliegenden Eingabeverteilung verursacht wurde oder ob sie durch einen Fehler in
einem der zahlreichen Verarbeitungsschritte verursacht wurde.
Das Schema, dem Ihre Merkmale folgen, kann sich im Laufe der Zeit ändern.
Wenn Sie keine Möglichkeit haben, Ihre Schemata zu versionieren und jedes Ihrer Features
auf das erwartete Schema abzubilden, könnte die Ursache für die gemeldete Warnung eher in dem
nicht übereinstimmenden Schema und nicht auf eine Änderung der Daten zurückzuführen sein.
Mit diesen Bedenken soll die Bedeutung der Feature-Überwachung nicht abgetan werden; Änderungen
Änderungen im Merkmalsraum sind eine nützliche Signalquelle für das Verständnis des Zustands Ihrer
ML-Systeme zu verstehen. Wir hoffen, dass die Berücksichtigung dieser Bedenken Ihnen bei der Auswahl einer für Sie geeigneten
Lösung zu wählen, die für Sie geeignet ist.
Überwachung von Roheingaben
Wie im vorangegangenen Abschnitt erläutert, kann eine Änderung der Merkmale durch
Probleme in den Verarbeitungsschritten und nicht durch Änderungen in den Daten verursacht werden. Was wäre, wenn wir die
Roheingaben überwachen, bevor sie verarbeitet werden? Die rohen Eingabedaten sind möglicherweise nicht einfacher zu
da sie aus verschiedenen Quellen in unterschiedlichen Formaten stammen können, die verschiedenen
Strukturen folgen. Die Art und Weise, wie viele ML-Workflows heute aufgebaut sind, macht es auch unmöglich
ML-Ingenieure keinen direkten Zugriff auf die rohen Eingabedaten haben, da diese oft von einem
von einem Datenplattformteam verwaltet werden, das die Daten verarbeitet und an einen Ort
wie ein Data Warehouse, und die ML-Ingenieure können nur Daten aus diesem Data Warehouse abfragen
Datenlager abfragen, in dem die Daten bereits teilweise verarbeitet wurden. Daher ist die Überwachung der Rohdaten
Eingaben oft in der Verantwortung des Datenplattformteams und nicht des Data-Science- oder ML
Team. Daher ist dies nicht Gegenstand dieses Buches.
Überwachung und Beobachtbarkeit | 255
49 Ian Malpass, "Measure Anything, Measure Everything," Code as Craft, 15. Februar 2011,
https://oreil.ly/3KF1K.
50 Andrew Morgan, "Data Engineering in Badoo: Handling 20 Billion Events Per Day," InfoQ, August 9, 2019,
https://oreil.ly/qnnuV.
51 Charity Majors, "Observability-A 3-Year Retrospective," The New Stack, 6. August 2019, https://oreil.ly/Logby.

Bisher haben wir verschiedene Arten von zu überwachenden Metriken besprochen, von betrieblichen Metriken
die allgemein für Softwaresysteme verwendet werden, bis hin zu ML-spezifischen Metriken, die Ihnen helfen, den Überblick über den
den Zustand Ihrer ML-Modelle zu überwachen. Im nächsten Abschnitt werden wir die Toolbox besprechen, die Sie
die Ihnen bei der Überwachung von Metriken hilft.
Überwachungs-Toolbox
Das Messen, Verfolgen und Interpretieren von Metriken für komplexe Systeme ist eine nicht triviale Aufgabe.
Aufgabe, und Ingenieure verlassen sich auf eine Reihe von Tools, die sie dabei unterstützen. Es ist üblich, dass
der Branche Metriken, Protokolle und Traces als die drei Säulen der Überwachung.
Ich finde ihre Unterscheidung jedoch undurchsichtig. Sie scheinen aus der Perspektive
Perspektive derjenigen, die Überwachungssysteme entwickeln: Traces sind eine Form von Protokollen und
Metriken können aus Protokollen berechnet werden. In diesem Abschnitt möchte ich mich auf den Satz von Werkzeugen konzentrieren
aus der Sicht der Benutzer der Überwachungssysteme: Protokolle, Dashboards und Alarme.
Protokolle
Herkömmliche Softwaresysteme stützen sich auf Protokolle zur Aufzeichnung von Ereignissen, die während der Laufzeit auftreten. Ein
Ereignis ist alles, was für die Systementwickler von Interesse sein kann, entweder zum Zeitpunkt des
zum Zeitpunkt des Ereignisses oder zu einem späteren Zeitpunkt für die Fehlersuche und Analyse. Beispiele für Ereignisse
sind z.B. der Start eines Containers, die Menge des benötigten Speichers, der Aufruf einer Funktion,
wann diese Funktion beendet wird, die anderen Funktionen, die diese Funktion aufruft, die
Eingabe und Ausgabe dieser Funktion usw. Vergessen Sie auch nicht, Abstürze, Stack Traces,
Fehlercodes und vieles mehr. Um es mit den Worten von Ian Malpass bei Etsy zu sagen: "Wenn es sich bewegt, verfolgen wir es."^49
Es werden auch Dinge aufgezeichnet, die sich noch nicht geändert haben, für den Fall, dass sie sich später bewegen.
Die Anzahl der Protokolle kann sehr schnell sehr groß werden. Zum Beispiel, im Jahr 2019,
die Dating-App Badoo 20 Milliarden Ereignisse pro Tag verarbeitet.^50 Wenn etwas schiefgeht
Wenn etwas schiefgeht, müssen Sie Ihre Protokolle nach der Abfolge der Ereignisse abfragen, die das Problem verursacht haben - ein
Ein Prozess, der sich wie die Suche nach einer Nadel im Heuhaufen anfühlen kann.
In den Anfängen der Softwarebereitstellung bestand eine Anwendung vielleicht aus einem einzigen Dienst.
Wenn etwas passierte, wusste man, wo es passiert war. Aber heute kann ein System
kann ein System aus vielen verschiedenen Komponenten bestehen: Container, Scheduler, Microservices,
polyglotter Persistenz, Mesh-Routing, ephemeren automatisch skalierenden Instanzen, serverlosen
Lambda-Funktionen. Eine Anfrage kann 20-30 Sprünge vom Senden bis zum
eine Antwort empfangen wird. Der schwierige Teil besteht vielleicht nicht darin, zu erkennen, wann etwas
passiert ist, sondern wo das Problem lag.^51
256 | Kapitel 8: Verschiebung der Datenverteilung und Überwachung
52 "Log Management Market Size, Share and Global Market Forecast to 2026", MarketsandMarkets, 2021,
https://oreil.ly/q0xgh.
53 Leser, die mit der Stream-Verarbeitung nicht vertraut sind, lesen bitte den Abschnitt "Batch-Verarbeitung versus Stream
Verarbeitung" auf Seite 78.

Wenn wir ein Ereignis protokollieren, wollen wir es uns so einfach wie möglich machen, es später zu finden.
Diese Praxis in der Microservice-Architektur wird als verteiltes Tracing bezeichnet. Wir wollen
jedem Prozess eine eindeutige ID geben, so dass die Fehlermeldung, wenn etwas schief geht
Fehlermeldung (hoffentlich) diese ID enthalten wird. So können wir nach den zugehörigen Protokollmeldungen suchen
die mit ihm verbunden sind. Außerdem wollen wir zu jedem Ereignis alle erforderlichen Metadaten aufzeichnen:
den Zeitpunkt des Ereignisses, den Dienst, bei dem es auftritt, die aufgerufene Funktion, den
Benutzer, der mit dem Prozess assoziiert ist, falls vorhanden, usw.
Da die Protokolle so umfangreich und schwierig zu verwalten sind, wurden viele
Tools entwickelt, die Unternehmen bei der Verwaltung und Analyse von Protokollen unterstützen. Der Markt für Protokollmanagement
Markt wird für das Jahr 2021 auf 2,3 Milliarden USD geschätzt und soll bis 2026 auf
4,1 Milliarden USD bis 2026 wachsen.^52
Die manuelle Analyse von Milliarden von protokollierten Ereignissen ist zwecklos, daher verwenden viele Unternehmen ML
um Protokolle zu analysieren. Ein Beispiel für den Einsatz von ML in der Protokollanalyse ist die Erkennung von Anomalien:
Es geht darum, abnormale Ereignisse im System zu erkennen. Ein ausgefeilteres Modell könnte sogar
jedes Ereignis nach seinen Prioritäten klassifizieren, wie z. B. normal, abnormal, Ausnahme, Fehler,
und fatal.
Ein weiterer Anwendungsfall von ML in der Protokollanalyse ist, dass es beim Ausfall eines Dienstes hilfreich sein kann
die Wahrscheinlichkeit zu kennen, dass verwandte Dienste betroffen sind. Dies könnte besonders nützlich sein
nützlich sein, wenn das System von einem Cyberangriff betroffen ist.
Viele Unternehmen verarbeiten Protokolle in Batch-Prozessen. In diesem Szenario sammeln Sie eine große
eine große Anzahl von Protokollen, die dann regelmäßig mit Hilfe von SQL nach bestimmten Ereignissen abgefragt
SQL nach bestimmten Ereignissen zu suchen oder sie mit einem Batch-Prozess wie in einem Spark-, Hadoop- oder Hive-Cluster zu verarbeiten.
Dies macht die Verarbeitung von Protokollen effizient, da Sie verteilte und
MapReduce-Prozesse nutzen können, um den Verarbeitungsdurchsatz zu erhöhen. Da Sie jedoch
Da Sie Ihre Protokolle jedoch periodisch verarbeiten, können Sie Probleme nur periodisch entdecken.
Um Anomalien in Ihren Protokollen zu entdecken, sobald sie auftreten, sollten Sie Ihre Ereignisse
Ereignisse zu verarbeiten, sobald sie protokolliert werden. Dies macht die Protokollverarbeitung zu einem Stream-Processing
Problem.^53 Sie können Echtzeit-Transportmittel wie Kafka oder Amazon Kinesis verwenden, um
Ereignisse zu transportieren, sobald sie protokolliert werden. Für die Suche nach Ereignissen mit bestimmten Merkmalen
in Echtzeit zu suchen, können Sie eine Streaming-SQL-Engine wie KSQL oder Flink SQL nutzen.
Überwachung und Beobachtbarkeit | 257
Dashboards

Ein Bild sagt mehr als tausend Worte. Eine Reihe von Zahlen mag für Sie nichts bedeuten
Ihnen nichts sagen, aber wenn Sie sie in einem Diagramm darstellen, können Sie die Beziehungen zwischen diesen Zahlen erkennen.
Zahlen. Dashboards zur Visualisierung von Metriken sind für die Überwachung von entscheidender Bedeutung.

Ein weiterer Nutzen von Dashboards besteht darin, die Überwachung für Nicht-Ingenieure zugänglich zu machen. Überwachung
toring ist nicht nur für die Entwickler eines Systems, sondern auch für nicht-technische Stakeholder wie Produktmanager und
wie Produktmanager und Geschäftsentwickler.

Auch wenn Diagramme für das Verständnis von Metriken sehr hilfreich sein können, reichen sie
allein nicht aus. Man braucht immer noch Erfahrung und statistisches Wissen. Betrachten Sie die beiden
Diagramme in Abbildung 8-8. Das einzige, was aus diesen Diagrammen ersichtlich ist, ist, dass der Verlust
stark schwankt. Ob es in einem der beiden Diagramme eine Verteilungsverschiebung gibt, kann ich nicht erkennen. Es ist
Es ist einfacher, ein Diagramm zu erstellen, um eine wackelnde Linie zu zeichnen, als zu verstehen, was diese wackelnde Linie
bedeutet.

Abbildung 8-8. Diagramme sind nützlich, um Zahlen zu verstehen, aber sie sind nicht ausreichend

Ein Übermaß an Metriken auf einem Dashboard kann auch kontraproduktiv sein, ein Phänomen
bekannt als Dashboard-Fäule. Es ist wichtig, die richtigen Metriken auszuwählen oder von den
abstrahieren, um Signale auf höherer Ebene zu berechnen, die für Ihre spezifischen
spezifische Aufgaben.

258 | Kapitel 8: Verschiebung der Datenverteilung und Überwachung

Warnungen

Wenn unser Überwachungssystem etwas Verdächtiges entdeckt, müssen wir die richtigen
richtigen Leute darüber zu informieren. Ein Alarm besteht aus den folgenden drei Komponenten:

Eine Alarmierungsrichtlinie
Diese beschreibt die Bedingung für eine Warnung. Sie möchten vielleicht eine Warnung erstellen, wenn
eine Metrik einen Schwellenwert überschreitet, optional über eine bestimmte Dauer. Zum Beispiel,
Sie möchten beispielsweise benachrichtigt werden, wenn die Genauigkeit eines Modells unter 90 % liegt, oder wenn die
HTTP-Antwortlatenz für mindestens 10 Minuten höher als eine Sekunde ist.

Benachrichtigungskanäle
Diese beschreiben, wer benachrichtigt werden soll, wenn die Bedingung erfüllt ist. Die Benachrichtigungen werden
werden in dem von Ihnen eingesetzten Überwachungsdienst angezeigt, z. B. Amazon CloudWatch
oder GCP Cloud Monitoring, aber Sie wollen auch die Verantwortlichen erreichen, wenn
wenn sie sich nicht in diesen Überwachungsdiensten befinden. Sie können zum Beispiel konfigurieren, dass Ihre
Alarme so konfigurieren, dass sie an eine E-Mail-Adresse wie mlops-monitoring@[Ihre Firmen
E-Mail-Domäne] gesendet werden, oder an einen Slack-Kanal wie #mlops-monitoring oder an
PagerDuty.

Eine Beschreibung des Alarms
Dies hilft der alarmierten Person zu verstehen, was vor sich geht. Die Beschreibung sollte
so detailliert wie möglich sein, zum Beispiel:

Genauigkeit des Empfehlungsmodells unter 90%
${Zeitstempel}: Diese Meldung stammt von dem Dienst ${service-name}
Abhängig von der Zielgruppe des Alerts ist es oft notwendig, den Alert
Anweisungen zur Schadensbegrenzung oder ein Runbook, eine Zusammenstellung von
eine Zusammenstellung von Routineverfahren und -vorgängen, die bei der Behandlung der Warnung hilfreich sein können.
Alarmmüdigkeit ist ein echtes Phänomen, wie bereits in diesem Kapitel beschrieben. Warnung
Müdigkeit kann demoralisierend sein - niemand wird gerne mitten in der Nacht geweckt
geweckt zu werden, was nicht in seinen Verantwortungsbereich fällt. Sie ist auch gefährlich - wenn man sich mit
trivialen Alarmen ausgesetzt zu sein, kann Menschen gegenüber kritischen Alarmen desensibilisieren. Es ist wichtig, sinnvolle Bedingungen
Bedingungen festzulegen, damit nur kritische Alarme gesendet werden.

Beobachtbarkeit
Seit Mitte der 2010er Jahre hat die Branche begonnen, den Begriff "Beobachtbarkeit"
anstelle von "Überwachung" zu verwenden. Bei der Überwachung werden keine Annahmen über die Beziehung
zwischen dem internen Zustand eines Systems und seinen Ergebnissen. Sie überwachen die externen
des Systems, um herauszufinden, wann etwas im System schief läuft.
Es gibt keine Garantie dafür, dass die externen Ausgaben Ihnen helfen, herauszufinden, was schief läuft.
schief läuft.

Überwachung und Beobachtbarkeit | 259
54 Suman Karumuri, Franco Solleza, Stan Zdonik, und Nesime Tatbul, "Towards Observability Data Manage-
ment at Scale," ACM SIGMOD Record 49, no. 4 (Dezember 2020): 18-23, https://oreil.ly/oS5hn.

In den Anfängen der Softwareentwicklung waren die Softwaresysteme so einfach, dass
die Überwachung externer Ausgaben für die Softwarewartung ausreichte. Ein System bestand
System bestand früher aus nur wenigen Komponenten, und ein Team hatte die Kontrolle über die gesamte
Code-Basis. Wenn etwas schief ging, war es möglich, Änderungen am System vorzunehmen, um
zu testen und herauszufinden, was falsch lief.
In den letzten zehn Jahren sind die Softwaresysteme jedoch erheblich komplexer geworden.
Jahrzehnt deutlich komplexer geworden. Heute besteht ein Softwaresystem aus vielen Komponenten. Viele dieser Kom- ponenten
Komponenten sind Dienste, die von anderen Unternehmen betrieben werden - Stichwort Cloud Native Services -, was bedeutet
was bedeutet, dass ein Team nicht einmal die Kontrolle über das Innere aller Komponenten des Systems hat.
ihres Systems hat. Wenn etwas schief geht, kann ein Team sein System nicht mehr einfach auseinandernehmen
System auseinandernehmen, um es herauszufinden. Das Team muss sich auf die externen Ergebnisse seines Systems verlassen, um
um herauszufinden, was intern vor sich geht.
Beobachtbarkeit ist ein Begriff, der verwendet wird, um diese Herausforderung anzugehen. Es handelt sich um ein Konzept aus der Kontrolltheorie
Es ist ein Konzept aus der Steuerungstheorie und bezieht sich darauf, "das komplexe
Verhalten von Software anhand von [Ausgaben], die zur Laufzeit vom System gesammelt werden"^54
Telemetrie
Die zur Laufzeit gesammelten Ausgaben eines Systems werden auch als Telemetrie bezeichnet. Telemetrie ist ein weiterer
Telemetrie ist ein weiterer Begriff, der in den letzten zehn Jahren in der Softwareüberwachungsbranche aufgekommen ist. Das
Wort "Telemetrie" kommt von den griechischen Wurzeln tele, was "fern" bedeutet, und metron,
was "messen" bedeutet. Telemetrie bedeutet also so viel wie "Fernmessung". Im Zusammenhang mit der Überwachung
Kontext bezieht sich der Begriff auf Protokolle und Metriken, die von entfernten Komponenten wie
Cloud-Dienste oder Anwendungen, die auf Kundengeräten laufen.
Mit anderen Worten: Observability geht von einer Annahme aus, die stärker ist als die des traditionellen Moni-
toring: dass die internen Zustände eines Systems aus der Kenntnis seiner externen
externen Ausgaben abgeleitet werden können. Interne Zustände können aktuelle Zustände sein, wie z. B. "die GPU-Auslastung
die GPU-Auslastung im Moment" und historische Zustände wie "die durchschnittliche GPU-Auslastung des letzten
Tag".
Wenn bei einem beobachtbaren System etwas schief läuft, sollten wir in der Lage sein, herauszufinden
Logs und Metriken des Systems herauszufinden, was schief gelaufen ist, ohne dass
ohne neuen Code in das System einspeisen zu müssen. Bei der Beobachtbarkeit geht es darum, Ihr System so zu instrumentieren
System so zu instrumentieren, dass genügend Informationen über die Laufzeit eines Systems gesammelt und
analysiert werden.
260 | Kapitel 8: Verschiebung der Datenverteilung und Überwachung
55 Siehe den Abschnitt "Bedeutung von Funktionen" auf Seite 142.

Bei der Überwachung geht es um Metriken, und Metriken werden in der Regel zusammengefasst. Beobachtbarkeit
ermöglicht feinere Metriken, so dass Sie nicht nur wissen können, wann die Leistung eines Modells
Leistung eines Modells nachlässt, sondern auch für welche Arten von Eingaben oder welche Untergruppen von Benutzern
oder über welchen Zeitraum die Leistung des Modells nachlässt. Sie sollten zum Beispiel in der Lage sein
Ihre Protokolle nach den Antworten auf Fragen wie: "Zeige mir alle Benutzer, für die
für die Modell A in der letzten Stunde falsche Vorhersagen lieferte, gruppiert nach ihren Postleitzahlen
gruppiert nach Postleitzahlen" oder "zeige mir die Ausreißer-Anfragen der letzten 10 Minuten" oder "zeige mir alle
Zwischenergebnisse dieser Eingabe durch das System". Um dies zu erreichen, müssen Sie
die Outputs Ihres Systems mit Tags und anderen identifizierenden Schlüsselwörtern protokolliert werden, damit
diese Ausgaben später entlang verschiedener Dimensionen Ihrer Daten in Scheiben und Würfel geschnitten werden können.
In der ML umfasst die Beobachtbarkeit auch die Interpretierbarkeit. Interpretierbarkeit hilft uns zu verstehen
verstehen, wie ein ML-Modell funktioniert, und die Beobachtbarkeit hilft uns zu verstehen, wie das gesamte
ML-System, das das ML-Modell einschließt, funktioniert. Wenn zum Beispiel die Leistung eines Modells
Wenn beispielsweise die Leistung eines Modells in der letzten Stunde nachlässt, kann man interpretieren, welches Merkmal
welches Merkmal am meisten zu den falschen Vorhersagen der letzten Stunde beiträgt, hilft dabei
herauszufinden, was mit dem System schief gelaufen ist und wie man es beheben kann.^55
In diesem Abschnitt haben wir mehrere Aspekte der Überwachung erörtert, von der Frage, welche Daten zu
Daten und Metriken zu überwachen sind, bis hin zu verschiedenen Werkzeugen für die Überwachung und
Beobachtbarkeit. Auch wenn die Überwachung ein leistungsfähiges Konzept ist, ist sie von Natur aus passiv.
Man wartet darauf, dass eine Veränderung eintritt, um sie zu erkennen. Überwachung hilft dabei, das Problem aufzudecken
ohne es zu korrigieren. Im nächsten Abschnitt wird das kontinuierliche Lernen vorgestellt, ein Para-
digm, das Ihnen aktiv dabei helfen kann, Ihre Modelle zu aktualisieren, um auf Verschiebungen zu reagieren.
Zusammenfassung
Dies war vielleicht das schwierigste Kapitel, das ich in diesem Buch geschrieben habe. Der
Grund dafür ist, dass es zwar wichtig ist, zu verstehen, wie und warum ML-Systeme in der
in der Produktion versagen, ist die Literatur dazu begrenzt. Normalerweise denken wir an Forschung
aber dies ist ein Bereich von ML, in dem die Forschung immer noch versucht, den Rückstand
mit der Produktion aufzuholen.
Um Ausfälle von ML-Systemen zu verstehen, haben wir zwischen zwei Arten von Ausfällen unterschieden:
Ausfälle von Softwaresystemen (Ausfälle, die auch bei Nicht-ML-Systemen auftreten) und ML-
spezifische Ausfälle. Auch wenn die meisten ML-Ausfälle heute nicht ML-spezifisch sind,
könnte sich dies mit der Weiterentwicklung der Werkzeuge und der Infrastruktur für MLOps ändern.
Wir haben drei Hauptursachen für ML-spezifische Ausfälle erörtert: Produktionsdaten, die sich
von den Trainingsdaten, Randfälle und degenerierte Rückkopplungsschleifen. Die ersten beiden Ursachen
hängen mit den Daten zusammen, während die letzte Ursache mit dem Systemdesign zusammenhängt, da sie
weil sie auftritt, wenn die Ausgaben des Systems die Eingaben desselben Systems beeinflussen.
Zusammenfassung | 261
Wir haben uns mit einem Fehler beschäftigt, der in den letzten Jahren viel Aufmerksamkeit erregt hat: Daten
Verteilungsverschiebungen. Wir haben drei Arten von Verschiebungen untersucht: Kovariatenverschiebung, Etikettenverschiebung
und Konzeptdrift. Obwohl die Untersuchung von Verteilungsverschiebungen ein wachsender Teilbereich der
ML-Forschung ist, hat die Forschungsgemeinschaft noch keine Standardbeschreibung gefunden. Verschiedene
Arbeiten bezeichnen die gleichen Phänomene mit unterschiedlichen Namen. Viele Studien basieren immer noch auf der
auf der Annahme, dass wir im Voraus wissen, wie sich die Verteilung verschieben wird, oder dass wir
die Bezeichnungen für die Daten sowohl der Quell- als auch der Zielverteilung kennen.
In Wirklichkeit wissen wir jedoch nicht, wie die zukünftigen Daten aussehen werden, und die Beschaffung von
Beschriftungen für neue Daten kann kostspielig, langsam oder einfach nicht durchführbar sein.

Um Verschiebungen erkennen zu können, müssen wir die von uns eingesetzten Systeme überwachen. Die Überwachung ist
ist eine wichtige Praxis für jedes Softwareentwicklungssystem in der Produktion, nicht
nicht nur für ML, und es ist ein Bereich von ML, in dem wir so viel wie möglich von der
DevOps-Welt lernen sollten.

Bei der Überwachung geht es um Metriken. Wir haben verschiedene Metriken besprochen, die wir überwachen müssen:
betriebliche Metriken - Metriken, die bei jedem Softwaresystem überwacht werden sollten
wie Latenz, Durchsatz und CPU-Auslastung, und ML-spezifische Metriken. Überwachung
toring kann auf genauigkeitsbezogene Metriken, Vorhersagen, Merkmale und/oder rohe
Eingaben.

Die Überwachung ist schwierig, denn auch wenn es billig ist, Metriken zu berechnen, ist das
Metriken nicht einfach zu verstehen ist. Es ist einfach, Dashboards zu erstellen, die Diagramme zeigen, aber es ist
viel schwieriger zu verstehen, was ein Diagramm bedeutet, ob es Anzeichen
Anzeichen einer Abweichung zeigt, und wenn es eine Abweichung gibt, ob diese durch eine zugrunde liegende
oder durch Fehler in der Pipeline verursacht wird. Ein Verständnis der Statistik könnte erforderlich sein
um den Sinn der Zahlen und Diagramme zu verstehen.

Der erste Schritt besteht darin, die Verschlechterung der Modellleistung in der Produktion zu erkennen. Der nächste
Schritt ist die Anpassung unserer Systeme an sich ändernde Umgebungen, die wir im
nächsten Kapitel.

262 | Kapitel 8: Verschiebung der Datenverteilung und Überwachung

KAPITEL 9

Kontinuierliches Lernen und Testen in der Produktion
In Kapitel 8 haben wir verschiedene Möglichkeiten diskutiert, wie ein ML-System in der Produktion versagen kann.
Wir haben uns auf ein besonders heikles Problem konzentriert, das sowohl in der Forschung als auch in der Praxis zu vielen Diskussionen geführt hat
sowohl in der Forschung als auch in der Praxis für viel Diskussionsstoff gesorgt hat: Verschiebungen in der Datenverteilung. Wir besprachen auch
mehrere Überwachungstechniken und -tools zur Erkennung von Verschiebungen in der Datenverteilung.

Dieses Kapitel ist eine Fortsetzung dieser Diskussion: Wie passen wir unsere Modelle an die
Verteilungsänderungen an? Die Antwort lautet, dass wir unsere ML-Modelle kontinuierlich aktualisieren. Wir beginnen
mit einer Diskussion darüber, was kontinuierliches Lernen ist und welche Herausforderungen damit verbunden sind.
Lernen ist weitgehend ein infrastrukturelles Problem. Dann werden wir einen Vier-Stufen-Plan aufstellen, um
um kontinuierliches Lernen Wirklichkeit werden zu lassen.

Nachdem Sie Ihre Infrastruktur so eingerichtet haben, dass Sie Ihre Modelle beliebig oft aktualisieren können, sollten Sie die Frage
können, sollten Sie sich die Frage stellen, die mir von fast allen ML-Ingenieuren gestellt wurde
von fast jedem ML-Ingenieur gestellt wurde, den ich getroffen habe: "Wie oft sollte ich meine Modelle neu trainieren?"
Diese Frage steht im Mittelpunkt des nächsten Abschnitts des Buches.

Wenn das Modell neu trainiert wird, um sich an die veränderte Umgebung anzupassen, reicht es nicht aus, es auf
ein stationäres Testset nicht ausreichen. Wir werden ein scheinbar erschreckendes, aber notwendiges
Konzept: Test in der Produktion. Dieser Prozess ist eine Möglichkeit, Ihre Systeme mit Live-Daten zu testen
Daten in der Produktion zu testen, um sicherzustellen, dass Ihr aktualisiertes Modell tatsächlich ohne katastrophale
Folgen.

Die Themen dieses Kapitels und des vorherigen Kapitels sind eng miteinander verknüpft. Testen in der Produktion
ist komplementär zur Überwachung. Wenn Überwachung bedeutet, passiv zu beobachten
die Ausgaben des verwendeten Modells zu verfolgen, bedeutet Test in der Produktion, proaktiv
die Auswahl des Modells, das die Ausgaben produziert, damit wir es bewerten können. Das Ziel von
Überwachung und Test in der Produktion ist es, die Leistung eines Modells zu verstehen und
herauszufinden, wann es aktualisiert werden muss. Das Ziel des kontinuierlichen Lernens ist es, sicher und effizient

263
1 Joan Serrà, Dídac Surís, Marius Miron, und Alexandros Karatzoglou, "Overcoming Catastrophic Forgetting
with Hard Attention to the Task," arXiv, January 4, 2018, https://oreil.ly/P95EZ.
die Aktualisierung zu automatisieren. All diese Konzepte ermöglichen es uns, ein ML-System zu entwickeln, das
wartbar und an veränderte Umgebungen anpassbar ist.

Dies ist das Kapitel, über das ich am liebsten schreiben würde, und ich hoffe, dass ich auch Sie dafür begeistern kann.
auch dafür begeistern kann!

Kontinuierliches Lernen
Wenn man "kontinuierliches Lernen" hört, denken viele Menschen an das Trainingsparadigma
bei dem sich ein Modell mit jeder eingehenden Probe in der Produktion aktualisiert. Nur sehr wenige
Unternehmen tun dies tatsächlich. Erstens, wenn Ihr Modell ein neuronales Netz ist, macht das Lernen mit
mit jeder eingehenden Stichprobe zu lernen, macht es anfällig für katastrophales Vergessen. Katastrophisches
Vergessen bezieht sich auf die Tendenz eines neuronalen Netzes, beim Lernen neuer Informationen
zu vergessen, wenn es neue Informationen lernt.^1

Zweitens kann das Training dadurch teurer werden - die meisten heutigen Hardware-Backends wurden
Stapelverarbeitung konzipiert, so dass die gleichzeitige Verarbeitung nur einer Probe eine enorme
Verschwendung von Rechenleistung und ist nicht in der Lage, die Datenparallelität zu nutzen.

Unternehmen, die kontinuierliches Lernen in der Produktion einsetzen, aktualisieren ihre Modelle in
Mikro-Batches. Sie könnten beispielsweise das bestehende Modell nach jeweils 512
oder 1.024 Beispielen aktualisieren - die optimale Anzahl von Beispielen in jedem Micro-Batch ist
abhängig.

Das aktualisierte Modell sollte erst dann eingesetzt werden, wenn es evaluiert worden ist. Das bedeutet, dass
Sie sollten Änderungen am bestehenden Modell nicht direkt vornehmen. Stattdessen erstellen Sie eine Replik
lica des bestehenden Modells und aktualisieren diese Replik mit neuen Daten und ersetzen das
Ersetzen Sie das bestehende Modell nur dann durch die aktualisierte Kopie, wenn sich die aktualisierte Kopie als besser erweist. Das
bestehende Modell wird als Champion-Modell bezeichnet und die aktualisierte Kopie als Challenger.
Dieser Prozess ist in Abbildung 9-1 dargestellt. Dies ist eine starke Vereinfachung des Prozesses zum
um das Verständnis zu erleichtern. In der Realität kann ein Unternehmen mehrere Herausforderer gleichzeitig haben
und der Umgang mit einem gescheiterten Herausforderer ist viel komplizierter als
als ihn einfach zu verwerfen.

264 | Kapitel 9: Kontinuierliches Lernen und Testen in der Produktion

2 Es handelt sich um ein "zustandsorientiertes Training" und nicht um ein "zustandsorientiertes Retraining", weil es hier kein Re-Training gibt. Das Modell setzt
vom letzten Zustand aus weiter trainiert.
Abbildung 9-1. Eine Vereinfachung, wie kontinuierliches Lernen in der Produktion funktionieren könnte. In
ist der Prozess der Behandlung eines gescheiterten Herausforderers viel anspruchsvoller als
als ihn einfach zu verwerfen.

Der Begriff "kontinuierliches Lernen" lässt jedoch vermuten, dass die Modelle sehr häufig aktualisiert werden
häufig, etwa alle 5 oder 10 Minuten. Viele Leute argumentieren, dass die meisten Unternehmen
ihre Modelle nicht so häufig aktualisieren müssen, und zwar aus zwei Gründen. Erstens, sie
Erstens haben sie nicht genug Datenverkehr (d. h. genügend neue Daten), als dass ein solcher Aktualisierungsplan sinnvoll wäre.
sinnvoll ist. Zweitens, ihre Modelle verfallen nicht so schnell. Ich stimme mit ihnen überein. Wenn die Änderung des
Umschulungszeitplans von einer Woche auf einen Tag bringt keinen Nutzen und verursacht mehr Aufwand,
gibt es keinen Grund, es zu tun.

Zustandsloses Retraining versus zustandsorientiertes Training
Beim kontinuierlichen Lernen geht es jedoch nicht um die Häufigkeit des Retrainings, sondern um die Art und Weise
Art und Weise, wie das Modell neu trainiert wird. Die meisten Unternehmen führen zustandsloses Retraining durch - das Modell
wird jedes Mal von Grund auf neu trainiert. Kontinuierliches Lernen bedeutet auch zustandsorientiertes
Training - das Modell wird anhand neuer Daten weiter trainiert.^2 Stateful Training ist auch bekannt als
als Feinabstimmung oder inkrementelles Lernen bekannt. Der Unterschied zwischen zustandsloser Umschulung
und zustandsorientiertem Training wird in Abbildung 9-2 veranschaulicht.

Kontinuierliches Lernen | 265
3 Alex Egg, "Online Learning for Recommendations at Grubhub," arXiv, July 15, 2021, https://oreil.ly/FBBUw.
Abbildung 9-2. Zustandslose Umschulung versus zustandsorientiertes Training

Das zustandsorientierte Training ermöglicht es Ihnen, Ihr Modell mit weniger Daten zu aktualisieren. Die Ausbildung eines Modells
von Grund auf zu trainieren, erfordert in der Regel viel mehr Daten als die Feinabstimmung desselben Modells. Für
Wenn Sie beispielsweise Ihr Modell von Grund auf neu trainieren, müssen Sie möglicherweise alle Daten
der letzten drei Monate verwenden. Wenn Sie jedoch eine Feinabstimmung Ihres Modells anhand des gestrigen
Kontrollpunktes vornehmen, müssen Sie nur die Daten des letzten Tages verwenden.

Grubhub hat herausgefunden, dass das zustandsabhängige Training eine schnellere Konvergenz ihrer Modelle ermöglicht und
viel weniger Rechenleistung benötigen. Die Umstellung von täglichem zustandslosen Training auf tägliches
zustandsbehaftetem Training reduzierte sich der Rechenaufwand für das Training um das 45-fache und erhöhte die
Durchkaufsrate um 20 %.^3

Eine schöne Eigenschaft, die oft übersehen wird, ist, dass es mit zustandsorientiertem Training möglich ist
möglich ist, die Speicherung von Daten ganz zu vermeiden. Bei der traditionellen zustandslosen Umschulung kann ein
Datenmuster während mehrerer Trainingsiterationen eines Modells wiederverwendet werden, was
was bedeutet, dass die Daten gespeichert werden müssen. Dies ist nicht immer möglich, insbesondere bei Daten mit
strengen Datenschutzanforderungen. Im Paradigma des zustandsorientierten Trainings wird jede Modellaktualisierung
wird jede Modellaktualisierung nur mit den frischen Daten trainiert, so dass ein Datenmuster nur einmal für das Training verwendet wird, wie
in Abbildung 9-2 dargestellt. Das bedeutet, dass Sie Ihr Modell trainieren können, ohne dass Sie
Daten in einem permanenten Speicher zu speichern, wodurch viele Bedenken hinsichtlich des
Datenschutz. Dies wird jedoch übersehen, weil die heutige Praxis des "Wir-halten-alles-auf-dem-Konto
Praxis viele Unternehmen immer noch zögern lässt, Daten wegzuwerfen.

266 | Kapitel 9: Kontinuierliches Lernen und Testen in der Produktion

4 Mu Li, Li Zhou, Zichao Yang, Aaron Li, Fei Xia, David G. Andersen, und Alexander Smola, "Parameter Server
for Distributed Machine Learning" (NIPS Workshop on Big Learning, Lake Tahoe, CA, 2013), https://oreil.ly/
xMmru.
5 Jonathan Raiman, Susan Zhang, und Christy Dennison, "Neural Network Surgery with Sets," arXiv, December
13, 2019, https://oreil.ly/SU0F1.
Stateful Training bedeutet nicht, dass man nicht von Grund auf trainieren muss. Die Unternehmen, die
Unternehmen, die das zustandsorientierte Training am erfolgreichsten einsetzen, trainieren ihr Modell gelegentlich auch von Grund auf
von Grund auf mit einer großen Menge an Daten, um es zu kalibrieren. Alternativ dazu können sie auch
von Grund auf parallel zum Stateful Training trainieren und dann beide Modelle
aktualisierte Modelle mit Techniken wie dem Parameterserver kombinieren.^4

Sobald Ihre Infrastruktur so eingerichtet ist, dass sie sowohl zustandslose Umschulung als auch zustandsabhängige
Training zu ermöglichen, ist die Trainingshäufigkeit nur noch ein Drehknopf, an dem Sie drehen müssen. Sie können Ihre Modelle aktualisieren
einmal pro Stunde, einmal pro Tag oder immer dann, wenn eine Verteilungsverschiebung festgestellt wird. Wie Sie den
optimalen Zeitplan für die Umschulung zu finden, wird im Abschnitt "Wie oft sollten Sie Ihre Modelle aktualisieren?
Ihre Modelle" auf Seite 279.

Beim kontinuierlichen Lernen geht es darum, die Infrastruktur so einzurichten, dass Sie, ein Datenwissenschaftler oder
Datenwissenschaftler oder ML-Ingenieur seine Modelle bei Bedarf aktualisieren kann, sei es von
zu aktualisieren, egal ob von Grund auf oder zur Feinabstimmung, und diese Aktualisierung schnell bereitzustellen.

Sie fragen sich vielleicht: Stateful Training klingt gut, aber wie funktioniert das, wenn ich
ein neues Merkmal oder eine weitere Schicht zu meinem Modell hinzufügen möchte? Um diese Frage zu beantworten, müssen wir zwischen
zwei Arten von Modellaktualisierungen unterscheiden:

Modell-Iteration
Ein neues Merkmal wird zu einer bestehenden Modellarchitektur hinzugefügt oder die Modellarchitek-
Architektur wird geändert.

Daten-Iteration
Die Modellarchitektur und die Funktionen bleiben gleich, aber Sie aktualisieren das Modell
mit neuen Daten.

Heutzutage wird das zustandsabhängige Training hauptsächlich für die Iteration von Daten verwendet, da eine
Modellarchitektur oder das Hinzufügen eines neuen Merkmals immer noch ein Training des resultierenden Modells
von Grund auf neu zu trainieren. Es gibt Untersuchungen, die zeigen, dass es möglich sein könnte, das
von Grund auf zu trainieren, indem Techniken wie Wissenstransfer
transfer (Google, 2015) und Modellchirurgie (OpenAI, 2019). Laut OpenAI,
"Chirurgie überträgt trainierte Gewichte von einem Netzwerk auf ein anderes, nachdem ein Auswahl
Auswahlverfahren, um festzustellen, welche Teile des Modells unverändert bleiben und welche neu initialisiert werden müssen.
neu initialisiert werden müssen."^5 Mehrere große Forschungslabors haben damit experimentiert; mir sind jedoch
sind mir jedoch keine eindeutigen Ergebnisse in der Industrie bekannt.

Kontinuierliches Lernen | 267
6 Diese Art von Problem wird auch als "dynamische Preisbildung" bezeichnet.
Terminologische Zweideutigkeit
Ich verwende den Begriff "kontinuierliches Lernen" anstelle von "Online-Lernen", denn wenn ich
Wenn ich "Online-Lernen" sage, denken die Leute normalerweise an Online-Bildung. Wenn Sie "Online-Lernen" bei Google eingeben
Lernen" bei Google eingibt, werden die ersten Ergebnisse wahrscheinlich von Online-Kursen handeln.
Manche Leute verwenden "Online-Lernen", um sich auf die spezielle Umgebung zu beziehen, in der ein Modell aus jeder neuen Probe lernt.
aus jeder neuen Probe lernt. In diesem Zusammenhang ist das kontinuierliche Lernen eine Verallgemeinerung
des Online-Lernens.
Ich verwende auch den Begriff "kontinuierliches Lernen" anstelle von "kontinuierliches Lernen". Kontinuierliches
Lernen bezieht sich auf das Regime, in dem Ihr Modell kontinuierlich mit jeder neuen Stichprobe lernt.
Stichprobe lernt, während beim kontinuierlichen Lernen das Lernen in einer Reihe von
Losen oder Mikro-Losen erfolgt.
Kontinuierliches Lernen wird manchmal auch als Bezeichnung für die kontinuierliche Bereitstellung von ML verwendet, die
das eng mit dem kontinuierlichen Lernen verbunden ist, da beide den Unternehmen helfen, den Iterationszyklus ihrer ML-Modelle zu beschleunigen.
tionszyklus ihrer ML-Modelle zu beschleunigen. Der Unterschied besteht jedoch darin, dass "kontinuierliches Lernen"
aus der DevOps-Perspektive um die Einrichtung der Pipeline für die kontinuierliche
für die kontinuierliche Bereitstellung, während "kontinuierliches Lernen" aus der ML-Perspektive stammt.
Aufgrund der Zweideutigkeit des Begriffs "kontinuierliches Lernen" hoffe ich, dass die Gemeinschaft
von diesem Begriff ganz wegbleiben kann.
Warum kontinuierliches Lernen?
Wir haben besprochen, dass es beim kontinuierlichen Lernen darum geht, eine Infrastruktur einzurichten, mit der Sie
Ihre Modelle aktualisieren und diese Änderungen so schnell wie möglich einführen können. Aber warum sollten Sie
brauchen Sie die Möglichkeit, Ihre Modelle so schnell zu aktualisieren, wie Sie wollen?

Der erste Anwendungsfall des kontinuierlichen Lernens ist die Bekämpfung von Verschiebungen in der Datenverteilung, insbesondere
wenn die Verschiebungen plötzlich auftreten. Stellen Sie sich vor, Sie erstellen ein Modell zur Bestimmung
Preise für einen Mitfahrdienst wie Lyft zu ermitteln.^6 Historisch gesehen ist die Nachfrage nach Fahrten an einem
Donnerstagabend in dieser bestimmten Gegend gering, so dass das Modell niedrige Preise vorhersagt.
Fahrpreise voraus, was es für die Fahrer weniger attraktiv macht, sich auf den Weg zu machen. Dennoch,
An diesem Donnerstagabend findet jedoch eine große Veranstaltung in der Nachbarschaft statt, und plötzlich steigt die
Nachfrage nach Fahrten in die Höhe. Wenn Ihr Modell nicht schnell genug auf diese Veränderung reagieren kann, indem es
indem es seine Preisvorhersage erhöht und mehr Fahrer in diese Gegend mobilisiert,
müssen die Fahrer lange auf eine Fahrt warten, was sich negativ auf das Nutzererlebnis auswirkt.
Sie könnten sogar zu einem Konkurrenten wechseln, was zu Umsatzeinbußen für Sie führt.

Ein weiterer Anwendungsfall des kontinuierlichen Lernens ist die Anpassung an seltene Ereignisse. Stellen Sie sich vor, Sie arbeiten
für eine E-Commerce-Website wie Amazon. Der Schwarze Freitag ist ein wichtiges Einkaufsereignis

268 | Kapitel 9: Kontinuierliches Lernen und Testen in der Produktion

7 Jon Russell, "Alibaba Acquires German Big Data Startup Data Artisans for $103M," TechCrunch, January
8, 2019, https://oreil.ly/4tf5c. Ein früher Kritiker erwähnte, dass es auch möglich ist, dass das Hauptziel dieser
Übernahme war, Alibabas Open-Source-Fußabdruck zu vergrößern, der im Vergleich zu anderen Tech-Giganten winzig ist.
8 Ebenso herausfordernd ist das Problem, wenn das Modell herausfinden soll, wann es einen neuen Film empfehlen soll, den noch niemand gesehen hat
Film empfehlen soll, den noch niemand gesehen und bewertet hat.
9 Lucas Bernardi, Jaap Kamps, Julia Kiseleva, und Melanie J. I. Müller, "The Continuous Cold Start Problem in
e-Commerce Recommender Systems," arXiv, August 5, 2015, https://oreil.ly/GWUyD.
10 Jacopo Tagliabue, Ciro Greco, Jean-Francis Roy, Bingqing Yu, Patrick John Chia, Federico Bianchi,
und Giovanni Cassani, "SIGIR 2021 E-Commerce Workshop Data Challenge," arXiv, April 19, 2021,
https://oreil.ly/8QxmS.

die nur einmal im Jahr stattfindet. Es ist unmöglich, genügend historische Daten zu sammeln
Daten zu sammeln, damit Ihr Modell genaue Vorhersagen darüber machen kann, wie Ihre Kunden
wie sich Ihre Kunden dieses Jahr am Black Friday verhalten werden. Um die Leistung zu verbessern,
sollte Ihr Modell den ganzen Tag über mit neuen Daten lernen. Im Jahr 2019 hat Alibaba
Data Artisans übernommen, das Team, das für die Entwicklung des Stream-Processing
des Stream-Processing-Frameworks Apache Flink, für 103 Millionen Dollar, um Flink für ML-Anwendungsfälle anzupassen.
Flink für ML-Anwendungsfälle zu adaptieren.^7 Ihr Hauptanwendungsfall war die Erstellung besserer Empfehlungen
am Singles Day, einem Shoppingtag in China, der dem Black Friday in den USA ähnelt.
Eine große Herausforderung für die heutige ML-Produktion, die durch kontinuierliches Lernen überwunden werden kann
ist das Problem des kontinuierlichen Kaltstarts. Das Kaltstartproblem tritt auf, wenn Ihr Modell
Vorhersagen für einen neuen Benutzer treffen muss, ohne dass historische Daten vorhanden sind. Zum Beispiel, um
um einem Benutzer zu empfehlen, welche Filme er als nächstes sehen möchte, muss ein Empfehlungssystem
muss ein Empfehlungssystem oft wissen, welche Filme dieser Nutzer zuvor gesehen hat. Aber wenn der Nutzer neu ist,
haben Sie keine Informationen über den Verlauf der Filme, die er gesehen hat, und müssen ihm etwas Generisches vorschlagen,
z. B. die beliebtesten Filme auf Ihrer Website im Moment.^8
Der kontinuierliche Kaltstart ist eine Verallgemeinerung des Kaltstartproblems,^9 denn er kann
nicht nur bei neuen Benutzern, sondern auch bei bestehenden Benutzern auftreten kann. Zum Beispiel kann es passieren
weil ein bestehender Benutzer von einem Laptop zu einem Mobiltelefon wechselt und sein Verhalten
auf einem Telefon anders ist als auf einem Laptop. Es kann passieren, weil Benutzer
nicht eingeloggt sind - die meisten Nachrichtenseiten verlangen nicht, dass sich die Leser zum Lesen anmelden.
Es kann auch vorkommen, dass ein Nutzer einen Dienst so selten besucht, dass die
Daten, die der Dienst über diesen Nutzer hat, veraltet sind. Die meisten Menschen buchen zum Beispiel nur
Hotels und Flüge nur ein paar Mal im Jahr. Coveo, ein Unternehmen, das Suchmaschinen
und Empfehlungssysteme für E-Commerce-Websites anbietet, hat herausgefunden, dass es für eine
dass mehr als 70 % der Käufer einer E-Commerce-Website diese weniger als dreimal im Jahr besuchen
drei Mal pro Jahr besuchen.^10
Wenn sich Ihr Modell nicht schnell genug anpasst, wird es nicht in der Lage sein, Empfehlungen
Wenn sich Ihr Modell nicht schnell genug anpasst, kann es bis zur nächsten Aktualisierung des Modells keine für diese Nutzer relevanten Empfehlungen abgeben. Zu diesem Zeitpunkt
könnten diese Nutzer den Dienst bereits verlassen haben, weil sie nichts finden, was
relevantes finden.
Kontinuierliches Lernen | 269
11 Catherine Wang, "Why TikTok Made Its User So Obsessive? The AI Algorithm That Got You Hooked,"
Towards Data Science, 7. Juni 2020, https://oreil.ly/BDWf8.
12 Siehe den Abschnitt "Datenübermittlung durch Echtzeit-Transport" auf Seite 74.

Wenn wir unsere Modelle so gestalten könnten, dass sie sich an jeden Nutzer während seiner Besuchssitzung anpassen, wären die
Modelle in der Lage sein, den Nutzern schon bei ihrem ersten Besuch genaue, relevante Vorhersagen
ersten Besuch. TikTok zum Beispiel hat erfolgreich kontinuierliches Lernen eingesetzt, um sein
das Empfehlungssystem innerhalb von Minuten an jeden Nutzer anzupassen. Sie laden die App herunter und,
nach ein paar Videos sind die Algorithmen von TikTok in der Lage, mit hoher Genauigkeit vorherzusagen, was
was man als Nächstes sehen möchte.^11 Ich glaube nicht, dass jeder versuchen sollte, etwas
wie TikTok zu entwickeln, aber es ist der Beweis dafür, dass kontinuierliches Lernen ein starkes
Vorhersagepotenzial erschließen kann.
Die Frage "Warum kontinuierliches Lernen?" sollte umformuliert werden in "Warum nicht kontinuierliches Lernen?"
Kontinuierliches Lernen ist eine Obermenge des Batch-Lernens, denn es ermöglicht alles, was
was das traditionelle Batch-Lernen auch kann. Kontinuierliches Lernen ermöglicht es Ihnen aber auch
Anwendungsfälle zu erschließen, die beim Batch-Lernen nicht möglich sind.
Wenn die Einrichtung des kontinuierlichen Lernens denselben Aufwand und dieselben Kosten verursacht wie das Batch
Batch-Lernen kostet, gibt es keinen Grund, nicht auf kontinuierliches Lernen zu setzen. Zum Zeitpunkt der Abfassung dieses Buches gibt es
gibt es immer noch eine Menge Herausforderungen bei der Einrichtung des kontinuierlichen Lernens, wie wir im
folgenden Abschnitt eingehen. Die MLOps-Werkzeuge für kontinuierliches Lernen werden jedoch immer ausgereifter,
was bedeutet, dass es eines nicht allzu fernen Tages so einfach sein könnte, kontinuierliches Lernen einzurichten
wie das Batch-Lernen.
Kontinuierliches Lernen - eine Herausforderung
Auch wenn es für kontinuierliches Lernen viele Anwendungsfälle gibt und viele Unternehmen
mit großem Erfolg angewandt haben, ist kontinuierliches Lernen noch mit vielen Herausforderungen verbunden. In diesem
Abschnitt werden wir drei große Herausforderungen erörtern: Zugang zu neuen Daten, Bewertung und
Algorithmen.
Herausforderung beim Zugriff auf neue Daten
Die erste Herausforderung ist die Beschaffung frischer Daten. Wenn Sie Ihr Modell stündlich aktualisieren
Modell stündlich aktualisieren will, braucht man jede Stunde neue Daten. Derzeit beziehen viele Unternehmen
neue Trainingsdaten aus ihren Data Warehouses. Die Geschwindigkeit, mit der Sie Daten
aus Ihren Data Warehouses ziehen können, hängt von der Geschwindigkeit ab, mit der diese Daten in
Data Warehouses gespeichert werden. Die Geschwindigkeit kann langsam sein, insbesondere wenn die Daten aus mehreren
Quellen stammen. Eine Alternative besteht darin, den Abruf von Daten zu ermöglichen, bevor sie in Data Warehouses abgelegt werden,
z. B. direkt von Echtzeit-Transportmitteln wie Kafka und Kinesis, die Daten
von Anwendungen zu Data Warehouses transportieren,^12 wie in Abbildung 9-3 dargestellt.
270 | Kapitel 9: Kontinuierliches Lernen und Testen in der Produktion
Abbildung 9-3. Das Abrufen von Daten direkt aus Echtzeittransporten, bevor sie in Data Warehouses abgelegt werden
Warehouses abgelegt werden, können Sie auf aktuellere Daten zugreifen

Es reicht nicht aus, frische Daten zu beziehen. Wenn Ihr Modell beschriftete Daten benötigt, um
zu aktualisieren, was bei den meisten Modellen heutzutage der Fall ist, müssen diese Daten ebenfalls beschriftet werden. In vielen
Anwendungen wird die Geschwindigkeit, mit der ein Modell aktualisiert werden kann, durch die Geschwindigkeit
Geschwindigkeit, mit der die Daten beschriftet werden.

Die besten Kandidaten für kontinuierliches Lernen sind Aufgaben, bei denen man natürliche Kennzeichnungen erhalten kann
mit kurzen Rückkopplungsschleifen. Beispiele für solche Aufgaben sind dynamische Preisgestaltung (basierend auf
geschätzter Nachfrage und Verfügbarkeit), Schätzung der Ankunftszeit, Vorhersage von Aktienkursen,
Vorhersage der Klickrate von Anzeigen und Empfehlungssysteme für Online-Inhalte wie
Tweets, Lieder, kurze Videos, Artikel usw.

Diese natürlichen Bezeichnungen werden jedoch in der Regel nicht als Bezeichnungen erzeugt, sondern als Verhaltens
iorale Aktivitäten, die in Labels extrahiert werden müssen. Lassen Sie uns ein Beispiel durchgehen
um dies zu verdeutlichen. Wenn Sie eine E-Commerce-Website betreiben, könnte Ihre Anwendung registrieren
dass Benutzer A um 22:33 Uhr auf das Produkt mit der ID 32345 klickt. Ihr System
muss in den Protokollen nachsehen, ob diese Produkt-ID dem Benutzer jemals empfohlen wurde.
Benutzer empfohlen wurde, und wenn ja, welche Abfrage zu dieser Empfehlung geführt hat, damit Ihr System
System diese Abfrage mit dieser Empfehlung abgleichen und diese Empfehlung als
als eine gute Empfehlung kennzeichnen kann, wie in Abbildung 9-4 dargestellt.

Abbildung 9-4. Eine Vereinfachung des Prozesses der Extraktion von Bezeichnungen aus dem Benutzerfeedback

Der Prozess, bei dem die Protokolle durchsucht werden, um Bezeichnungen zu extrahieren, wird als Bezeichnungsberechnung bezeichnet.
Bei einer großen Anzahl von Protokollen kann dies recht kostspielig sein. Die Berechnung von Kennzeichnungen kann erfolgen
mit Stapelverarbeitung: z. B. Warten auf die Ablage von Protokollen in Data Warehouses

Kontinuierliches Lernen | 271
13 Siehe den Abschnitt "Batch Processing Versus Stream Processing" auf Seite 78.
14 Tyler Akidau, "Snowflake Streaming: Now Hiring! Helfen Sie mit, die Zukunft von Big Data und Stream Processing zu entwerfen und aufzubauen.
Processing", Snowflake Blog, 26. Oktober 2020, https://oreil.ly/Knh2Y.
15 Arjun Narayan, "Materialize Raises a $60M Series C, Bringing Total Funding to Over $100M," Materialize,
30. September 2021, https://oreil.ly/dqxRb.
16 Khristopher J. Brooks, "Disparity in Home Lending Costs Minorities Millions, Researchers Find," CBS News,
November 15, 2019, https://oreil.ly/SpZ1N; Lee Brown, "Tesla Driver Killed in Crash Posted Videos Driving
Without His Hands on the Wheel," New York Post, May 16, 2021, https://oreil.ly/uku9S; "A Tesla Driver Is
Charged in a Crash Involving Autopilot That Killed 2 People," NPR, January 18, 2022, https://oreil.ly/WWaRA.

bevor Sie einen Batch-Job ausführen, um alle Etiketten auf einmal aus den Protokollen zu extrahieren. Wie jedoch
wie bereits erwähnt, bedeutet dies jedoch, dass wir zunächst auf die Hinterlegung der Daten warten müssen,
und dann auf die Ausführung des nächsten Batch-Jobs warten. Ein viel schnellerer Ansatz wäre die Nutzung der
Stromverarbeitung zu nutzen, um die Etiketten direkt aus den Echtzeittransporten zu extrahieren.^13
Wenn die Iterationsgeschwindigkeit Ihres Modells durch die Beschriftungsgeschwindigkeit beeinträchtigt wird, ist es auch möglich
den Beschriftungsprozess zu beschleunigen, indem man programmatische Beschriftungstools wie Snorkel einsetzt
schnelle Beschriftungen mit minimalem menschlichem Eingriff erzeugen. Es könnte auch möglich sein
Crowdsourced-Labels zu nutzen, um neue Daten schnell zu annotieren.
Da die Werkzeuge für das Streaming noch im Entstehen begriffen sind, ist die Entwicklung einer effizienten
Streaming-First-Infrastruktur für den Zugriff auf frische Daten und die Extraktion schneller Beschriftungen aus
Echtzeit-Transporten kann technisch aufwändig und kostspielig sein. Die gute Nachricht ist, dass
Werkzeuge für das Streaming wachsen schnell. Confluent, die auf Kafka aufbauende Plattform
Kafka aufbaut, ist ein 16-Milliarden-Dollar-Unternehmen (Stand: Oktober 2021). Ende 2020 gründete Snowflake ein
Snowflake hat Ende 2020 ein Team gegründet, das sich auf Streaming konzentriert.^14 Im September 2021 hat Materialize 100 Millionen Dollar
für die Entwicklung einer Streaming-SQL-Datenbank aufgebracht.^15 Mit der Reifung der Werkzeuge für Streaming wird es für
wird es für Unternehmen viel einfacher und billiger sein, eine Streaming-First-Infrastruktur
für ML zu entwickeln.
Herausforderung der Auswertung
Die größte Herausforderung beim kontinuierlichen Lernen besteht nicht darin, eine Funktion zu schreiben, die das Modell kontinuierlich aktualisiert.
Modell zu aktualisieren - das können Sie mit einem Skript erledigen! Die größte Herausforderung besteht darin
sicherzustellen, dass diese Aktualisierung gut genug ist, um eingesetzt zu werden. In diesem Buch haben wir
diskutiert, wie ML-Systeme in der Produktion katastrophale Fehler machen, von Millionen von
Minderheiten, denen zu Unrecht Kredite verweigert werden, bis hin zu Fahrern, die dem Autopiloten zu sehr vertrauen und
in tödliche Unfälle verwickelt werden.^16
Die Risiken für katastrophale Fehler steigen mit fortlaufendem Lernen. Erstens, je häufiger
je häufiger Sie Ihre Modelle aktualisieren, desto mehr Möglichkeiten gibt es, dass Updates
scheitern.
Zweitens macht das kontinuierliche Lernen Ihre Modelle anfälliger für koordinierte
Manipulation und gegnerische Angriffe. Da Ihre Modelle online aus
272 | Kapitel 9: Kontinuierliches Lernen und Testen in der Produktion
17 James Vincent, "Twitter Taught Microsofts Friendly AI Chatbot to Be a Racist Asshole in Less Than a Day,"
The Verge, 24. Mai 2016, https://oreil.ly/NJEVF.
18 Ihr Betrugserkennungssystem besteht aus mehreren ML-Modellen.
19 Im Abschnitt "Bandits" auf Seite 287 erfahren wir, wie Bandits als eine dateneffizientere Alternative zu A/B-Tests verwendet werden können.
Alternative zu A/B-Tests verwendet werden können.

Daten aus der realen Welt macht es den Nutzern leichter, bösartige Daten einzugeben, um Modifikationen zu überlisten.
zu verleiten, falsche Dinge zu lernen. Im Jahr 2016 veröffentlichte Microsoft Tay, einen Chatbot, der
der durch "zwanglose und spielerische Konversation" auf Twitter lernen kann. Kaum war Tay
startete, begannen Trolle, dem Bot rassistische und frauenfeindliche Bemerkungen zu twittern. Der Bot begann bald
hetzerische und beleidigende Tweets zu posten, was Microsoft dazu veranlasste, den
Bot 16 Stunden nach seinem Start abzuschalten.^17
Um ähnliche oder noch schlimmere Vorfälle zu vermeiden, ist es wichtig, dass Sie jedes Ihrer Modell
Modell-Updates gründlich zu testen, um die Leistung und Sicherheit zu gewährleisten, bevor die
breiteren Publikum. Wir haben die Offline-Evaluierung von Modellen bereits in Kapitel 6 besprochen und werden
Online-Evaluierung (Test in der Produktion) wird in diesem Kapitel behandelt.
Wenn Sie die Evaluierungspipeline für kontinuierliches Lernen entwerfen, sollten Sie bedenken, dass
Evaluierung Zeit braucht, was ein weiterer Engpass für die Häufigkeit der Modellaktualisierung sein kann.
Ein großes Online-Zahlungsunternehmen, mit dem ich zusammengearbeitet habe, hat zum Beispiel ein ML-System
um betrügerische Transaktionen zu erkennen.^18 Die Betrugsmuster ändern sich schnell, daher möchte
ihr System schnell aktualisieren, um es an die sich ändernden Muster anzupassen. Sie können nicht
Das neue Modell kann erst dann eingesetzt werden, wenn es im Vergleich zum aktuellen Modell einem A/B-Test unterzogen wurde. Doch aufgrund
Aufgrund der unausgewogenen Natur der Aufgabe - die meisten Transaktionen sind kein Betrug - dauert es
dauert es etwa zwei Wochen, bis sie genügend Betrugstransaktionen sehen, um genau
um genau beurteilen zu können, welches Modell besser ist.^19 Daher können sie ihr System nur alle zwei
Wochen aktualisieren.
Herausforderung für den Algorithmus
Im Vergleich zur Herausforderung mit den frischen Daten und der Bewertung ist dies eine "weichere" Herausforderung
da sie nur bestimmte Algorithmen und bestimmte Trainingshäufigkeiten betrifft. Um genau zu sein,
betrifft sie nur matrix- und baumbasierte Modelle, die sehr schnell aktualisiert werden müssen
(z. B. stündlich).
Zur Veranschaulichung dieses Punktes betrachten wir zwei verschiedene Modelle: ein neuronales Netz und ein Matrix-
basiertes Modell, wie z. B. ein kollaboratives Filtermodell. Das Modell des kollaborativen Filterns
verwendet eine Benutzer-Element-Matrix und eine Dimensionsreduktionstechnik.
Das neuronale Netzmodell kann mit einem beliebig großen Datenstapel aktualisiert werden. Sie können sogar
den Aktualisierungsschritt mit nur einer Datenprobe durchführen. Wenn Sie jedoch das
Wenn Sie jedoch das kollaborative Filtermodell aktualisieren möchten, müssen Sie zunächst den gesamten Datensatz verwenden, um die
Benutzer-Element-Matrix aufbauen, bevor Sie eine Dimensionalitätsreduktion durchführen. Natürlich können Sie
Natürlich können Sie die Dimensionalitätsreduktion jedes Mal auf Ihre Matrix anwenden, wenn Sie die Matrix aktualisieren
Kontinuierliches Lernen | 273
20 Manche nennen diese Einstellung "Lernen mit partiellen Informationen", aber das Lernen mit partiellen Informationen bezieht sich auf eine andere Einstellung
aber Lernen mit partieller Information bezieht sich auf eine andere Einstellung, wie sie in dem Papier "Subspace Learning with Partial Information" von Gonen et al.
(2016).
21 Pedro Domingos und Geoff Hulten, "Mining High-Speed Data Streams," in Proceedings of the Sixth
International Conference on Knowledge Discovery and Data Mining (Boston: ACM Press, 2000), 71-80;
Albert Bifet und Ricard Gavaldà, "Adaptive Parameter-free Learning from Evolving Data Streams", 2009,
https://oreil.ly/XIMpl.
22 Zohar Karnin, Kevin Lang, and Edo Liberty, "Optimal Quantile Approximation in Streams," arXiv, March 17,
2016, https://oreil.ly/bUu4H.

mit einer neuen Datenprobe, aber wenn Ihre Matrix groß ist, wäre der Schritt der Dimensionalitätsreduktion
zu langsam und zu teuer, um ihn häufig durchzuführen. Daher ist dieses Modell weniger
für das Lernen mit einem partiellen Datensatz weniger geeignet als das vorangegangene neuronale Netzmodell.^20
Es ist viel einfacher, Modelle wie neuronale Netze an das kontinuierliche Lernparadies anzupassen als matrix- und baumbasierte Modelle.
basierte Modelle an das Paradigma des kontinuierlichen Lernens anzupassen. Allerdings gibt es Algo-
rithmen zur Erstellung baumbasierter Modelle, die aus inkrementellen Datenmengen lernen können,
vor allem der Hoeffding-Baum und seine Varianten Hoeffding-Fensterbaum und Hoeffding
Adaptive Tree,^21 aber ihre Anwendung ist noch nicht weit verbreitet.
Nicht nur der Lernalgorithmus muss mit Teildatensätzen arbeiten, sondern auch der
Code zur Merkmalsextraktion muss das auch. Wir haben im Abschnitt "Skalierung" auf Seite 126
besprochen, dass es oft notwendig ist, die Merkmale mithilfe von Statistiken wie Min., Max,
Median und Varianz. Um diese Statistiken für einen Datensatz zu berechnen, müssen Sie oft
einen Durchlauf über den gesamten Datensatz. Wenn Ihr Modell jeweils nur eine kleine Teilmenge der Daten sehen kann
kann, können Sie diese Statistiken theoretisch für jede Teilmenge der Daten berechnen. Allerdings
bedeutet dies jedoch, dass diese Statistiken zwischen den verschiedenen Teilmengen stark schwanken werden. Die
Statistiken, die aus einer Teilmenge berechnet werden, können sich stark von der nächsten Teilmenge unterscheiden, was
Das macht es für das auf einer Teilmenge trainierte Modell schwierig, auf die nächste Teilmenge zu verallgemeinern.
Um diese Statistiken über verschiedene Teilmengen hinweg stabil zu halten, sollten Sie
diese Statistiken online berechnen. Anstatt den Mittelwert oder die Varianz aus allen Daten auf einmal zu verwenden
zu verwenden, berechnen oder approximieren Sie diese Statistiken schrittweise, sobald Sie neue Daten erhalten,
wie z. B. die Algorithmen, die in "Optimale Quantilsapproximation in Streams"^22 beschrieben werden.
Beliebte Frameworks bieten heute eine gewisse Kapazität für die Berechnung laufender Statistiken.
StandardScaler von sklearn verfügt beispielsweise über einen partial_fit, der die Verwendung eines Feature-Scalers
mit laufenden Statistiken verwendet werden kann - aber die eingebauten Methoden sind langsam und unterstützen keine
eine breite Palette von laufenden Statistiken.
Vier Stadien des kontinuierlichen Lernens
Wir haben erörtert, was kontinuierliches Lernen ist, warum kontinuierliches Lernen wichtig ist und welche
Herausforderungen des kontinuierlichen Lernens. Als Nächstes werden wir erörtern, wie man diese Herausforderungen überwindet
und kontinuierliches Lernen zu verwirklichen. Zum Zeitpunkt der Erstellung dieses Buches ist kontinuierliches
ist kontinuierliches Lernen nichts, womit Unternehmen beginnen. Der Schritt zu kontinuierlichem Lernen
274 | Kapitel 9: Kontinuierliches Lernen und Testen in der Produktion
23 Wir werden ML-Plattformen im Abschnitt "ML-Plattform" auf Seite 319 behandeln.

Das Lernen erfolgt in vier Phasen, die im Folgenden beschrieben werden. Wir gehen darauf ein, was in jeder Phase passiert
was in den einzelnen Phasen passiert und welche Voraussetzungen erfüllt sein müssen, um von einer vorherigen Phase zu dieser
Stufe zu gelangen.
Stufe 1: Manuelle, zustandslose Umschulung
Zu Beginn konzentriert sich das ML-Team oft auf die Entwicklung von ML-Modellen, um möglichst viele Geschäftsprobleme zu lösen.
Geschäftsprobleme wie möglich zu lösen. Wenn es sich bei Ihrem Unternehmen zum Beispiel um eine E-Commerce
Website ist, könnten Sie vier Modelle in der folgenden Reihenfolge entwickeln:
1.1. Ein Modell zur Erkennung betrügerischer Transaktionen
2.2. Ein Modell, um den Nutzern relevante Produkte zu empfehlen
3.3. Ein Modell zur Vorhersage, ob ein Verkäufer ein System missbraucht
4.4. Ein Modell zur Vorhersage, wie lange es dauert, eine Bestellung zu versenden
Da sich Ihr Team auf die Entwicklung neuer Modelle konzentriert, tritt die Aktualisierung der bestehenden Modelle
in den Hintergrund getreten. Sie aktualisieren ein bestehendes Modell nur, wenn die folgenden zwei Bedingungen
Bedingungen erfüllt sind: Die Leistung des Modells hat sich so weit verschlechtert, dass es mehr
mehr schadet als nützt, und Ihr Team hat Zeit, es zu aktualisieren. Einige Ihrer Modelle werden
einmal alle sechs Monate aktualisiert. Einige werden einmal im Quartal aktualisiert. Einige sind
sind seit einem Jahr in freier Wildbahn und wurden überhaupt nicht mehr aktualisiert.
Der Prozess der Aktualisierung eines Modells ist manuell und ad hoc. Jemand, normalerweise ein Daten
Ingenieur, muss das Data Warehouse nach neuen Daten abfragen. Jemand anderes bereinigt diese neuen
bereinigt diese neuen Daten, extrahiert daraus Merkmale, trainiert das Modell von Grund auf mit den alten und
neuen Daten und exportiert dann das aktualisierte Modell in ein Binärformat. Dann nimmt jemand
dieses Binärformat und setzt das aktualisierte Modell ein. Oftmals wird der Code
Daten, Merkmale und Modelllogik während des Umschulungsprozesses geändert.
Prozesses geändert, aber diese Änderungen wurden nicht in die Produktion übertragen, was zu Fehlern führt, die
schwer aufzuspüren sind.
Wenn Ihnen dieser Prozess schmerzlich bekannt vorkommt, sind Sie nicht allein. Eine große Mehrheit von
Unternehmen außerhalb der Technologiebranche - z. B. jedes Unternehmen, das ML vor weniger als
vor weniger als drei Jahren eingeführt haben und kein ML-Plattformteam haben, befinden sich in dieser Phase.^23
Stufe 2: Automatisierte Umschulung
Nach ein paar Jahren ist es Ihrem Team gelungen, Modelle zur Lösung der meisten
offensichtlichen Probleme zu lösen. Sie haben zwischen 5 und 10 Modelle in Produktion.
Ihre Priorität liegt nicht mehr in der Entwicklung neuer Modelle, sondern in der Pflege und Verbesserung
bestehenden Modelle. Der manuelle Ad-hoc-Prozess der Aktualisierung von Modellen, der von der
Kontinuierliches Lernen | 275
24 Möglicherweise müssen Sie Ihr Einbettungsmodell häufiger trainieren, wenn Sie jeden Tag viele neue Elemente haben.

Die vorherige Phase hat sich zu einem Problem entwickelt, das nicht mehr ignoriert werden kann. Ihr Team beschließt
ein Skript zu schreiben, das alle Umschulungsschritte automatisch ausführt. Dieses Skript wird dann
in regelmäßigen Abständen mit einem Batch-Prozess wie Spark ausgeführt.
Die meisten Unternehmen mit einer einigermaßen ausgereiften ML-Infrastruktur befinden sich in diesem Stadium. Einige
Unternehmen führen Experimente durch, um die optimale Umschulungsfrequenz zu ermitteln.
quenz zu ermitteln. Für die meisten Unternehmen in dieser Phase wird die Häufigkeit der Nachschulung jedoch
nach dem Bauchgefühl festgelegt, z. B. "einmal am Tag scheint richtig zu sein" oder "lassen Sie uns den
Umschulungsprozess jede Nacht starten, wenn wir freie Rechenleistung haben".
Wenn Sie Skripte zur Automatisierung des Umschulungsprozesses für Ihr System erstellen, müssen Sie
müssen Sie berücksichtigen, dass verschiedene Modelle in Ihrem System möglicherweise unterschiedliche
Umschulungszeitpläne erfordern. Nehmen wir zum Beispiel ein Empfehlungssystem, das aus folgenden Modellen besteht
zwei Modellen besteht: ein Modell zur Erzeugung von Einbettungen für alle Produkte und ein weiteres Modell
um die Relevanz der einzelnen Produkte bei einer Anfrage zu bewerten. Das Einbettungsmodell könnte
muss möglicherweise viel seltener neu trainiert werden als das Ranglistenmodell. Da sich die Produkt
sich die Eigenschaften der Produkte nicht so häufig ändern, können Sie Ihre Einbettungen
Einbettungsmodelle einmal pro Woche neu zu trainieren,^24 wohingegen Ihre Ranking-Modelle möglicherweise
einmal pro Tag neu trainiert werden müssen.
Das Automatisierungsskript kann sogar noch komplizierter werden, wenn es Abhängigkeiten
zwischen Ihren Modellen gibt. Da zum Beispiel das Ranking-Modell von den Einbettungen abhängt
dings abhängt, sollte bei einer Änderung der Einbettungen auch das Ranking-Modell aktualisiert werden.
Anforderungen. Wenn Ihr Unternehmen ML-Modelle in Produktion hat, ist es wahrscheinlich, dass Ihr
Unternehmen bereits über die meisten Infrastrukturkomponenten verfügt, die für ein automatisches Retrain-
benötigen. Die Machbarkeit dieser Phase dreht sich um die Durchführbarkeit des Schreibens eines Skripts zur
Arbeitsablauf zu automatisieren und Ihre Infrastruktur automatisch zu konfigurieren:
1.1. Daten abrufen.
2.2. Downsample oder Upsample dieser Daten, falls erforderlich.
3.3. Extrahieren von Merkmalen.
4.4. Verarbeiten und/oder annotieren Sie Labels, um Trainingsdaten zu erstellen.
5.5. Starten des Trainingsprozesses.
6.6. Evaluierung des neu trainierten Modells.
7.7. Einsetzen des Modells.
Wie lange es dauert, dieses Skript zu schreiben, hängt von vielen Faktoren ab, einschließlich der
Kompetenz des Skriptschreibers ab. Im Allgemeinen sind die drei Hauptfaktoren, die die Durchführbarkeit des Skripts beeinflussen
die Durchführbarkeit dieses Skripts beeinflussen: Scheduler, Daten und Modellspeicher.
276 | Kapitel 9: Kontinuierliches Lernen und Testen in der Produktion
Ein Scheduler ist im Grunde ein Werkzeug, das die Planung von Aufgaben übernimmt, was wir im
Abschnitt "Cron, Scheduler und Orchestratoren" auf Seite 311 behandelt. Wenn Sie nicht bereits einen
einen Scheduler haben, werden Sie Zeit brauchen, um einen einzurichten. Wenn Sie jedoch bereits über einen Scheduler
wie z. B. Airflow oder Argo, sollte es nicht allzu schwierig sein, die Skripte miteinander zu verbinden.

Der zweite Faktor ist die Verfügbarkeit und Zugänglichkeit Ihrer Daten. Müssen Sie
selbst Daten für Ihr Data Warehouse sammeln? Müssen Sie Daten aus verschiedenen
mehreren Organisationen zusammenführen? Müssen Sie viele Funktionen von Grund auf extrahieren? Wird
müssen Sie Ihre Daten auch kennzeichnen? Je mehr Fragen Sie mit Ja beantworten, desto mehr
desto mehr Zeit wird die Einrichtung des Skripts in Anspruch nehmen. Stefan Krawczyk, ML/Data Platform Manager bei
Stitch Fix, kommentierte, dass er vermutet, dass die meiste Zeit hier verbracht werden könnte.

Der dritte Faktor, den Sie benötigen, ist ein Modellspeicher zur automatischen Versionierung und Speicherung aller
Artefakte, die zur Reproduktion eines Modells benötigt werden. Der einfachste Modellspeicher ist wahrscheinlich nur ein
S3-Bucket, der serialisierte Blobs von Modellen in einer strukturierten Weise speichert. Allerdings
Blob-Speicher wie S3 sind jedoch weder besonders gut für die Versionierung von Artefakten geeignet noch für den Menschen lesbar.
Sie benötigen möglicherweise einen ausgereifteren Modellspeicher wie Amazon SageMaker (verwalteter
Service) und MLflow von Databricks (Open Source). Was ein Modellspeicher ist und wie verschiedene
Modellspeicher ist, und bewerten verschiedene Modellspeicher im Abschnitt "Modellspeicher" auf Seite 321.

Wiederverwendung von Merkmalen (Protokollieren und Warten)
Wenn Sie Trainingsdaten aus neuen Daten erstellen, um Ihr Modell zu aktualisieren,
denken Sie daran, dass die neuen Daten bereits den Vorhersagedienst durchlaufen haben.
dienst durchlaufen haben. Dieser Vorhersagedienst hat bereits Merkmale aus den neuen Daten extrahiert
aus diesen neuen Daten extrahiert, um sie in Modelle für Vorhersagen einzugeben. Einige
Unternehmen verwenden diese extrahierten Merkmale erneut für die Modellschulung,
Das spart Berechnungen und ermöglicht die Konsistenz zwischen
Vorhersage und Training ermöglicht. Dieser Ansatz ist als "log and wait" bekannt.
Es ist ein klassischer Ansatz zur Verringerung der in
Kapitel 8 besprochen wurde (siehe Abschnitt "Produktionsdaten unterscheiden sich von Trainingsdaten
Daten" auf Seite 229).
Log and wait ist noch nicht sehr verbreitet, aber es wird immer beliebter.
beliebter. Faire hat einen großartigen Blogbeitrag veröffentlicht, in dem die Vor- und Nachteile des
ihrem "Log and wait"-Ansatz.
Stufe 3: Automatisiertes, zustandsorientiertes Training

In Phase 2 trainieren Sie Ihr Modell jedes Mal von Grund auf neu (zustandslose
Umschulung). Das macht die Umschulung kostspielig, insbesondere bei häufigeren Umschulungen.
Häufigkeit. Sie lesen den Abschnitt "Zustandsloses Retraining versus zustandsorientiertes Training" auf
Seite 265 und entscheiden, dass Sie zustandsorientiert trainieren möchten - warum sollten Sie jeden Tag mit den Daten der letzten drei Monate trainieren
Daten der letzten drei Monate trainieren, wenn Sie das Training nur mit den Daten des letzten Tages
dem letzten Tag trainieren können?

Kontinuierliches Lernen | 277
In dieser Phase konfigurieren Sie also Ihr Skript zur automatischen Aktualisierung so um, dass es beim Start der
Modellaktualisierung gestartet wird, zuerst den vorherigen Kontrollpunkt findet und in den Speicher lädt
Speicher geladen wird, bevor das Training an diesem Kontrollpunkt fortgesetzt wird.

Anforderungen. Das Wichtigste in dieser Phase ist eine Änderung der Denkweise:
Die Umschulung von Grund auf ist eine solche Norm - viele Unternehmen sind so daran gewöhnt, dass Datenwissenschaftler
viele Unternehmen sind es so gewohnt, dass Datenwissenschaftler ein Modell an Ingenieure weitergeben, die es jedes Mal von Grund auf neu implementieren, dass viele
dass viele Unternehmen nicht daran denken, ihre Infrastruktur so einzurichten, dass zustandsorientiertes Training möglich ist.

Sobald Sie sich für zustandsorientiertes Training entschieden haben, ist die Neukonfiguration des Aktualisierungsskripts
einfach. Das Wichtigste, was Sie in dieser Phase brauchen, ist eine Möglichkeit zur Verfolgung Ihrer Daten und
Modelllinie zu verfolgen. Stellen Sie sich vor, Sie laden zuerst die Modellversion 1.0 hoch. Dieses Modell wird aktualisiert
mit neuen Daten aktualisiert, um Modellversion 1.1 zu erstellen, und so weiter, um Modell 1.2 zu erstellen. Dann
wird ein weiteres Modell hochgeladen und Modellversion 2.0 genannt. Dieses Modell wird aktualisiert mit
neuen Daten aktualisiert, um die Modellversion 2.1 zu erstellen. Nach einiger Zeit haben Sie vielleicht Modellversion
3.32, Modellversion 2.11, Modellversion 1.64. Vielleicht möchten Sie wissen, wie sich diese
Modelle sich im Laufe der Zeit entwickeln, welches Modell als Basismodell verwendet wurde und welche Daten
verwendet wurde, um es zu aktualisieren, damit Sie es reproduzieren und debuggen können. Soweit ich weiß, hat kein
existierende Modellspeicher diese Kapazität, so dass Sie die Lösung wahrscheinlich selbst entwickeln müssen.
Lösung selbst entwickeln.

Wenn Sie frische Daten aus den Echtzeittransporten statt aus Data Warehouses beziehen wollen
wie im Abschnitt "Herausforderung beim Zugriff auf frische Daten" auf Seite 270 beschrieben,
und Ihre Streaming-Infrastruktur nicht ausgereift genug ist, müssen Sie möglicherweise Ihre
Ihre Streaming-Pipeline überarbeiten.

Stufe 4: Kontinuierliches Lernen

In Stufe 3 werden Ihre Modelle immer noch nach einem festen Zeitplan aktualisiert, der von den
Entwicklern. Die Suche nach dem optimalen Zeitplan ist nicht einfach und kann situationsabhängig sein.
abhängig sein. Letzte Woche ist zum Beispiel nicht viel auf dem Markt passiert, so dass Ihre
Modelle nicht so schnell verfielen. In dieser Woche ereignen sich jedoch viele Ereignisse, so dass Ihre
Modelle viel schneller ab und erfordern einen viel schnelleren Umschulungsplan.

Anstatt sich auf einen festen Zeitplan zu verlassen, möchten Sie vielleicht, dass Ihre Modelle auto-
automatisch aktualisiert werden, wenn sich die Datenverteilungen ändern und die Leistung des Modells
einbricht.

Der heilige Gral ist die Kombination aus ständigem Lernen und Spitzeneinsatz. Stellen Sie sich vor -
Sie können ein Basismodell mit einem neuen Gerät - einem Telefon, einer Uhr, einer Drohne usw. - ausliefern.
und das Modell auf diesem Gerät wird kontinuierlich aktualisiert und an seine Umgebung angepasst, ohne
Umgebung anpasst, ohne mit einem zentralen Server synchronisiert werden zu müssen. Es besteht keine Notwendigkeit für einen
zentraler Server, was bedeutet, dass keine Kosten für einen zentralen Server anfallen. Es besteht auch keine Notwendigkeit
Daten zwischen dem Gerät und der Cloud hin und her zu übertragen, was eine bessere
Sicherheit und Datenschutz!

278 | Kapitel 9: Kontinuierliches Lernen und Testen in der Produktion

Anforderungen. Der Schritt von Stufe 3 zu Stufe 4 ist steil. Sie brauchen zunächst einen Mecha-
nismus, um Modellaktualisierungen auszulösen. Dieser Auslöser kann sein:

Zeitbasierte
Zum Beispiel, alle fünf Minuten

Leistungsabhängig
Zum Beispiel, wenn die Leistung des Modells sinkt

Volumenbasiert
Wenn zum Beispiel die Gesamtmenge der gekennzeichneten Daten um 5 % steigt

Drift-basiert
Zum Beispiel, wenn eine größere Verschiebung der Datenverteilung festgestellt wird

Damit dieser Auslösemechanismus funktioniert, benötigen Sie eine solide Überwachungslösung. Wir
im Abschnitt "Überwachung und Beobachtbarkeit" auf Seite 250 beschrieben, dass die Schwierigkeit
Teil nicht darin besteht, die Änderungen zu erkennen, sondern zu bestimmen, welche dieser Änderungen von Bedeutung sind.
Wenn Ihre Überwachungslösung eine Menge falscher Alarme ausgibt, wird Ihr Modell
viel häufiger aktualisiert, als es eigentlich nötig wäre.

Außerdem brauchen Sie eine solide Pipeline, um Ihre Modellaktualisierungen kontinuierlich zu bewerten. Das Schreiben von
einer Funktion zur Aktualisierung Ihrer Modelle unterscheidet sich nicht wesentlich von dem, was Sie in Phase 3 tun würden.
Der schwierige Teil besteht darin, sicherzustellen, dass das aktualisierte Modell richtig funktioniert. Wir gehen auf
verschiedene Testtechniken, die Sie im Abschnitt "Testen in der Produktion" auf Seite

Wie oft Sie Ihre Modelle aktualisieren sollten
Jetzt, wo Ihre Infrastruktur so eingerichtet ist, dass Sie ein Modell schnell aktualisieren können, stellen Sie sich
die Frage zu stellen, die ML-Ingenieure in Unternehmen jeder Art und Größe immer wieder beschäftigt
und Größen quält: "Wie oft sollte ich meine Modelle aktualisieren?" Bevor wir versuchen, diese Frage zu beantworten
Frage zu beantworten, müssen wir zunächst herausfinden, wie viel Gewinn Ihr Modell durch die
mit frischen Daten aktualisiert wird. Je mehr Gewinn Ihr Modell aus frischen Daten ziehen kann, desto
desto häufiger sollte es neu trainiert werden.

Wert der Aktualität der Daten

Die Frage, wie oft ein Modell aktualisiert werden sollte, wird viel einfacher, wenn wir wissen, wie
wie sehr sich die Leistung des Modells durch die Aktualisierung verbessern wird. Wenn wir zum Beispiel
wenn wir unser Modell nicht mehr jeden Monat, sondern jede Woche neu trainieren, wie viel Leistungsgewinn
können wir erreichen? Was ist, wenn wir zu einer täglichen Umschulung übergehen? Es heißt immer wieder, dass sich die Daten
Verteilungen verschieben sich, also sind frischere Daten besser, aber wie viel besser sind frischere Daten?

Kontinuierliches Lernen | 279
25 Xinran He, Junfeng Pan, Ou Jin, Tianbing Xu, Bo Liu, Tao Xu, Tanxin Shi, u.a., "Practical Lessons from
Predicting Clicks on Ads at Facebook," in ADKDD '14: Proceedings of the Eighth International Workshop on
Data Mining for Online Advertising (August 2014): 1-9, https://oreil.ly/oS16J.
26 Qian Yu, "Machine Learning with Flink in Weibo," QCon 2019, video, 17:57, https://oreil.ly/Yia6v.

Eine Möglichkeit, den Gewinn herauszufinden, besteht darin, Ihr Modell auf Daten aus verschiedenen
Zeitfenstern in der Vergangenheit trainiert und mit den Daten von heute ausgewertet wird, um zu sehen, wie sich die
Leistung ändert. Nehmen wir zum Beispiel an, dass Sie Daten aus dem Jahr 2020 haben.
Um den Wert der Datenaktualität zu messen, können Sie mit dem Training von Modell
Version A mit den Daten von Januar bis Juni 2020, Modellversion B mit den Daten von
April bis September und Modellversion C mit den Daten von Juni bis November trainieren und dann
testen Sie dann jede dieser Modellversionen mit den Daten von Dezember, wie in Abbildung 9-5 dargestellt.
Der Unterschied in der Leistung dieser Versionen gibt Ihnen ein Gefühl für den
Leistungsgewinn, den Ihr Modell durch frischere Daten erzielen kann. Wenn das Modell, das auf Daten
von vor einem Quartal trainiert wurde, viel schlechter ist als das Modell, das mit Daten von vor einem Monat trainiert wurde,
wissen Sie, dass Sie nicht ein Quartal warten sollten, um Ihr Modell neu zu trainieren.
Abbildung 9-5. Um ein Gefühl für den Leistungsgewinn zu bekommen, den Sie durch frischere Daten erzielen können, trainieren Sie
Sie Ihr Modell mit Daten aus verschiedenen Zeitfenstern in der Vergangenheit und testen es mit Daten von heute
um zu sehen, wie sich die Leistung ändert.
Dies ist ein einfaches Beispiel, um zu veranschaulichen, wie das Experiment zur Datenfrische funktioniert. Unter
Praxis sollten Sie Ihre Experimente viel feinkörniger gestalten und nicht
nicht in Monaten, sondern in Wochen, Tagen, sogar Stunden oder Minuten. Im Jahr 2014 führte Facebook
ein ähnliches Experiment für die Vorhersage der Klickrate von Anzeigen durchgeführt und herausgefunden, dass sie
den Verlust des Modells um 1 % reduzieren konnte, indem es von wöchentlichem auf tägliches Training umstieg.
Dieser Leistungsgewinn war so signifikant, dass das Unternehmen seine
Umschulung von wöchentlich auf täglich umzustellen.^25 Angesichts der Tatsache, dass Online-Inhalte heute so
viel vielfältiger sind und sich die Aufmerksamkeit der Nutzer online viel schneller ändert, können wir uns vorstellen
dass der Wert der Datenfrische für die Klickrate von Anzeigen noch höher ist. Einige der
Unternehmen mit einer hochentwickelten ML-Infrastruktur haben genug Leistungs
Leistungsgewinn gefunden, um ihre Umschulungspipeline auf alle paar Minuten umzustellen.^26
280 | Kapitel 9: Kontinuierliches Lernen und Testen in der Produktion
Modelliteration versus Dateniteration

Wir haben bereits in diesem Kapitel besprochen, dass nicht alle Modellaktualisierungen gleich sind. Wir
unterschieden zwischen Modelliteration (Hinzufügen einer neuen Funktion zu einer bestehenden Modell
Architektur oder Änderung der Modellarchitektur) und Dateniteration (gleiche Modell
Architektur und Funktionen, aber Sie aktualisieren dieses Modell mit neuen Daten). Sie könnten
nicht nur fragen, wie oft Sie Ihr Modell aktualisieren sollten, sondern auch, welche Art von Modell
Aktualisierungen durchgeführt werden sollen.

Theoretisch können Sie beide Arten von Aktualisierungen vornehmen, und in der Praxis sollten Sie beide von Zeit zu Zeit durchführen.
von Zeit zu Zeit durchführen. Je mehr Ressourcen Sie jedoch für einen Ansatz aufwenden, desto weniger
Ressourcen, die Sie für einen anderen Ansatz aufwenden können.

Wenn Sie auf der einen Seite feststellen, dass die Iteration Ihrer Daten Ihnen keinen großen
Leistungssteigerung bringt, sollten Sie Ihre Ressourcen für die Suche nach einem besseren Modell verwenden.
Andererseits, wenn die Suche nach einer besseren Modellarchitektur das 100-fache an Rechenleistung für
Training erfordert und Ihnen 1 % Leistung bringt, während die Aktualisierung desselben Modells mit Daten
Daten der letzten drei Stunden nur 1X Rechenaufwand erfordert und ebenfalls 1 % mehr Leistung bringt
bringt, ist es besser, mit den Daten zu iterieren.

Vielleicht werden wir in naher Zukunft mehr theoretische Erkenntnisse gewinnen, um zu wissen, in welcher
Situation ein Ansatz besser funktioniert (Stichwort "Aufruf zur Forschung"), aber bis heute kann kein
Buch die Antwort darauf geben, welcher Ansatz für Ihr spezifisches Modell und Ihre
Modell für Ihre spezielle Aufgabe besser funktioniert. Um das herauszufinden, müssen Sie Experimente machen.

Die Frage, wie oft Sie Ihr Modell aktualisieren sollten, ist schwierig zu beantworten, und ich
Ich hoffe, dass dieser Abschnitt die Nuancen ausreichend erläutert hat. Am Anfang, wenn
Ihre Infrastruktur noch im Entstehen begriffen ist und die Aktualisierung eines Modells manuell und
langsam ist, lautet die Antwort: so oft wie möglich.

Wenn Ihre Infrastruktur jedoch ausgereift ist und der Prozess der Aktualisierung eines Modells
teilweise automatisiert ist und in wenigen Stunden, wenn nicht Minuten, durchgeführt werden kann, hängt die Antwort
auf diese Frage von der Antwort auf die folgende Frage abhängig: "Wie viel
Leistungsgewinn würde ich durch frischere Daten erzielen?" Es ist wichtig, Experimente durchzuführen, um
den Wert der Datenfrische für Ihre Modelle zu quantifizieren.

Test in der Produktion
In diesem Buch, einschließlich dieses Kapitels, haben wir über die Gefahr gesprochen, Modelle
Einsatz von Modellen, die nicht ausreichend evaluiert wurden. Um Ihre Modelle ausreichend zu evaluieren
zu evaluieren, benötigen Sie zunächst eine Mischung aus Offline-Evaluierung, die in Kapitel 6
besprochenen Offline-Evaluierung und der in diesem Abschnitt behandelten Online-Evaluierung. Um zu verstehen, warum Offline-Evaluierung
nicht ausreicht, gehen wir auf zwei wichtige Testarten für die Offline-Evaluierung ein: Test-Splits und
Backtests.

Test in der Produktion | 281
Die erste Art der Modellevaluation, an die Sie denken könnten, sind die guten alten Test-Splits
die Sie zur Offline-Evaluierung Ihrer Modelle verwenden können, wie in Kapitel 6 beschrieben. Diese Test
sind in der Regel statisch und müssen statisch sein, damit Sie einen zuverlässigen Benchmark zum Vergleich mehrerer Modelle haben.
mehrere Modelle zu vergleichen. Es ist schwierig, die Testergebnisse von zwei Modellen zu vergleichen, wenn
sie auf unterschiedlichen Testsätzen getestet werden.

Wenn Sie jedoch das Modell aktualisieren, um es an eine neue Datenverteilung anzupassen, reicht es nicht aus
es nicht aus, dieses neue Modell anhand von Testsplits aus der alten Verteilung zu bewerten. Unter der Annahme
dass je frischer die Daten sind, desto wahrscheinlicher ist es, dass sie aus der aktuellen Verteilung stammen,
Eine Idee ist, das Modell mit den neuesten Daten zu testen, auf die Sie Zugriff haben. Also,
nachdem Sie Ihr Modell anhand der Daten des letzten Tages aktualisiert haben, sollten Sie
dieses Modell mit den Daten der letzten Stunde testen (unter der Annahme, dass die Daten der letzten Stunde
nicht in den Daten enthalten waren, die zur Aktualisierung Ihres Modells verwendet wurden). Die Methode zum Testen eines
Vorhersagemodells anhand von Daten aus einem bestimmten Zeitraum in der Vergangenheit ist bekannt als
Backtest.

Die Frage ist, ob Backtests ausreichen, um statische Testsplits zu ersetzen. Nicht ganz.
Wenn etwas mit Ihrer Datenpipeline schief gelaufen ist und einige Daten der letzten Stunde
beschädigt sind, reicht es nicht aus, Ihr Modell nur anhand dieser jüngsten Daten zu bewerten.

Bei Backtests sollten Sie Ihr Modell immer noch anhand eines statischen Testsatzes bewerten, den Sie
die Sie ausgiebig studiert haben und denen Sie (meistens) vertrauen, als eine Art "Sanity Check".

Da sich die Datenverteilungen verschieben, bedeutet die Tatsache, dass ein Modell mit den Daten der letzten Stunde gut funktioniert
der letzten Stunde gut funktioniert, bedeutet nicht, dass es auch in Zukunft gut funktionieren wird.
Die einzige Möglichkeit, herauszufinden, ob ein Modell in der Produktion gut funktioniert, besteht darin, es einzusetzen.
Diese Erkenntnis führte zu einem scheinbar erschreckenden, aber notwendigen Konzept: Testen in der Produktion.
Tests in der Produktion müssen jedoch nicht beängstigend sein. Es gibt Techniken, die Ihnen helfen
Ihre Modelle in der Produktion (meistens) sicher zu evaluieren. In diesem Abschnitt behandeln wir
die folgenden Techniken: Shadow Deployment, A/B-Tests, Kanarienvogel-Analyse, verschachtelte
Experimente und Bandits.

Schatten-Implementierung
Die Schattenbereitstellung ist möglicherweise die sicherste Methode zur Bereitstellung Ihres Modells oder einer Software
aktualisieren. Die Schattenbereitstellung funktioniert wie folgt:

1.1. Einsatz des Kandidatenmodells parallel zum bestehenden Modell.
2.2. Jede eingehende Anfrage wird an beide Modelle weitergeleitet, um Vorhersagen zu treffen, aber nur
die Vorhersage des bestehenden Modells an den Benutzer weiter.
3.3. Protokollieren Sie die Vorhersagen des neuen Modells zu Analysezwecken.
Erst wenn Sie festgestellt haben, dass die Vorhersagen des neuen Modells zufriedenstellend sind, können Sie
ersetzen Sie das bestehende Modell durch das neue Modell.

282 | Kapitel 9: Kontinuierliches Lernen und Testen in der Produktion

27 Ron Kohavi und Stefan Thomke, "The Surprising Power of Online Experiments", Harvard Business Review,
September-Oktober 2017, https://oreil.ly/OHfj0.

Da Sie den Nutzern die Vorhersagen des neuen Modells erst dann zur Verfügung stellen, wenn Sie sich vergewissert haben
sichergestellt haben, dass die Vorhersagen des Modells zufriedenstellend sind, ist das Risiko, dass das neue Modell
etwas anstellt, ist gering, zumindest nicht höher als bei dem bestehenden Modell. Allerdings ist diese
Technik ist jedoch nicht immer vorteilhaft, weil sie teuer ist. Sie verdoppelt die Anzahl der Vor
Vorhersagen, die Ihr System generieren muss, was in der Regel eine Verdoppelung der Inferenz
Rechenkosten.
A/B-Tests
Beim A/B-Testing werden zwei Varianten eines Objekts miteinander verglichen, in der Regel durch das Testen von Reaktionen auf diese beiden Varianten.
Dabei werden in der Regel die Reaktionen auf diese beiden Varianten getestet, und es wird ermittelt, welche der beiden Varianten effektiver ist.
In unserem Fall haben wir das bestehende Modell als eine Variante und das Kandidatenmodell (das
(das kürzlich aktualisierte Modell) als eine weitere Variante. Wir verwenden A/B-Tests, um festzustellen, welches
Modell nach einigen vordefinierten Metriken besser ist.
A/B-Tests sind mittlerweile so weit verbreitet, dass Unternehmen wie Microsoft und Google im Jahr 2017
Google jedes Jahr über 10.000 A/B-Tests durchführen.^27 Für viele ML-Ingenieure ist es die erste
Antwort auf die Frage, wie ML-Modelle in der Produktion bewertet werden können. A/B-Tests funktionieren wie folgt:
1.1. Einsetzen des Kandidatenmodells neben dem bestehenden Modell.
2.2. Ein bestimmter Prozentsatz des Datenverkehrs wird für Vorhersagen an das neue Modell weitergeleitet; der Rest
wird für Vorhersagen an das bestehende Modell weitergeleitet. Es ist üblich, dass beide Varianten
gleichzeitig für den Vorhersageverkehr genutzt werden. Es gibt jedoch Fälle, in denen
die Vorhersagen eines Modells die Vorhersagen eines anderen Modells beeinflussen können - z. B. bei der dynamischen Preisgestaltung von Ride-Sharing
Mitfahrgelegenheiten mit dynamischer Preisgestaltung können die vorhergesagten Preise eines Modells die Anzahl
Anzahl der verfügbaren Fahrer und Mitfahrer beeinflussen, die wiederum die Vorhersagen des anderen Modells beeinflussen.
tionen beeinflussen. In solchen Fällen müssen Sie Ihre Varianten möglicherweise alternativ ausführen, z. B.
Modell A an einem Tag und dann Modell B am nächsten Tag.
3.3. Überwachen und analysieren Sie die Vorhersagen und gegebenenfalls das Nutzerfeedback beider Modelle
um festzustellen, ob der Unterschied in der Leistung der beiden Modelle statistisch signifikant ist.
statistisch signifikant ist.
Um A/B-Tests richtig durchführen zu können, müssen viele Dinge richtig gemacht werden. In diesem Buch werden wir
zwei wichtige Dinge besprechen. Erstens: A/B-Tests bestehen aus einem randomisierten Experiment:
Der Verkehr, der zu jedem Modell geleitet wird, muss wirklich zufällig sein. Wenn nicht, ist das Testergebnis
ungültig sein. Wenn es zum Beispiel einen Selektionsfehler in der Art und Weise gibt, wie der Verkehr zu den beiden Modellen geleitet wird
zwei Modelle geleitet wird, z. B. weil die Nutzer, die Modell A ausgesetzt sind, in der Regel am Telefon sind
während die Benutzer von Modell B in der Regel an ihren Desktops sitzen, dann ist Modell A
Test in der Produktion | 283
bessere Genauigkeit als Modell B hat, können wir nicht sagen, ob dies daran liegt, dass A besser ist als B oder
ob das "Telefonieren" die Qualität der Vorhersage beeinflusst.

Zweitens sollte Ihr A/B-Test mit einer ausreichenden Anzahl von Stichproben durchgeführt werden, um ein
genug Vertrauen in das Ergebnis zu gewinnen. Wie berechnet man die Anzahl der Stichproben
für einen A/B-Test zu berechnen, ist eine einfache Frage mit einer sehr komplizierten Antwort, und ich
Ich empfehle den Lesern, ein Buch über A/B-Tests zu lesen, um mehr zu erfahren.

Das Wesentliche dabei ist, dass Sie, wenn das Ergebnis Ihres A/B-Tests zeigt, dass ein Modell besser ist als ein anderes
mit statistischer Signifikanz, können Sie feststellen, welches Modell tatsächlich besser ist. Um
statistische Signifikanz zu messen, werden bei A/B-Tests statistische Hypothesentests wie
Zwei-Stichproben-Tests. Wir haben die Zwei-Stichproben-Tests in Kapitel 8 kennengelernt, als wir sie zur Feststellung von
Verteilungsverschiebungen. Zur Erinnerung: Ein Zwei-Stichproben-Test ist ein Test, mit dem festgestellt wird, ob
der Unterschied zwischen diesen beiden Populationen statistisch signifikant ist. Im Anwendungsfall der Verteilungs
Wenn ein statistischer Unterschied darauf hindeutet, dass die beiden Populationen aus unterschiedlichen Verteilungen stammen
aus unterschiedlichen Verteilungen stammen, bedeutet dies, dass sich die ursprüngliche Verteilung verschoben hat. Unter
dem Anwendungsfall A/B-Testing bedeuten statistische Unterschiede, dass wir genügend Beweise gesammelt haben
um zu zeigen, dass eine Variante besser ist als die andere Variante.

Statistische Signifikanz ist zwar nützlich, aber nicht narrensicher. Angenommen, wir führen einen Test mit zwei Stichproben durch und
erhalten das Ergebnis, dass Modell A besser ist als Modell B mit einem p-Wert von p = 0,05 oder
5 %, und wir definieren statistische Signifikanz als p ≤ 0,5. Das bedeutet, dass, wenn wir das
(100 - 5 =) 95 % der Zeit das Ergebnis, dass A besser ist als B, wenn wir dasselbe A/B-Testing-Experiment mehrmals durchführen.
Ergebnis, dass A besser ist als B, und in den anderen 5 % der Fälle ist B besser als A. Selbst wenn das Ergebnis also
Wenn das Ergebnis statistisch signifikant ist, ist es möglich, dass wir bei einer erneuten Durchführung des Experiments
wir ein anderes Modell wählen werden.

Selbst wenn das Ergebnis Ihres A/B-Tests statistisch nicht signifikant ist, bedeutet das nicht, dass dieser A/B-Test
Test fehlschlägt. Wenn Sie Ihren A/B-Test mit einer großen Anzahl von Stichproben durchgeführt haben und der Unterschied zwischen
zwischen den beiden getesteten Modellen statistisch nicht signifikant ist, gibt es vielleicht keinen großen Unterschied
zwischen diesen beiden Modellen, und Sie können wahrscheinlich beide verwenden.

Lesern, die mehr über A/B-Tests und andere für ML wichtige statistische Konzepte erfahren möchten
Konzepte, die für ML wichtig sind, empfehle ich Ron Kohavs Buch Trustworthy Online
Controlled Experiments (A Practical Guide to A/B Testing) (Cambridge University
Press) und Michael Barbers großartige Einführung in die Statistik für Datenwissenschaft (viel
kürzer).

In der Produktion haben Sie oft nicht nur einen Kandidaten, sondern mehrere Kandidaten
Modelle. Es ist möglich, A/B-Tests mit mehr als zwei Varianten durchzuführen, was bedeutet, dass wir
A/B/C-Tests oder sogar A/B/C/D-Tests durchgeführt werden können.

284 | Kapitel 9: Kontinuierliches Lernen und Testen in der Produktion

28 Danilo Sato, "CanaryRelease," 25. Juni 2014, MartinFowler.com, https://oreil.ly/YtKJE.

Canary-Freigabe
Canary Release ist eine Technik, die das Risiko der Einführung einer neuen Softwareversion
in der Produktion zu reduzieren, indem die Änderung langsam an eine kleine Untergruppe von Benutzern
bevor sie auf die gesamte Infrastruktur ausgeweitet und für alle verfügbar gemacht wird.^28 Im
Kontext der ML-Bereitstellung funktioniert die Kanarienfreigabe wie folgt:
1.1. Bereitstellung des Kandidatenmodells neben dem bestehenden Modell. Das Kandidatenmodell
wird Kanarienvogel genannt.
2.2. Ein Teil des Datenverkehrs wird an das Kandidatenmodell weitergeleitet.
3.3. Wenn seine Leistung zufriedenstellend ist, wird der Verkehr zum Kandidatenmodell erhöht. Wenn
nicht, brechen Sie den Kanarienvogel ab und leiten den gesamten Verkehr zurück zum bestehenden Modell.
4.4. Abbrechen, wenn entweder der Kanarienvogel den gesamten Verkehr bedient (das Kandidatenmodell hat
das bestehende Modell ersetzt hat) oder wenn der Kanarienvogel abgebrochen wird.
Die Leistung des Kandidatenmodells wird im Vergleich zur Leistung des bestehenden Modells gemessen.
Leistung des bestehenden Modells anhand der für Sie wichtigen Kennzahlen gemessen. Wenn sich die Schlüsselmetriken des Kandidatenmodells
des Kandidatenmodells erheblich verschlechtern, wird der Canary abgebrochen und der gesamte Verkehr wird an das
bestehende Modell geleitet.
Canary-Versionen können aufgrund der Ähnlichkeiten in ihrem Aufbau zur Durchführung von A/B-Tests verwendet werden.
Einstellungen. Sie können die Canary-Analyse jedoch auch ohne A/B-Tests durchführen. Zum Beispiel müssen Sie
müssen Sie beispielsweise den Datenverkehr nicht nach dem Zufallsprinzip zu den einzelnen Modellen leiten. Ein plausibles Szenario ist, dass
dass Sie das Kandidatenmodell zunächst in einem weniger kritischen Markt einführen, bevor Sie es für alle
alle.
Für Leser, die sich dafür interessieren, wie die Freigabe von Kanarienvögeln in der Branche funktioniert, haben Netflix und
Google einen großartigen gemeinsamen Blogbeitrag darüber, wie die automatisierte Canary-Analyse in
ihren Unternehmen eingesetzt wird.
Verschachtelte Experimente
Stellen Sie sich vor, Sie haben zwei Empfehlungssysteme, A und B, und Sie möchten bewerten
welches von beiden besser ist. Jedes Mal empfiehlt ein Modell 10 Artikel, die den Benutzern gefallen könnten. Mit
A/B-Tests würden Sie Ihre Benutzer in zwei Gruppen aufteilen: eine Gruppe erhält A und
die andere Gruppe bekommt B zu sehen. Jeder Nutzer wird mit den Empfehlungen eines
von einem Modell.
Wie wäre es, wenn wir einen Benutzer nicht nur den Empfehlungen eines Modells aussetzen, sondern
Empfehlungen von beiden Modellen aussetzen und sehen, auf welche Empfehlungen des Modells
Empfehlungen er anklicken wird? Das ist die Idee hinter der Verschachtelung von Experimenten, ursprünglich
Test in der Produktion | 285
29 Thorsten Joachims, "Optimizing Search Engines using Clickthrough Data", KDD 2002, https://oreil.ly/XnH5G.
30 Joshua Parks, Juliette Aurisset, und Michael Ramm, "Innovating Faster on Personalization Algorithms at
Netflix Using Interleaving," Netflix Technology Blog, November 29, 2017, https://oreil.ly/lnvDY.

Thorsten Joachims im Jahr 2002 für die Probleme von Suchrankings vorgeschlagen.^29 In
Experimenten fand Netflix heraus, dass Interleaving "zuverlässig die besten Algorithmen identifiziert
mit einer wesentlich geringeren Stichprobengröße im Vergleich zu traditionellen A/B-Tests"^30.
Abbildung 9-6 zeigt, wie sich Interleaving von A/B-Tests unterscheidet. Bei A/B-Tests werden die wichtigsten
Kennzahlen wie Verbleib und Streaming gemessen und zwischen den beiden Gruppen verglichen.
Gruppen verglichen. Beim Interleaving können die beiden Algorithmen durch die Messung der Nutzerpräferenzen verglichen werden.
erenzen. Da das Interleaving von den Benutzerpräferenzen bestimmt werden kann, gibt es keine Garantie
dass die Benutzerpräferenz zu besseren Kernmetriken führt.
Abbildung 9-6. Veranschaulichung von Interleaving und A/B-Testing. Quelle: Angepasst von einem
Bild von Parks et al.
Wenn wir den Nutzern Empfehlungen aus mehreren Modellen zeigen, ist es wichtig zu
dass die Position einer Empfehlung beeinflusst, wie wahrscheinlich es ist, dass ein Nutzer darauf klickt.
anklicken wird. Die Wahrscheinlichkeit, dass ein Nutzer auf die oberste Empfehlung klickt, ist zum Beispiel viel größer
als auf die unterste Empfehlung. Damit die Verschachtelung gültige Ergebnisse liefert, müssen wir
müssen wir sicherstellen, dass eine Empfehlung an einer bestimmten Position mit gleicher Wahrscheinlichkeit von A oder B generiert wird.
Um dies zu gewährleisten, können wir unter anderem das Team-Draft-Interleaving verwenden,
das den Drafting-Prozess im Sport nachahmt. Für jede Empfehlungsposition,
Für jede Empfehlungsposition wählen wir zufällig A oder B mit gleicher Wahrscheinlichkeit aus, und das gewählte Modell wählt die
286 | Kapitel 9: Kontinuierliches Lernen und Testen in der Produktion
31 Olivier Chapelle, Thorsten Joachims, Filip Radlinski, und Yisong Yue, "Large-Scale Validation and Analysis
of Interleaved Search Evaluation," ACM Transactions on Information Systems 30, no. 1 (Februar 2012): 6,
https://oreil.ly/lccvK.
32 Parks et al. "Innovating Faster on Personalization Algorithms".

Top-Empfehlung, die noch nicht ausgewählt wurde.^31 Eine Veranschaulichung, wie diese
Team-Drafting-Methode funktioniert, ist in Abbildung 9-7 dargestellt.
Abbildung 9-7. Verschachtelung von Videoempfehlungen aus zwei Ranking-Algorithmen mit
Team-Draft. Quelle: Parks et al.^32
Banditen
Für diejenigen, die damit nicht vertraut sind: Banditen-Algorithmen haben ihren Ursprung im Glücksspiel. Ein Kasino hat mehrere
Spielautomaten mit unterschiedlichen Auszahlungen. Ein Spielautomat ist auch bekannt als einarmiger
Bandit bezeichnet, daher der Name. Sie wissen nicht, welcher Spielautomat die höchste Auszahlung bietet.
Sie können mit der Zeit experimentieren, um herauszufinden, welcher Spielautomat am besten ist und gleichzeitig Ihre Auszahlung maximiert.
Ihre Auszahlung zu maximieren. Mehrarmige Banditen sind Algorithmen, die es Ihnen ermöglichen, ein Gleichgewicht
zwischen Ausbeutung (Auswahl des Spielautomaten, der in der Vergangenheit am meisten ausgezahlt hat)
und Exploration (Auswahl anderer Spielautomaten, die vielleicht noch mehr Gewinn abwerfen).
Test in der Produktion | 287
33 Greg Rafferty, "A/B-Testing - Gibt es einen besseren Weg? An Exploration of Multi-Armed Bandits," Towards Data
Science, 22. Januar 2020, https://oreil.ly/MsaAK.

Heutzutage ist die Standardmethode für das Testen von Modellen in der Produktion das A/B-Testing.
Bei A/B-Tests leiten Sie den Datenverkehr nach dem Zufallsprinzip an jedes Modell weiter, um Vorhersagen zu treffen und
und messen am Ende des Versuchs, welches Modell besser funktioniert. A/B-Tests sind zustandslos:
Sie können den Datenverkehr zu jedem Modell leiten, ohne die aktuelle Leistung der Modelle zu kennen.
Leistung. Sie können A/B-Tests auch mit Batch-Vorhersagen durchführen.
Wenn Sie mehrere Modelle zu bewerten haben, kann jedes Modell als Spielautomat betrachtet werden
Spielautomat betrachtet werden, dessen Auszahlung (d. h. Vorhersagegenauigkeit) Sie nicht kennen. Mit Bandits können Sie
zu bestimmen, wie der Datenverkehr für die Vorhersage an jedes Modell weitergeleitet werden soll, um das beste Modell zu ermitteln
Modell zu bestimmen und gleichzeitig die Vorhersagegenauigkeit für Ihre Nutzer zu maximieren. Bandit ist zustandsabhängig: Bevor
Bevor Sie eine Anfrage an ein Modell weiterleiten, müssen Sie die aktuelle Leistung aller Modelle berechnen.
Dies erfordert drei Dinge:
Ihr Modell muss in der Lage sein, Online-Vorhersagen zu treffen.
-Vorzugsweise kurze Rückkopplungsschleifen: Sie müssen eine Rückmeldung erhalten, ob eine Vorhersage
Vorhersage gut ist oder nicht. Dies gilt in der Regel für Aufgaben, bei denen die Kennzeichnungen
aus den Rückmeldungen der Benutzer ermittelt werden können, wie bei Empfehlungen - wenn Benutzer auf eine Empfehlung klicken, wird
auf eine Empfehlung klicken, wird daraus geschlossen, dass sie gut ist. Wenn die Feedbackschleifen kurz sind, können Sie die
Auszahlung der einzelnen Modelle schnell aktualisieren.
-Ein Mechanismus zum Sammeln von Feedback, zum Berechnen und Verfolgen der Leistung der einzelnen Modelle
Leistung jedes Modells zu berechnen und zu verfolgen und Vorhersageanfragen an verschiedene Modelle
aktuellen Leistung.
Bandits sind in der Wissenschaft gut erforscht und haben sich als wesentlich dateneffizienter erwiesen
effizienter als A/B-Tests (in vielen Fällen sind Bandits sogar optimal). Bandits erfordern
Bandits benötigen weniger Daten, um zu ermitteln, welches Modell das beste ist, und verringern gleichzeitig die Opportunitätskosten, da sie den Verkehr an die
nity-Kosten, da sie den Datenverkehr schneller auf das bessere Modell umleiten. Siehe Diskussionen über
Bandits bei LinkedIn, Netflix, Facebook und Dropbox, Zillow und Stitch Fix. Für eine
mehr theoretische Sichtweise finden Sie in Kapitel 2 von Reinforcement Learning (Sutton und Barto
2020).
In einem Experiment von Greg Rafferty von Google waren für A/B-Tests über 630.000
Stichproben, um ein Konfidenzintervall von 95 % zu erhalten, während ein einfacher Bandit-Algorithmus
(Thompson Sampling) mit weniger als 12.000 Stichproben feststellte, dass ein Modell 5 % besser war als das andere.
weniger als 12.000 Stichproben.^33
Bandits sind jedoch sehr viel schwieriger zu implementieren als A/B-Tests, weil
weil sie die Berechnung und Verfolgung der Auszahlungen der Modelle erfordern. Daher werden Bandit-Algo-
rithmen in der Branche nur bei einigen wenigen großen Technologieunternehmen weit verbreitet.
288 | Kapitel 9: Kontinuierliches Lernen und Testen in der Produktion
34 William R. Thompson, "On the Likelihood that One Unknown Probability Exceeds Another in View of the
Evidence of Two Samples", Biometrika 25, no. 3/4 (Dezember 1933): 285-94, https://oreil.ly/TH1HC.
35 Peter Auer, "Using Confidence Bounds for Exploitation-Exploration Trade-offs", Journal of Machine Learning
Research 3 (November 2002): 397-422, https://oreil.ly/vp9mI.

Bandit-Algorithmen
Viele der Lösungen für das mehrarmige Banditenproblem können hier verwendet werden. Der
einfachste Algorithmus zur Erkundung ist ε-greedy. Für einen bestimmten Prozentsatz der Zeit, sagen wir 90% der
der Zeit oder ε = 0,9, leitet man den Verkehr zu dem Modell, das derzeit die beste Leistung erbringt
und in den anderen 10 % der Zeit wird der Verkehr zu einem Zufallsmodell geleitet. Dies
bedeutet, dass für jede der von Ihrem System generierten Vorhersagen 90 % von dem
dem zu diesem Zeitpunkt besten Modell.
Zwei der beliebtesten Explorationsalgorithmen sind Thompson Sampling und Upper
Konfidenzschranke (UCB). Thompson Sampling wählt ein Modell mit einer Wahrscheinlichkeit
dass dieses Modell angesichts des aktuellen Wissensstandes optimal ist.^34 In unserem Fall bedeutet dies, dass
der Algorithmus das Modell auf der Grundlage seiner Wahrscheinlichkeit auswählt, einen höheren Wert
(bessere Leistung) als alle anderen Modelle. Auf der anderen Seite wählt UCB das Element
mit der höchsten oberen Konfidenzgrenze.^35 Wir sagen, dass UCB Optimismus angesichts
angesichts der Ungewissheit einen "Ungewissheitsbonus", auch "Explorationsbonus" genannt
für die Elemente, bei denen es unsicher ist.
Kontextuelle Bandits als Explorationsstrategie
Wenn Bandits zur Modellbewertung die Auszahlung (d. h. die Vorhersagegenauigkeit) jedes Modells bestimmen sollen
eines jeden Modells zu bestimmen, so sollen kontextuelle Bandits die Auszahlung jeder Aktion bestimmen. Unter
Fall von Empfehlungen/Anzeigen ist eine Aktion ein Artikel/eine Anzeige, der/die den Nutzern gezeigt wird, und die
Auszahlung ist die Wahrscheinlichkeit, dass ein Nutzer darauf klickt. Kontextbezogene Bandits sind, wie andere Bandits auch,
sind eine erstaunliche Technik zur Verbesserung der Dateneffizienz Ihres Modells.
Manche Leute nennen Bandits für die Modellevaluation auch "kontextuelle
bandits". Das macht die Konversationen verwirrend, deshalb wird in diesem Buch
"kontextuelle Bandits" auf Explorationsstrategien zur Bestimmung der
Auszahlung von Vorhersagen.
Stellen Sie sich vor, Sie bauen ein Empfehlungssystem mit 1.000 zu empfehlenden Artikeln,
Das macht es zu einem 1.000-armigen Banditenproblem. Jedes Mal können Sie nur die
jeweils nur die 10 relevantesten Artikel einem Benutzer empfehlen. In Banditenform ausgedrückt, müssen Sie die besten
10 Arme. Die angezeigten Elemente erhalten ein Nutzerfeedback, das daraus abgeleitet wird, ob der Nutzer auf sie klickt
sie klickt. Aber Sie erhalten kein Feedback zu den anderen 990 Elementen. Dies wird als partieller
Test in der Produktion | 289
36 Lihong Li, Wei Chu, John Langford, und Robert E. Schapire, "A Contextual-Bandit Approach to Personalized
News Article Recommendation," arXiv, February 28, 2010, https://oreil.ly/uaWHm.
37 Laut Wikipedia ist Multi-Armed Bandit ein klassisches Problem des Verstärkungslernens, das beispielhaft ist für
das Dilemma zwischen Exploration und Ausbeutung veranschaulicht (s.v., "Multi-armed bandit", https://oreil.ly/ySjwo). Der Name
kommt von der Vorstellung eines Spielers an einer Reihe von Spielautomaten (manchmal auch als "einarmige Banditen" bekannt), der
der entscheiden muss, an welchen Automaten er spielt, wie oft er jeden Automaten spielt und in welcher Reihenfolge er ihn spielt,
und ob er mit dem aktuellen Automaten weiterspielen oder einen anderen Automaten ausprobieren soll.

Feedback-Problem, auch bekannt als Bandit-Feedback. Man kann sich kontextuelle Bandits auch als
Bandits als ein Klassifizierungsproblem mit Bandit-Feedback betrachten.
Nehmen wir an, dass jedes Mal, wenn ein Benutzer auf ein Element klickt, dieses Element 1 Wertpunkt erhält. Wenn ein
Element 0 Wertpunkte hat, könnte das entweder daran liegen, dass das Element noch nie einem Benutzer gezeigt wurde
Benutzer gezeigt wurde, oder weil er zwar gezeigt, aber nicht angeklickt wurde. Sie möchten den Benutzern die Elemente
mit dem höchsten Wert für sie zeigen, aber wenn Sie den Nutzern immer nur die Artikel mit
mit den meisten Wertpunkten zeigen, werden Sie immer wieder die gleichen beliebten Artikel empfehlen und die
nie zuvor gezeigten Artikel werden weiterhin 0 Wertpunkte haben.
Kontextabhängige Banditen sind Algorithmen, die Ihnen helfen, ein Gleichgewicht zwischen der Anzeige von Artikeln, die
die ihnen gefallen werden, und der Anzeige der Artikel, zu denen Sie Feedback wünschen.^36 Es ist derselbe
Exploration-Ausbeutung-Abwägung, die viele Leser vielleicht schon beim
Verstärkungslernen kennen. Contextual Bandits werden auch als "One-Shot"-Probleme des Reinforcement Learning
^37 Beim Reinforcement Learning müssen Sie möglicherweise eine Reihe von Aktionen durchführen
Aktionen durchführen, bevor man die Belohnungen sieht. Bei kontextuellen Bandits kann man Bandit-Feedback erhalten
sofort nach einer Aktion erhalten - z. B. nach der Empfehlung einer Anzeige erhalten Sie eine Rückmeldung darüber
ob ein Nutzer auf diese Empfehlung geklickt hat.
Kontextabhängige Bandits sind gut erforscht und verbessern nachweislich die Leistung der Modelle
Leistung der Modelle erheblich verbessern (siehe Berichte von Twitter und Google). Allerdings sind kontextuelle
Bandits sind jedoch noch schwieriger zu implementieren als Modell-Bandits, da die Explorations
Strategie von der Architektur des ML-Modells abhängt (z. B. ob es sich um einen Entscheidungsbaum
oder ein neuronales Netz ist), was die Verallgemeinerbarkeit für verschiedene Anwendungsfälle erschwert. Leserinnen und Leser
die an der Kombination von kontextuellen Bandits mit Deep Learning interessiert sind, sollten sich ein
von einem Team bei Twitter verfasstes Papier lesen: "Deep Bayesian Bandits: Erkundung in Online
Personalized Recommendations" (Guo et al. 2020).
Bevor wir diesen Abschnitt abschließen, möchte ich noch einen Punkt hervorheben. Wir haben uns
verschiedene Arten von Tests für ML-Modelle durchlaufen. Es ist jedoch wichtig zu beachten
dass es bei einer guten Evaluierungspipeline nicht nur darum geht, welche Tests durchgeführt werden sollen, sondern auch darum
wer diese Tests durchführen sollte. Bei ML wird der Evaluierungsprozess oft von Datenwissenschaftlern durchgeführt
Datenwissenschaftler - dieselben Personen, die das Modell entwickelt haben, sind auch für die Evaluierung
es. Datenwissenschaftler neigen dazu, ihr neues Modell ad hoc zu evaluieren, indem sie die Testsätze verwenden, die
sie mögen. Erstens ist dieser Prozess mit Vorurteilen behaftet - Datenwissenschaftler haben Kontexte über
über ihre Modelle, die die meisten Benutzer nicht haben, was bedeutet, dass sie das Modell wahrscheinlich nicht so verwenden werden
nicht so verwenden, wie es die meisten ihrer Nutzer tun. Zweitens bedeutet die Ad-hoc-Natur des Prozesses
290 | Kapitel 9: Kontinuierliches Lernen und Testen in der Produktion
dass die Ergebnisse unterschiedlich ausfallen können. Ein Datenwissenschaftler könnte eine Reihe von Tests durchführen und
dass Modell A besser ist als Modell B, während ein anderer Datenwissenschaftler vielleicht zu einem anderen Ergebnis kommt.
anders.

Das Fehlen einer Möglichkeit, die Qualität der Modelle in der Produktion zu gewährleisten, hat dazu geführt, dass viele Modi
scheitern, was wiederum die Angst der Datenwissenschaftler vor dem Einsatz von Modellen schürt.
Einsatz von Modellen. Um dieses Problem zu entschärfen, ist es für jedes Team wichtig, klare
Pipelines zu skizzieren, wie Modelle evaluiert werden sollen: z. B. die auszuführenden Tests, die Reihenfolge, in der
Reihenfolge, in der sie ausgeführt werden sollen, und die Schwellenwerte, die sie erreichen müssen, um in die nächste
Stufe. Besser noch, diese Pipelines sollten automatisiert und bei jeder
neue Modellaktualisierung. Die Ergebnisse sollten gemeldet und überprüft werden, ähnlich wie bei der
Integration/Continuous Deployment (CI/CD) bei der traditionellen Softwareentwicklung
Entwicklung. Es ist wichtig zu verstehen, dass ein guter Evaluierungsprozess nicht nur beinhaltet
nicht nur darum geht, welche Tests durchgeführt werden sollen, sondern auch darum, wer diese Tests durchführen soll.

Zusammenfassung
Dieses Kapitel befasst sich mit einem Thema, das meiner Meinung nach zu den spannendsten, aber noch zu wenig
erforschten Themen gehört: wie Sie Ihre Modelle in der Produktion kontinuierlich aktualisieren, um sie
um sie an sich ändernde Datenverteilungen anzupassen. Wir haben die vier Phasen besprochen, die ein Unternehmen
Unternehmen bei der Modernisierung ihrer Infrastruktur für kontinuierliches Lernen durchlaufen
Lernens durchlaufen kann: von der manuellen, von Grund auf trainierten Phase bis hin zum automatisierten, zustandslosen
Lernen.

Dann haben wir die Frage untersucht, die ML-Ingenieure in Unternehmen jeder Art und Größe umtreibt
"Wie oft sollte ich meine Modelle aktualisieren?", indem wir sie aufforderten, den
Wert der Aktualität der Daten für ihre Modelle und die Kompromisse zwischen Modelliteration
und Datenaktualisierung.

Ähnlich wie bei der in Kapitel 7 erörterten Online-Vorhersage erfordert das kontinuierliche Lernen
eine ausgereifte Streaming-Infrastruktur. Der Trainingsteil des kontinuierlichen Lernens kann
Der Trainingsteil des kontinuierlichen Lernens kann im Batch-Verfahren durchgeführt werden, aber der Teil der Online-Auswertung erfordert Streaming. Viele Ingenieure
befürchten, dass Streaming schwierig und kostspielig ist. Das war auch vor drei Jahren so, aber die Streaming
haben sich die Streaming-Technologien seither erheblich weiterentwickelt. Mehr und mehr Unternehmen
bieten Lösungen an, die Unternehmen den Umstieg auf Streaming erleichtern, darunter
Spark Streaming, Snowflake Streaming, Materialize, Decodable, Vectorize, usw.

Kontinuierliches Lernen ist ein ML-spezifisches Problem, das jedoch weitgehend eine infrastrukturelle Lösung erfordert.
turelle Lösung. Um den Iterationszyklus zu beschleunigen und Fehler in neuen
Modellaktualisierungen schnell zu erkennen, müssen wir unsere Infrastruktur richtig einrichten. Diese
Dazu müssen das Data Science/ML-Team und das Plattformteam zusammenarbeiten. Wir werden
Infrastruktur für ML im nächsten Kapitel besprechen.

Zusammenfassung | 291
KAPITEL 10

Infrastruktur und Hilfsmittel für MLOps
In den Kapiteln 4 bis 6 haben wir die Logik für die Entwicklung von ML-Systemen erörtert. In den Kapiteln
7 bis 9 haben wir die Überlegungen zum Einsatz, zur Überwachung und zur kontinuierlichen
Aktualisierung eines ML-Systems. Bis jetzt sind wir davon ausgegangen, dass ML-Fachleute
Zugang zu allen Werkzeugen und der Infrastruktur haben, die sie zur Umsetzung dieser Logik und
diese Überlegungen anzustellen. Diese Annahme ist jedoch bei weitem nicht zutreffend. Viele Daten
Datenwissenschaftler haben mir gesagt, dass sie wissen, was sie für ihre ML-Systeme tun müssen, aber
aber sie können es nicht tun, weil ihre Infrastruktur nicht so eingerichtet ist, dass sie
dies zu tun.

ML-Systeme sind komplex. Je komplexer ein System ist, desto mehr kann es von einer
einer guten Infrastruktur profitieren. Wenn die Infrastruktur richtig eingerichtet ist, kann sie zur Automatisierung von Prozessen beitragen,
wodurch der Bedarf an Fachwissen und Entwicklungszeit verringert wird. Dies wiederum kann
die Entwicklung und Bereitstellung von ML-Anwendungen beschleunigen, die Oberfläche für
für Fehler reduzieren und neue Anwendungsfälle ermöglichen. Wenn die Infrastruktur jedoch falsch eingerichtet ist, ist sie
mühsam in der Anwendung und teuer im Austausch. In diesem Kapitel wird erörtert, wie man die
Infrastruktur für ML-Systeme richtig einrichtet.

Bevor wir ins Detail gehen, ist es wichtig zu wissen, dass die Infrastrukturbedürfnisse eines jeden Unternehmens
unterschiedlich sind. Die für Sie erforderliche Infrastruktur hängt von der Anzahl der Anwendungen ab
Sie entwickeln und wie spezialisiert die Anwendungen sind. Am einen Ende des Spektrums
gibt es Unternehmen, die ML für Ad-hoc-Geschäftsanalysen verwenden, z. B. um die
die Zahl der neuen Nutzer im nächsten Jahr zu prognostizieren, um sie bei der vierteljährlichen Planungsbesprechung
präsentieren. Diese Unternehmen müssen wahrscheinlich nicht in eine Infrastruktur investieren.
Jupyter Notebooks, Python und Pandas wären ihre besten Freunde. Wenn Sie
nur einen einfachen ML-Anwendungsfall haben, z. B. eine Android-App zur Objekterkennung, die Sie
Objekterkennung, die Sie Ihren Freunden zeigen wollen, brauchen Sie wahrscheinlich auch keine Infrastruktur - Sie brauchen nur ein
Android-kompatibles ML-Framework wie TensorFlow Lite.

293
1 Kunal Shah, "Das macht SEO so wichtig für jedes Unternehmen", Entrepreneur India, 11. Mai 2020,
https://oreil.ly/teQlX.
2 Um einen Einblick in Teslas Recheninfrastruktur für ML zu erhalten, empfehle ich, die Aufzeichnung des
Tesla AI Day 2021 auf YouTube anzusehen.
3 Die Definition für "vernünftige Größe" wurde von Jacopo Tagliabue in seinem Artikel "You Do Not Need a
Bigger Boat: Recommendations at Reasonable Scale in a (Mostly) Serverless and Open Stack", arXiv, 15. Juli,
2021, https://oreil.ly/YNRZQ. Für eine weitere Diskussion über vernünftige Skalierung, siehe "ML and MLOps at a Reasonable
Skala" von Ciro Greco (Oktober 2021).
4 FAAAM ist die Abkürzung für Facebook, Apple, Amazon, Alphabet, Microsoft.
5 Reza Shiftehfar, "Uber's Big Data Platform: 100+ Petabytes with Minute Latency," Uber Engineering, October
17, 2018, https://oreil.ly/6Ykd3; Kaushik Krishnamurthi, "Building a Big Data Pipeline to Process Clickstream
Data," Zillow, April 6, 2018, https://oreil.ly/SGmNe.
6 Nathan Bronson und Janet Wiener, "Facebook's Top Open Data Problems," Meta, October 21, 2014,
https://oreil.ly/p6QjX.
Am anderen Ende des Spektrums gibt es Unternehmen, die an Anwendungen arbeiten
mit einzigartigen Anforderungen arbeiten. Selbstfahrende Autos haben zum Beispiel besondere Anforderungen an Genauigkeit und
Latenzzeit - der Algorithmus muss innerhalb von Millisekunden reagieren können
und seine Genauigkeit muss nahezu perfekt sein, da eine falsche Vorhersage zu schweren Unfällen führen kann.
Unfällen führen kann. Auch die Google-Suche stellt besondere Anforderungen an den Umfang, da die meisten Unternehmen
Unternehmen nicht 63.000 Suchanfragen pro Sekunde verarbeiten, was 234 Millionen
Suchanfragen pro Stunde, wie es Google tut.^1 Diese Unternehmen müssen wahrscheinlich
ihre eigene hochspezialisierte Infrastruktur entwickeln. Google entwickelte einen großen Teil
seiner internen Infrastruktur für die Suche entwickelt; dies gilt auch für selbstfahrende Autos wie Tesla
und Waymo.^2 Es ist üblich, dass ein Teil der spezialisierten Infrastruktur später öffentlich gemacht
und von anderen Unternehmen übernommen wird. Zum Beispiel hat Google seine interne Cloud-Infrastruktur
Infrastruktur für die Öffentlichkeit zugänglich gemacht, was zur Google Cloud Platform führte.

In der Mitte des Spektrums befindet sich die Mehrheit der Unternehmen, die ML
für mehrere gängige Anwendungen - ein Modell zur Betrugserkennung, ein Preisoptimierungsmodell
ein Modell zur Preisoptimierung, ein Modell zur Vorhersage der Kundenabwanderung, ein Empfehlungssystem usw. - in angemessenem Umfang einsetzen.
"Angemessener Umfang" bezieht sich auf Unternehmen, die mit Daten in der Größenordnung von Gigabytes
und Terabytes arbeiten, anstatt mit Petabytes pro Tag. Ihr Data-Science-Team kann zwischen
10 bis zu Hunderten von Ingenieuren reichen.^3 Zu dieser Kategorie kann jedes Unternehmen gehören, von einem
20-Personen-Startup bis hin zu einem Unternehmen in der Größenordnung von Zillow, aber nicht in der Größenordnung von FAAAM.^4
Zum Beispiel fügte Uber im Jahr 2018 täglich Dutzende von Terabytes an Daten zu seinem Datensee hinzu.
und der größte Datensatz von Zillow brachte 2 Terabyte unkomprimierter Daten pro Tag ein
pro Tag.^5 Im Gegensatz dazu generierte Facebook selbst im Jahr 2014 noch 4 Petabyte an Daten pro
Tag.^6

294 | Kapitel 10: Infrastruktur und Werkzeuge für MLOps

7 Wikipedia, s.v. "Infrastruktur", https://oreil.ly/YaIk8.
Unternehmen in der Mitte des Spektrums werden wahrscheinlich von einer generalisierten ML
Infrastruktur profitieren, die zunehmend standardisiert wird (siehe Abbildung 10-1). In diesem Buch,
Buch konzentrieren wir uns auf die Infrastruktur für die große Mehrheit der ML-Anwendungen in einem
fähigen Maßstab.

Abbildung 10-1. Infrastrukturanforderungen für Unternehmen in verschiedenen Produktionsgrößenordnungen

Um die richtige Infrastruktur für Ihre Bedürfnisse einzurichten, ist es wichtig zu verstehen
was Infrastruktur genau bedeutet und woraus sie besteht. Laut Wikipedia,
ist die Infrastruktur in der physischen Welt "die Gesamtheit der grundlegenden Einrichtungen und Systeme
die die nachhaltige Funktionsfähigkeit von Haushalten und Unternehmen unterstützen."^7 In der ML-Welt,
ist die Infrastruktur die Gesamtheit der grundlegenden Einrichtungen, die die Entwicklung und
Wartung von ML-Systemen unterstützen. Was als "grundlegende Einrichtungen" zu betrachten ist
ist von Unternehmen zu Unternehmen sehr unterschiedlich, wie bereits weiter oben in diesem Kapitel erläutert. In diesem
Abschnitt werden wir die folgenden vier Ebenen untersuchen:

Speicherung und Datenverarbeitung
In der Speicherebene werden die Daten gesammelt und gespeichert. Die Berechnungsschicht stellt die
die Rechenleistung, die für die Ausführung von ML-Workloads benötigt wird, z. B. für das Training eines Modells,
Berechnen von Merkmalen, Generieren von Merkmalen, usw.

Ressourcenmanagement
Die Ressourcenverwaltung umfasst Werkzeuge zur Planung und Orchestrierung Ihrer Arbeits
Lasten zu planen und zu koordinieren, um die verfügbaren Rechenressourcen optimal zu nutzen. Beispiele für
Tools dieser Kategorie sind Airflow, Kubeflow und Metaflow.

Infrastruktur und Werkzeuge für MLOps | 295
ML-Plattform
Diese bietet Werkzeuge zur Unterstützung der Entwicklung von ML-Anwendungen wie Modellspeicher
Modellspeicher, Funktionsspeicher und Überwachungswerkzeuge. Beispiele für Werkzeuge in dieser Kategorie
sind SageMaker und MLflow.

Entwicklungsumgebung
Diese Umgebung wird gewöhnlich als Entwicklungsumgebung bezeichnet; hier wird der Code geschrieben und
Experimente durchgeführt werden. Der Code muss versioniert und getestet werden. Experimente müssen
nachverfolgt werden.

Diese vier verschiedenen Ebenen sind in Abbildung 10-2 dargestellt. Daten und Rechenleistung sind die
wesentlichen Ressourcen, die für jedes ML-Projekt benötigt werden, und daher bildet die Speicher- und Rechenschicht
die infrastrukturelle Grundlage für jedes Unternehmen, das ML anwenden möchte. Diese
Schicht ist auch die abstrakteste für einen Datenwissenschaftler. Wir werden diese Schicht zuerst besprechen, weil
diese Ressourcen am einfachsten zu erklären sind.

Abbildung 10-2. Verschiedene Ebenen der Infrastruktur für ML

Die Entwicklungsumgebung ist das, womit Datenwissenschaftler täglich zu tun haben, und daher,
ist sie für sie am wenigsten abstrakt. Wir werden diese Kategorie als Nächstes besprechen, und dann werden wir
Ressourcenmanagement, ein umstrittenes Thema unter Datenwissenschaftlern - es wird immer noch
Die Leute diskutieren immer noch darüber, ob ein Datenwissenschaftler diese Ebene kennen muss oder nicht. Weil "ML
Plattform" ein relativ neues Konzept ist, dessen verschiedene Komponenten noch nicht ausgereift sind,
werden wir diese Kategorie als letzte behandeln, nachdem wir uns mit allen anderen Kategorien vertraut gemacht haben.
Kategorien vertraut gemacht haben. Eine ML-Plattform erfordert Vorabinvestitionen eines Unternehmens, aber wenn sie
richtig gemacht, kann sie das Leben von Datenwissenschaftlern in verschiedenen Geschäftsbereichen des Unternehmens
so viel einfacher machen.

296 | Kapitel 10: Infrastruktur und Werkzeuge für MLOps

8 Ich habe ein Unternehmen gesehen, dessen Daten über Amazon Redshift und GCP BigQuery verteilt sind, und die Ingenieure sind
nicht sehr glücklich darüber.
9 Wir sprechen hier nur über Datenspeicherung, da wir in Kapitel 2 über Datensysteme gesprochen haben.
Selbst wenn zwei Unternehmen genau die gleichen Infrastrukturanforderungen haben, kann die daraus resultierende Infrastruktur
Infrastruktur unterschiedlich aussehen, je nachdem, wie sie bei der Entscheidung "bauen oder kaufen
d.h., was sie intern aufbauen wollen und was sie an andere Unternehmen auslagern wollen.
an andere Unternehmen auslagern. Wir werden im letzten Teil dieses Kapitels auf die Entscheidung zwischen Eigenbau und Kauf eingehen.
Kapitel erörtern, wo wir auch die Hoffnung auf standardisierte und einheitliche Abstraktionen
für die ML-Infrastruktur.

Lasst uns eintauchen!

Speicherung und Rechenleistung
ML-Systeme arbeiten mit einer Vielzahl von Daten, die irgendwo gespeichert werden müssen. Die
Speicherschicht ist der Ort, an dem die Daten gesammelt und gespeichert werden. In ihrer einfachsten Form kann die Speicherebene
Speicherschicht kann eine Festplatte (HDD) oder eine Solid State Disk (SSD) sein. Die Speicherebene
kann sich an einem Ort befinden, z. B. können Sie alle Ihre Daten in Amazon S3 oder in Snowflake speichern,
oder über mehrere Standorte verteilt sein.^8 Ihre Speicherebene kann sich vor Ort in einem privaten
Rechenzentrum oder in der Cloud sein. In der Vergangenheit haben Unternehmen möglicherweise versucht, ihre
eigene Speicherebene zu verwalten. In den letzten zehn Jahren wurde die Speicherebene jedoch weitgehend
und in die Cloud verlagert. Die Datenspeicherung ist so billig geworden, dass die meisten
dass die meisten Unternehmen einfach alle Daten speichern, die sie haben, ohne dass ihnen dadurch Kosten entstehen.^9 Wir haben uns in Kapitel 3 intensiv mit der Datenschicht
In Kapitel 3 haben wir uns intensiv mit der Datenschicht befasst, daher konzentrieren wir uns in diesem Kapitel auf die Rechenschicht.

Die Datenverarbeitungsschicht bezieht sich auf alle Datenverarbeitungsressourcen, auf die ein Unternehmen Zugriff hat, sowie auf den
Mechanismus, der bestimmt, wie diese Ressourcen genutzt werden können. Die Menge der verfügbaren
Ressourcen bestimmt die Skalierbarkeit Ihrer Workloads. Sie können sich die
Rechenschicht als den Motor für die Ausführung Ihrer Aufträge betrachten. In ihrer einfachsten Form kann die Rechenschicht
Schicht nur aus einem einzigen CPU- oder GPU-Kern bestehen, der alle Berechnungen durchführt.
Die gebräuchlichste Form ist die von einem Cloud-Anbieter verwaltete Cloud-Rechenschicht, z. B. AWS
Elastic Compute Cloud (EC2) oder GCP.

Die Rechenschicht kann in der Regel in kleinere Recheneinheiten unterteilt werden, die gleichzeitig genutzt werden können.
gleichzeitig verwendet werden. Ein CPU-Kern kann zum Beispiel zwei gleichzeitige Threads unterstützen; jeder
Thread wird als Recheneinheit verwendet, um seinen eigenen Auftrag auszuführen. Oder mehrere CPU-Kerne können
zusammengeschlossen werden, um eine größere Recheneinheit zur Ausführung eines größeren Auftrags zu bilden. Eine Recheneinheit
Unit kann für einen bestimmten, kurzlebigen Auftrag erstellt werden, z. B. für eine AWS Step Function oder einen
GCP Cloud Run - die Einheit wird nach Beendigung des Auftrags wieder entfernt. Eine Berechnungseinheit
kann auch "dauerhaft" erstellt werden, d. h. ohne an einen Auftrag gebunden zu sein, wie eine
virtuelle Maschine. Eine dauerhaftere Recheneinheit wird manchmal als "Instanz" bezeichnet.

Speicher und Rechenleistung | 297
10 Zum Zeitpunkt der Erstellung dieses Buches benötigt eine ML-Arbeitslast in der Regel zwischen 4 GB und 8 GB Speicher; 16 GB
Speicher ist ausreichend, um die meisten ML-Arbeitslasten zu bewältigen.
11 Siehe Betriebsfusion im Abschnitt "Modelloptimierung" auf Seite 216.
12 "What Is FLOP/s and Is It a Good Measure of Performance?", Stack Overflow, zuletzt aktualisiert am 7. Oktober 2020,
https://oreil.ly/M8jPP.

Die Berechnungsschicht verwendet jedoch nicht immer Threads oder Kerne als Recheneinheiten.
Es gibt Berechnungsschichten, die von den Begriffen der Kerne abstrahieren und andere
Berechnungseinheiten verwenden. Berechnungsmaschinen wie Spark und Ray verwenden zum Beispiel
"Job" als Einheit, und Kubernetes verwendet "Pod", einen Wrapper für Container, als
kleinste einsetzbare Einheit. Während Sie mehrere Container in einem Pod haben können, können Sie nicht
verschiedene Container im selben Pod unabhängig voneinander starten oder stoppen.
Um einen Auftrag auszuführen, müssen Sie zunächst die erforderlichen Daten in den Speicher Ihrer Compute Unit laden
Speicher Ihrer Recheneinheit laden und dann die erforderlichen Operationen - Addition, Multiplikation, Division,
Faltung usw. an diesen Daten aus. Um zum Beispiel zwei Arrays zu addieren, müssen Sie zunächst
diese beiden Arrays in den Speicher laden und dann die Addition mit den beiden Arrays durchführen.
Wenn die Recheneinheit nicht über genügend Speicher verfügt, um diese beiden Arrays zu laden, ist die
Operation ohne einen Algorithmus zum Umgang mit Out-of-Memory-Compu- tation unmöglich.
tation. Daher wird eine Recheneinheit hauptsächlich durch zwei Parameter charakterisiert: wie viel
Speicher sie hat und wie schnell sie eine Operation ausführt.
Die Speichermetrik kann in Einheiten wie GB angegeben werden und ist im Allgemeinen einfach
einfach zu bewerten: Eine Recheneinheit mit 8 GB Speicher kann mehr Daten im
Speicher verarbeiten als eine Recheneinheit mit nur 2 GB, und sie ist im Allgemeinen teurer.^10
Für einige Unternehmen ist nicht nur wichtig, wie viel Speicher eine Recheneinheit hat, sondern auch
wie schnell Daten in den und aus dem Speicher geladen werden können, weshalb einige Cloud-Anbieter damit werben
ihre Instanzen als "Speicher mit hoher Bandbreite" an oder geben die E/A-Bandbreite ihrer Instanzen
Bandbreite an.
Die Betriebsgeschwindigkeit ist umstrittener. Die gebräuchlichste Kennzahl ist FLOPS-
Gleitkommaoperationen pro Sekunde. Wie der Name schon sagt, gibt diese Kennzahl an, wie viele
Anzahl der Fließkommaoperationen, die eine Recheneinheit pro Sekunde ausführen kann. Sie können
ein Hardwarehersteller damit werben, dass seine GPUs, TPUs oder IPUs (Intelligence
Verarbeitungseinheiten) über teraFLOPS (eine Billion FLOPS) oder eine andere enorme Anzahl
von FLOPS.
Diese Metrik ist jedoch umstritten, weil erstens die Unternehmen, die diese Metrik messen
unterschiedliche Vorstellungen davon haben, was als Operation gezählt wird. Wenn zum Beispiel eine
Maschine zwei Operationen zu einer verschmilzt und diese verschmolzene Operation ausführt,^11 zählt dies
als eine Operation oder als zwei? Zweitens: Nur weil eine Recheneinheit in der Lage ist
eine Billion FLOPS leisten kann, heißt das nicht, dass Sie Ihre Aufgabe mit der Geschwindigkeit
einer Billion FLOPS ausführen können. Das Verhältnis zwischen der Anzahl der FLOPS, die ein Auftrag ausführen kann, und der Anzahl
FLOPS, die ein Auftrag ausführen kann, zur Anzahl der FLOPS, die eine Recheneinheit verarbeiten kann, wird als Auslastung bezeichnet.^12 Wenn eine Instanz
298 | Kapitel 10: Infrastruktur und Tooling für MLOps
13 Lesern, die sich für FLOPS und Bandbreite und deren Optimierung für Deep Learning-Modelle interessieren, empfehle ich
empfehle ich den Beitrag "Making Deep Learning Go Brrrr From First Principles" (He 2022).
14 Laut Amazon "unterstützen EC2-Instanzen Multithreading, wodurch mehrere Threads gleichzeitig auf einem CPU-Kern ausgeführt werden können.
gleichzeitig auf einem einzigen CPU-Kern laufen. Jeder Thread wird als virtuelle CPU (vCPU) auf der Instanz dargestellt. Eine Instanz
hat eine Standardanzahl von CPU-Kernen, die je nach Instanztyp variiert. Zum Beispiel hat ein m5.xlarge
Instanztyp hat beispielsweise standardmäßig zwei CPU-Kerne und zwei Threads pro Kern - insgesamt vier vCPUs" ("Optimize CPU
Options," Amazon Web Services, letzter Zugriff im April 2020, https://oreil.ly/eeOtd).

eine Million FLOPs leisten kann und Ihr Auftrag mit 0,3 Millionen FLOPs läuft, ist das
eine Auslastungsrate von 30 %. Natürlich möchten Sie Ihre Auslastungsrate so hoch wie möglich halten.
wie möglich. Es ist jedoch nahezu unmöglich, eine Auslastungsrate von 100 % zu erreichen. Abhängig
dem Hardware-Backend und der Anwendung kann eine Auslastungsrate von 50 % als
als gut oder schlecht angesehen werden. Die Auslastung hängt auch davon ab, wie schnell Sie Daten in den Speicher laden können
Speicher geladen werden können, um die nächsten Operationen durchzuführen - daher die Bedeutung der E/A-Bandbreite.^13
Bei der Evaluierung einer neuen Recheneinheit ist es wichtig, zu ermitteln, wie lange diese Recheneinheit
diese Recheneinheit für die Ausführung gängiger Arbeitslasten benötigt. MLPerf ist zum Beispiel ein beliebter
Benchmark für Hardwarehersteller, um ihre Hardwareleistung zu messen, indem sie zeigen
wie lange ihre Hardware braucht, um ein ResNet-50-Modell auf dem ImageNet
Datensatz zu trainieren oder ein BERT-large-Modell zu verwenden, um Vorhersagen für den SQuAD-Datensatz zu erstellen.
Da es nicht sehr sinnvoll ist, über FLOPS nachzudenken, wird zur Vereinfachung bei der
der Einfachheit halber bei der Bewertung der Rechenleistung nur die Anzahl der Kerne
die eine Recheneinheit hat. Sie könnten also eine Instanz mit 4 CPU-Kernen und 8 GB
Speicher. Beachten Sie, dass AWS das Konzept der vCPU verwendet, was für virtuelle
CPU steht und für praktische Zwecke als ein halber physischer Kern angesehen werden kann.^14
Die Anzahl der Kerne und des Arbeitsspeichers, die von einigen AWS EC2- und GCP
Instanzen in Abbildung 10-3.
Abbildung 10-3. Beispiele für GPU- und TPU-Instanzen, die auf AWS und GCP verfügbar sind (Stand
Februar 2022. Quelle: Screenshots von AWS- und GCP-Websites
Speicher und Rechenleistung | 299
15 Was $26,688/Stunde kostet.
16 On-Demand-Instances sind Instances, die verfügbar sind, wenn Sie sie anfordern. Spot-Instances sind Instances
die verfügbar sind, wenn sie von niemand anderem genutzt werden. Cloud-Anbieter bieten Spot-Instances in der Regel zu einem günstigeren Preis
im Vergleich zu On-Demand-Instanzen.
17 Synergy Research Group, "2020-The Year That Cloud Service Revenues Finally Dwarfed Enterprise Spend-
ing on Data Centers," March 18, 2021, https://oreil.ly/uPx94.

Öffentliche Cloud versus private Rechenzentren
Wie die Datenspeicherung ist auch die Datenverarbeitungsschicht weitgehend standardisiert. Das bedeutet, dass
eigene Rechenzentren für die Speicherung und Berechnung einzurichten, können Unternehmen
Cloud-Anbieter wie AWS und Azure für genau die Menge an Rechenleistung, die sie benötigen.
Cloud Compute macht es für Unternehmen extrem einfach, mit dem Aufbau zu beginnen, ohne
ohne sich um die Datenverarbeitungsschicht kümmern zu müssen. Das ist besonders für Unternehmen interessant, die
Arbeitslasten variabler Größe haben. Stellen Sie sich vor, dass Ihre Arbeitslasten an einem Tag des Jahres 1.000 CPU-Kerne benötigen
Tag des Jahres 1.000 CPU-Kerne und den Rest des Jahres nur 10 CPU-Kerne. Wenn Sie Ihre eigenen
Rechenzentren bauen, müssen Sie für 1.000 CPU-Kerne im Voraus bezahlen. Mit Cloud Compute,
müssen Sie nur an einem Tag im Jahr für 1.000 CPU-Kerne und den Rest des Jahres für 10 CPU-Kerne
den Rest des Jahres. Es ist praktisch, einfach mehr Rechenleistung hinzuzufügen oder
die meisten Cloud-Anbieter machen das sogar automatisch für Sie.
und so den technischen Aufwand zu reduzieren. Dies ist besonders bei ML nützlich, da
Datenwissenschaftliche Workloads sind schnelllebig. Datenwissenschaftler neigen dazu, während der Entwicklung ein paar Wochen lang
einige Wochen lang Experimente durchzuführen, was eine große Menge an Rechenleistung erfordert. Später, in der
während der Produktion, ist die Arbeitslast gleichmäßiger.
Denken Sie daran, dass Cloud Compute zwar elastisch, aber nicht magisch ist. Sie hat nicht wirklich
unendlich viel Rechenleistung. Die meisten Cloud-Anbieter bieten Beschränkungen für die Rechenressourcen, die Sie
Ressourcen, die Sie gleichzeitig nutzen können. Einige, aber nicht alle, dieser Grenzen können durch Petitionen angehoben werden. Für
Beispiel: Zum Zeitpunkt der Erstellung dieses Buches ist die größte Instanz von AWS EC2 X1e mit 128 vCPUs
und fast 4 TB Speicher.^15 Viele Rechenressourcen zu haben, bedeutet nicht, dass
dass es immer einfach ist, sie zu nutzen, vor allem, wenn Sie mit Spot-Instanzen arbeiten müssen, um Kosten zu sparen.
Kosten zu sparen.^16
Aufgrund der Elastizität und Benutzerfreundlichkeit der Cloud entscheiden sich immer mehr Unternehmen
für die Cloud zu bezahlen, anstatt ihre eigene Speicher- und Berechnungsschicht aufzubauen und zu pflegen.
Schicht. Die Studie der Synergy Research Group zeigt, dass im Jahr 2020 "die Unternehmensausgaben
Ausgaben von Unternehmen für Cloud-Infrastrukturdienste um 35 % auf fast 130 Milliarden US-Dollar steigen werden", während
"die Unternehmensausgaben für Rechenzentren um 6 % auf unter 90 Mrd. US-Dollar sinken"^17 , wie
in Abbildung 10-4 dargestellt.
300 | Kapitel 10: Infrastruktur und Tooling für MLOps
18 Sarah Wang und Martin Casado, "The Cost of Cloud, a Trillion Dollar Paradox", a16z, https://oreil.ly/3nWU3.
19 Wang und Casado, "The Cost of Cloud".

Abbildung 10-4. Im Jahr 2020 stiegen die Unternehmensausgaben für Cloud-Infrastrukturdienste um 35 %
während die Ausgaben für Rechenzentren um 6 % sinken. Quelle: Nach einer Abbildung von
Synergy Research Group
Während die Nutzung der Cloud den Unternehmen tendenziell höhere Renditen beschert als der Aufbau
als der Aufbau eigener Speicher- und Berechnungsebenen, wird dies jedoch weniger vertretbar, wenn ein
Unternehmen wächst. Auf der Grundlage der offengelegten Ausgaben für Cloud-Infrastruktur durch öffentliche Software
Softwareunternehmen zeigt die Risikokapitalfirma a16z, dass die Cloud-Ausgaben für
etwa 50 % der Umsatzkosten dieser Unternehmen ausmachen.^18
Die hohen Kosten der Cloud haben Unternehmen dazu veranlasst, ihre Arbeitslasten
zurück in ihre eigenen Rechenzentren zu verlagern, ein Prozess, der als "Cloud-Repatriierung" bezeichnet wird. Dropboxs S-1
von Dropbox aus dem Jahr 2018 zeigt, dass das Unternehmen in den zwei Jahren vor dem Börsengang 75 Mio. USD einsparen konnte.
zwei Jahren vor dem Börsengang durch die Optimierung der Infrastruktur einsparen konnte - ein großer Teil davon
bestand in der Verlagerung der Arbeitslasten von der öffentlichen Cloud in die eigenen Rechenzentren. Ist
die hohen Cloud-Kosten nur bei Dropbox, weil Dropbox in der Datenspeicherung tätig ist?
Geschäft ist? Nicht ganz. In der oben erwähnten Analyse schätzt a16z, dass "über 50
der führenden öffentlichen Softwareunternehmen, die derzeit eine Cloud-Infrastruktur nutzen, ein
schätzungsweise 100 Milliarden Dollar an Marktwert durch die Auswirkungen der Cloud auf
Margen - im Vergleich zum Betrieb der Infrastruktur selbst - verloren gehen."^19
Während der Einstieg in die Cloud einfach ist, ist der Ausstieg aus der Cloud schwierig.
Die Rückführung in die Cloud erfordert nicht unerhebliche Vorabinvestitionen sowohl in Rohstoffe als auch in
Speicher und Rechenleistung | 301
20 Laurence Goasduff, "Why Organizations Choose a Multicloud Strategy", Gartner, 7. Mai 2019,
https://oreil.ly/ZiqzQ.
21 Goasduff, "Why Organizations Choose a Multicloud Strategy" (Warum Unternehmen eine Multi-Cloud-Strategie wählen).

Engineering-Aufwand. Immer mehr Unternehmen verfolgen einen hybriden Ansatz:
Sie behalten den Großteil ihrer Workloads in der Cloud, erhöhen aber langsam ihre Investitionen
in Rechenzentren.
Multicloud-Strategie
Eine weitere Möglichkeit für Unternehmen, ihre Abhängigkeit von einem einzigen Cloud-Anbieter zu verringern
ist die Verfolgung einer Multi-Cloud-Strategie: Sie verteilen ihre Arbeitslasten auf mehrere Cloud-Anbieter.
20 Dies ermöglicht es Unternehmen, ihre Systeme so zu gestalten, dass sie mit mehreren Clouds kompatibel sind.
mit mehreren Clouds kompatibel sind, so dass sie die besten und kostengünstigsten
verfügbaren Technologien zu nutzen, anstatt auf die Dienste eines einzigen Cloud-Anbieters festgelegt zu sein.
Cloud-Anbieters gebunden zu sein, eine Situation, die als Vendor Lock-in bekannt ist. Eine Studie von Gartner aus dem Jahr 2019 zeigt
dass 81 % der Unternehmen mit zwei oder mehr Public Cloud-Anbietern zusammenarbeiten.^21 Ein
häufiges Muster, das ich für ML-Workloads gesehen habe, ist das Training auf GCP oder Azure,
und die Bereitstellung auf AWS.
Die Multicloud-Strategie erfolgt in der Regel nicht freiwillig. Wie Josh Wills, einer unserer
Josh Wills, einer unserer ersten Rezensenten, sagte: "Niemand, der bei Verstand ist, hat die Absicht, Multicloud zu nutzen." Es ist
Es ist unglaublich schwer, Daten zu verschieben und Workloads über verschiedene Clouds hinweg zu orchestrieren.
Multicloud wird oft nur deshalb eingesetzt, weil verschiedene Teile des Unternehmens unabhängig voneinander arbeiten
unabhängig voneinander arbeiten und jeder Teil seine eigene Cloud-Entscheidung trifft. Es kann auch passieren
nach einer Übernahme: Das übernommene Team arbeitet bereits in einer anderen Cloud als das
Host-Organisation und die Migration ist noch nicht erfolgt.
In meiner Arbeit habe ich erlebt, dass Multicloud aufgrund strategischer Investitionen entstanden ist. Microsoft und
Google sind große Investoren im Startup-Ökosystem, und mehrere Unternehmen, mit denen ich arbeite
mit denen ich zusammenarbeite, die zuvor AWS nutzten, sind zu Azure/GCP gewechselt, nachdem Microsoft/Google
in sie investiert haben.
Entwicklungsumgebung
In der Entwicklungsumgebung schreiben ML-Ingenieure Code, führen Experimente durch und
mit der Produktionsumgebung interagieren, in der Champion-Modelle eingesetzt
und Herausforderer-Modelle bewertet werden. Die Entwicklungsumgebung besteht aus den folgenden
Komponenten: IDE (integrierte Entwicklungsumgebung), Versionierung und CI/CD.
Wenn Sie ein Datenwissenschaftler oder ML-Ingenieur sind, der täglich Code schreibt, sind Sie wahrscheinlich sehr
mit all diesen Tools vertraut und fragen sich vielleicht, was es über sie zu sagen gibt. Nach meiner
meiner Erfahrung nach ist die Entwicklungsumgebung außerhalb einer Handvoll Technologieunternehmen stark
302 | Kapitel 10: Infrastruktur und Tooling für MLOps
22 Ville Tuulos, Effective Data Science Infrastructure (Manning, 2022).

in den meisten Unternehmen unterschätzt und zu wenig investiert wird. Laut Ville Tuulos in seinem Buch
seinem Buch Effective Data Science Infrastructure, "wären Sie überrascht, wie viele
viele Unternehmen eine gut abgestimmte, skalierbare Produktionsinfrastruktur haben, aber die Frage
aber die Frage, wie der Code überhaupt entwickelt, debuggt und getestet wird, wird ad-hoc gelöst.
ad-hoc gelöst."^22
Er schlug vor: "Wenn Sie Zeit haben, nur ein Stück Infrastruktur einzurichten
dann sollte es die Entwicklungsumgebung für Datenwissenschaftler sein". Da die Entwicklungsumgebung
Da die Entwicklungsumgebung der Ort ist, an dem die Ingenieure arbeiten, führen Verbesserungen in der Entwicklungsumgebung
direkt in Verbesserungen der technischen Produktivität.
In diesem Abschnitt werden wir zunächst die verschiedenen Komponenten der Entwicklungsumgebung behandeln und dann
dann die Standardisierung der Entwicklungsumgebung, bevor wir erörtern, wie Sie
Änderungen aus der Entwicklungsumgebung in die Produktionsumgebung mit
Containern.
Einrichtung der Entwicklungsumgebung
Die Entwicklungsumgebung sollte so eingerichtet werden, dass sie alle Tools enthält, die den Entwicklern die Arbeit erleichtern.
die Arbeit der Entwickler erleichtern. Sie sollte auch Werkzeuge für die Versionierung enthalten. Zum Zeitpunkt dieser
Unternehmen eine Reihe von Ad-hoc-Tools für die Versionierung ihrer ML-Workflows, z. B.
Git zur Versionskontrolle von Code, DVC zur Versionierung von Daten, Weights & Biases oder Comet.ml zur
Experimente während der Entwicklung zu verfolgen, und MLflow, um Artefakte von Modellen bei
Einsatz zu verfolgen. Claypot AI arbeitet an einer Plattform, die Sie bei der Versionierung und
alle Ihre ML-Workflows an einem Ort zu verfolgen. Die Versionskontrolle ist für jedes Software
Softwareentwicklungsprojekte, aber noch mehr für ML-Projekte, da man sowohl die schiere
Anzahl der Dinge, die Sie ändern können (Code, Parameter, die Daten selbst, etc.) und die Notwendigkeit
frühere Durchläufe zu verfolgen und später zu reproduzieren. Wir haben dies in dem Abschnitt
"Verfolgung und Versionierung von Experimenten" auf Seite 162.
Die Entwicklungsumgebung sollte auch mit einer CI/CD-Testsuite eingerichtet werden, um Ihren Code zu testen
zu testen, bevor er in die Staging- oder Produktionsumgebung übertragen wird. Beispiele für Werkzeuge zur
zur Orchestrierung Ihrer CI/CD-Testsuite sind GitHub Actions und CircleCI. Da CI/CD ein
ein Thema der Softwareentwicklung ist, geht es über den Rahmen dieses Buches hinaus.
In diesem Abschnitt werden wir uns auf den Ort konzentrieren, an dem Ingenieure ihren Code schreiben: die IDE.
IDE
Die IDE ist der Editor, in dem Sie Ihren Code schreiben. IDEs unterstützen in der Regel mehrere
Programmiersprachen. IDEs können native Anwendungen wie VS Code oder Vim sein. IDEs können
browserbasiert sein, d. h. sie werden in Browsern ausgeführt, wie z. B. AWS Cloud9.
Entwicklungsumgebung | 303
23 Zum Zeitpunkt der Erstellung dieses Buches bietet Google Colab sogar kostenlose GPUs für seine Nutzer an.

Viele Datenwissenschaftler schreiben ihren Code nicht nur in IDEs, sondern auch in Notebooks wie Jupyter
Notebooks und Google Colab.^23 Notebooks sind mehr als nur Orte zum Schreiben von Code.
Sie können beliebige Artefakte wie Bilder, Diagramme, Daten in schönen Tabellenformaten usw. enthalten,
usw., was Notebooks sehr nützlich für die explorative Datenanalyse und die Analyse der
Modell-Trainingsergebnisse.
Notebooks haben eine schöne Eigenschaft: Sie sind zustandsorientiert, d. h. sie können Zustände nach der Ausführung beibehalten.
Wenn Ihr Programm auf halbem Weg fehlschlägt, können Sie es ab dem fehlgeschlagenen Schritt erneut ausführen, anstatt
müssen Sie das Programm nicht von Anfang an ausführen. Dies ist besonders hilfreich, wenn Sie
mit großen Datensätzen umgehen müssen, deren Laden lange dauern kann. Mit Notebooks,
brauchen Sie Ihre Daten nur einmal zu laden - Notebooks können diese Daten im Speicher halten -
statt sie jedes Mal laden zu müssen, wenn Sie Ihren Code ausführen wollen. Wie in
Abbildung 10-5 gezeigt, müssen Sie, wenn Ihr Code bei Schritt 4 in einem Notebook fehlschlägt, nur Schritt 4
und nicht vom Anfang des Programms an.
Abbildung 10-5. Wenn in Jupyter Notebooks Schritt 4 fehlschlägt, müssen Sie nur Schritt 4 erneut ausführen,
anstatt die Schritte 1 bis 4 erneut ausführen zu müssen.
Beachten Sie, dass diese Zustandsabhängigkeit ein zweischneidiges Schwert sein kann, denn sie ermöglicht es Ihnen, Ihre Zellen
Zellen in einer anderen Reihenfolge auszuführen. In einem normalen Skript muss zum Beispiel Zelle 4 nach Zelle 3
und Zelle 3 muss nach Zelle 2 ausgeführt werden. In Notebooks können Sie jedoch die Zelle 2, 3, dann 4
oder Zelle 4, 3, dann 2. Dies erschwert die Reproduzierbarkeit von Notebooks, es sei denn, Ihr Notebook
eine Anweisung enthält, in welcher Reihenfolge die Zellen ausgeführt werden sollen. Diese Schwierigkeit wird
in einem Witz von Chris Albon dargestellt (siehe Abbildung 10-6).
304 | Kapitel 10: Infrastruktur und Werkzeuge für MLOps
24 Michelle Ufford, M. Pacer, Matthew Seal, und Kyle Kelley, "Beyond Interactive: Notebook-Innovation bei
Netflix," Netflix Technology Blog, August 16, 2018, https://oreil.ly/EHvAe.

Abbildung 10-6. Die Zustandsabhängigkeit von Notizbüchern ermöglicht die Ausführung von Zellen außerhalb der Reihenfolge, wodurch es
Notebook schwer zu reproduzieren
Da Notebooks so nützlich für die Datenexploration und Experimente sind, sind Notebooks
zu einem unverzichtbaren Werkzeug für Datenwissenschaftler und ML geworden. Einige Unternehmen
haben Notebooks zum Zentrum ihrer Data-Science-Infrastruktur gemacht. In ihrem bahnbrechenden
Artikel "Beyond Interactive: Notebook Innovation at Netflix", hat Netflix eine Liste von
Infrastruktur-Tools, die verwendet werden können, um Notebooks noch leistungsfähiger zu machen.^24
Liste umfasst:
Papermill
Um mehrere Notebooks mit unterschiedlichen Parametersätzen zu erstellen - zum Beispiel, wenn
wenn Sie verschiedene Experimente mit unterschiedlichen Parametersätzen durchführen und
gleichzeitig ausführen wollen. Es kann auch bei der Zusammenfassung von Metriken aus einer Sammlung
von Notizbüchern.
Pendler
Ein Notebook-Hub zum Anzeigen, Suchen und Freigeben von Notebooks innerhalb einer
Organisation.
Entwicklungsumgebung | 305
25 Für Uneingeweihte kann ein neuer Pull Request als ein neues Stück Code verstanden werden, das der Codebasis hinzugefügt wird.

Ein weiteres interessantes Projekt zur Verbesserung der Erfahrung mit Notebooks ist nbdev, eine
Bibliothek, die auf Jupyter Notebooks aufsetzt und Sie ermutigt, die Dokumentation und
Tests an der gleichen Stelle zu schreiben.
Standardisierung von Entwicklungsumgebungen
Der erste Punkt bei der Entwicklungsumgebung ist, dass sie standardisiert sein sollte, wenn nicht
wenn nicht unternehmensweit, so doch zumindest teamweit. Wir werden eine Geschichte durchgehen, um zu verstehen, was es bedeutet
was es bedeutet, die Entwicklungsumgebung zu standardisieren und warum dies notwendig ist.
In den Anfängen unseres Startups arbeitete jeder von uns von seinem eigenen Computer aus. Wir hatten eine
bash-Datei, die ein neues Teammitglied ausführen konnte, um eine neue virtuelle Umgebung zu erstellen - in
In unserem Fall verwenden wir conda für virtuelle Umgebungen und installieren die erforderlichen Pakete
die für die Ausführung unseres Codes erforderlich sind. Die Liste der benötigten Pakete war die gute alte require-
ments.txt, die wir immer wieder ergänzten, wenn wir ein neues Paket einsetzten. Manchmal war einer
einer von uns faul und fügte einfach einen Paketnamen hinzu (z.B. torch), ohne anzugeben
welche Version des Pakets es war (z.B. torch==1.10.0+cpu). Gelegentlich lief eine neue
Pull-Request auf meinem Computer, aber nicht auf dem Computer eines anderen Mitarbeiters,^25
und wir fanden in der Regel schnell heraus, dass es daran lag, dass wir unterschiedliche Versionen
des gleichen Pakets verwendeten. Wir haben beschlossen, immer den Paketnamen zusammen mit der
Paketversion anzugeben, wenn wir ein neues Paket zur requirements.txt hinzufügen, und das
eine Menge unnötiger Kopfschmerzen beseitigt.
Eines Tages stießen wir auf diesen merkwürdigen Fehler, der nur bei einigen Durchläufen auftrat und bei anderen nicht.
anderen. Ich bat meinen Kollegen, sich das anzusehen, aber er war nicht in der Lage, den Fehler zu reproduzieren.
Ich sagte ihm, dass der Fehler nur manchmal auftrat, so dass er den Code vielleicht
20 Mal ausführen, nur um sicherzugehen. Er führte den Code 20 Mal aus und fand immer noch
nichts. Wir verglichen unsere Pakete und alles stimmte überein. Nach ein paar Stunden der
Frustration entdeckten wir, dass es sich um ein Gleichzeitigkeitsproblem handelte, das nur bei
nur bei Python Version 3.8 oder früher auftritt. Ich hatte Python 3.8 und mein Kollege hatte Python
3.9, also konnte er den Fehler nicht sehen. Wir haben beschlossen, dass alle dieselbe Python
Version zu verwenden, was uns einige weitere Kopfschmerzen ersparte.
Dann bekam mein Kollege eines Tages einen neuen Laptop. Es war ein MacBook mit dem damals neuen
M1-Chip. Er versuchte, unsere Einrichtungsschritte auf diesem neuen Laptop zu befolgen, stieß aber auf Schwierigkeiten.
Das lag daran, dass der M1-Chip neu war und einige der von uns verwendeten Tools, darunter
Docker, funktionierten noch nicht gut mit M1-Chips. Nachdem ich gesehen hatte, wie er mit der Einrichtung
Nachdem wir gesehen hatten, wie er sich einen Tag lang mit der Einrichtung der Umgebung abmühte, beschlossen wir, auf eine Cloud-Entwicklungsumgebung umzustellen.
Das bedeutet, dass wir die virtuelle Umgebung sowie die Tools und Pakete immer noch standardisieren,
aber jetzt verwendet jeder die virtuelle Umgebung und die Tools und Pakete auf demselben
Art von Maschine, die von einem Cloud-Anbieter bereitgestellt wird.
306 | Kapitel 10: Infrastruktur und Tooling für MLOps
26 Siehe Editor-Krieg, die jahrzehntelange, hitzige Debatte über Vim versus Emacs.

Wenn Sie eine Cloud-Entwicklungsumgebung verwenden, können Sie eine Cloud-Entwicklungsumgebung verwenden, die auch
mit einer Cloud-IDE wie AWS Cloud9 (die keine integrierten Notebooks hat) und
Amazon SageMaker Studio (das mit gehostetem JupyterLab ausgestattet ist). Zum Zeitpunkt der Erstellung dieses
Buches scheint Amazon SageMaker Studio weiter verbreitet zu sein als Cloud9. Wie auch immer,
die meisten Ingenieure, die ich kenne, die Cloud-IDEs verwenden, installieren IDEs ihrer Wahl,
wie Vim, auf ihren Cloud-Instanzen.
Eine viel beliebtere Option ist die Verwendung einer Cloud-Entwicklungsumgebung mit einer lokalen IDE. Für
Sie können z. B. VS Code verwenden, das auf Ihrem Computer installiert ist, und die lokale IDE
über ein sicheres Protokoll wie Secure Shell (SSH) mit der Cloud-Umgebung verbinden.
Es ist zwar allgemein anerkannt, dass Tools und Pakete standardisiert werden sollten,
zögern einige Unternehmen, IDEs zu standardisieren. Ingenieure können eine emotionale Bindung
an IDEs binden, und einige haben sich sehr bemüht, die IDE ihrer Wahl zu verteidigen,^26
Es wird also schwierig sein, alle zur Verwendung derselben IDE zu zwingen. Im Laufe der Jahre haben sich jedoch einige
IDEs herauskristallisiert, die am beliebtesten sind. Unter ihnen ist VS Code eine gute Wahl
da sie eine einfache Integration mit Cloud-Entwicklungsinstanzen ermöglicht.
Bei unserem Startup haben wir uns für GitHub Codespaces als Cloud-Entwicklungsumgebung entschieden, aber
eine AWS EC2 oder eine GCP-Instanz, in die Sie sich per SSH einloggen können, ist ebenfalls eine gute Option.
Bevor wir auf Cloud-Umgebungen umgestiegen sind, haben wir uns wie viele andere Unternehmen Sorgen gemacht
Was wäre, wenn wir vergessen würden, unsere Instanzen abzuschalten, wenn sie nicht gebraucht werden, und
sie uns weiterhin Geld in Rechnung stellen? Diese Sorge ist jedoch aus zwei Gründen verschwunden.
Erstens schalten Tools wie GitHub Codespaces Ihre Instanz automatisch nach 30
Minuten der Inaktivität ab. Zweitens sind einige Instanzen ziemlich billig. Zum Beispiel ist eine AWS
Instanz mit 4 vCPUs und 8 GB Speicher kostet etwa 0,1 $/Stunde, was
was ungefähr 73 $/Monat entspricht, wenn Sie sie nie abschalten. Da Entwicklungszeit teuer ist
Wenn Sie mit einer Cloud-Entwicklungsumgebung ein paar Stunden Entwicklungszeit pro Monat einsparen können
im Monat einsparen können, lohnt sich das für viele Unternehmen.
Der Wechsel von lokalen Entwicklungsumgebungen zu Cloud-Entwicklungsumgebungen hat viele weitere
Vorteile. Erstens wird der IT-Support so viel einfacher - stellen Sie sich vor, Sie müssten
Stellen Sie sich vor, Sie müssten 1.000 verschiedene lokale Maschinen betreuen, anstatt nur einen Typ von Cloud
Instanz. Zweitens ist es praktisch für die Arbeit aus der Ferne - Sie können sich einfach per SSH in Ihre Entwicklungsumgebung
Umgebung einwählen, egal wo Sie sich befinden, von jedem Computer aus. Drittens: Cloud-Entwicklungsumgebungen
zur Sicherheit beitragen. Wenn zum Beispiel der Laptop eines Mitarbeiters gestohlen wird, können Sie einfach
können Sie einfach den Zugriff auf Cloud-Instanzen von diesem Laptop aus sperren, um den Dieb am Zugriff auf
auf Ihre Codebasis und geschützte Informationen zuzugreifen. Natürlich können einige Unternehmen nicht
auch wegen Sicherheitsbedenken nicht in der Lage sein, auf Cloud-Entwicklungsumgebungen umzusteigen. Für
Sie dürfen zum Beispiel ihren Code oder ihre Daten nicht in der Cloud haben.
Entwicklungsumgebung | 307
Der vierte Vorteil, der meines Erachtens der größte Vorteil für Unternehmen ist, die
Unternehmen, die ihre Produktion in der Cloud betreiben, ist, dass die Entwicklungsumgebung in der Cloud
die Kluft zwischen der Entwicklungsumgebung und der Produktionsumgebung verringert. Wenn
Produktionsumgebung in der Cloud ist, ist es nur natürlich, die Entwicklungsumgebung in die
in die Cloud zu verlegen, ist nur natürlich.

Gelegentlich muss ein Unternehmen seine Entwicklungsumgebungen in die Cloud verlagern, nicht nur
Vorteile, sondern auch aus der Notwendigkeit heraus. Für die Anwendungsfälle, in denen Daten nicht
nicht heruntergeladen oder auf einem lokalen Rechner gespeichert werden können, ist der einzige Weg, auf sie über ein Notebook
in der Cloud (SageMaker Studio) zugreifen, das die Daten aus S3 lesen kann, sofern es über die
Berechtigungen hat.

Natürlich eignen sich Cloud-Entwicklungsumgebungen nicht für jedes Unternehmen, sei es aus Kostengründen,
Sicherheit oder aus anderen Gründen. Die Einrichtung von Cloud-Entwicklungsumgebungen erfordert auch einige
Anfangsinvestitionen, und möglicherweise müssen Sie Ihre Datenwissenschaftler in Sachen Cloud-Hygiene schulen.
Hygiene, einschließlich der Einrichtung sicherer Verbindungen zur Cloud, der Einhaltung von Sicherheitsvorschriften
oder die Vermeidung einer verschwenderischen Cloud-Nutzung. Allerdings kann die Standardisierung von Entwicklungsumgebungen
kann Ihren Datenwissenschaftlern das Leben leichter machen und Ihnen langfristig Geld sparen.

Von Dev zu Prod: Container
Während der Entwicklung arbeiten Sie normalerweise mit einer festen Anzahl von Maschinen oder
Instanzen (in der Regel eine), weil Ihre Arbeitslasten nicht stark schwanken - Ihr Modell
Ihr Modell ändert sich nicht plötzlich von nur 1.000 Anfragen pro Stunde auf 1 Million
Anfragen pro Stunde.

Ein Produktionsdienst hingegen kann auf mehrere Instanzen verteilt sein.
Die Anzahl der Instanzen ändert sich von Zeit zu Zeit in Abhängigkeit von den eingehenden
Arbeitslasten, die manchmal unvorhersehbar sein können. Zum Beispiel twittert ein Prominenter über
Ihre noch junge App und plötzlich steigt der Datenverkehr um das 10-fache an. Sie müssen dann neue Instanzen einschalten
Instanzen einschalten, und diese Instanzen müssen mit den erforderlichen Tools und
Paketen eingerichtet werden, um Ihre Workloads auszuführen.

Früher mussten Sie die Instanzen selbst hoch- und herunterfahren, aber die meisten öffentlichen
Cloud-Anbieter haben den Teil der automatischen Skalierung übernommen. Allerdings müssen Sie sich immer noch
um die Einrichtung neuer Instanzen kümmern.

Wenn Sie immer mit der gleichen Instanz arbeiten, können Sie die Abhängigkeiten
einmal installieren und sie immer dann verwenden, wenn Sie diese Instanz benutzen. In der Produktion, wenn Sie dynamisch
Instanzen nach Bedarf dynamisch zuweisen, ist Ihre Umgebung von Natur aus zustandslos. Wenn eine neue
Instanz für Ihre Arbeitslast zugewiesen wird, müssen Sie die Abhängigkeiten anhand einer Liste
von vordefinierten Anweisungen installieren.

Es stellt sich die Frage, wie man eine Umgebung auf einer neuen Instanz neu erstellen kann.
Die Containertechnologie, von der Docker die beliebteste ist, soll diese Frage beantworten.
diese Frage zu beantworten. Mit Docker erstellen Sie eine Dockerdatei mit Schritt-für-Schritt-Anweisungen für

308 | Kapitel 10: Infrastruktur und Werkzeuge für MLOps

wie Sie eine Umgebung neu erstellen, in der Ihr Modell ausgeführt werden kann: Installieren Sie dieses Paket,
laden Sie dieses trainierte Modell herunter, setzen Sie Umgebungsvariablen, navigieren Sie in einen Ordner, usw.
Diese Anweisungen ermöglichen es der Hardware, Ihren Code überall auszuführen.

Zwei Schlüsselbegriffe in Docker sind Image und Container. Wenn Sie alle Anweisungen in
einer Dockerdatei erhalten Sie ein Docker-Abbild. Wenn Sie dieses Docker-Image ausführen, erhalten Sie
einen Docker-Container. Sie können sich ein Dockerfile als das Rezept für die Konstruktion einer Form vorstellen,
die ein Docker-Abbild ist. Aus dieser Form können Sie mehrere laufende Instanzen erstellen;
jede ist ein Docker-Container.

Sie können ein Docker-Image entweder von Grund auf oder aus einem anderen Docker-Image erstellen.
Zum Beispiel könnte NVIDIA ein Docker-Image bereitstellen, das TensorFlow und
alle notwendigen Bibliotheken enthält, um TensorFlow für GPUs zu optimieren. Wenn Sie eine Anwendung bauen wollen
Anwendung bauen wollen, die TensorFlow auf GPUs ausführt, ist es keine schlechte Idee, dieses Docker
Image als Basis zu verwenden und die Abhängigkeiten, die für Ihre Anwendung spezifisch sind, über dieses
Basis-Image zu installieren.

In einer Container-Registry können Sie ein Docker-Image freigeben oder ein Image finden, das
das von anderen Personen erstellt wurde und öffentlich oder nur mit Personen innerhalb ihrer Organisation geteilt werden soll.
Zu den gängigen Container-Registrierungen gehören Docker Hub und AWS ECR (Elastic Container
Registry).

Hier ist ein Beispiel für ein einfaches Dockerfile, das die folgenden Anweisungen ausführt. Das
Beispiel soll zeigen, wie Dockerdateien im Allgemeinen funktionieren, und ist möglicherweise nicht ausführbar.

1.1. Laden Sie das neueste PyTorch-Basis-Image herunter.
2.2. Klonen Sie das NVIDIA apex-Repository auf GitHub, navigieren Sie zu dem neu erstellten apex
Ordner und installiere apex.
3.3. Legen Sie fancy-nlp-project als Arbeitsverzeichnis fest.
4.4. Klonen Sie das transformers-Repository von Hugging Face auf GitHub, navigieren Sie zum neu erstellten
erstellten Ordner transformers und installieren Sie transformers.
FROM pytorch/pytorch:latest
RUN git clone https://github.com/NVIDIA/apex
RUN cd apex && \
python3 setup.py install && \
pip install -v --no-cache-dir --global-option="--cpp_ext" \
--global-option="--cuda_ext" ./
WORKDIR /fancy-nlp-project
RUN git clone https://github.com/huggingface/transformers.git && \
cd transformers && \
python3 -m pip install --no-cache-dir.
Entwicklungsumgebung | 309
27 Chip Huyen, "Why Data Scientists Shouldn't Need to Know Kubernetes," September 13, 2021, https://huyen.
chip.com/2021/09/13/data-science-infrastructure.html; Neil Conway und David Hershey, "Data Scientists Don't
Care About Kubernetes," Determined AI, November 30, 2020, https://oreil.ly/FFDQW; I Am Developer on
Twitter (@iamdevloper): "Ich verstehe kaum meine eigenen Gefühle, wie soll ich dann Kubernetes verstehen?"
June 26, 2021, https://oreil.ly/T2eQE.

Wenn Ihre Anwendung etwas Interessantes tut, benötigen Sie wahrscheinlich mehr als einen
Container. Betrachten Sie den Fall, dass Ihr Projekt aus dem Featurizing-Code besteht, der
schnell auszuführen ist, aber viel Speicherplatz benötigt, und dem Code für die Modellschulung, der langsam
langsam läuft, aber weniger Speicher benötigt. Wenn Sie beide Teile des Codes auf denselben
GPU-Instanzen ausführen, benötigen Sie GPU-Instanzen mit hohem Speicherbedarf, was sehr
teuer sein kann. Stattdessen können Sie Ihren Featurizing-Code auf CPU-Instanzen und den
Modelltrainingscode auf GPU-Instanzen ausführen. Das bedeutet, Sie benötigen einen Container für
Featurisierung und einen weiteren Container für das Training.
Verschiedene Container können auch notwendig sein, wenn verschiedene Schritte in Ihrer Pipeline
widersprüchliche Abhängigkeiten haben, z. B. wenn Ihr Featurizer-Code NumPy 0.8 benötigt, aber
Ihr Modell erfordert NumPy 1.0.
Wenn Sie 100 Microservices haben und jeder Microservice seinen eigenen Container benötigt, können Sie
können Sie 100 Container gleichzeitig laufen lassen. Manuelles Erstellen, Ausführen,
und das Beenden von 100 Containern kann eine mühsame Aufgabe sein. A
Werkzeug, das Ihnen bei der Verwaltung mehrerer Container hilft, heißt Container-Orchestrierung. Docker
Compose ist ein leichtgewichtiger Container-Orchestrator, der Container auf einem
einzelnen Host verwalten kann.
Allerdings könnte jeder Ihrer Container auf einem eigenen Host laufen, und hier stößt
Docker Compose an seine Grenzen stößt. Kubernetes (K8s) ist ein Werkzeug für genau diese Aufgabe. K8s
schafft ein Netzwerk, über das Container kommunizieren und Ressourcen gemeinsam nutzen können. Es kann Ihnen helfen
Container auf mehr Instanzen zu spinnen, wenn Sie mehr Rechenleistung/Speicher benötigen, sowie
Container abzuschalten, wenn Sie sie nicht mehr benötigen, und es hilft, die Hochverfügbarkeit
Hochverfügbarkeit für Ihr System.
K8s war eine der am schnellsten wachsenden Technologien in den 2010er Jahren. Seit ihrer Einführung im Jahr
2014 ist sie heute in Produktionssystemen allgegenwärtig. Jeremy Jordan hat eine großartige
Einführung in K8s für Leser, die mehr darüber erfahren möchten. Allerdings ist K8s nicht
nicht das datenwissenschaftlerfreundlichste Tool, und es gab viele Diskussionen darüber, wie
wie man Data-Science-Workloads davon wegbewegen kann.^27 Im nächsten Abschnitt gehen wir näher auf K8s
Abschnitt.
310 | Kapitel 10: Infrastruktur und Tooling für MLOps
Ressourcenmanagement
In der Welt vor der Cloud (und auch heute noch in Unternehmen, die ihre eigenen Rechenzentren unterhalten)
eigenen Rechenzentren) waren Speicher und Rechenleistung endlich. Das Ressourcenmanagement konzentrierte sich damals
wie man das Beste aus den begrenzten Ressourcen herausholen kann. Eine Erhöhung der Ressourcen für
für eine Anwendung konnte bedeuten, dass die Ressourcen für andere Anwendungen sinken.
Logik war erforderlich, um die Ressourcennutzung zu maximieren, auch wenn dies einen
mehr Entwicklungszeit erforderte.

In der Cloud-Welt, in der die Speicher- und Rechenressourcen viel elastischer sind
elastisch sind, geht es nicht mehr darum, die Ressourcenauslastung zu maximieren, sondern
Ressourcen kosteneffizient zu nutzen. Das Hinzufügen von mehr Ressourcen für eine Anwendung bedeutet nicht
bedeutet keine Verringerung der Ressourcen für andere Anwendungen, was die
Zuweisungsproblem erheblich vereinfacht. Viele Unternehmen haben kein Problem damit, mehr Ressourcen für eine
solange die zusätzlichen Kosten durch den Ertrag gerechtfertigt sind, z. B. durch zusätzliche Einnahmen oder
eingesparte Entwicklungszeit.

In der überwiegenden Mehrheit der Welt, wo die Zeit der Ingenieure wertvoller ist als
mehr Ressourcen einzusetzen, wenn dies bedeutet, dass die Ingenieure dadurch produktiver werden können.
Ingenieure produktiver werden. Das bedeutet, dass es für Unternehmen sinnvoll sein kann
Unternehmen sinnvoll sein, in die Automatisierung ihrer Arbeitslasten zu investieren, was den Einsatz von Ressourcen weniger
weniger effizient ist als die manuelle Planung der Arbeitsbelastung, aber die Ingenieure können sich
auf Arbeiten mit höherem Ertrag zu konzentrieren. Wenn ein Problem gelöst werden kann, indem entweder mehr
mehr nicht-menschliche Ressourcen (z. B. mehr Rechenleistung) oder durch mehr menschliche
Ressourcen (z. B. mehr Zeit für die Neukonzeption), wird oft die erste Lösung
bevorzugt werden.

In diesem Abschnitt wird erörtert, wie Ressourcen für ML-Workflows verwaltet werden können. Wir werden uns
Wir konzentrieren uns auf Cloud-basierte Ressourcen; die diskutierten Ideen können jedoch auch für
private Rechenzentren.

Cron, Planer und Orchestratoren
Es gibt zwei Hauptmerkmale von ML-Workflows, die ihre Ressourcenverwaltung beeinflussen
Verwaltung beeinflussen: Wiederholbarkeit und Abhängigkeiten.

In diesem Buch haben wir ausführlich erörtert, dass die Entwicklung von ML-Systemen ein iterativer
Prozess ist. Auch ML-Arbeitslasten sind selten einmalige Vorgänge, sondern etwas Wiederholtes.
itive. Sie können zum Beispiel jede Woche ein Modell trainieren oder alle vier Stunden einen neuen Stapel
Vorhersagen alle vier Stunden. Diese sich wiederholenden Prozesse können geplant und
und orchestriert werden, damit sie reibungslos und kosteneffizient mit den verfügbaren Ressourcen ablaufen.

Die Planung von sich wiederholenden Aufgaben zu festen Zeiten ist genau das, was cron tut. Das ist auch
alles, was cron tut: ein Skript zu einer bestimmten Zeit ausführen und Ihnen mitteilen, ob der Auftrag
erfolgreich war oder nicht. Es kümmert sich nicht um die Abhängigkeiten zwischen den Aufträgen, die es ausführt - Sie

Ressourcenverwaltung | 311
Sie können Job A nach Job B mit Cron ausführen, aber Sie können nichts Kompliziertes planen wie
B ausführen, wenn A erfolgreich ist, und C ausführen, wenn A fehlschlägt.

Dies führt uns zum zweiten Merkmal: Abhängigkeiten. Die Schritte in einem ML-Workflow
können komplexe Abhängigkeitsbeziehungen zueinander haben. Zum Beispiel kann ein ML
Arbeitsablauf beispielsweise aus den folgenden Schritten bestehen:

1.1. Abrufen der Daten der letzten Woche aus den Data Warehouses.
2.2. Extrahieren von Merkmalen aus diesen gezogenen Daten.
3.3. Trainieren von zwei Modellen, A und B, auf den extrahierten Merkmalen.
4.4. Vergleich von A und B auf dem Testsatz.
5.5. Einsatz von A, wenn A besser ist; andernfalls Einsatz von B.
Jeder Schritt hängt vom Erfolg des vorangegangenen Schritts ab. Schritt 5 bezeichnen wir als bedingte
tionale Abhängigkeit: Die Aktion für diesen Schritt hängt vom Ergebnis des vorherigen
Schrittes ab. Die Reihenfolge der Ausführung und die Abhängigkeiten zwischen diesen Schritten lassen sich
Graph dargestellt werden, wie in Abbildung 10-7 gezeigt.

Abbildung 10-7. Ein Graph, der die Reihenfolge der Ausführung eines einfachen ML-Workflows zeigt, der
im Wesentlichen ein DAG (gerichteter azyklischer Graph) ist

Viele Leser werden erkennen, dass Abbildung 10-7 ein DAG ist: ein gerichteter azyklischer Graph. Er
muss gerichtet sein, um die Abhängigkeiten zwischen den Schritten auszudrücken. Er kann keine Zyklen enthalten
denn sonst würde der Auftrag einfach ewig weiterlaufen. DAG ist eine gängige Methode
um Computer-Workflows im Allgemeinen darzustellen, nicht nur ML-Workflows. Die meisten Workflow
Management-Tools verlangen, dass Sie Ihre Arbeitsabläufe in Form von DAGs spezifizieren.

312 | Kapitel 10: Infrastruktur und Werkzeuge für MLOps

28 Abhishek Verma, Luis Pedrosa, Madhukar Korupolu, David Oppenheimer, Eric Tune, und John Wilkes,
"Large-Scale Cluster Management at Google with Borg," EuroSys '15: Proceedings of the Tenth European
Conference on Computer Systems (April 2015): 18, https://oreil.ly/9TeTM.

Scheduler sind Cron-Programme, die mit Abhängigkeiten umgehen können. Sie nehmen die DAG eines
eines Arbeitsablaufs und plant jeden Schritt entsprechend ein. Sie können sogar planen, dass ein
Job basierend auf einem ereignisbasierten Auslöser starten, z. B. einen Job starten, wenn ein Ereignis X eintritt.
Mit Schedulern können Sie auch festlegen, was zu tun ist, wenn ein Job fehlschlägt oder erfolgreich ist, z. B. wenn er fehlschlägt,
wie oft er es erneut versuchen soll, bevor er aufgibt.
Scheduler verwenden in der Regel Warteschlangen, um die Aufträge zu verfolgen. Aufträge können in eine Warteschlange gestellt, priorisiert
und die für die Ausführung benötigten Ressourcen zugewiesen werden. Dies bedeutet, dass die Planer
die verfügbaren und die für die Ausführung der einzelnen Aufträge benötigten Ressourcen kennen.
Die benötigten Ressourcen werden entweder als Optionen bei der Planung eines Auftrags angegeben oder
vom Scheduler geschätzt. Wenn ein Job zum Beispiel 8 GB Speicher und zwei CPUs benötigt, muss der
Scheduler unter den von ihm verwalteten Ressourcen eine Instanz mit 8 GB
Speicher und zwei CPUs finden und warten, bis die Instanz keine anderen Aufträge ausführt, um
diesen Auftrag auf der Instanz auszuführen.
Hier ist ein Beispiel für die Planung eines Jobs mit dem beliebten Scheduler Slurm, bei dem
Sie den Jobnamen, die Zeit, zu der der Job ausgeführt werden soll, und die
Menge an Speicher und CPUs, die für den Job zugewiesen werden sollen:
#!/bin/bash
#SBATCH -J JobName
#SBATCH --time=11:00:00 # Wann soll der Job gestartet werden?
#SBATCH --mem-per-cpu=4096 # Speicher, in MB, der pro CPU zugewiesen werden soll
#SBATCH --cpus-per-task=4 # Anzahl der Kerne pro Task
Scheduler sollten auch für die Ressourcennutzung optimieren, da sie über Informationen
über die verfügbaren Ressourcen, die auszuführenden Aufgaben und die für die Ausführung der einzelnen Aufgaben benötigten Ressourcen verfügen. Wie-
Allerdings ist die von den Benutzern angegebene Anzahl von Ressourcen nicht immer korrekt. Ein Beispiel,
Ich könnte schätzen und daher angeben, dass ein Auftrag 4 GB Arbeitsspeicher benötigt, aber dieser
aber dieser Auftrag benötigt nur 3 GB Arbeitsspeicher oder 4 GB Arbeitsspeicher in Spitzenzeiten und nur 1-2
GB Arbeitsspeicher sonst. Ausgefeilte Scheduler wie Googles Borg schätzen, wie
wie viele Ressourcen ein Auftrag tatsächlich benötigt und fordern ungenutzte Ressourcen für andere Aufträge zurück,^28
was die Ressourcennutzung weiter optimiert.
Die Entwicklung eines Allzweck-Schedulers ist schwierig, denn dieser Scheduler muss
in der Lage sein muss, eine fast beliebige Anzahl von gleichzeitigen Maschinen und Arbeitsabläufen zu verwalten. Wenn
Wenn Ihr Planer ausfällt, wird jeder einzelne Arbeitsablauf, den dieser Planer berührt
unterbrochen.
Wenn sich die Planer damit befassen, wann die Aufträge ausgeführt werden und welche Ressourcen dafür benötigt werden
benötigt werden, kümmern sich die Orchestratoren darum, wo diese Ressourcen zu finden sind.
Scheduler befassen sich mit Job-Typ-Abstraktionen wie DAGs, Prioritäts-Warteschlangen, User-Level
Ressourcenverwaltung | 313
Quoten (d. h. die maximale Anzahl von Instanzen, die ein Benutzer zu einem bestimmten Zeitpunkt nutzen kann) usw.
Orchestratoren befassen sich mit Abstraktionen auf niedrigerer Ebene wie Maschinen, Instanzen, Clustern,
Gruppierung auf Dienstebene, Replikation usw. Wenn der Orchestrator feststellt, dass es mehr Aufträge gibt
als der Pool der verfügbaren Instanzen, kann er die Anzahl der Instanzen im
verfügbaren Instanz-Pool erhöhen. Wir sagen, dass er mehr Computer "bereitstellt", um die
die Arbeitslast zu bewältigen. Scheduler werden oft für periodische Aufträge verwendet, während Orchestratoren
für Dienste verwendet werden, bei denen Sie einen lang laufenden Server haben, der auf
Anfragen reagiert.

Der bekannteste Orchestrator ist heute zweifelsohne Kubernetes, der Container
Orchestrator, den wir im Abschnitt "Von Dev zu Prod: Container" auf Seite

K8s kann vor Ort verwendet werden (sogar auf Ihrem Laptop über Minikube). Allerdings habe ich
niemanden getroffen, der gerne seine eigenen K8s-Cluster einrichtet, daher nutzen die meisten
K8s als gehosteten Dienst, der von ihren Cloud-Anbietern verwaltet wird, wie z. B. AWS's Elastic
Kubernetes Service (EKS) oder Google Kubernetes Engine (GKE).
Viele Leute verwenden Scheduler und Orchestratoren synonym, da Scheduler
in der Regel auf Orchestrierern laufen. Scheduler wie Slurm und Googles Borg haben
eine gewisse Orchestrierungskapazität, und Orchestrierungsprogramme wie HashiCorp Nomad und K8s
verfügen über eine gewisse Planungskapazität. Sie können aber auch separate Scheduler und
Orchestratoren haben, z. B. den Job Scheduler von Spark auf Kubernetes oder den AWS
Batch-Scheduler auf EKS. Orchestratoren wie HashiCorp Nomad und daten
Datenwissenschaft-spezifische Orchestratoren wie Airflow, Argo, Prefect und Dagster haben
ihre eigenen Scheduler.

Data Science Workflow Management
Wir haben die Unterschiede zwischen Schedulern und Orchestrierern besprochen und wie sie
für die Ausführung von Workflows im Allgemeinen verwendet werden können. Leser, die mit Workflow-Management
Workflow-Management-Tools wie Airflow, Argo, Prefect, Kubeflow,
Metaflow usw. vertraut sind, fragen sich vielleicht, wo sie in dieser Diskussion zwischen Scheduler und Orchestrator
Diskussion passen. Auf dieses Thema gehen wir hier ein.

In ihrer einfachsten Form verwalten Workflow-Management-Tools Arbeitsabläufe. Im Allgemeinen
ermöglichen es Ihnen, Ihre Workflows als DAGs zu spezifizieren, ähnlich wie in Abbildung 10-7 dargestellt. A
Arbeitsablauf kann aus einem Schritt zur Featurisierung, einem Schritt zur Modellschulung und einem
Schritt. Workflows können entweder über Code (Python) oder Konfigurationsdateien
(YAML). Jeder Schritt in einem Workflow wird als Aufgabe bezeichnet.

Fast alle Tools zur Verwaltung von Arbeitsabläufen verfügen über einen Zeitplaner,
Sie können sich diese als Planer vorstellen, die sich nicht auf einzelne Aufträge, sondern auf den
auf den Workflow als Ganzes konzentrieren. Sobald ein Workflow definiert ist, arbeitet der zugrunde liegende Scheduler
normalerweise mit einem Orchestrator zusammen, um Ressourcen für die Ausführung des Workflows zuzuweisen, wie
in Abbildung 10-8 dargestellt.

314 | Kapitel 10: Infrastruktur und Werkzeuge für MLOps

Abbildung 10-8. Nachdem ein Workflow definiert wurde, werden die Aufgaben in diesem Workflow geplant und
orchestriert

Es gibt viele Artikel im Internet, in denen verschiedene Tools zur Verwaltung von Data Science Workflows
werkzeuge. In diesem Abschnitt gehen wir auf fünf der gängigsten Tools ein: Airflow,
Argo, Prefect, Kubeflow und Metaflow. Dieser Abschnitt ist nicht als umfassender
Vergleich dieser Tools sein, sondern soll Ihnen eine Vorstellung von den verschiedenen Funktionen geben, die ein Work-Flow
Workflow-Management-Tool benötigen könnte.

Ursprünglich bei Airbnb entwickelt und 2014 veröffentlicht, ist Airflow einer der frühesten
Workflow-Orchestratoren. Es ist ein erstaunlicher Aufgabenplaner, der mit einer riesigen Bibliothek
die es einfach macht, Airflow mit verschiedenen Cloud-Anbietern, Datenbanken
Datenbanken, Speicheroptionen und so weiter. Airflow ist ein Verfechter des Prinzips "Konfiguration als
Code"-Prinzip. Seine Schöpfer waren der Meinung, dass Daten-Workflows komplex sind und
Code (Python) definiert werden sollten und nicht in YAML oder einer anderen deklarativen Sprache. Hier ist ein
Beispiel eines Airflow-Workflows, das dem GitHub-Repository der Plattform entnommen wurde:

from datetime import datetime, timedelta
von airflow importiere DAG
from airflow.operators.bash import BashOperator
from airflow.providers.docker.operators.docker import DockerOperator
dag = DAG(
'docker_sample',
default_args={'retries': 1},
schedule_interval=timedelta(minutes=10),
start_date=datetime(2021, 1, 1),
catchup=False,
)
t1 = BashOperator(task_id='print_date', bash_command='date', dag=dag)
t2 = BashOperator(task_id='sleep', bash_command='sleep 5', retries=3, dag=dag)
t3 = DockerOperator(
docker_url='tcp://localhost:2375', # Setzen Sie Ihre Docker-URL
command='/bin/sleep 30',
image='centos:latest',
Ressourcenverwaltung | 315
network_mode='bridge',
task_id='docker_op_tester',
dag=dag,
)
t4 = BashOperator(
task_id='print_hello',
bash_command='echo "hello world!!!"',
dag=dag
)
t1 >> t2
t1 >> t3
t3 >> t4
Da Airflow jedoch früher als die meisten anderen Tools entwickelt wurde, hatte es kein Tool, von dem es
gelernt und leidet unter vielen Nachteilen, die in einem Blogbeitrag von Uber Engineering ausführlich
Blogbeitrag von Uber Engineering beschrieben werden. Hier gehen wir nur auf drei ein, um Ihnen einen Eindruck zu vermitteln.

Erstens ist Airflow monolithisch, d. h. es packt den gesamten Arbeitsablauf in einen
Container. Wenn zwei verschiedene Schritte in Ihrem Workflow unterschiedliche Anforderungen haben, können Sie
können Sie theoretisch mit dem DockerOperator von Airflow verschiedene Container für sie erstellen,
aber das ist nicht so einfach.

Zweitens sind die DAGs von Airflow nicht parametrisiert, was bedeutet, dass Sie keine Parameter an Ihre Workflows übergeben können.
Parameter in Ihre Arbeitsabläufe übergeben können. Wenn Sie also dasselbe Modell mit unterschiedlichen
Lernraten ausführen möchten, müssen Sie verschiedene Workflows erstellen.

Drittens sind die DAGs von Airflow statisch, d. h. es können nicht automatisch neue Schritte
während der Laufzeit erstellen. Stellen Sie sich vor, Sie lesen aus einer Datenbank und möchten einen Schritt erstellen
einen Schritt erstellen, um jeden Datensatz in der Datenbank zu verarbeiten (z. B. um eine Vorhersage zu treffen), aber Sie
wissen aber nicht im Voraus, wie viele Datensätze in der Datenbank vorhanden sind. Airflow wird nicht
nicht verarbeiten können.

Die nächste Generation von Workflow-Orchestrierern (Argo, Prefect) wurde entwickelt, um
verschiedene Nachteile von Airflow zu beheben.

Der CEO von Prefect, Jeremiah Lowin, war einer der Hauptakteure von Airflow. Ihre frühe Werbekampagne
Marketing-Kampagne zog einen intensiven Vergleich zwischen Prefect und Airflow nach sich. Prefects
Arbeitsabläufe von Prefect sind parametrisiert und dynamisch, eine enorme Verbesserung im Vergleich zu Airflow.
Außerdem folgt es dem Prinzip "Konfiguration als Code", d. h. die Arbeitsabläufe werden in
Python.

Wie bei Airflow sind jedoch auch bei Prefect die Schritte im Container nicht die erste Priorität. Sie können
Sie können jeden Schritt in einem Container ausführen, aber Sie müssen sich trotzdem mit Dockerdateien befassen und
Ihr Docker mit Ihren Arbeitsabläufen in Prefect registrieren.

Argo löst das Containerproblem. Jeder Schritt in einem Argo-Workflow wird in seinem
eigenen Container ausgeführt. Die Arbeitsabläufe von Argo sind jedoch in YAML definiert, was Ihnen erlaubt

316 | Kapitel 10: Infrastruktur und Werkzeuge für MLOps

um jeden Schritt und seine Anforderungen in derselben Datei zu definieren. Das folgende Codebeispiel,
aus dem Argo GitHub Repository, zeigt, wie man einen Workflow erstellt, um
einen Münzwurf zu zeigen:

apiVersion : argoproj.io/v1alpha1
Art : Arbeitsablauf
metadata :
generateName : coinflip-
annotations :
workflows.argoproj.io/description : |
Dies ist ein Beispiel für einen Münzwurf, der als eine Folge von bedingten Schritten definiert ist.
Sie können es auch in Python ausführen:
https://couler-proj.github.io/couler/examples/#coin-flip
spec :
Einstiegspunkt : coinflip
templates :
name : münzwurf
Schritte :
name : münze umdrehen
Vorlage : flip-coin
name : kopf
Vorlage : Köpfe
when : "{{steps.flip-coin.outputs.result}} == Kopf"
name : tails
Vorlage : tails
wenn : "{{steps.flip-coin.outputs.result}} == tails"
name : flip-münze
script :
bild : python:alpine3.6
befehl : [python]
source : |
importieren zufällig
Ergebnis = "Kopf" if random.randint(0,1) == 0 sonst "Zahl"
print(ergebnis)
Name : Köpfe
container :
image : alpine:3.6
befehl : [sh, -c]
args : ["echo "es war Kopf""]
name : tails
container :
image : alpin:3.6
befehl : [sh, -c]
args : ["echo "es war tails""]
Ressourcenverwaltung | 317
Der größte Nachteil von Argo, abgesehen von den unübersichtlichen YAML-Dateien, ist, dass es nur auf
auf K8s-Clustern laufen kann, die nur in der Produktion verfügbar sind. Wenn Sie den gleichen
testen wollen, müssen Sie minikube verwenden, um einen K8s auf Ihrem Laptop zu simulieren, was
chaotisch werden kann.

Hier kommen Kubeflow und Metaflow ins Spiel, zwei Tools, die Ihnen dabei helfen sollen, den Workflow
sowohl in Entwicklungs- als auch in Produktionsumgebungen zu unterstützen, indem sie den
Code abstrahieren, der normalerweise für die Ausführung von Airflow oder Argo erforderlich ist. Sie versprechen, Datenwissenschaftlern
Zugriff auf die volle Rechenleistung der Prod-Umgebung über lokale Notebooks,
Dies ermöglicht es Datenwissenschaftlern, denselben Code in Entwicklungs- und
Umgebungen zu verwenden.

Obwohl beide Tools über eine gewisse Planungskapazität verfügen, sind sie für den Einsatz
mit einem echten Scheduler und Orchestrator verwendet werden. Eine Komponente von Kubeflow ist Kube-
flow Pipelines, das auf Argo aufbaut und für die Verwendung mit K8s gedacht ist.
Metaflow kann mit AWS Batch oder K8s verwendet werden.

Beide Werkzeuge sind vollständig parametrisiert und dynamisch. Derzeit ist Kubeflow das
populärer. Aus Sicht der Benutzererfahrung ist Metaflow meiner Meinung nach jedoch überlegen.
meiner Meinung nach überlegen. In Kubeflow können Sie zwar Ihren Arbeitsablauf in Python definieren, müssen aber trotzdem
eine Dockerdatei und eine YAML-Datei schreiben, um die Spezifikationen der einzelnen Komponenten
(z.B. Daten verarbeiten, trainieren, deployen), bevor man sie in einem Python-Workflow zusammenfügen
Arbeitsablauf zusammenfügen. Im Grunde genommen hilft Kubeflow Ihnen, die Boilerplate anderer Tools zu abstrahieren, indem
indem es Sie dazu bringt, Kubeflow-Boilerplate zu schreiben.

In Metaflow können Sie einen Python-Dekorator @conda verwenden, um die Anforderungen für
für jeden Schritt angeben - erforderliche Bibliotheken, Speicher- und Rechenanforderungen - und Metaflow
wird automatisch einen Container mit all diesen Anforderungen erstellen, um den Schritt auszuführen.
Sie sparen sich Dockerdateien oder YAML-Dateien.

Mit Metaflow können Sie nahtlos mit Entwicklungs- und Produktivumgebungen arbeiten.
demselben Notebook/Skript. Sie können Experimente mit kleinen Datensätzen auf lokalen
Maschinen durchführen, und wenn Sie bereit sind, den großen Datensatz in der Cloud auszuführen, fügen Sie einfach
fügen Sie einfach den @batch-Dekorator hinzu, um es auf AWS Batch auszuführen. Sie können sogar verschiedene Schritte im
desselben Workflows in verschiedenen Umgebungen ausführen. Wenn zum Beispiel ein Schritt einen geringen
Speicherplatz benötigt, kann er auf Ihrem lokalen Rechner ausgeführt werden. Wenn jedoch der nächste Schritt einen
einen großen Speicherbedarf, können Sie einfach @batch hinzufügen, um ihn in der Cloud auszuführen.

# Beispiel: Skizze eines Empfehlungssystems, das ein Ensemble aus zwei Modellen verwendet.
# Modell A wird auf Ihrem lokalen Rechner ausgeführt und Modell B wird auf AWS ausgeführt.
class RecSysFlow (FlowSpec):
@step
def start(self):
self.data = load_data()
self.next(self.fitA, self.fitB)
318 | Kapitel 10: Infrastruktur und Werkzeuge für MLOps

# fitA erfordert eine andere Version von NumPy als fitB
@conda(libraries={"scikit-learn":"0.21.1", "numpy":"1.13.0"})
@step
def fitA(self):
self.model = fit(self.data, model="A")
self.next(self.ensemble)
@conda(libraries={"numpy": "0.9.8"})
# Benötigt 2 GPUs mit 16GB Speicher
@batch(gpu=2, memory=16000)
@Schritt
def fitB(self):
self.model = fit(self.data, model="B")
self.next(self.ensemble)
@step
def ensemble(self, inputs):
self.outputs = (
(inputs.fitA.model.predict(self.data) +
inputs.fitB.model.predict(self.data)) / 2
for input in inputs
)
self.next(self.end)
def end(self):
print (self.outputs)
ML-Plattform
Der Leiter des ML-Plattform-Teams bei einem großen Streaming-Unternehmen erzählte mir die
Geschichte, wie sein Team entstanden ist. Er kam ursprünglich zu dem Unternehmen, um an
Empfehlungssysteme zu arbeiten. Um ihre Empfehlungssysteme einzusetzen, mussten sie
Tools für die Verwaltung von Merkmalen, die Modellverwaltung, die Überwachung usw. entwickelt werden.
Letztes Jahr erkannte sein Unternehmen, dass diese Tools auch für andere ML
Anwendungen genutzt werden können, nicht nur für Empfehlungssysteme. Es wurde ein neues Team gegründet, das ML-Plat-
form-Team, mit dem Ziel, eine gemeinsame Infrastruktur für ML-Anwendungen bereitzustellen.
Da das Team für Empfehlungssysteme über das ausgereifteste Tool verfügte, wurden seine Tools
anderen Teams übernommen, und einige Mitglieder des Teams für Empfehlungssysteme
wurden gebeten, dem neuen ML-Plattformteam beizutreten.

Diese Geschichte steht für einen wachsenden Trend seit Anfang 2020. Da jedes Unternehmen Anwendungen
für ML in immer mehr Anwendungen findet, kann man mehr gewinnen, wenn man die
Tools für mehrere Anwendungen zu nutzen, anstatt für jede Anwendung ein eigenes Toolset
für jede Anwendung zu unterstützen. Dieser gemeinsame Satz von Werkzeugen für den ML-Einsatz bildet die ML
Plattform.

Da ML-Plattformen relativ neu sind, ist das, was genau eine ML-Plattform ausmacht
ist von Unternehmen zu Unternehmen unterschiedlich. Selbst innerhalb desselben Unternehmens ist es eine laufende

ML-Plattform | 319
Diskussion. Hier werde ich mich auf die Komponenten konzentrieren, die ich am häufigsten in ML-Plattformen sehe,
Dazu gehören die Modellentwicklung, der Modellspeicher und der Funktionsspeicher.

Die Bewertung eines Tools für jede dieser Kategorien hängt von Ihrem Anwendungsfall ab. Dennoch,
hier sind zwei allgemeine Aspekte, die Sie im Auge behalten sollten:

Ob das Tool mit Ihrem Cloud-Anbieter zusammenarbeitet oder Sie es in Ihrem eigenen Rechenzentrum nutzen können
Rechenzentrum
Sie müssen Ihre Modelle über eine Rechenschicht ausführen und bereitstellen, und in der Regel unterstützen die Tools
nur die Integration mit einer Handvoll von Cloud-Anbietern. Niemand mag es, wenn er
einen neuen Cloud-Anbieter für ein anderes Tool übernehmen zu müssen.

Ob Open Source oder verwalteter Dienst
Wenn es sich um Open Source handelt, können Sie es selbst hosten und müssen sich weniger Sorgen um
Datensicherheit und Datenschutz. Selbst gehostet zu werden bedeutet jedoch, dass zusätzliche technische Zeit
für die Wartung erforderlich. Wenn es sich um einen verwalteten Dienst handelt, werden Ihre Modelle und wahrscheinlich auch einige Ihrer
Ihre Modelle und wahrscheinlich auch einige Ihrer Daten auf dem Dienst, was bei bestimmten Vorschriften nicht zulässig sein könnte.
Einige verwaltete Dienste arbeiten mit virtuellen privaten Clouds, die es Ihnen ermöglichen
Ihre Maschinen in Ihren eigenen Cloud-Clustern bereitstellen können, was die Einhaltung von Vorschriften erleichtert. Wir werden
Wir werden dies im Abschnitt "Bauen statt Kaufen" auf Seite 327 näher erläutern.

Beginnen wir mit der ersten Komponente: der Modellbereitstellung.

Einsatz des Modells
Sobald ein Modell trainiert (und hoffentlich getestet) ist, möchten Sie seine Vorhersagefähigkeit
Vorhersagefähigkeit den Benutzern zugänglich machen. In Kapitel 7 haben wir ausführlich darüber gesprochen, wie ein Modell
seine Vorhersagen bereitstellen kann: Online- oder Batch-Vorhersage. Wir haben auch besprochen, dass die einfachste
einfachste Weg zur Bereitstellung eines Modells darin besteht, das Modell und seine Abhängigkeiten an einen Ort zu verschieben
der in der Produktion zugänglich ist, und dann das Modell als Endpunkt für die Benutzer bereitstellt. Wenn Sie
Online-Vorhersage durchführen, wird dieser Endpunkt Ihr Modell veranlassen, eine Vorhersage zu erstellen.
Wenn Sie eine Batch-Vorhersage durchführen, ruft dieser Endpunkt eine vorberechnete Vorhersage ab.

Ein Deployment Service kann dabei helfen, Ihre Modelle und deren Abhängigkeiten
Modelle und ihre Abhängigkeiten in die Produktion zu bringen und Ihre Modelle als Endpunkte bereitzustellen. Da die Bereitstellung der
der Name des Spiels ist, ist die Bereitstellung die ausgereifteste aller ML-Plattform
Komponenten am ausgereiftesten, und es gibt viele Tools dafür. Alle großen Cloud-Anbieter bieten Tools
für die Bereitstellung an: AWS mit SageMaker, GCP mit Vertex AI, Azure mit Azure ML,
Alibaba mit Machine Learning Studio, und so weiter. Außerdem gibt es eine Vielzahl von Startups
die Tools zur Modellbereitstellung anbieten, wie MLflow Models, Seldon, Cortex, Ray
Serve, und so weiter.

Bei der Auswahl eines Einsatzwerkzeugs ist es wichtig zu prüfen, wie einfach es ist, sowohl
Online-Vorhersage und Batch-Vorhersage mit dem Tool möglich ist. Während es normalerweise
Online-Vorhersage in kleinerem Maßstab mit den meisten Deployment-Tools problemlos

320 | Kapitel 10: Infrastruktur und Werkzeuge für MLOps

29 Bei der Online-Vorhersage in kleinerem Maßstab kann man einfach einen Endpunkt mit Nutzdaten treffen und erhält dann
Vorhersagen. Die Batch-Vorhersage erfordert die Einrichtung von Batch-Aufträgen und die Speicherung von Vorhersagen.

Dienste ist die Batch-Vorhersage in der Regel schwieriger.^29 Einige Tools ermöglichen es, Anfragen
Anfragen für die Online-Vorhersage zusammenzufassen, was sich von der Batch-Vorhersage unterscheidet.
Viele Unternehmen haben separate Bereitstellungspipelines für die Online-Vorhersage und die Batch
Vorhersage. Sie könnten zum Beispiel Seldon für die Online-Vorhersage verwenden, während sie
Databricks für die Stapelvorhersage.
Ein offenes Problem bei der Modellbereitstellung ist die Frage, wie man die Qualität eines Modells
bevor es eingesetzt wird. In Kapitel 9 haben wir über verschiedene Techniken für den Test in
Produktion wie Shadow Deployment, Canary Release, A/B-Tests und so weiter. Wenn
Auswahl eines Bereitstellungsdienstes sollten Sie prüfen, ob dieser Dienst
die Durchführung der von Ihnen gewünschten Tests erleichtert.
Modellspeicher
Viele Unternehmen lehnen Modellspeicher ab, weil sie einfach klingen. In dem Abschnitt
"Modellbereitstellung" auf Seite 320 haben wir besprochen, wie Sie ein Modell bereitstellen können, indem Sie
Ihr Modell verpacken und an einen Ort hochladen müssen, der in der Produktion zugänglich ist. Modell
store suggeriert, dass es Modelle speichert - Sie können dies tun, indem Sie Ihre Modelle auf
Speicher wie S3. Ganz so einfach ist es jedoch nicht. Stellen Sie sich nun vor, dass die Leistung Ihres Modells
Leistung Ihres Modells für eine Gruppe von Eingaben abfällt. Die Person, die auf das Problem aufmerksam gemacht wurde
Die Person, die auf das Problem aufmerksam gemacht wurde, ist eine DevOps-Ingenieurin, die nach der Untersuchung des Problems beschloss, dass sie
dass sie den Datenwissenschaftler, der dieses Modell erstellt hat, informieren muss. Aber es gibt vielleicht 20
Datenwissenschaftler im Unternehmen geben; wen sollte sie anpingen?
Stellen Sie sich nun vor, dass der richtige Datenwissenschaftler eingeweiht ist. Der Datenwissenschaftler möchte zunächst
die Probleme lokal reproduzieren. Sie hat noch das Notebook, mit dem sie dieses Modell erstellt hat
und das endgültige Modell, also startet sie das Notebook und verwendet das Modell mit den
problematischen Eingabesätzen. Zu ihrer Überraschung sind die Ergebnisse, die das Modell lokal erzeugt
von den in der Produktion erzeugten Ergebnissen ab. Diese Diskrepanz kann viele Ursachen haben
Diese Diskrepanz kann viele Ursachen haben; hier sind nur einige Beispiele:
-Das Modell, das jetzt in der Produktion verwendet wird, ist nicht das gleiche Modell, das sie
lokal hat. Vielleicht hat sie die falsche Modellbinärdatei in die Produktion hochgeladen?
-Das Modell, das in der Produktion verwendet wird, ist korrekt, aber die Liste der verwendeten Merkmale
ist falsch. Vielleicht hat sie vergessen, den Code lokal neu zu erstellen, bevor sie ihn in die
Produktion zu übertragen?
ML Platform | 321
-Das Modell ist korrekt, die Merkmalsliste ist korrekt, aber der Featurisierungscode ist
veraltet.
-Das Modell ist korrekt, die Feature-Liste ist korrekt, der Featurisierungscode ist korrekt,
aber irgendetwas stimmt nicht mit der Datenverarbeitungspipeline.
Ohne die Ursache des Problems zu kennen, wird es sehr schwierig sein, es zu beheben. In diesem
einfachen Beispiel gehen wir davon aus, dass der verantwortliche Datenwissenschaftler noch Zugriff auf den
Code hat, mit dem das Modell generiert wurde. Was ist, wenn dieser Datenwissenschaftler keinen Zugriff mehr auf
auf das Notebook hat oder bereits gekündigt hat oder im Urlaub ist?

Viele Unternehmen haben erkannt, dass die Speicherung des Modells im Blob-Speicher allein nicht
nicht ausreicht. Um die Fehlersuche und Wartung zu erleichtern, ist es wichtig, so viele
Informationen, die mit einem Modell verbunden sind, zu verfolgen. Hier sind acht Arten von Artefakten, die
Sie möglicherweise speichern möchten. Beachten Sie, dass viele der hier genannten Artefakte Informationen sind
Informationen sind, die in die Modellkarte aufgenommen werden sollten, wie im Abschnitt "Erstellen von Modellkarten
Karten" auf Seite 351 beschrieben.

Modelldefinition
Dies sind die Informationen, die benötigt werden, um die Form des Modells zu erstellen, z. B. welche Verlustfunktion
Funktion es verwendet. Wenn es sich um ein neuronales Netz handelt, gehört dazu, wie viele versteckte Schichten es
Schichten hat und wie viele Parameter sich in jeder Schicht befinden.

Modellparameter
Dies sind die aktuellen Werte der Parameter Ihres Modells. Diese Werte werden
dann mit der Form des Modells kombiniert, um ein Modell zu erstellen, mit dem
Vorhersagen zu machen. Einige Frameworks ermöglichen es Ihnen, sowohl die Parameter
und die Modelldefinition zusammen zu exportieren.

Funktionen charakterisieren und vorhersagen
Wie extrahiert man bei einer Vorhersageanforderung Merkmale und gibt diese Merkmale
in das Modell ein, um eine Vorhersage zu erhalten? Die Featurize- und Predict-Funktionen bieten
liefern die Anweisung dazu. Diese Funktionen sind normalerweise in Endpunkten verpackt.

Abhängigkeiten
Die Abhängigkeiten - z. B. die Python-Version und die Python-Pakete -, die für die Ausführung Ihres
Modells benötigt werden, sind normalerweise in einem Container zusammengefasst.

Daten
Die Daten, die zum Trainieren dieses Modells verwendet werden, können Zeiger auf den Ort sein, an dem die
Daten gespeichert sind, oder der Name/die Version Ihrer Daten. Wenn Sie Tools wie DVC verwenden, um
Version Ihrer Daten verwenden, kann dies der DVC-Commit sein, der die Daten erzeugt hat.

322 | Kapitel 10: Infrastruktur und Werkzeuge für MLOps

Code für die Modellerstellung
Dies ist der Code, der angibt, wie Ihr Modell erstellt wurde, z. B.:

-welche Rahmenbedingungen er verwendet hat
-Wie es trainiert wurde
-Einzelheiten zur Erstellung der Splits train/valid/test
-die Anzahl der durchgeführten Experimente
-Der Bereich der berücksichtigten Hyperparameter
-Der tatsächliche Satz von Hyperparametern, der im endgültigen Modell verwendet wurde
Sehr oft erstellen Datenwissenschaftler Modelle, indem sie Code in Notebooks schreiben. Unternehmen -
Unternehmen mit ausgereifteren Pipelines lassen ihre Datenwissenschaftler den
Modellgenerierungscode in ihre Git-Repos auf GitHub oder GitLab zu übertragen. Doch in vielen
Unternehmen erfolgt dieser Prozess jedoch ad hoc, und die Datenwissenschaftler schauen nicht einmal in ihren
Notebooks. Wenn der für das Modell verantwortliche Datenwissenschaftler das Notizbuch verliert oder
kündigt oder in Urlaub geht, gibt es keine Möglichkeit, ein Modell in der Produktion dem Code zuzuordnen, der es
Code abzubilden, der es zum Debuggen oder zur Wartung erzeugt hat.
Experiment-Artefakte
Dies sind die Artefakte, die während des Modellentwicklungsprozesses erzeugt werden, wie
wie im Abschnitt "Verfolgung und Versionierung von Experimenten" auf Seite 162 beschrieben. Diese
Artefakte können Diagramme wie die Verlustkurve sein. Diese Artefakte können rohe Zahlen sein wie
die Leistung des Modells auf dem Testsatz.

Schlagwörter
Dazu gehören Tags, die bei der Modellsuche und -filterung helfen, wie z. B. Eigentümer (die Person oder das Team, die
Person oder das Team, das Eigentümer dieses Modells ist) oder Aufgabe (das Geschäftsproblem
das dieses Modell löst, z. B. Betrugserkennung).

Die meisten Unternehmen speichern eine Teilmenge, aber nicht alle dieser Artefakte. Die Artefakte, die ein Unternehmen
speichern, befinden sich möglicherweise nicht am selben Ort, sondern sind verstreut. Beispielsweise können Modelldefinitionen
und Modellparameter möglicherweise in S3. Container, die Abhängigkeiten enthalten, könnten
in ECS (Elastic Container Service) liegen. Die Daten könnten sich in Snowflake befinden. Experiment
Artefakte könnten sich in Weights & Biases befinden. Featurize- und Vorhersagefunktionen befinden sich möglicherweise in
AWS Lambda. Einige Datenwissenschaftler können diese Speicherorte manuell in einer Datei nachverfolgen,
z. B. in einer README, aber diese Datei kann leicht verloren gehen.

ML-Plattform | 323
Ein Modellspeicher, der genügend allgemeine Anwendungsfälle speichern kann, ist noch lange kein gelöstes
Problem. Zum Zeitpunkt der Erstellung dieses Buches ist MLflow zweifellos der beliebteste Modellspeicher
Modellspeicher, der nicht mit einem großen Cloud-Anbieter verbunden ist. Dennoch sind drei der sechs wichtigsten
MLflow-Fragen auf Stack Overflow drehen sich um die Speicherung und den Zugriff auf Artefakte in
MLflow, wie in Abbildung 10-9 dargestellt. Modellspeicher sind fällig für eine Überarbeitung, und ich hoffe
Ich hoffe, dass in naher Zukunft ein Startup dieses Problem lösen wird.

Abbildung 10-9. MLflow ist der beliebteste Modellspeicher, aber weit davon entfernt, das Artefaktproblem zu lösen.
Problem. Drei der sechs häufigsten MLflow-Fragen auf Stack Overflow drehen sich um die Speicherung
und den Zugriff auf Artefakte in MLflow. Quelle: Screenshot der Stack Overflow-Seite

324 | Kapitel 10: Infrastruktur und Werkzeuge für MLOps

30 Neal Lathia, "Building a Feature Store", 5. Dezember 2020, https://oreil.ly/DgsvA; Jordan Volz, "Why You Need
einen Feature Store," Continual, 28. September 2021, https://oreil.ly/kQPMb; Mike Del Balso, "Was ist ein Feature
Store?" Tecton, 20. Oktober 2020, https://oreil.ly/pzy0I.

Da es keine gute Modellspeicherlösung gibt, beschließen Unternehmen wie Stitch Fix
ihren eigenen Modellspeicher zu bauen. Abbildung 10-10 zeigt die Artefakte, die der Modellspeicher von Stitch Fix
Modellspeicher verfolgt. Wenn ein Modell in den Modellspeicher hochgeladen wird, wird dieses Modell
mit dem Link zum serialisierten Modell, den Abhängigkeiten, die zum Ausführen des Modells
(Python-Umgebung), den Git-Commit, in dem die Modellcodegenerierung erstellt wurde
(Git-Informationen), Tags (um zumindest das Team anzugeben, dem das Modell gehört), usw.
Abbildung 10-10. Artefakte, die der Modellspeicher von Stitch Fix verfolgt. Quelle: Angepasst von einer Folie
von Stefan Krawczyk für CS 329S (Stanford).
Merkmalsspeicher
"Feature Store" ist ein zunehmend belasteter Begriff, der von verschiedenen Personen
sehr unterschiedliche Dinge bezeichnen kann. Es hat viele Versuche von ML-Praktikern gegeben
zu definieren, welche Eigenschaften ein Feature Store haben sollte.^30 Im Kern gibt es drei
Hauptprobleme, bei denen ein Feature Store helfen kann: Feature Management, Feature
ML-Plattform | 325
31 Jeremy Hermann und Mike Del Balso, "Meet Michelangelo: Uber's Machine Learning Platform," Uber Engi-
neering, September 5, 2017, https://oreil.ly/XteNy.
32 Manche Leute verwenden den Begriff "Feature Transformation".

Transformation und Merkmalskonsistenz. Eine Merkmalspeicherlösung kann eines dieser Probleme
oder eine Kombination dieser Probleme:
Verwaltung von Merkmalen
Ein Unternehmen verfügt möglicherweise über mehrere ML-Modelle, wobei jedes Modell eine Vielzahl von Funktionen verwendet.
Im Jahr 2017 hatte Uber etwa 10.000 Features in allen Teams!^31 Es ist oft der Fall
dass Funktionen, die für ein Modell verwendet werden, für ein anderes Modell nützlich sein können. Zum Beispiel,
Team A hat vielleicht ein Modell zur Vorhersage, wie wahrscheinlich es ist, dass ein Nutzer abwandert, und Team
B hat ein Modell, mit dem sich vorhersagen lässt, wie wahrscheinlich es ist, dass ein kostenloser Nutzer in einen zahlenden Nutzer umgewandelt wird.
Es gibt viele Merkmale, die diese beiden Modelle gemeinsam nutzen können. Wenn Team A feststellt, dass
Funktion X sehr nützlich ist, kann Team B diese vielleicht auch nutzen.
Ein Funktionsspeicher kann Teams dabei helfen, Funktionen gemeinsam zu nutzen und zu entdecken, sowie
Rollen und Freigabeeinstellungen für jede Funktion zu verwalten. Zum Beispiel möchten Sie vielleicht nicht
nicht jeder im Unternehmen Zugang zu sensiblen Finanzdaten des Unternehmens
des Unternehmens oder seiner Nutzer haben. In dieser Hinsicht kann man sich einen Feature Store als
als einen Feature-Katalog betrachten. Beispiele für Tools zur Verwaltung von Funktionen sind Amundsen
(entwickelt bei Lyft) und DataHub (entwickelt bei LinkedIn).
Feature-Berechnung^32
Nachdem die Logik des Feature-Engineerings definiert wurde, muss sie berechnet werden. Zum Beispiel
Beispiel: Die Logik des Merkmals könnte lauten: Verwende die durchschnittliche Zubereitungszeit einer Mahlzeit von
gestern. Der Teil der Berechnung besteht darin, Ihre Daten zu untersuchen und
diesen Durchschnitt zu berechnen.
Im vorigen Punkt haben wir besprochen, wie mehrere Modelle ein Merkmal gemeinsam nutzen können.
Wenn die Berechnung dieses Merkmals nicht zu teuer ist, kann es akzeptabel sein
dieses Merkmal jedes Mal zu berechnen, wenn es von einem Modell benötigt wird. Wenn jedoch die
Berechnung teuer ist, sollten Sie sie vielleicht nur einmal ausführen, wenn sie zum ersten Mal benötigt wird
ausführen und dann für die Verwendung von Merkmalen speichern.
Ein Merkmalspeicher kann sowohl bei der Durchführung der Merkmalsberechnung als auch bei der Speicherung
der Ergebnisse dieser Berechnungen helfen. In dieser Eigenschaft funktioniert ein Merkmalsspeicher wie ein Data
Lagerhaus.
Feature-Konsistenz
In Kapitel 7 haben wir über das Problem gesprochen, zwei getrennte Pipelines
für ein und dasselbe Modell zu haben: Die Trainingspipeline extrahiert Batch-Features aus historischen
Daten und die Inferenzpipeline extrahiert Streaming-Features. Während der Entwicklung,
können Datenwissenschaftler Features definieren und Modelle mit Python erstellen. Produktion
326 | Kapitel 10: Infrastruktur und Werkzeuge für MLOps
Code kann jedoch aus Leistungsgründen in einer anderen Sprache, wie Java oder C, geschrieben werden.
Leistung.
Das bedeutet, dass Merkmalsdefinitionen, die während der Entwicklung in Python geschrieben wurden, möglicherweise
geschrieben wurden, in die in der Produktion verwendeten Sprachen konvertiert werden müssen. Sie müssen also
die gleichen Merkmale zweimal schreiben, einmal für das Training und einmal für die Inferenz. Das ist erstens
lästig und zeitraubend. Zweitens schafft es zusätzliche Oberfläche für Fehler, da ein
ein oder mehrere Merkmale in der Produktion von ihren Gegenstücken im Training abweichen können,
was zu merkwürdigem Modellverhalten führt.
Ein wichtiges Verkaufsargument für moderne Feature-Stores ist, dass sie die Logik sowohl für
Batch-Features und Streaming-Features vereinheitlichen und so die Konsistenz zwischen Features
während des Trainings und zwischen den Merkmalen während der Inferenz.
Feature-Stores sind eine neuere Kategorie, die sich erst um 2020 herum durchzusetzen begann. Während
ist man sich allgemein einig, dass Feature-Stores Feature-Definitionen verwalten und die
Feature-Konsistenz gewährleisten sollen, variieren ihre genauen Kapazitäten von Anbieter zu Anbieter. Einige Feature
verwaltet nur Feature-Definitionen, ohne Features aus Daten zu berechnen; einige
Merkmalspeicher tun beides. Einige Merkmalspeicher führen auch eine Merkmalsvalidierung durch, d. h. sie erkennen
wenn ein Merkmal nicht mit einem vordefinierten Schema übereinstimmt, und einige Merkmalspeicher überlassen
diesen Aspekt einem Überwachungswerkzeug.

Zum Zeitpunkt der Erstellung dieses Buches ist Feast der beliebteste Open-Source-Feature-Speicher. Allerdings,
Die Stärke von Feast liegt jedoch in Batch-Funktionen, nicht in Streaming-Funktionen. Tecton ist ein vollständig verwalteter
Feature Store, der verspricht, sowohl Batch-Features als auch Online-Features verarbeiten zu können.
Features zu bewältigen, aber seine tatsächliche Verbreitung ist langsam, da er eine tiefe Integration erfordert. Plat-
Plattformen wie SageMaker und Databricks bieten ebenfalls ihre eigenen Interpretationen von Feature
speichert. Von den 95 Unternehmen, die ich im Januar 2022 befragte, nutzen nur etwa 40 % einen Feature Store.
verwenden einen Feature Store. Von denjenigen, die einen Feature Store verwenden, baut die Hälfte ihren eigenen
Feature-Store.

Bauen statt kaufen
Zu Beginn dieses Kapitels haben wir erörtert, wie schwierig es ist, die richtige
Infrastruktur für Ihre ML-Anforderungen einzurichten. Welche Infrastruktur Sie benötigen, hängt von den
Anwendungen und dem Umfang, in dem Sie diese Anwendungen ausführen.

Wie viel Sie in die Infrastruktur investieren müssen, hängt auch davon ab, was Sie
was Sie selbst aufbauen und was Sie kaufen wollen. Wenn Sie zum Beispiel vollständig verwaltete
verwaltete Databricks-Cluster nutzen wollen, brauchen Sie wahrscheinlich nur einen Ingenieur. Wenn Sie jedoch
Ihre eigenen Spark Elastic MapReduce-Cluster hosten möchten, benötigen Sie möglicherweise fünf weitere
Mitarbeiter.

Im einen Extremfall können Sie alle Ihre ML-Anwendungsfälle an ein Unternehmen auslagern, das
ML-Anwendungen von Anfang bis Ende anbietet, und dann ist vielleicht der einzige Teil der Infrastruktur, den

Erstellen statt Kaufen | 327
Datenbewegung: Verschieben Ihrer Daten von Ihren Anwendungen zu Ihrem Anbieter,
und die Übertragung von Vorhersagen von diesem Anbieter zurück zu Ihren Benutzern. Der Rest Ihrer
Infrastruktur wird von Ihrem Anbieter verwaltet.

Wenn Sie hingegen ein Unternehmen sind, das mit sensiblen Daten umgeht, die Sie daran hindern
nicht die Dienste eines anderen Unternehmens in Anspruch nehmen können, müssen Sie möglicherweise
Ihre gesamte Infrastruktur selbst aufbauen und warten, sogar mit eigenen Rechenzentren.

Die meisten Unternehmen befinden sich jedoch in keinem dieser Extreme. Wenn Sie für eines dieser Unternehmen arbeiten
arbeiten, werden Sie wahrscheinlich einige Komponenten von anderen Unternehmen verwalten lassen
und einige Komponenten werden intern entwickelt. Zum Beispiel könnte Ihre Datenverarbeitung
von AWS EC2 und Ihr Data Warehouse von Snowflake verwaltet, aber Sie
aber Sie haben Ihren eigenen Funktionsspeicher und Ihre eigenen Überwachungs-Dashboards.

Ihre Entscheidung für oder gegen einen Kauf hängt von vielen Faktoren ab. Hier besprechen wir drei
die mir häufig begegnen, wenn ich mit Leitern von Infrastrukturen darüber spreche
wie sie diese Entscheidungen bewerten:

Die Phase, in der sich Ihr Unternehmen befindet
Am Anfang möchten Sie vielleicht Anbieterlösungen nutzen, um schnellstmöglich
um so schnell wie möglich loszulegen, damit Sie Ihre begrenzten Ressourcen auf die
Kernangebot Ihres Produkts konzentrieren können. Wenn Ihre Anwendungsfälle jedoch wachsen, können die Kosten der Anbieter
exorbitant werden und es könnte für Sie billiger sein, in Ihre eigene Lösung zu investieren.
Lösung zu investieren.

Was Ihrer Meinung nach der Schwerpunkt oder die Wettbewerbsvorteile Ihres Unternehmens sind
Stefan Krawczyk, Leiter des ML-Plattformteams bei Stitch Fix, erklärte mir
mir seine Entscheidung zwischen Bauen und Kaufen: "Wenn es etwas ist, das wir wirklich gut können wollen
gut sein wollen, werden wir es intern verwalten. Wenn nicht, nehmen wir einen Anbieter. Für die große Mehrheit
Mehrheit der Unternehmen außerhalb des Technologiesektors - z. B. Unternehmen im Einzelhandel,
Bankwesen, Fertigung - ist die ML-Infrastruktur nicht ihr Schwerpunkt, so dass sie dazu neigen
zum Kauf. Wenn ich mit diesen Unternehmen spreche, bevorzugen sie Managed Services,
sogar Punktlösungen (z. B. Lösungen, die ein Geschäftsproblem für sie lösen, wie
ein Dienst für Nachfrageprognosen). Für viele Technologieunternehmen, bei denen die Technologie
Wettbewerbsvorteil ist und deren starke Ingenieurteams lieber die
Kontrolle über ihre Stacks zu haben, tendieren sie eher zum Bauen. Wenn sie einen verwalteten
Dienst nutzen, bevorzugen sie es vielleicht, dass dieser Dienst modular und anpassbar ist, so dass
dass sie jede beliebige Komponente einbauen und nutzen können.

Der Reifegrad der verfügbaren Tools
Ihr Team könnte zum Beispiel beschließen, dass Sie einen Modellspeicher benötigen, und Sie würden
Sie hätten lieber einen Anbieter benutzt, aber es gibt keinen Anbieter, der reif genug für Ihre
Anforderungen ausgereift ist, so dass Sie Ihren eigenen Funktionsspeicher erstellen müssen, vielleicht auf der Grundlage einer Open-Source-Lösung.
quelloffenen Lösung.

328 | Kapitel 10: Infrastruktur und Werkzeuge für MLOps

33 Erik Bernhardsson auf Twitter (@bernhardsson), 29. September 2021, https://oreil.ly/GnxOH.

So sieht es in den ersten Tagen der Einführung von ML in der Industrie aus. Unternehmen -
Unternehmen, die zu den Early Adopters gehören, d. h. große Tech-Unternehmen, bauen ihre eigene Infrastruktur auf
Infrastruktur auf, weil es keine Lösungen gibt, die für ihre Bedürfnisse ausgereift sind. Dies
führt dazu, dass die Infrastruktur jedes Unternehmens anders ist. Ein paar
Jahre später sind die Lösungsangebote ausgereift. Allerdings ist es für diese Angebote schwierig, an
großen Technologieunternehmen zu verkaufen, weil es unmöglich ist, eine Lösung zu entwickeln, die mit der
mit der Mehrheit der kundenspezifischen Infrastrukturen funktioniert.
Während wir Claypot AI aufbauen, haben uns andere Gründer sogar geraten
nicht an große Technologieunternehmen zu verkaufen, weil wir sonst in das hineingezogen werden, was sie
Integrationshölle" nennen - wir verbringen mehr Zeit mit der Integration unserer Lösung in die
Infrastruktur zu integrieren, anstatt unsere Kernfunktionen auszubauen. Sie rieten uns
uns auf Startups zu konzentrieren, die eine viel sauberere Basis haben, auf der wir aufbauen können.
Manche Leute denken, dass Bauen billiger ist als Kaufen, aber das ist nicht unbedingt
der Fall ist. Bauen bedeutet, dass man mehr Ingenieure einstellen muss, um die eigene Infrastruktur aufzubauen und
Ihre eigene Infrastruktur zu bauen und zu warten. Es kann auch zukünftige Kosten mit sich bringen: die Kosten für
Innovation. Eine eigene, maßgeschneiderte Infrastruktur macht es schwer, neue Technologien zu übernehmen
aufgrund von Integrationsproblemen.
Die Entscheidung für oder gegen eine eigene Infrastruktur ist komplex, stark kontextabhängig und wahrscheinlich
was die Leiter von Infrastrukturen viel Zeit damit verbringen, darüber nachzudenken. Erik Bernhardsson,
Ex-CTO von Better.com, sagte in einem Tweet, dass "eine der wichtigsten Aufgaben eines
CTOs ist die Auswahl von Anbietern/Produkten, und die Bedeutung dieser Aufgabe steigt
jedes Jahr rapide an, da der Infrastrukturbereich so schnell wächst."^33 Es ist unmöglich, dass ein kleiner
Abschnitt alle Nuancen ansprechen kann. Aber ich hoffe, dass dieser Abschnitt Ihnen einige
einige Anhaltspunkte für den Beginn der Diskussion.
Zusammenfassung
Wenn Sie mir bis hierher gefolgt sind, werden Sie mir hoffentlich zustimmen, dass die Einführung von ML-Modellen in die
Produktion ein infrastrukturelles Problem ist. Um Datenwissenschaftler in die Lage zu versetzen, ML-Modelle zu entwickeln und
ML-Modelle zu entwickeln und einzusetzen, ist es entscheidend, die richtigen Tools und die richtige Infrastruktur einzurichten.
In diesem Kapitel haben wir die verschiedenen Infrastrukturebenen behandelt, die für ML-Systeme erforderlich sind.
Wir haben mit der Speicher- und Rechenebene begonnen, die wichtige Ressourcen für
jedes technische Projekt, das intensive Daten- und Rechenressourcen benötigt, wie ML
Projekte. Die Speicher- und Berechnungsebene ist stark standardisiert, was bedeutet, dass
die meisten Unternehmen Cloud-Dienste für genau die Menge an Speicher- und Rechenleistung bezahlen
die sie benötigen, anstatt eigene Rechenzentren einzurichten. Doch während Cloud-Anbieter
einem Unternehmen den Einstieg erleichtern, werden ihre Kosten jedoch unerschwinglich, wenn diese
Zusammenfassung | 329
Unternehmen wächst, und immer mehr große Unternehmen erwägen eine Rückführung
aus der Cloud in private Rechenzentren.

Anschließend haben wir die Entwicklungsumgebung besprochen, in der Datenwissenschaftler
tisten Code schreiben und mit der Produktionsumgebung interagieren. Da die Entwicklungs
Da die Entwicklungsumgebung der Ort ist, an dem Ingenieure die meiste Zeit verbringen, führen Verbesserungen
Umgebung direkt in Produktivitätsverbesserungen umgesetzt werden. Eine der ersten
Unternehmen die Entwicklungsumgebung verbessern kann, ist die Standardisierung der Entwicklungs
Umgebung für Datenwissenschaftler und ML-Ingenieure zu standardisieren, die im selben Team arbeiten. Wir
haben in diesem Kapitel erörtert, warum eine Standardisierung empfehlenswert ist und wie man sie durchführt.

Anschließend haben wir ein infrastrukturelles Thema diskutiert, dessen Bedeutung für Datenwissenschaftler
in den letzten Jahren stark diskutiert wurde: das Ressourcenmanagement. Ressourcenmanagement
ist wichtig für datenwissenschaftliche Arbeitsabläufe, aber die Frage ist, ob Datenwissenschaftler
erwartet werden sollte, dass sie damit umgehen. In diesem Abschnitt haben wir die Entwicklung von Ressourcen
Ressourcenmanagement-Tools von Cron über Scheduler bis hin zu Orchestrierern. Wir haben auch erörtert, warum
ML-Workflows sich von anderen Softwareentwicklungs-Workflows unterscheiden und warum
sie ihre eigenen Workflow-Management-Tools benötigen. Wir verglichen verschiedene Workflow
Workflow-Management-Tools wie Airflow, Argo und Metaflow verglichen.

Die ML-Plattform ist ein Team, das in jüngster Zeit mit der zunehmenden Verbreitung von ML entstanden ist. Da es sich um ein
Konzept ist, gibt es immer noch Meinungsverschiedenheiten darüber, was eine ML-Plattform
bestehen sollte. Wir haben uns entschieden, uns auf die drei Werkzeuggruppen zu konzentrieren, die für die meisten ML
Plattformen wesentlich sind: Bereitstellung, Modellspeicher und Funktionsspeicher. Wir haben die Überwachung der
ML-Plattform übersprungen, da sie bereits in Kapitel 8 behandelt wird.

Bei der Arbeit an der Infrastruktur stellt sich für technische Leiter und CTOs immer wieder die Frage
und CTOs gleichermaßen: bauen oder kaufen? Wir haben dieses Kapitel mit ein paar Diskussionspunkten abgeschlossen
die Ihnen oder Ihrem Team hoffentlich einen ausreichenden Kontext bieten, um diese
schwierigen Entscheidungen.

330 | Kapitel 10: Infrastruktur und Werkzeuge für MLOps

1 Manchmal erhält man unterschiedliche Ergebnisse, wenn man dasselbe Modell mit derselben Eingabe zweimal
zur exakt gleichen Zeit ausführen.
KAPITEL 11

Die menschliche Seite des maschinellen Lernens
In diesem Buch haben wir uns mit vielen technischen Aspekten der Entwicklung eines ML
Systems behandelt. ML-Systeme sind jedoch nicht nur technisch. Sie beziehen geschäftliche Entscheidungsträger
Entscheidungsträger, Benutzer und natürlich die Entwickler der Systeme. Wir haben die Interessengruppen
und ihre Ziele in den Kapiteln 1 und 2 behandelt. In diesem Kapitel werden wir erörtern, wie Benutzer und
Entwickler von ML-Systemen mit diesen Systemen interagieren können.

Wir werden zunächst untersuchen, wie die Nutzererfahrung durch die probabilistische Natur von
probabilistischen Natur von ML-Modellen. Anschließend werden wir die Organisationsstruktur diskutieren
die es verschiedenen Entwicklern desselben ML-Systems ermöglicht, effektiv zusammenzuarbeiten.
Das Kapitel endet mit der Frage, wie ML-Systeme die Gesellschaft als Ganzes beeinflussen können, und zwar im
Abschnitt "Verantwortungsvolle KI" auf Seite 339.

Benutzererfahrung
Wir haben ausführlich erörtert, wie sich ML-Systeme anders verhalten als herkömmliche Soft- ware-Systeme.
ware-Systeme. Erstens sind ML-Systeme probabilistisch und nicht deterministisch. Normalerweise, wenn
Wenn Sie dieselbe Software zweimal zu verschiedenen Zeiten auf dieselbe Eingabe anwenden, können Sie
das gleiche Ergebnis erwarten. Lässt man jedoch dasselbe ML-System zweimal zu unterschiedlichen Zeiten auf
gleichen Eingaben ausführt, kann es zu unterschiedlichen Ergebnissen kommen.^1 Zweitens sind die Vorhersagen von ML-Systemen aufgrund dieser probabilistischen
sind die Vorhersagen von ML-Systemen meist korrekt, und das Schwierige daran ist, dass wir in der Regel
nicht wissen, für welche Eingaben das System korrekt sein wird! Drittens, ML-Systeme können auch
groß sein und können unerwartet lange brauchen, um eine Vorhersage zu treffen.

Diese Unterschiede bedeuten, dass ML-Systeme die Benutzererfahrung unterschiedlich beeinflussen können, insbesondere für
insbesondere für Benutzer, die bisher an traditionelle Software gewöhnt waren. Aufgrund der relativ

331
neuen Einsatz von ML in der realen Welt ist die Auswirkung von ML-Systemen auf die Nutzererfahrung noch nicht
gut untersucht. In diesem Abschnitt werden wir drei Herausforderungen diskutieren, die ML-Systeme für
für eine gute Benutzererfahrung darstellen und wie man sie angehen kann.

Sicherstellung einer konsistenten Benutzererfahrung
Bei der Nutzung einer App oder einer Website erwarten die Nutzer ein gewisses Maß an Konsistenz. Für
Ich bin zum Beispiel daran gewöhnt, dass bei Chrome die Schaltfläche "Minimieren" in der linken oberen Ecke
auf meinem MacBook. Wenn Chrome diese Schaltfläche nach rechts verschieben würde, wäre ich verwirrt oder sogar
frustriert.

ML-Vorhersagen sind probabilistisch und inkonsistent, was bedeutet, dass Vorhersagen, die
Vorhersagen, die heute für einen Nutzer erstellt werden, sich von den Vorhersagen für denselben
Nutzer am nächsten Tag anders ausfallen, je nach Kontext der Vorhersagen. Für Aufgaben, die
Aufgaben, die ML nutzen wollen, um die Benutzererfahrung zu verbessern, kann die Inkonsistenz der ML-Vorhersagen
ein Hindernis sein.

Um dies zu verdeutlichen, betrachten wir eine Fallstudie, die von Booking.com im Jahr 2020 veröffentlicht wurde.
Wenn Sie auf Booking.com eine Unterkunft buchen, können Sie etwa 200 Filter
können Sie Ihre Präferenzen angeben, z. B. "Frühstück inklusive", "haustierfreundlich" und
"Nichtraucherzimmer". Es gibt so viele Filter, dass es einige Zeit dauert, bis die Nutzer die
die Filter zu finden, die sie wollen. Das angewandte ML-Team bei Booking.com wollte ML nutzen, um
automatisch Filter vorschlagen, die ein Benutzer auf der Grundlage der Filter, die er in einer
in einer bestimmten Browsing-Sitzung.

Die Herausforderung bestand darin, dass die Benutzer verwirrt werden könnten, wenn ihr ML-Modell jedes Mal andere
Filter vorschlägt, könnten die Nutzer verwirrt werden, vor allem wenn sie einen Filter nicht finden, den
den sie bereits zuvor angewendet hatten. Das Team löste diese Herausforderung, indem es eine Regel erstellte, die
die Bedingungen festlegte, unter denen das System dieselben Filterempfehlungen zurückgeben muss (z. B.
Filterempfehlungen zurückgeben muss (z. B. wenn der Benutzer einen Filter angewendet hat) und die Bedingungen, unter denen das System
neue Empfehlungen zurückgeben kann (z. B. wenn der Benutzer sein Ziel ändert).
Dies wird als Kompromiss zwischen Konsistenz und Genauigkeit bezeichnet, da die Empfehlungen
Empfehlungen, die das System für am genauesten hält, nicht unbedingt die Empfehlungen sind, die
Benutzer Konsistenz bieten.

Bekämpfung von "größtenteils korrekten" Vorhersagen
Im vorigen Abschnitt haben wir darüber gesprochen, wie wichtig es ist, die Konsistenz
der Vorhersagen eines Modells. In diesem Abschnitt werden wir darüber sprechen, dass wir in manchen Fällen
weniger Konsistenz und mehr Vielfalt in den Vorhersagen eines Modells.

Seit 2018 haben das große Sprachmodell GPT und seine Nachfolger GPT-2 und GPT-3,
die Welt im Sturm erobert. Ein Vorteil dieser großen Sprachmodelle
ist, dass sie in der Lage sind, Vorhersagen für eine Vielzahl von Aufgaben zu erstellen, für die wenig bis keine
aufgabenspezifische Trainingsdaten erforderlich sind. Sie können zum Beispiel die Anforderungen für eine

332 | Kapitel 11: Die menschliche Seite des maschinellen Lernens

Webseite als Eingabe für das Modell und gibt den React-Code aus, der für die Erstellung der
diese Webseite zu erstellen, wie in Abbildung 11-1 gezeigt.

Abbildung 11-1. GPT-3 kann Ihnen beim Schreiben von Code für Ihre Website helfen. Quelle: Angepasst von
Bildschirmfotos aus einem Video von Sharif Shameem

Ein Nachteil dieser Modelle ist jedoch, dass die Vorhersagen nicht immer korrekt sind,
und es ist sehr kostspielig, sie anhand aufgabenspezifischer Daten fein abzustimmen, um ihre
Vorhersagen zu verbessern. Diese meist korrekten Vorhersagen können für Benutzer nützlich sein, die sie leicht korrigieren können.
korrigieren können. Im Fall des Kundensupports zum Beispiel können ML-Systeme für jede Kundenanfrage
können ML-Systeme für jede Kundenanfrage weitgehend korrekte Antworten geben, die von den menschlichen Mitarbeitern
können diese Antworten schnell bearbeiten. Dies kann die Antwort beschleunigen, als wenn man
als wenn die Antwort von Grund auf neu geschrieben werden müsste.

Diese meist richtigen Vorhersagen sind jedoch nicht sehr nützlich, wenn die Benutzer nicht wissen
nicht wissen, wie sie die Antworten korrigieren können. Betrachten wir die gleiche Aufgabe, ein Sprachmodell zu verwenden
Modells zur Generierung von React-Code für eine Webseite. Der generierte Code könnte nicht funktionieren,
oder wenn doch, dann wird er vielleicht nicht zu einer Webseite gerendert, die den Anforderungen entspricht.
Ein React-Ingenieur kann diesen Code vielleicht schnell korrigieren, aber viele Benutzer dieser
Anwendung kennen React vielleicht nicht. Und diese Anwendung könnte eine Menge Benutzer anziehen
die sich mit React nicht auskennen - deshalb brauchten sie diese Anwendung ja überhaupt erst!

Um dieses Problem zu lösen, werden den Nutzern mehrere Vorhersagen für dieselbe
Vorhersagen für dieselbe Eingabe zu zeigen, um die Wahrscheinlichkeit zu erhöhen, dass zumindest eine davon richtig ist. Diese
Vorhersagen sollten so dargestellt werden, dass auch Benutzer, die keine Experten sind, sie auswerten können.
In diesem Fall kann das Modell bei einer Reihe von Anforderungen, die von den Benutzern eingegeben werden, Folgendes tun

Benutzererfahrung | 333
mehrere React-Code-Schnipsel erzeugen. Die Codeschnipsel werden in visuellen Webseiten gerendert
Webseiten gerendert, so dass Benutzer, die keine Techniker sind, beurteilen können, welcher der beste für sie ist.

Dieser Ansatz ist sehr verbreitet und wird manchmal als "Human-in-the-Loop"-KI bezeichnet, da er
der Mensch die besten Vorhersagen auswählt oder die maschinell erstellten Vorhersagen verbessert
Vorhersagen. Lesern, die sich für Human-in-the-Loop-KI interessieren, empfehle ich dringend
Jessy Lin's "Rethinking Human-AI Interaction".

Sanftes Scheitern
Wir haben ausführlich über die Auswirkungen der Inferenzlatenz eines ML-Modells auf die Benutzererfahrung gesprochen
im Abschnitt "Berechnungsprioritäten" auf Seite 15. Wir haben auch besprochen
wie man Modelle komprimieren und für eine schnellere Inferenzgeschwindigkeit optimieren kann, im Abschnitt
"Modellkomprimierung" auf Seite 206. Allerdings können normalerweise schnelle Modelle bei bestimmten
Zeit bei bestimmten Abfragen benötigen. Dies kann insbesondere bei Modellen vorkommen, die sich mit
sequentiellen Daten wie Sprachmodellen oder Zeitreihenmodellen - z. B. benötigt das Modell
längere Zeit für die Verarbeitung langer Reihen als für kürzere Reihen. Was sollten wir mit den Abfragen tun
bei denen die Modelle zu lange brauchen, um zu antworten?

Einige Unternehmen, mit denen ich zusammengearbeitet habe, verwenden ein Backup-System, das weniger optimal ist als
das weniger optimal ist als das Hauptsystem, aber garantiert schnelle Vorhersagen liefert. Diese Systeme können
Heuristiken oder einfache Modelle sein. Es kann sich sogar um zwischengespeicherte vorberechnete Vorhersagen handeln.
Das bedeutet, dass Sie eine Regel haben könnten, die besagt: Wenn das Hauptmodell länger als
als X Millisekunden braucht, um Vorhersagen zu erstellen, verwenden Sie stattdessen das Ersatzmodell. Einige
Unternehmen haben anstelle dieser einfachen Regel ein anderes Modell, das vorhersagt, wie lange
wie lange das Hauptmodell braucht, um Vorhersagen für eine bestimmte Abfrage zu treffen, und leiten diese
Vorhersage entweder an das Hauptmodell oder an das Sicherungsmodell weiter. Natürlich kann dieses
zusätzliche Modell kann natürlich auch zusätzliche Latenzzeiten für Ihr System bedeuten.

Dies hängt mit dem Kompromiss zwischen Geschwindigkeit und Genauigkeit zusammen: Ein Modell kann eine schlechtere
schlechtere Leistung als ein anderes Modell, kann aber viel schneller schlussfolgern. Dieses suboptimale
aber schnelle Modell kann dem Benutzer schlechtere Vorhersagen liefern, aber in
Situationen, in denen die Latenzzeit entscheidend ist. Viele Unternehmen müssen ein Modell dem anderen vorziehen.
einem anderen Modell entscheiden, aber mit einem Backup-System können Sie beides tun.

Struktur des Teams
An einem ML-Projekt sind nicht nur Datenwissenschaftler und ML-Ingenieure beteiligt, sondern auch andere
Ingenieure wie DevOps-Ingenieure und Plattform-Ingenieure sowie
nicht-entwickelnde Beteiligte wie Fachexperten (KMU). Angesichts einer Vielzahl von
der Beteiligten stellt sich die Frage nach der optimalen Struktur für die Organisation von ML
Teams. Wir werden uns auf zwei Aspekte konzentrieren: die funktionsübergreifende Zusammenarbeit von Teams und die viel
die viel diskutierte Rolle eines End-to-End-Datenwissenschaftlers.

334 | Kapitel 11: Die menschliche Seite des maschinellen Lernens

Funktionsübergreifende Teams Zusammenarbeit
KMU (Ärzte, Anwälte, Banker, Landwirte, Friseure usw.) werden bei der Entwicklung von ML-Systemen oft übersehen.
Entwurf von ML-Systemen übersehen, aber viele ML-Systeme würden ohne Fachwissen
Fachwissen. Sie sind nicht nur Nutzer, sondern auch Entwickler von ML-Systemen.

Die meisten Menschen denken nur in der Phase der Datenbeschriftung an Fachwissen.
Man braucht z. B. geschulte Fachleute, um zu beurteilen, ob ein CT-Scan einer Lunge
Anzeichen von Krebs zeigt. Da jedoch das Training von ML-Modellen zu einem fortlaufenden Prozess in der
in der Produktion zu einem fortlaufenden Prozess wird, kann auch die Beschriftung und Neuetikettierung
über den gesamten Projektlebenszyklus hinweg. Ein ML-System würde sehr davon profitieren, wenn KMUs
Lebenszyklus beteiligt sind, wie z. B. Problemformulierung, Feature Engineering, Fehleranalyse
Fehleranalyse, Modellbewertung, Neueinstufung von Vorhersagen und Benutzerschnittstelle: Wie können die
wie die Ergebnisse den Nutzern und/oder anderen Teilen des Systems am besten präsentiert werden.

Die Arbeit mit mehreren unterschiedlichen Profilen an einem Projekt bringt viele Herausforderungen mit sich
an einem Projekt arbeiten. Wie erklärt man zum Beispiel die Grenzen und Fähigkeiten von ML-Algorithmen
den KMUs erklären, die möglicherweise keinen technischen oder statistischen Hintergrund haben? Um
ein ML-System aufzubauen, wollen wir, dass alles versioniert wird, aber wie übersetzen Sie
Fachwissen (z. B. wenn in diesem Bereich zwischen X und Y ein kleiner Punkt zu sehen ist, könnte das
ein Anzeichen für Krebs sein könnte) in Code um und versioniert diesen?

Viel Glück bei dem Versuch, Ihren Arzt zur Verwendung von Git zu bewegen.

Es ist wichtig, die KMU frühzeitig in die Projektplanungsphase einzubeziehen und sie zu befähigen
zu befähigen, Beiträge zu leisten, ohne die Ingenieure damit zu belasten, ihnen Zugang zu gewähren.
Um beispielsweise KMU zu helfen, sich stärker an der Entwicklung von ML-Systemen zu beteiligen, bauen viele Unternehmen
bauen viele Unternehmen No-Code/Low-Code-Plattformen auf, die es den Mitarbeitern ermöglichen, Änderungen vorzunehmen
Änderungen vorzunehmen, ohne Code zu schreiben. Die meisten no-code ML-Lösungen für KMU sind
befinden sich derzeit in der Phase der Kennzeichnung, Qualitätssicherung und des Feedbacks, aber es werden weitere Plattformen
Plattformen werden entwickelt, um an anderen kritischen Punkten wie der Erstellung von Datensätzen und
Ansichten zur Untersuchung von Problemen, die den Beitrag von KMU erfordern.

End-to-End-Datenwissenschaftler
Ich hoffe, ich konnte Sie mit diesem Buch davon überzeugen, dass die ML-Produktion nicht nur ein
ML-Problem ist, sondern auch ein Infrastrukturproblem. Um MLOps durchzuführen, brauchen wir nicht nur
ML-Expertise, sondern auch Ops (operationale) Expertise, insbesondere im Bereich der Bereitstellung,
Containerisierung, Job-Orchestrierung und Workflow-Management.

Um all diese Fachgebiete in ein ML-Projekt einbringen zu können, neigen Unternehmen dazu
einen der beiden folgenden Ansätze zu verfolgen: ein separates Team für die Verwaltung aller
Ops-Aspekte zu verwalten, oder sie nehmen Data Scientists in das Team auf und lassen sie den gesamten
Prozess.

Schauen wir uns einmal genauer an, wie jeder dieser Ansätze in der Praxis funktioniert.

Teamstruktur | 335
Ansatz 1: Ein eigenes Team für die Verwaltung der Produktion

Bei diesem Ansatz entwickelt das Data Science/ML-Team Modelle in der Entwicklungsumgebung.
Ein separates Team, in der Regel das Ops/Plattform/ML-Engineering-Team, produziert die Modelle in der Produktionsumgebung.
die Modelle in der Produktivumgebung. Dieser Ansatz erleichtert die Einstellung, da es einfacher ist, Mitarbeiter
Es ist einfacher, Mitarbeiter mit einem Satz von Fähigkeiten einzustellen, als Mitarbeiter mit mehreren Sätzen von Fähigkeiten. Es könnte auch
für alle Beteiligten einfacher, da sie sich nur auf eine Aufgabe konzentrieren müssen
(z. B. auf die Entwicklung von Modellen oder den Einsatz von Modellen). Dieser Ansatz hat jedoch viele
Nachteile:

Aufwand für Kommunikation und Koordination
Ein Team kann zu Blockierern für andere Teams werden. Nach Frederick P. Brooks,
"Was ein Programmierer in einem Monat schaffen kann, schaffen zwei Programmierer in zwei
Monaten."

Herausforderungen bei der Fehlersuche
Wenn etwas fehlschlägt, weiß man nicht, ob der Code des eigenen Teams oder der eines anderen
Team oder der Code eines anderen Teams die Ursache sein könnte. Vielleicht liegt es auch gar nicht am Code Ihres Unternehmens.
ny's Code. Sie brauchen die Zusammenarbeit mehrerer Teams, um herauszufinden, was
falsch ist.

Schuldzuweisungen
Selbst wenn Sie herausgefunden haben, was schief gelaufen ist, kann es sein, dass jedes Team denkt, es sei
dass ein anderes Team dafür verantwortlich ist, es zu beheben.

Enger Kontext
Niemand hat Einblick in den gesamten Prozess, um ihn zu optimieren/verbessern. Ein Beispiel,
Das Plattformteam hat Ideen, wie die Infrastruktur verbessert werden kann, kann aber
kann aber nur auf Anfragen von Datenwissenschaftlern reagieren, die sich aber nicht mit der
Datenwissenschaftler müssen sich jedoch nicht mit der Infrastruktur befassen, so dass sie weniger Anreize haben, proaktiv Änderungen vorzunehmen.

Ansatz 2: Datenwissenschaftler sind für den gesamten Prozess verantwortlich

Bei diesem Ansatz muss sich das Data-Science-Team auch um die Produktion von
Modelle kümmern. Datenwissenschaftler werden zu mürrischen Einhörnern, von denen erwartet wird, dass sie alles über den Prozess wissen.
alles über den Prozess wissen, und am Ende schreiben sie vielleicht mehr Standardcode als Data Science.

Vor etwa einem Jahr twitterte ich über eine Reihe von Fähigkeiten, die meiner Meinung nach wichtig sind, um ein
ML-Ingenieur oder Datenwissenschaftler zu werden, wie in Abbildung 11-2 dargestellt. Die Liste deckt fast jeden
Teil des Arbeitsablaufs ab: Abfragen von Daten, Modellierung, verteiltes Training und Einrichten von
Endpunkte. Sie umfasst sogar Tools wie Kubernetes und Airflow.

336 | Kapitel 11: Die menschliche Seite des maschinellen Lernens

2 Eugene Yan, "Unpopular Opinion-Data Scientists Should be More End-to-End," EugeneYan.com, August 9,
2020, https://oreil.ly/A6oPi.
3 Eric Colson, "Beware the Data Science Pin Factory: Die Macht des Full-Stack-Data-Science-Generalisten und
the Perils of Division of Labor Through Function," MultiThreaded, March 11, 2019, https://oreil.ly/m6WWu.
Abbildung 11-2. Ich dachte immer, dass ein Datenwissenschaftler all diese Dinge wissen muss

Der Tweet scheint bei meinem Publikum Anklang zu finden. Eugene Yan schrieb auch darüber, wie
"Datenwissenschaftler mehr End-to-End sein sollten."^2 Eric Colson, Chief Algorithms Officer bei Stitch Fix
Officer von Stitch Fix (der zuvor auch VP Data Science and Engineering bei Netflix war), schrieb einen
Post über "die Macht des Full-Stack Data Science Generalisten und die Gefahren der Arbeitsteilung
der Arbeitsteilung durch Funktion"^3

Teamstruktur | 337
4 Erik Bernhardsson auf Twitter (@bernhardsson), 20. Juli 2021, https://oreil.ly/7X4J9.
5 Colson, "Beware the Data Science Pin Factory".
Als ich diesen Tweet schrieb, war ich der Meinung, dass Kubernetes für den ML-Workflow unerlässlich ist.
flow. Dieser Gedanke entsprang der Frustration über meinen eigenen Job - mein Leben als ML
Ingenieur wäre viel einfacher gewesen, wenn ich mich mit K8s besser auskennen würde.

Als ich jedoch mehr über die Low-Level-Infrastruktur erfuhr, wurde mir klar, wie unangemessen
wie unvernünftig es ist, von Datenwissenschaftlern zu erwarten, dass sie sich damit auskennen. Die Infrastruktur erfordert eine ganz
andere Fähigkeiten als Datenwissenschaftler. Theoretisch kann man beide Arten von Fähigkeiten erlernen.
In der Praxis ist es so, dass je mehr Zeit man mit dem einen verbringt, desto weniger Zeit verbringt man mit dem
anderen. Ich liebe Erik Bernhardssons Analogie, dass die Erwartung, dass Datenwissenschaftler etwas über
Infrastruktur zu erwarten ist, als würde man von App-Entwicklern erwarten, dass sie wissen, wie Linux-Kernel
^4 Ich bin zu einem ML-Unternehmen gegangen, weil ich mehr Zeit mit Daten verbringen wollte, nicht
mit dem Aufsetzen von AWS-Instanzen, dem Schreiben von Dockerdateien, dem Planen/Skalieren von Clustern oder
YAML-Konfigurationsdateien zu debuggen.

Damit Datenwissenschaftler den gesamten Prozess beherrschen können, brauchen wir gute Werkzeuge. Mit anderen Worten, wir
brauchen wir eine gute Infrastruktur.

Was wäre, wenn wir eine Abstraktion hätten, die es Datenwissenschaftlern ermöglicht, den Prozess von Anfang bis Ende selbst zu steuern
ohne sich um die Infrastruktur kümmern zu müssen?

Was wäre, wenn ich diesem Tool einfach sagen könnte: "Hier speichere ich meine Daten (S3), hier sind die
Schritte zur Ausführung meines Codes (Featurisierung, Modellierung), wo mein Code ausgeführt werden soll (EC2
Instanzen, serverloses Zeug wie AWS Batch, Function usw.), hier ist, was mein Code braucht
Schritt auszuführen (Abhängigkeiten)", und dieses Tool verwaltet dann die gesamte Infrastruktur
Sachen für mich?

Sowohl Stitch Fix als auch Netflix zufolge hängt der Erfolg eines Full-Stack-Datenwissenschaftlers
von den Werkzeugen ab, die ihnen zur Verfügung stehen. Sie benötigen Tools, die "die Datenwissenschaftler von der Komplexität
Komplexität von Containerisierung, verteilter Verarbeitung, automatischer Ausfallsicherung und
und anderen fortgeschrittenen Informatikkonzepten abstrahieren."^5

Im Modell von Netflix erstellen die Spezialisten - die Personen, die ursprünglich einen Teil des Projekts
Projekt gehörten, zuerst Tools, die ihre Teile automatisieren, wie in Abbildung 11-3 dargestellt. Daten
Datenwissenschaftler können diese Tools nutzen, um ihre Projekte von Anfang bis Ende zu verwalten.

338 | Kapitel 11: Die menschliche Seite des maschinellen Lernens

6 "Full Cycle Developers at Netflix-Operate What You Build," Netflix Technology Blog, 17. Mai 2018,
https://oreil.ly/iYgQs.
Abbildung 11-3. Full-Cycle-Entwickler bei Netflix. Quelle: Adaptiert von einer Abbildung von Netflix^6

Wir haben darüber gesprochen, wie ML-Systeme die Benutzererfahrung beeinflussen können und wie die Organisationsstruktur die Produktivität von ML-Projekten beeinflussen kann.
Organisationsstruktur die Produktivität von ML-Projekten beeinflussen kann. In der zweiten Hälfte des
dieses Kapitels werden wir uns auf einen noch wichtigeren Aspekt konzentrieren: wie ML-Systeme
Gesellschaft auswirken können und was ML-Systementwickler tun sollten, um sicherzustellen, dass die
Systeme, die sie entwickeln, mehr Nutzen als Schaden anrichten.

Verantwortungsvolle KI
Dieser Abschnitt wurde unter großzügiger Mitwirkung von Abhishek Gupta, Gründer
und Hauptforscher am Montreal AI Ethics Institute. Seine Arbeit konzentriert sich auf
angewandte technische und politische Maßnahmen zum Aufbau ethischer, sicherer und integrativer KI-Systeme.

Die Frage, wie intelligente Systeme verantwortlich gemacht werden können, ist nicht nur
nicht nur für ML-Systeme, sondern auch für allgemeine künstliche Intelligenz
(KI)-Systeme. KI ist ein weiter gefasster Begriff, der ML einschließt. Daher wird in
diesem Abschnitt verwenden wir KI statt ML.
Verantwortungsvolle KI | 339
Verantwortungsvolle KI ist die Praxis des Designs, der Entwicklung und des Einsatzes von KI-Systemen
in guter Absicht und mit ausreichendem Bewusstsein, um die Nutzer zu befähigen und Vertrauen zu schaffen,
und um faire und positive Auswirkungen auf die Gesellschaft zu gewährleisten. Sie umfasst Bereiche wie Fairness,
Datenschutz, Transparenz und Verantwortlichkeit.

Diese Begriffe sind nicht mehr nur philosophische Träumereien, sondern ernsthafte Überlegungen für
sowohl für politische Entscheidungsträger als auch für Praktiker. Angesichts der Tatsache, dass ML in fast jedem
in fast jedem Aspekt unseres Lebens eingesetzt wird, kann es katastrophale Folgen haben, wenn wir unsere ML-Systeme nicht fair und ethisch korrekt gestalten.
kann zu katastrophalen Folgen führen, wie in dem Buch Weapons of Math
Zerstörung (Cathy O'Neil, Crown Books, 2016) und in anderen Fallstudien
die in diesem Buch erwähnt werden.

Als Entwickler von ML-Systemen haben Sie die Verantwortung, nicht nur darüber nachzudenken, wie
wie sich Ihre Systeme auf die Nutzer und die Gesellschaft insgesamt auswirken werden, sondern auch, um allen Beteiligten zu helfen
ihre Verantwortung gegenüber den Nutzern besser wahrzunehmen, indem sie Ethik, Sicherheit und
Ethik, Sicherheit und Inklusivität in Ihre ML-Systeme zu integrieren. Dieser Abschnitt ist eine kurze Einführung in die
was passieren kann, wenn nicht genügend Anstrengungen unternommen werden, um ML-Systeme
verantwortlich zu machen. Wir beginnen mit zwei Fallstudien über recht unglückliche und öffentliche Misserfolge
von ML. Anschließend schlagen wir einen vorläufigen Rahmen für Datenwissenschaftler und ML
Ingenieure vorschlagen, um die Werkzeuge und Richtlinien auszuwählen, die am besten helfen, ML
Systeme verantwortlich zu machen.

Haftungsausschluss: Verantwortungsvolle KI ist ein komplexes Thema mit wachsender Literatur, das eine
einen eigenen Abschnitt verdient und leicht mehrere Bücher umfassen kann. Dieser Abschnitt ist weit davon entfernt, ein
erschöpfenden Leitfaden. Wir wollen ML-Entwicklern lediglich einen Überblick geben, damit sie sich effektiv
Entwicklungen auf diesem Gebiet zu navigieren. Wer an weiterführender Literatur interessiert ist, dem
empfehlen wir die folgenden Ressourcen:

-NIST Special Publication 1270: Towards a Standard for Identifying and Manag-
ing Bias in künstlicher Intelligenz
ACM-Konferenz über Fairness, Verantwortlichkeit und Transparenz (ACM FAccT)
Veröffentlichungen
-Trustworthy ML's Liste empfohlener Ressourcen und grundlegender Papiere für
Forscher und Praktiker, die mehr über vertrauenswürdiges ML erfahren möchten
-Sara Hookers großartiger Foliensatz über Fairness, Sicherheit und Governance in
maschinellem Lernen (2022)
-Timnit Gebru und Emily Dentons Tutorials zu Fairness, Rechenschaftspflicht, Transparenz und
Transparenz und Ethik (2020)
340 | Kapitel 11: Die menschliche Seite des maschinellen Lernens

7 Elliot Jones und Cansu Safak, "Can Algorithms Ever Make the Grade?" Blog des Ada Lovelace Instituts, 2020,
https://oreil.ly/ztTxR.
8 Tom Simonite, "Skewed Grading Algorithms Fuel Backlash Beyond the Classroom", Wired, 19. August 2020,
https://oreil.ly/GFRet.
9 Ofqual, "Awarding GCSE, AS & A Levels in Summer 2020: Interim Report," Gov.uk, 13. August 2020,
https://oreil.ly/r22iz.
Unverantwortliche KI: Fallstudien
Wir beginnen diesen Abschnitt mit der Betrachtung von zwei Fehlern von KI-Systemen, die nicht nur für die Nutzer dieser Systeme
nicht nur für die Nutzer dieser Systeme, sondern auch für die Organisationen, die sie entwickelt haben.
die diese Systeme entwickelt haben. Wir werden nachzeichnen, wo die Unternehmen Fehler gemacht haben
Fehler begangen haben und was die Praktiker hätten tun können, um diese Punkte
Versagen zu verhindern. Diese Schlaglichter dienen als Hintergrund, wenn wir uns mit dem technischen
Rahmen für verantwortungsvolle KI.

Es gibt weitere interessante Beispiele für "KI-Vorfälle", die in der AI Incident
Datenbank. Denken Sie daran, dass die folgenden zwei Beispiele und die in der
der AI Incident Database die Aufmerksamkeit auf sich gezogen haben, gibt es viele weitere
Fälle von unverantwortlicher KI, die im Stillen geschehen.

Fallstudie I: Voreingenommenheit des automatisierten Beurteilers

Im Sommer 2020 hat das Vereinigte Königreich die A-Levels, die anspruchsvollen Prüfungen
Prüfungen, die über die Einstufung an Hochschulen entscheiden, wegen der COVID-19-Pandemie ab. Ofqual,
die Aufsichtsbehörde für Bildung und Prüfungen im Vereinigten Königreich, genehmigte die Verwendung
die Verwendung eines automatisierten Systems zur Zuweisung von A-Level-Abschlussnoten an Schüler - ohne dass diese
ohne dass diese die Prüfung abgelegt haben. Laut Jones und Safak vom Ada Lovelace Institute "war die Vergabe von
Noten von Schülern auf der Grundlage von Lehrerbeurteilungen wurde ursprünglich von Ofqual mit der
Ungerechtigkeit zwischen den Schulen, Unvergleichbarkeit zwischen den Generationen und
Entwertung der Ergebnisse aufgrund von Noteninflation. Die fairere Option, so vermutete Ofqual,
war es, frühere Leistungsdaten und Lehrerbewertungen zu kombinieren, um Noten zu vergeben,
unter Verwendung eines bestimmten statistischen Modells - eines 'Algorithmus'."^7

Die von diesem Algorithmus veröffentlichten Ergebnisse erwiesen sich jedoch als ungerecht und
unzuverlässig. Sie führten schnell zu einem öffentlichen Aufschrei, der die Abschaffung des Algorithmus forderte, wobei Hunderte
von Studenten, die mit Sprechchören protestierten.^8

Was hat den Aufschrei der Öffentlichkeit ausgelöst? Der erste Blick scheint auf die schlechte Leistung des Algorithmus
schlechte Leistung. Ofqual erklärte, dass ihr Modell, das mit Daten aus dem Jahr 2019 getestet wurde, eine durchschnittliche Genauigkeit von etwa
60 % durchschnittliche Genauigkeit in den A-Level-Fächern hatte.^9 Das bedeutet, dass sie erwarteten, dass 40 % der
dass die von diesem Modell zugewiesenen Noten von den tatsächlichen Noten der Schüler abweichen würden.

Verantwortungsvolle KI | 341
10 Ofqual, "Vergabe von GCSE, AS & A levels".
11 Jones und Safak, "Can Algorithms Ever Make the Grade?"
12 Jones und Safak, "Can Algorithms Ever Make the Grade?"

Obwohl die Genauigkeit des Modells gering erscheint, verteidigte Ofqual ihren Algorithmus als
Algorithmus als weitgehend vergleichbar mit der Genauigkeit von menschlichen Prüfern. Beim Vergleich der Noten eines Prüfers
er mit den Noten eines leitenden Prüfers, liegt die Übereinstimmung ebenfalls bei etwa 60 %.^10
Die Genauigkeit sowohl der menschlichen Prüfer als auch des Algorithmus verdeutlicht die zugrunde liegende
Unsicherheit bei der Bewertung von Schülern zu einem einzigen Zeitpunkt,^11 was die Verunsicherung der Öffentlichkeit weiter anheizt.
der Öffentlichkeit.
Wenn Sie dieses Buch bis hierher gelesen haben, wissen Sie, dass grobkörnige Genauigkeit allein
bei weitem nicht ausreicht, um die Leistung eines Modells zu bewerten, insbesondere bei
ein Modell, dessen Leistung die Zukunft so vieler Schüler beeinflussen kann. Ein genauerer
Betrachtet man diesen Algorithmus genauer, so erkennt man mindestens drei große Fehler im Prozess der
bei der Konzeption und Entwicklung dieses automatischen Benotungssystems:
-Versagen, das richtige Ziel zu setzen
-Versäumnis, eine feinkörnige Bewertung durchzuführen, um potenzielle Verzerrungen aufzudecken
-Versäumnis, das Modell transparent zu machen
Wir werden auf jedes dieser Versäumnisse im Detail eingehen. Denken Sie daran, dass selbst wenn diese
Versäumnisse behoben werden, könnte die Öffentlichkeit immer noch über das automatische Bewertungssystem verärgert sein.
Fehler 1: Festlegung des falschen Ziels. In Kapitel 2 wurde erörtert, wie sich das Ziel eines
eines ML-Projekts die Leistung des daraus resultierenden ML-Systems beeinflussen wird. Bei der Entwicklung eines
automatisches System zur Benotung von Schülern zu entwickeln, hätten Sie gedacht, dass das Ziel dieses
Ziel dieses Systems die "Benotungsgenauigkeit für Schüler" wäre.
Das Ziel, das Ofqual anscheinend optimieren wollte, war jedoch die "Aufrechterhaltung
Standards" zwischen den Schulen zu optimieren, indem die vom Modell vorhergesagten Noten an die historischen
Verteilungen der einzelnen Schulen. Wenn zum Beispiel Schule A in der Vergangenheit besser als Schule B war
Schule B in der Vergangenheit besser abgeschnitten hat, wollte Ofqual einen Algorithmus, der im Durchschnitt auch
Schüler von Schule A bessere Noten erhalten als Schüler von Schule B. Ofqual gab der
Fairness zwischen den Schulen gegenüber Fairness zwischen den Schülern - sie bevorzugten ein Modell, das
das die Ergebnisse auf Schulebene richtig einschätzt, gegenüber einem anderen Modell, das die Noten der einzelnen Schüler
richtig.
Aufgrund dieser Zielsetzung stufte das Modell leistungsstarke Jahrgänge aus historisch schwachen Schulen unverhältnismäßig stark herab.
Kohorten aus Schulen mit historisch schlechten Leistungen. Ein Schüler aus Klassen, in denen
A-Schüler aus Klassen, in denen die Schüler in der Vergangenheit durchweg Ds erhalten hatten, wurden auf Bs und Cs zurückgestuft.^12
342 | Kapitel 11: Die menschliche Seite des maschinellen Lernens
13 Ofqual, "Vergabe von GCSE, AS & A Levels".
14 Jones und Safak, "Can Algorithms Ever Make the Grade?"

Ofqual hat es versäumt, die Tatsache zu berücksichtigen, dass Schulen mit mehr Ressourcen tendenziell
besser abschneiden als Schulen mit weniger Ressourcen. Durch die Bevorzugung der historischen Leistung der Schulen
Leistung der Schulen gegenüber der aktuellen Leistung der Schüler, bestrafte dieser automatische
ressourcenarmen Schulen bestraft, in denen tendenziell mehr Schüler aus unterprivilegierten
Hintergrund haben.
Fehler 2: Unzureichende feinkörnige Modellbewertung zur Aufdeckung von Verzerrungen. Voreingenommenheit gegenüber Schü-
von Schülern aus Schulen mit historisch schlechten Leistungen ist nur eine der vielen Verzerrungen, die nach der
die nach der Veröffentlichung der Ergebnisse über dieses Modell bekannt wurden. Das automatische
Das automatisierte Benotungssystem berücksichtigte die Bewertungen der Lehrer als Inputs, versäumte es aber, auf die
die Uneinheitlichkeit der Bewertung von Lehrern in verschiedenen demografischen Gruppen. Außerdem "berücksichtigt es nicht
die Auswirkungen von Mehrfachbenachteiligungen für einige geschützte Gruppen
geschützten Gruppen [gemäß dem] Gleichstellungsgesetz von 2010, die durch niedrige
durch niedrige Erwartungen an die Lehrer, [und] rassistische Diskriminierung, die in einigen
Schulen."^13
Da das Modell die historischen Leistungen jeder Schule berücksichtigte, räumte Ofqual
eingeräumt, dass ihr Modell nicht genügend Daten für kleine Schulen enthielt. Für diese
Schulen wurde dieser Algorithmus nicht für die Vergabe der Endnoten verwendet, sondern nur die von den Lehrern
bewertete Noten. In der Praxis führte dies zu "besseren Noten für Privatschüler, die
tendenziell kleinere Klassen haben."^14
Es wäre möglich gewesen, diese Verzerrungen durch die öffentliche Veröffentlichung der
der vorausgesagten Noten des Modells mit einer feinkörnigen Auswertung, um die Leistung des Modells
Leistung des Modells für verschiedene Datenbereiche zu verstehen - z. B. durch die Bewertung der Genauigkeit des Modells für
Schulen unterschiedlicher Größe und für Schüler mit unterschiedlichem Hintergrund.
Misserfolg 3: Mangelnde Transparenz. Transparenz ist der erste Schritt, um Vertrauen in Systeme zu schaffen.
Ofqual hat es jedoch versäumt, wichtige Aspekte seines Auto-Graders zu veröffentlichen, bevor
bevor es zu spät war. Zum Beispiel wurde der Öffentlichkeit nicht mitgeteilt, dass das Ziel des
dass das Ziel ihres Systems darin besteht, Fairness zwischen den Schulen zu wahren, bis zu dem Tag, an dem die Noten
veröffentlicht wurden. Die Öffentlichkeit konnte daher ihre Bedenken bezüglich dieses Ziels nicht äußern, als
das Modell entwickelt wurde.
Außerdem teilte Ofqual den Lehrern erst nach der Veröffentlichung der Noten mit, wie ihre Beurteilungen von
wie ihre Beurteilungen vom Auto-Grader verwendet werden, erst nachdem die Beurteilungen und die Rangfolge der Schüler eingereicht worden waren.
Ofqual wollte damit verhindern, dass die Lehrkräfte versuchen, ihre Beurteilungen zu ändern, um
Vorhersagen des Modells zu beeinflussen. Ofqual entschied sich dafür, das genaue Modell
zu veröffentlichen, um sicherzustellen, dass alle Schüler ihre Ergebnisse zur gleichen Zeit erfahren.
Zeit erfahren.
Verantwortungsvolle KI | 343
15 "Royal Statistical Society Response to the House of Commons Education Select Committee Call for Evidence:
The Impact of COVID-19 on Education and Children's Services Inquiry," Royal Statistical Society, 8. Juni,
2020, https://oreil.ly/ernho.

Diese Überlegungen entsprangen einer guten Absicht, doch die Entscheidung von Ofqual, die
ihre Modellentwicklung im Dunkeln zu lassen, bedeutete, dass ihr System nicht ausreichend
unabhängige, externe Prüfung. Jedes System, das sich auf das Vertrauen der Öffentlichkeit stützt
sollte von unabhängigen Experten, denen die Öffentlichkeit vertraut, überprüft werden können. Die Royal Statisti-
cal Society (RSS) äußerte in ihrer Untersuchung über die Entwicklung dieses Auto-Graders
Bedenken hinsichtlich der Zusammensetzung der "technischen Beratungsgruppe", die Ofqual
um das Modell zu bewerten. Die RSS wies darauf hin, dass "ohne eine stärkere Verfahrens
ohne eine stärkere Verfahrensgrundlage, die statistische Strenge gewährleistet, und ohne größere Transparenz hinsichtlich der von Ofqual
untersucht"^15 die Legitimität des statistischen Modells von Ofqual in Frage steht.
Diese Fallstudie zeigt, wie wichtig Transparenz bei der Entwicklung eines Modells ist, das
das einen direkten Einfluss auf das Leben so vieler Menschen haben kann, und welche Folgen es haben kann
Konsequenzen haben kann, wenn wichtige Aspekte des Modells nicht rechtzeitig offengelegt werden. Die Website
zeigt auch, wie wichtig es ist, das richtige Ziel für die Optimierung zu wählen, da das falsche
falsche Zielsetzung (z. B. Fairness unter den Schulen) kann nicht nur dazu führen, dass Sie ein Modell wählen
Modell zu wählen, das im Hinblick auf das richtige Ziel unterdurchschnittlich abschneidet, sondern auch Vorurteile aufrechterhalten kann.
Dies ist auch ein Beispiel für die derzeit unscharfe Grenze zwischen dem, was durch Algorithmen automatisiert
durch Algorithmen automatisiert werden sollte und was nicht. Es muss Leute in der britischen Regierung geben
die es für in Ordnung halten, dass die Benotung von Abiturprüfungen durch Algorithmen automatisiert wird, aber es ist auch möglich
aber man kann auch argumentieren, dass die A-Level-Bewertung aufgrund der potenziell katastrophalen Folgen
A-Level-Bewertung niemals hätte automatisiert werden dürfen. Bis es
eine klarere Grenze gibt, wird es mehr Fälle des Missbrauchs von KI-Algorithmen geben. Eine klarere
Grenze kann nur durch mehr Investitionen in Zeit und Ressourcen sowie durch
sowie ernsthafte Überlegungen von KI-Entwicklern, der Öffentlichkeit und den Behörden.
Fallstudie II: Die Gefahr von "anonymisierten" Daten
Diese Fallstudie ist für mich interessant, weil hier der Algorithmus nicht explizit als
Übeltäter ist. Vielmehr ist es die Art und Weise, wie die Schnittstelle und die Datenerfassung gestaltet sind, die das
das Durchsickern sensibler Daten ermöglicht. Da die Entwicklung von ML-Systemen stark von der Qualität der Daten abhängt
von der Qualität der Daten abhängt, ist es wichtig, dass Nutzerdaten gesammelt werden. Die Forschungskommu-
Die Forschungsgemeinschaft braucht Zugang zu hochwertigen Datensätzen, um neue Techniken zu entwickeln. Praktiker
und Unternehmen benötigen Zugang zu Daten, um neue Anwendungsfälle zu entdecken und neue
KI-gestützte Produkte zu entwickeln.
344 | Kapitel 11: Die menschliche Seite des maschinellen Lernens
16 "Guidance on the Protection of Personal Identifiable Information", US Department of Labor,
https://oreil.ly/FokAV.
17 Sasha Lekach, "Strava's Fitness Heatmap Has a Major Security Problem for the Military," Mashable, January 28,
2018, https://oreil.ly/9ogYx.
18 Jeremy Hsu, "The Strava Heat Map and the End of Secrets", Wired, 29. Januar 2018, https://oreil.ly/mB0GD.
19 Matt Burgess, "Strava's Heatmap Data Lets Anyone See the Names of People Exercises on Military Bases,"
Wired, 30. Januar 2018, https://oreil.ly/eJPdj.
20 Matt Burgess, "Strava's Heatmap Data Lets Anyone See"; Rosie Spinks, "Using a Fitness App Taught
Me the Scary Truth About Why Privacy Settings Are a Feminist Issue," Quartz, August 1, 2017,
https://oreil.ly/DO3WR.

Das Sammeln und Weitergeben von Datensätzen kann jedoch die Privatsphäre und die Sicherheit der
der Nutzer, deren Daten Teil dieser Datensätze sind, verletzen. Um die Nutzer zu schützen, wurden
die Anonymisierung von personenbezogenen Daten (PII) gefordert. Nach Angaben des
des US-Arbeitsministeriums sind PII definiert als "jede Darstellung von Informationen
die einen Rückschluss auf die Identität einer Person, auf die sich die Informationen beziehen, zulässt
direkt oder indirekt auf die Identität einer Person schließen lässt, auf die sich die Informationen beziehen", wie z. B. Name, Adresse oder
Telefonnummer.^16
Die Anonymisierung ist jedoch möglicherweise keine ausreichende Garantie zur Verhinderung von Datenmissbrauch
Datenmissbrauch und eine Aushöhlung der Erwartungen an die Privatsphäre zu verhindern. Im Jahr 2018 veröffentlichte der Online-Fitness-Tracker Strava
eine Heatmap veröffentlicht, die die Wege seiner Nutzer auf der ganzen Welt aufzeichnet
während sie Sport treiben, z. B. laufen, joggen oder schwimmen. Die Heatmap wurde aggregiert
von einer Milliarde Aktivitäten, die zwischen 2015 und September 2017 aufgezeichnet wurden und 27
Milliarden Kilometern an Distanz. Strava erklärte, dass die verwendeten Daten anonymisiert wurden,
und "schließt Aktivitäten aus, die als privat markiert wurden, sowie benutzerdefinierte
Zonen."^17
Da Strava von Militärangehörigen genutzt wurde, konnten deren öffentliche Daten trotz Anonymisierung,
trotz der Anonymisierung Muster zu entdecken, die die Aktivitäten von US-Militärbasen auf
Militärbasen über dem Meer aufdeckten, darunter die "vorgeschobenen Operationsbasen in Afghanistan, türkische Militärpatrouillen
in Syrien und eine mögliche Wachpatrouille im russischen Operationsgebiet in Syrien."^18 Ein
Beispiel für diese Unterscheidungsmuster ist in Abbildung 11-4 dargestellt. Einige Analysten haben sogar
vorgeschlagen, dass die Daten die Namen und Herzfrequenzen der einzelnen Strava
Benutzer enthüllen könnten.^19
Was ist also bei der Anonymisierung schief gelaufen? Erstens war die Standard-Datenschutzeinstellung von Strava
"Opt-out", d. h. die Nutzer müssen sich manuell abmelden, wenn sie nicht wollen, dass ihre Daten
Daten gesammelt werden sollen. Die Nutzer haben jedoch darauf hingewiesen, dass diese Datenschutzeinstellungen nicht
nicht immer klar sind und bei den Nutzern zu Überraschungen führen können.^20 Einige der Datenschutzeinstellungen können nur
über die Strava-Website und nicht über die mobile App geändert werden. Dies zeigt, wie
wie wichtig es ist, die Nutzer über ihre Datenschutzeinstellungen aufzuklären. Besser, Daten Opt-in (Daten
(das Sammeln von Daten ist nicht Standard), nicht Opt-out, sollte der Standard sein.
Verantwortungsvolle KI | 345
21 "Fitness App Strava Lights Up Staff at Military Bases," BBC News, January 29, 2018, https://oreil.ly/hXwpN.
22 Matt Burgess, "Strava's Heatmap Data Lets Anyone See".

Abbildung 11-4. Das Bild wurde auf der Grundlage einer Analyse von BBC News erstellt^21
Als das Problem mit der Strava-Heatmap öffentlich wurde, wurden einige der Verantwortlichkeiten
wurden einige Verantwortlichkeiten auf die Nutzer verlagert: z. B. dass Militärangehörige keine nicht-militärischen
nicht-militärische Geräte mit GPS-Ortung verwenden und die Ortungsdienste deaktivieren sollten.^22
346 | Kapitel 11: Die menschliche Seite des maschinellen Lernens
Die Einstellungen zum Schutz der Privatsphäre und die Wahlmöglichkeiten der Nutzer behandeln das Problem jedoch nur oberflächlich.
Ebene. Das eigentliche Problem ist, dass die Geräte, die wir heute benutzen, ständig Daten über uns sammeln und melden.
Daten über uns sammeln und weitergeben. Diese Daten müssen bewegt und irgendwo gespeichert werden,
Das schafft Möglichkeiten, sie abzufangen und zu missbrauchen. Die Daten, die Strava hat
sind klein im Vergleich zu viel weiter verbreiteten Anwendungen wie Amazon, Facebook,
Google, usw. Der Fehler von Strava könnte die Aktivitäten von Militärstützpunkten aufgedeckt haben, aber andere
aber andere Datenschutzverletzungen könnten noch mehr Gefahren mit sich bringen, nicht nur für den Einzelnen, sondern auch für die
Gesellschaft als Ganzes.

Das Sammeln und Teilen von Daten ist für die Entwicklung datengesteuerter Technologien wie
Technologien wie KI. Diese Fallstudie zeigt jedoch die versteckte Gefahr des Sammelns und
und Weitergabe von Daten, selbst wenn die Daten angeblich anonymisiert und in guter Absicht
Absicht freigegeben werden. Entwickler von Anwendungen, die Nutzerdaten sammeln, müssen verstehen, dass ihre
dass ihre Nutzer möglicherweise nicht das technische Know-how und das Bewusstsein für die Privatsphäre haben, um die
die richtigen Datenschutzeinstellungen für sich selbst zu wählen, und deshalb müssen die Entwickler proaktiv darauf hinarbeiten
Daher müssen die Entwickler proaktiv darauf hinarbeiten, dass die richtigen Einstellungen standardmäßig verwendet werden, auch wenn dies mit einer geringeren Datenerfassung verbunden ist.

Ein Rahmen für verantwortungsvolle KI
In diesem Abschnitt werden wir die Grundlagen für Sie als ML-Praktiker schaffen, um
Modellverhalten zu prüfen, und legen Richtlinien fest, die Ihnen am besten helfen, die Anforderungen
Ihrer Projekte zu erfüllen. Dieser Rahmen ist nicht für jeden Anwendungsfall ausreichend. Es gibt bestimmte
Anwendungen, bei denen der Einsatz von KI insgesamt unangemessen oder unethisch sein könnte (z. B.,
strafrechtliche Verurteilungen, vorausschauende Polizeiarbeit), unabhängig davon, welchen Rahmen
Sie folgen.

Quellen für Modellverzerrungen entdecken

Als jemand, der die Diskussionen über die Gestaltung von ML-Systemen verfolgt hat, wissen Sie
wissen Sie, dass sich während des gesamten Arbeitsablaufs Verzerrungen in Ihr System einschleichen können. Ihr erster
Schritt ist es, herauszufinden, wie sich diese Verzerrungen einschleichen können. Im Folgenden finden Sie einige Beispiele für
der Datenquellen, aber bedenken Sie, dass diese Liste bei weitem nicht vollständig ist. Eine
Grund, warum es so schwer ist, Vorurteile zu bekämpfen, ist die Tatsache, dass Vorurteile bei jedem Schritt
während eines Projektlebenszyklus entstehen können.

Verantwortungsvolle KI | 347
23 Michael Feldman, Sorelle Friedler, John Moeller, Carlos Scheidegger, und Suresh Venkatasubramanian, "Certi-
fying and Removing Disparate Impact," arXiv, July 16, 2015, https://oreil.ly/FjSve.

Trainingsdaten
Sind die für die Entwicklung Ihres Modells verwendeten Daten repräsentativ für die Daten, die Ihr Modell
die Ihr Modell in der realen Welt verarbeiten wird? Wenn nicht, ist Ihr Modell möglicherweise gegenüber den
Gruppen von Nutzern mit weniger Daten in den Trainingsdaten.
Beschriftung
Wenn Sie Ihre Daten durch menschliche Kommentatoren beschriften lassen, wie messen Sie dann die Qualität
dieser Beschriftungen? Wie stellen Sie sicher, dass die Kommentatoren Standardrichtlinien befolgen
anstatt sich bei der Beschriftung Ihrer Daten auf subjektive Erfahrungen zu verlassen? Je mehr Annotatoren
Je mehr sich die Kommentatoren auf ihre subjektive Erfahrung verlassen müssen, desto größer ist der Spielraum für menschliche Verzerrungen.
Merkmalstechnik
Verwendet Ihr Modell ein Merkmal, das sensible Informationen enthält? Hat Ihr
Modell eine ungleiche Auswirkung auf eine Untergruppe von Menschen? Ungleiche Auswirkungen
liegt vor, "wenn ein Auswahlverfahren für verschiedene Gruppen zu sehr unterschiedlichen Ergebnissen führt
^23 Dies kann der Fall sein, wenn die Entscheidung eines Modells auf
Entscheidung eines Modells auf Informationen beruht, die mit gesetzlich geschützten Klassen korrelieren (z. B. ethnische Zugehörigkeit,
Geschlecht, Religionsausübung), auch wenn diese Informationen nicht direkt in das Training des
Modells verwendet werden. So kann beispielsweise ein Einstellungsverfahren zu ungleichen Auswirkungen nach Rasse führen
wenn es Variablen verwendet, die mit der Rasse korreliert sind, wie Postleitzahl und High-School
Diplome. Um diese potenziellen ungleichen Auswirkungen abzuschwächen, können Sie
die von Feldman et al. in "Certifying and Removing Disparate Impact" vorgeschlagenen Techniken zur Beseitigung
Removing Disparate Impact" vorgeschlagen wurden, oder die Funktion DisparateImpactRemover
die von AI Fairness 360 (AIF360) implementiert wurde. Sie können auch versteckte Verzerrungen in
Variablen (die dann aus dem Trainingsset entfernt werden können) mit der Infogram
Methode, die in H2O implementiert ist.
Das Ziel des Modells
Optimieren Sie Ihr Modell mit einem Ziel, das allen Nutzern gerecht wird?
Nutzer ermöglicht? Priorisieren Sie zum Beispiel die Leistung Ihres Modells bei allen Nutzern,
was Ihr Modell in Richtung der Mehrheitsgruppe der Nutzer verzerrt?
Bewertung
Führen Sie eine angemessene, feinkörnige Bewertung durch, um die Leistung Ihres Modells
die Leistung Ihres Modells bei verschiedenen Benutzergruppen zu verstehen? Dies wird in dem Abschnitt
"Ausschnittbasierte Auswertung" auf Seite 185. Eine faire, angemessene Evaluation hängt von der
Existenz von fairen, angemessenen Evaluationsdaten ab.
348 | Kapitel 11: Die menschliche Seite des maschinellen Lernens
24 Wikipedia, s.v. "Differential privacy", https://oreil.ly/UcxzZ.

Verstehen Sie die Grenzen des datengesteuerten Ansatzes
ML ist ein datengesteuerter Ansatz zur Lösung von Problemen. Es ist jedoch wichtig zu verstehen
dass Daten allein nicht ausreichen. Daten betreffen Menschen in der realen Welt, mit sozioeco-
nomische und kulturelle Aspekte zu berücksichtigen. Wir müssen ein besseres Verständnis für
die blinden Flecken, die durch eine zu starke Abhängigkeit von Daten entstehen. Dies bedeutet oft das Überschreiten
über disziplinäre und funktionale Grenzen hinweg, sowohl innerhalb als auch außerhalb der Organisation, damit
damit wir die gelebten Erfahrungen derjenigen berücksichtigen können, die von den
Systeme, die wir aufbauen.
Um zum Beispiel ein gerechtes automatisches Benotungssystem zu entwickeln, ist es wichtig
mit Fachleuten zusammenzuarbeiten, um die demografische Verteilung der Schülerschaft zu verstehen
und wie sich sozioökonomische Faktoren in den historischen Leistungsdaten
Daten widerspiegeln.
Verstehen der Kompromisse zwischen verschiedenen Desideraten
Beim Aufbau eines ML-Systems gibt es verschiedene Eigenschaften, die das System
System haben soll. Zum Beispiel könnte Ihr System eine niedrige Inferenzlatenz haben
Latenzzeit haben, was durch Modellkomprimierungstechniken wie Pruning erreicht werden kann.
Vielleicht möchten Sie auch, dass Ihr Modell eine hohe Vorhersagegenauigkeit hat, was durch
durch Hinzufügen weiterer Daten erreicht werden kann. Sie möchten vielleicht auch, dass Ihr Modell fair und
transparent sein, was erfordern könnte, dass das Modell und die zur Entwicklung des Modells verwendeten Daten
zur öffentlichen Überprüfung zugänglich gemacht werden.
Die ML-Literatur geht oft von der unrealistischen Annahme aus, dass die Optimierung für eine Eigenschaft, wie die Modellgenauigkeit, gilt.
Eigenschaft, wie die Modellgenauigkeit, alle anderen statisch hält. Man könnte Techniken diskutieren, um die
die Fairness eines Modells zu verbessern, unter der Annahme, dass die Genauigkeit oder Latenz dieses Modells
gleich bleibt. In Wirklichkeit kann die Verbesserung einer Eigenschaft jedoch dazu führen, dass andere
Eigenschaften verschlechtern. Hier sind zwei Beispiele für diese Abwägungen:
Abwägung zwischen Privatsphäre und Genauigkeit
Laut Wikipedia ist differentieller Datenschutz "ein System zur öffentlichen Freigabe von
Informationen über einen Datensatz durch die Beschreibung der Muster von Gruppen innerhalb des Datensatzes
Datensatzes, während Informationen über Einzelpersonen im Datensatz zurückgehalten werden. Die Idee
Die Idee hinter der differentiellen Privatsphäre ist, dass, wenn die Auswirkung einer beliebigen einzelnen
Substitution in der Datenbank klein genug ist, kann das Abfrageergebnis nicht dazu verwendet werden
nicht viel über eine einzelne Person ableiten und bietet daher Privatsphäre."^24
Differentielle Vertraulichkeit ist eine beliebte Technik, die bei Trainingsdaten für ML-Modelle eingesetzt wird.
Der Nachteil dabei ist, dass je höher der Grad der Vertraulichkeit ist, den die differentielle Vertraulichkeit
bieten kann, desto geringer ist die Genauigkeit des Modells. Allerdings ist diese Verringerung der Genauigkeit
ist jedoch nicht für alle Stichproben gleich. Wie Bagdasaryan und Shmatikov (2019) hervorheben,
Responsible AI | 349
25 Eugene Bagdasaryan und Vitaly Shmatikov, "Differential Privacy Has Disparate Impact on Model Accuracy,"
arXiv, 28. Mai 2019, https://oreil.ly/nrJGK.
26 Sarah Hooker, Aaron Courville, Gregory Clark, Yann Dauphin, und Andrea Frome, "What Do Compressed
Deep Neural Networks Forget?", arXiv, November 13, 2019, https://oreil.ly/bgfFX.
27 Sara Hooker, Nyalleng Moorosi, Gregory Clark, Samy Bengio, and Emily Denton, "Characterising Bias in
Compressed Models," arXiv, October 6, 2020, https://oreil.ly/ZTI72.
28 Hooker et al., "Characterising Bias in Compressed Models".

"Die Genauigkeit der differenziellen Privatsphärenmodelle sinkt viel stärker für die unterre-
Klassen und Untergruppen ab."^25
Abwägung zwischen Kompaktheit und Fairness
In Kapitel 7 haben wir uns ausführlich mit verschiedenen Techniken zur Modellkomprimierung
wie Pruning und Quantisierung. Wir haben gelernt, dass es möglich ist, die Größe eines
Größe eines Modells mit minimalen Kosten für die Genauigkeit erheblich zu reduzieren, z. B. die
Parameteranzahl eines Modells um 90 % bei minimalen Kosten für die Genauigkeit.
Die minimalen Genauigkeitskosten sind in der Tat minimal, wenn sie gleichmäßig über alle Klassen verteilt sind.
Klassen verteilt sind, aber was ist, wenn sich die Kosten auf nur wenige Klassen konzentrieren? In ihrem 2019
Veröffentlichung "What Do Compressed Deep Neural Networks Forget?" haben Hooker et al.
festgestellt, dass "Modelle mit einer völlig unterschiedlichen Anzahl von Gewichten vergleichbare
vergleichbare Leistungskennzahlen haben, sich aber in einem engen Teil des Datensatzes beträchtlich unterscheiden.
Teilmenge des Datensatzes erheblich voneinander abweichen."^26 Sie fanden zum Beispiel heraus, dass Komprimierungstechniken
den algorithmischen Schaden verstärken, wenn das geschützte Merkmal (z. B. Geschlecht, Rasse, Behinderung)
im langen Schwanz der Verteilung liegt. Das bedeutet, dass die Komprimierung unterproportionale
überproportional auf unterrepräsentierte Merkmale auswirkt.^27
Eine weitere wichtige Erkenntnis aus ihrer Arbeit ist, dass alle untersuchten Komprimierungsverfahren
eine ungleichmäßige Auswirkung haben, aber nicht alle Techniken haben den gleichen
gleichen Grad an ungleicher Auswirkung haben. Pruning hat eine weitaus größere ungleiche Auswirkung als
als bei den untersuchten Quantisierungstechniken.^28
Ähnliche Abwägungen werden auch weiterhin entdeckt. Es ist wichtig, sich dieser Kompromisse bewusst zu sein
Kompromisse zu kennen, damit wir fundierte Entscheidungen für unsere ML-Systeme treffen können. Wenn Sie
Wenn Sie mit einem System arbeiten, das komprimiert oder differentiell privat ist, sollten Sie mehr
Ressourcen für die Überprüfung des Modellverhaltens zuzuweisen, um unbeabsichtigte Schäden zu vermeiden.
Frühzeitig handeln
Stellen Sie sich ein neues Gebäude vor, das in der Innenstadt gebaut wird. Ein Bauunternehmer wurde beauftragt
etwas zu bauen, das die nächsten 75 Jahre Bestand haben wird. Um Kosten zu sparen, verwendet der
Bauunternehmer minderwertigen Zement. Der Eigentümer investiert nicht in die Überwachung, da er
da er Gemeinkosten vermeiden will, um schnell handeln zu können. Der Bauunternehmer fährt fort
Der Bauunternehmer baut auf diesem schlechten Fundament weiter und stellt das Gebäude pünktlich fertig.
350 | Kapitel 11: Die menschliche Seite des maschinellen Lernens
29 Jonette M. Stecklein, Jim Dabney, Brandon Dick, Bill Haskins, Randy Lovell, und Gregory Moroney,
"Error Cost Escalation Through the Project Life Cycle", NASA Technical Reports Server (NTRS),
https://oreil.ly/edzaB.
30 Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena
Spitzer, Inioluwa Deborah Raji, und Timnit Gebru, "Model Cards for Model Reporting," arXiv, October 5,
2018, https://oreil.ly/COpah.
31 Mitchell et al., "Model Cards for Model Reporting".

Innerhalb eines Jahres zeigen sich Risse, und es sieht so aus, als könnte das Gebäude einstürzen.
Die Stadt beschließt, dass das Gebäude ein Sicherheitsrisiko darstellt, und fordert seinen
abzureißen. Die Entscheidung des Bauunternehmers, Kosten zu sparen, und die Entscheidung des Eigentümers, Zeit zu sparen
Die Entscheidung des Bauunternehmers, Kosten zu sparen, und die Entscheidung des Bauherrn, Zeit zu sparen, kosten den Bauherrn am Ende viel mehr Geld und Zeit.
Diese Geschichte ist in ML-Systemen häufig anzutreffen. Unternehmen könnten beschließen
ethische Fragen in ML-Modellen zu umgehen, um Kosten und Zeit zu sparen, nur um in der Zukunft
Risiken in der Zukunft zu entdecken, wenn sie am Ende viel mehr kosten, wie in den vorangegangenen Fallstudien von
Ofqual und Strava.
Je früher man im Entwicklungszyklus eines ML-Systems beginnt, darüber nachzudenken
je früher im Entwicklungszyklus eines ML-Systems damit begonnen wird, sich Gedanken darüber zu machen, wie sich das System auf das Leben der Nutzer auswirkt und welche
je früher im Entwicklungszyklus eines ML-Systems man sich Gedanken darüber macht, wie sich das System auf das Leben der Nutzer auswirkt und welche Verzerrungen es haben könnte. Eine Studie der NASA zeigt, dass bei der
Softwareentwicklung die Kosten für Fehler in jeder Phase des Projektlebenszyklus um eine Größenordnung ansteigen.
Lebenszyklus eines Projekts um eine Größenordnung steigen.^29
Erstellen von Modellkarten
Modellkarten sind kurze Dokumente, die trainierte ML-Modelle begleiten und
Informationen darüber, wie diese Modelle trainiert und bewertet wurden. Modellkarten zeigen auch
den Kontext, in dem die Modelle verwendet werden sollen, sowie ihre Grenzen.
tionen.^30 Den Autoren des Modellkarten-Papiers zufolge "ist das Ziel von Modellkarten
Ziel der Modellkarten ist es, die ethische Praxis und die Berichterstattung zu standardisieren, indem sie den Beteiligten
Modelle, die für den Einsatz in Frage kommen, nicht nur anhand traditioneller Bewertungskriterien
sondern auch entlang der Achsen ethischer, inklusiver und fairer Überlegungen".
Die folgende Liste wurde vom Inhalt des Papiers "Model Cards for
Model Reporting" übernommen, um die Informationen zu zeigen, die Sie für Ihre Modelle angeben möchten
Modelle:^31
Modell-Details: Grundlegende Informationen über das Modell.
--Person oder Organisation, die das Modell entwickelt hat
--Datum des Modells
-Version des Modells
--Modell-Typ
Verantwortliche KI | 351
--Informationen über Trainingsalgorithmen, Parameter, Fairness-Beschränkungen oder
andere angewandte Ansätze und Merkmale
-Veröffentlichung oder andere Quelle für weitere Informationen
-Zitatangaben
-Lizenz
-Anschrift für Fragen oder Kommentare zum Modell
-Verwendungszweck: Anwendungsfälle, die während der Entwicklung angedacht wurden.
-Primäre beabsichtigte Verwendungszwecke
-Primär beabsichtigte Benutzer
-Out-of-scope Anwendungsfälle
-Faktoren: Zu den Faktoren können demographische oder phänotypische Gruppen, Umweltbedingungen
Umweltbedingungen, technische Eigenschaften oder andere.
-Relevante Faktoren
-Evaluierungsfaktoren
-Messgrößen: Die Messgrößen sollten so gewählt werden, dass sie die potenziellen realen Auswirkungen des
Modells widerspiegeln.
-Leistungsmaße des Modells
-Entscheidungsschwellen
-Variationsansätze
-Evaluationsdaten: Einzelheiten zu dem/den für die quantitativen Analysen verwendeten Datensatz/en in der
Karte.
-Datensätze
--Motivation
-Vorverarbeitung
-Trainingsdaten: Kann in der Praxis nicht immer zur Verfügung gestellt werden. Wenn möglich, sollte dieser
sollte dieser Abschnitt die Evaluierungsdaten widerspiegeln. Wenn eine solche Detaillierung nicht möglich ist, sollten minimale
sollten hier nur minimale Informationen angegeben werden, wie z. B. Details der Verteilung
über verschiedene Faktoren in den Trainingsdatensätzen.
-Quantitative Analysen
-Einheitliche Ergebnisse
--Intersektionelle Ergebnisse
-Ethische Überlegungen
-Warnungen und Empfehlungen
352 | Kapitel 11: Die menschliche Seite des maschinellen Lernens

Modellkarten sind ein Schritt zu mehr Transparenz bei der Entwicklung von ML
Modellen. Sie sind besonders wichtig in Fällen, in denen die Nutzer eines Modells nicht dieselben sind
nicht die gleichen Personen sind, die das Modell entwickelt haben.

Beachten Sie, dass die Modellkarten immer dann aktualisiert werden müssen, wenn ein Modell aktualisiert wird. Für
Modelle, die häufig aktualisiert werden, kann dies für Datenwissenschaftler einen ziemlichen Overhead bedeuten, wenn
Modellkarten manuell erstellt werden. Daher ist es wichtig, Werkzeuge zu haben, die automatisch
Modellkarten automatisch zu generieren, entweder durch die Nutzung der Modellkartengenerierungsfunktion von
Tools wie TensorFlow, Metaflow und scikit-learn oder durch die Erstellung dieser Funktion im eigenen Haus.
Da sich die Informationen, die in der Karte eines Modells verfolgt werden sollten, mit den
Informationen, die von einem Modellspeicher verfolgt werden sollten, würde es mich nicht überraschen, wenn in
in naher Zukunft Modellspeicher entwickelt werden, die automatisch Modellkarten generieren.

Einführung von Verfahren zur Abschwächung von Vorurteilen

Der Aufbau einer verantwortungsvollen KI ist ein komplexer Prozess, und je ad hoc der Prozess ist,
desto größer ist die Gefahr von Fehlern. Für Unternehmen ist es wichtig, systematische Prozesse zu etablieren
Prozesse zu etablieren, um ihre ML-Systeme verantwortungsvoll zu gestalten.

Vielleicht möchten Sie ein Portfolio interner Tools erstellen, die für verschiedene
Beteiligten zugänglich sind. Große Unternehmen verfügen über Tool-Sets, auf die Sie zurückgreifen können. Zum Beispiel,
Google hat empfohlene Best Practices für verantwortungsvolle KI veröffentlicht und IBM hat
AI Fairness 360 veröffentlicht, das eine Reihe von Metriken, Erklärungen und Algorithmen enthält
Algorithmen zur Abschwächung von Verzerrungen in Datensätzen und Modellen enthält. Sie könnten auch die Nutzung von
Audits durch Dritte.

Bleiben Sie auf dem Laufenden über verantwortungsvolle AI

KI ist ein sich schnell entwickelnder Bereich. Es werden ständig neue Quellen für Verzerrungen in der KI entdeckt,
und es ergeben sich ständig neue Herausforderungen für eine verantwortungsvolle KI. Neue Techniken zur Bewältigung
um diese Vorurteile und Herausforderungen zu bewältigen, werden aktiv entwickelt. Es ist wichtig, auf dem
Es ist wichtig, auf dem neuesten Stand der Forschung im Bereich der verantwortungsvollen KI zu bleiben. Verfolgen Sie zum Beispiel die
ACM FAccT-Konferenz, die Partnership on AI, die Fairness.Transparency.Privacy-Gruppe des Alan Turing Institute und das AI Now Institute,
Transparency, Privacy group" des Alan Turing Institute und das AI Now Institute.

Zusammenfassung
Trotz der technischen Natur von ML-Lösungen kann die Entwicklung von ML-Systemen nicht auf den technischen Bereich beschränkt werden.
nicht auf den technischen Bereich beschränken. Sie werden von Menschen entwickelt, von Menschen genutzt und hinterlassen
ihre Spuren in der Gesellschaft. In diesem Kapitel sind wir vom technischen Thema der letzten acht Kapitel abgewichen
der letzten acht Kapitel ab und konzentrieren uns auf die menschliche Seite von ML.

Wir haben uns zunächst darauf konzentriert, wie die probabilistische, meist korrekte und latenzreiche Natur
von ML-Systemen die Benutzererfahrung auf verschiedene Weise beeinflussen kann. Die probabilistische Natur
kann zu Inkonsistenzen in der Benutzererfahrung führen, die Frustration verursachen können - "Hey,

Zusammenfassung | 353
Ich habe diese Option gerade hier gesehen und kann sie nun nirgendwo finden." Die meist
korrekte Natur eines ML-Systems könnte es nutzlos machen, wenn die Benutzer diese Vorhersagen nicht einfach
Vorhersagen zu korrigieren. Um dem entgegenzuwirken, könnten Sie den Benutzern mehrere
"Vorhersagen für dieselbe Eingabe zeigen, in der Hoffnung, dass zumindest eine davon
richtig sein wird.

Der Aufbau eines ML-Systems erfordert oft eine Vielzahl von Fähigkeiten, und eine Organisation könnte sich fragen
sich fragen, wie diese erforderlichen Fähigkeiten verteilt werden sollen: sollen verschiedene Teams mit
Teams mit unterschiedlichen Fähigkeiten einbeziehen oder erwarten, dass ein und dasselbe Team (z. B. Datenwissenschaftler) über alle
Fähigkeiten. Wir haben die Vor- und Nachteile der beiden Ansätze untersucht. Der Hauptnachteil des ersten
Ansatzes ist der Overhead bei der Kommunikation. Der Hauptnachteil des zweiten Ansatzes ist
dass es schwierig ist, Datenwissenschaftler einzustellen, die den Prozess der Entwicklung eines ML
Systems von Anfang bis Ende beherrschen. Und selbst wenn sie es könnten, würden sie es vielleicht nicht gerne tun. Allerdings ist der
zweite Ansatz könnte jedoch möglich sein, wenn diese End-to-End-Datenwissenschaftler
mit ausreichenden Werkzeugen und Infrastrukturen ausgestattet werden, was der Schwerpunkt von Kapitel 10 war.

Wir haben das Kapitel mit dem meiner Meinung nach wichtigsten Thema dieses Buches abgeschlossen
Buches: verantwortungsvolle KI. Verantwortungsvolle KI ist nicht mehr nur eine Abstraktion, sondern eine wesentliche
Praxis in der heutigen ML-Industrie, die dringend Maßnahmen erfordert. Die Einbeziehung von ethischen
Prinzipien in Ihre Modellierungs- und Organisationspraktiken wird Ihnen nicht nur helfen
sich als professioneller und hochmoderner Datenwissenschaftler und ML-Ingenieur zu profilieren
sondern auch dazu beitragen, dass Ihre Organisation das Vertrauen Ihrer Kunden und Nutzer gewinnt. Es wird auch
Wettbewerbsvorteil auf dem Markt zu erlangen, da immer mehr Kunden und
Kunden und Nutzer ihren Bedarf an verantwortungsvollen KI-Produkten und -Dienstleistungen betonen.

Es ist wichtig, dass wir diese verantwortungsvolle KI nicht nur als eine Aktivität zum Ankreuzen von Kästchen betrachten
die wir durchführen, um die Compliance-Anforderungen für unsere Organisation zu erfüllen. Es ist wahr
dass der in diesem Kapitel vorgeschlagene Rahmen Ihnen helfen wird, die Compliance
Compliance-Anforderungen für Ihr Unternehmen zu erfüllen, aber er ersetzt nicht das kritische Nachdenken
darüber, ob ein Produkt oder eine Dienstleistung überhaupt entwickelt werden sollte.

354 | Kapitel 11: Die menschliche Seite des maschinellen Lernens

Epilog.
Wow, Sie haben es geschafft! Du hast gerade ein ziemlich technisches Buch mit 100.000 Wörtern und
über 100 Illustrationen, geschrieben von einer Autorin, die Englisch als zweite Sprache spricht.
Mit der Hilfe vieler Kollegen und Mentoren habe ich wirklich hart an diesem Buch gearbeitet,
und ich bin dankbar, dass Sie es unter den vielen Büchern, die es gibt, lesen wollen. Ich hoffe, dass
die Erkenntnisse, die Sie aus diesem Buch ziehen können, Ihre Arbeit ein wenig erleichtern.

Mit den bewährten Praktiken und Werkzeugen, die wir jetzt haben, gibt es bereits unglaublich viele
ML-Anwendungsfälle, die unser tägliches Leben beeinflussen. Ich habe keinen Zweifel, dass die Zahl der
Anwendungsfälle mit der Zeit zunehmen wird, wenn die Werkzeuge reifen, und Sie könnten
die dafür sorgen werden, dass dies geschieht. Ich freue mich darauf zu sehen, was Sie bauen!

ML-Systeme sind mit vielen Herausforderungen verbunden. Nicht alle davon machen Spaß, aber alle sind
Chancen für Wachstum und Wirkung. Wenn Sie über diese Herausforderungen und Chancen sprechen möchten
sprechen möchten, zögern Sie nicht, mich zu kontaktieren. Sie finden mich auf Twitter unter @chipro oder per
E-Mail an chip@claypot.ai.

355
Index.
Symbole
1NF (erste Normalform), 59
2NF (zweite Normalform), 59

A
A/B-Tests, 283 -284, 288
Genauigkeitsbezogene Metriken, 252
ACID (Atomarität, Konsistenz, Isolation, Dauerhaftigkeit
bilität), 68
Aktives Lernen, 101 -102
Ad-hoc-Analytik, 162
Anpassungsfähigkeit, 31
gegnerische Angriffe, 272
gegnerische Augmentation, 116
KI (künstliche Intelligenz), Ethik, 339 , 347-348
Grenzen des datengesteuerten Ansatzes, 349
unverantwortlich, Fallstudien, 341 -347
Entschärfung von Verzerrungen, 353
Modellkarten, 351 -353
Abstriche, 349
Luftströmung, 315 -316
Warnmüdigkeit, 255 , 259
Alarmierungsstrategien, 259
Algorithmen
Bandit-Algorithmen, 287 -291
Kontinuierliches Lernen und, 273 -274
Bedeutung von Merkmalen, 142
Analytische Verarbeitung, 67
Apache Iceberg, 69
Architektonische Suche, 174
Argo, 316 -318
Artefakte, 162
Künstliche Intelligenz (siehe AI)
Asynchrone Vorhersage, 198

automatische Umschulung, 275-277
AutoML
Architektur-Suche, 174 -178
harte AutoML, 174 -178
Hyperparameter-Abstimmung, 173 -174
Gelernter Optimierer, 174 -178
weiche AutoML, 173 -174
Autoskalierung, 30
B
Bagging, Ensembles, 158 -159
Bandit-Algorithmen, 287 -291
BASE (grundsätzlich verfügbar, weicher Zustand und gleichmäßige
tuale Konsistenz), 68
Basis-Lernende, 156
Basismodell, Feinabstimmung, 100
Basislinien, Offline-Modellbewertung, 179
bestehende Lösungen, 181
menschlich, 180
zufällig, 180
einfache Heuristik, 180
Null-Regel, 180
Batch-Pipeline, 203 -205
Batch-Vorhersage, 197 -201
Umstellung auf Online-Vorhersage, 201 -203
Stapelverarbeitung, 78 -79
Stapel, Überanpassung, 167
Binäre Klassifizierung, 37
Binärdaten, 57
Größe von Binärdateien, 57
Verstärkung, Ensembles, 159 -161
Borg, 314
Markenüberwachung, 12
Browser, ML (maschinelles Lernen) und, 222
357
Bauen versus Kaufen, 327 -329
Unternehmensanalyse, 35
Geschäftsziele, 26 -28

C
Kalibrierung, 183 -184
Kanarienvogel-Freigabe, 285
Kardinalität, Klassifizierungsaufgaben und, 37
katastrophales Vergessen, 264
kategoriale Merkmale, 129 -132
Champion-Modell, 264
Abwanderungsvorhersage, 104
Ungleichgewicht der Klassen, 102
Methoden auf Algorithmus-Ebene, 110
Klassenausgeglichener Verlust, 112
Kostensensitives Lernen, 111
fokaler Verlust, 112
Herausforderungen, 103 -105
Bewertungsmetriken, 106 -108
Neuabtastung, 109 -110
Klassenausgeglichener Verlust, 112
Klassifizierung
als Regressionsproblem, 107
binär, 37
hierarchisch, 38
hohe Kardinalität, 37
Mehrklassig, 37 , 38
mehrstufig, 38
Stimmungsanalyse, 120
Klassifizierungsmodelle, 36
Cloud Computing, 212 , 300-302
Elastizität, 300
Multicloud-Strategie, 302
Code-Versionierung, 164
Spaltenlöschung, 125
Spalten-Hauptformate, 54 -56
Pandas, 56
Parkett, 54
Pendler, 305
kompakte Faltungsfilter, 206
Berechnungsprioritäten, 15
rechenintensive Probleme, 6
Konzeptabweichung, 238 , 241
Vertrauensmessung, 185
Behälter, 308 -310
kontextbezogene Banditen, 289
Kontinuierliches Lernen, 35 , 264, 268-270
Algorithmen und, 273 -274
Bewertung und, 272 -273

Wiederverwendung von Merkmalen, 277
frischer Datenzugriff, 270 -272
zustandsorientierte Ausbildung, 265 -268
automatisiert, 277 -278
zustandslose Umschulung, 265 -268
manuell, 275
Schulung, automatisierte Umschulung, 275 -277
versus Online-Lernen, 268
Zufallsstichproben, 83
Kostensensitives Lernen, 111
Verschiebung der Kovariantendatenverteilung, 238 -240
cron, Zeitplaner, 313 -314
funktionsübergreifende Zusammenarbeit, Teams, 335
CSV (Komma-getrennte Werte), Zeilen-Haupt- für-
mat, 54
D
DAG (gerichteter azyklischer Graph), 312
Dashboards, Überwachung und, 258
Daten, 5, 18
Geist versus Daten, 43 -46
Training (siehe Trainingsdaten)
ungesehene Daten, 6
Datenerweiterung, 113
gegnerische Datenerweiterung, 116
Datensynthese, 116 -117
Störung, 114 -116
einfache markenerhaltende Transformationen,
114
Datenverteilungs-Verschiebungen
Adressierung, 248-250
Erkennung
statistische Methoden, 243 -244
Zeitskalenfenster, 245 -247
ML-Systemausfall, 237
Konzeptdrift, 238 , 241
Kovariatenverschiebung, 238 -240
Merkmalsänderung, 241
Änderung des Bezeichnungsschemas, 241
Verschiebung von Bezeichnungen, 238 , 240
Datenduplizierung, Datenverluste und, 139
Datentechnik, 34
Datenformate, 53
Binär, 57
Spalten-Hauptformat, 54 -56
JSON, 54
multimodale Daten, 53
relationales Modell, NoSQL, 63 -66
Zeilenmajor, 54 -56
358 | Index

Text, 57
Datenfrische, Modellaktualisierungen und, 279 -280
Datengenerierung, Datenlecks und, 140
Daten-Iteration, 267
Modellaktualisierungen und, 281
Datenlecks, 135
Datenduplizierung vor der Aufteilung, 139
Datengenerierungsprozess und, 140
Erkennung, 140
Gruppenlecks, 139
Kaggle-Wettbewerb, 136
Skalierung vor der Aufteilung, 138
Statistiken von Testaufteilungen, fehlende Daten und,
138
Zeitkorrelierte Daten, 137
Datenmodelle
relational, 59 -62
strukturierte Daten, 66 -67
unstrukturierte Daten, 66 -67
Datennormalisierung, 59
Datenparallelität, verteiltes Training und,
168 -170
Datenwissenschaftler, Teams, 336 -339
Datenquellen, 50
Datenbanken, interne, 52
Protokolle, 51
Smartphones und, 52
System-generierte Daten, 50
Daten Dritter, 52
Benutzereingaben, 50
Datensynthese, 116 -117
Datengesteuerter Ansatz, KI-Ethik und, 349
Datenbanken und Datenfluss, 72
Datenfluss, 72
Modell der Nachrichtenwarteschlange, 77
Weiterleitung durch Datenbanken, 72
Durchleitung durch Echtzeittransport, 74-77
Weitergabe durch Dienste, 73 -74
Anfragegesteuert, 75
DataFrame, Pandas und, 56
Fehlersuche, 165
Entscheidungsbäume, Beschneidung, 208 -209
deklarative ML-Systeme, 62
Deep Learning
ML (maschinelles Lernen) und, 1
ML-Algorithmen und, 150
Entartete Rückkopplungsschleifen, Ausfall von ML-Systemen,
233
Korrigieren, 235 -236

Abhängigkeiten, 312
ML-Modelle, Modellspeicher, 322
Ausfall von Abhängigkeiten, 227
Bereitstellung, 34 , 192
Endpunkte, Offenlegung, 192
Ausfall, 227
ML-Modelle, 320
Mythen
begrenzte Modelle auf einmal, 194 -195
Modellaktualisierung, 196
Leistung, 195
Maßstab, 196
Trennung der Zuständigkeiten, 193
Schatteneinsatz, 282
Entwicklungsumgebung, Infrastruktur, 296 ,
302
Container, 308 -310
Einrichtung, 303
IDE, 303 -306
Standardisierung, 306 -308
gerichteter azyklischer Graph (DAG), 312
gerichtete Erwartungstests, 183
Diskretisierung, Merkmalstechnik und, 128 -129
Verteiltes Training, 168
Datenparallelität und, 168
Modellparallelität und, 170 -172
Docker Compose, 310
Docker-Bilder, 308 -310
Dockerdateien, 308 -310
Dokumentenmodell, 63
Schemata, 64
Ausfallzeit, 228
Treiberverwaltungsdienst, 73
dynamische Probenahme, 110
E
Grenzfälle
Ausfall und, 231 -231
Ausreißer und, 232
Kantenberechnung, 213
Modelloptimierung, 214 -221
EKS (Elastischer Kubernetes-Dienst), 314
Einbettung
positionale Einbettung, 133 -135
Wort-Einbettungen, 133
Endpunkt, Offenlegung, 192
Ensembles
Bagging, 158 -159
Basis-Lerner, 156
Index | 359
Boosten, 159 -161
Spam-Klassifikatoren, 157
Stapeln, 161
Ethik in der KI, 339 -347
ETL (Extrahieren, Transformieren, Laden), 70 -72
Bewertung, offline
Vertrauensmessung, 185
gerichtete Erwartungstests, 183
Invarianz-Tests, 182
Modellkalibrierung, 183 -184
Störungstests, 181 -182
scheibenbasiert, 185 -188
vorhandene Daten, 5
Experiment-Artefakte, Entwicklung und, 323
Experimentverfolgung, 162 -163
Werkzeuge von Drittanbietern, 163
Modelle exportieren, 193

F
F1-Kennzahlen, 107
Faktorisierung, niedriger Rang, 206 -208
Fairness, 19
Merkmalsänderung, 241
Merkmalstechnik, 120 -122
kategorische Merkmale, 129 -132
Diskretisierung, 128 -129
Merkmalskreuzung, 132
Merkmalsverallgemeinerung, 144 -146
Bedeutung von Merkmalen, 142
Fehlende Werte und, 123
Löschung, 125
Imputation, 125 -126
MAR (fehlende Werte nach dem Zufallsprinzip), 124
MCAR (vollständig zufällig fehlende Werte),
124
MNAR (nicht zufällig fehlend), 124
NLP (natürliche Sprachverarbeitung) und, 122
Positionsbezogene Einbettungen, 133 -135
Vorhersagekraft von Merkmalen, 140
Skalierung, 126 -128
Unbrauchbare Merkmale, 141
Merkmalsskalierung, 126 -128
Merkmalsspeicher, 325 -327
Merkmale
Berechnung, 326
Konsistenz, 326
Extrahieren, 255
Ausfälle und, 166
gelernt, 120 -122

Verwaltung, 326
Überwachung, 253 -255
Online, 199
Wiederverwendung, 277
Streaming, 199
Rückkopplungsschleifen, 288
ML-Systemausfall, 234
Rückmeldungen, Benutzer, 93
Positionsfixierte Einbettungen, 135
Festkomma-Inferenz, 210
FLOPS (Gleitkommaoperationen pro Sekunde),
298
Vorhersage der Kundennachfrage, 11
Fourier-Merkmale, 135
Betrugserkennung, 11 , 104
G
GDPR (General Data Protection Regulation),
164
Generalisierung, Merkmale, 144 -146
GKE (Google Kubernetes Engine), 314
Google Translate, 1
Graphenmodell, 65
H
H20 AutoML, 62
Handbeschriftungen, 88
Abstammung, 90
Multiplizität, 89 -90
harte AutoML, 174 -178
Hardware-Ausfall, 228
Hash-Funktionen, 130
Heuristik, LFs (Beschriftungsfunktionen), 95
Heuristik-basiertes Slicing, 188
Hierarchische Klassifizierung, 38
menschliche Basislinien, 180
Hyperparameter
Ausfälle und, 166
Abstimmung, 173 -174
Werte im Zeitverlauf, 163
I
IDE (integrierte Entwicklungsumgebung),
303
Cloud-Entwicklungsumgebung, 307
Notebooks und, 304
Wichtigkeit von Stichproben, 87
Infrastruktur, 293 , 295
360 | Index

Bauen versus Kaufen, 327 -329
Cloud Computing und, 300 -302
Schicht der Entwicklungsumgebung, 296 , 302
Einrichtung, 303 -306
Grundlegende Einrichtungen, 295
ML-Plattform-Schicht, 296
Anforderungen, 295
Ressourcenverwaltungsschicht, 295
Speicher- und Berechnungsschicht, 295 , 296, 297
Berechnungsressourcen, 297
FLOPS, 298
Private Rechenzentren, 300 -302
öffentliche Wolke, 300 -302
Einheiten, 297
Eingabe, Überwachung, 255
Instanzen auf Abruf, 300
Integrierte Entwicklungsumgebung (siehe IDE)
Verschachtelung von Experimenten, 285 -287
interne Datenbanken, 52
Interpretierbarkeit, 20
Invarianz-Tests, 182
IR (Zwischendarstellung), 215
Iterative Prozesse
Modellentwicklung und, 34
Leistungsüberprüfung, 149
Modellaktualisierungen und, 281
Training des Modells und, 32 -33
Datentechnik, 34
Projekt-Scoping, 34

J
JSON (JavaScript-Objektnotation), 54
Urteilsstichproben, 83

K
k-means-Clustermodelle, 150
Kaggle, Datenlecks, 136
Wissensdestillation, 208
Kubeflow, 318
Kubernetes (K8s), 310 , 314
EKS (Elastischer Kubernetes-Dienst), 314
GKE (Google Kubernetes Engine), 314

L
Label-Berechnung, 271
Änderung des Etikettenschemas, 241
Etikettverschiebung, 238 , 240
Etikettierung, 88

Klassenungleichgewicht und, 102
Fehler, Klassenungleichgewicht und, 105
Handzettel, 88
Abstammung, 90
Vielfältigkeit, 89 -90
Fehlen von Etiketten, 94
Aktives Lernen, 101 -102
Semi-Supervision, 98 -99
Transfer-Lernen, 99 -101
schwache Überwachung, 95 -98
ML-Algorithmen, 151
natürliche Bezeichnungen, 91
Länge der Rückkopplungsschleife, 92
Empfehlungssysteme, 91
Störung, 114 -116
einfache kennzeichenerhaltende Transformationen,
114
Sprachmodellierung, Sampling und, 83
Latenz, 16
Latenz gegenüber Durchsatz, 16 -18
Lernen, 3
LFs (Beschriftungsfunktionen), 95
Heuristiken, 95
Protokolle, 51 , 51
Experimentverfolgung, 162
Überwachung und, 256 -257
Speicherung, 51
Schleifenkacheln, Modelloptimierung, 218
Verlustkurve, 162
Verlustfunktionen, 40
(siehe auch Zielfunktionen)
Niederrangige Faktorisierung, 206 -208
M
Wartbarkeit, 31
Manning, Christoph, 44
MAR-Werte (fehlende Zufallswerte), 124
MCAR (völlig zufällig fehlende Werte),
124
Zusammenführungskonflikte, 164
Nachrichtenwarteschlange, Datenfluss und, 77
Metaflow, 318
Metriken
Überwachung und, 250
Genauigkeitsbezogene Metriken, 252
Merkmale, 253 -255
Vorhersagen, 252 -253
Roheingabe, 255
Leistungsmetriken, 162
Index | 361
Systemleistung, 163
Geist versus Daten, 43 -46
zufällig fehlend (MAR), 124
Vollständig fehlende Zufallsdaten (MCAR), 124
Fehlende Daten, Test-Split-Statistik und, 138
fehlende nicht zufällige Daten (MNAR), 124
ML (maschinelles Lernen)
Browser und, 222 , 223
Cloud Computing, 212 -223
komplexe Muster, 4
Deep Learning und, 1
Edge-Computing, 212 -223
vorhandene Daten und, 5
Lernen, 3
Modelloptimierung, 220 -221
Vorhersagen und, 6
Produktion und, 12 -21
Wiederholung, 7
Forschung und, 12 -21
Maßstab, 7
Smartphones und, 9
ungesehene Daten, 6
Anwendungsfälle, 9-12
wann zu verwenden, 3-12
ML-Algorithmen, 2, 149
Deep Learning und, 150
Kennzeichnungen, 151
versus neuronale Netze, 150
ML-Modell-Logik, 191
ML-Modelle
Kontinuierliches Lernen, 35
Daten-Iteration, 267
Fehlersuche, 165
Einsatz, 320
Edge Computing, Optimierung, 214 -221
Ensembles, 156 , 157
Bagging, 158 -159
Basis-Lerner, 156
Boosten, 159 -161
Stapeln, 161
Bewertung, 150
Test in der Produktion, 281 -291
Experimentverfolgung, 162 -163
Exportieren, 193
Ausfälle
Chargen, Überanpassung, 167
Komponenten, 167
Datenprobleme, 166
Merkmalsauswahl, 166

Hyperparameter und, 166
schlechte Modellimplementierung, 166
Zufällige Seeds, 167
Theoretische Beschränkungen, 166
Iteration, 267
Überwachung, 35
Offline-Bewertung, 178
Grundlinien, 179 -181
Methoden, 181 -188
Optimierung, 220 -221
Parameter, Modellspeicher, 322
Leistungsmetriken, 162
Auswahlkriterien, 151
menschliche Voreingenommenheit, 153
Modell, 155
Leistung jetzt und später, 153
Einfache Modelle, 152
State-of-the-Art-Falle, 152
Kompromisse, 154
Geschwindigkeit, 163
Ausbildung, 32 -33
Datentechnik, 34
Verteilt, 168 -172
Aktualisierungshäufigkeit, 279
Datenfrische und, 279 -280
Dateniteration und, 281
Modell-Iteration und, 281
Aktualisierungen, 267
Versionierung, 163 -165
ML-Plattform, 319
Modellbereitstellung, 320
Modellspeicher, 321 - 325
ML-Plattformschicht, Infrastruktur, 296
ML-Systemausfälle
Verschiebung der Datenverteilung, 237
Adressierung, 248 -250
Konzeptdrift, 238 , 241
Kovariate, 238 -240
Erkennung, 242 -247
Merkmalsänderung, 241
Änderung des Bezeichnungsschemas, 241
Etikettenverschiebungen, 238 , 240
ML-systemspezifisch
Entartete Rückkopplungsschleifen, 233 -236
Grenzfälle, 231
Produktionsdaten unterscheiden sich von Trainingsdaten
Daten, 229 -231
Betriebliche Erwartungsverletzungen, 227
Software
362 | Index

Abstürze, 228
Abhängigkeitsfehler, 227
Fehler bei der Bereitstellung, 227
Ausfallzeit, 228
Hardware-Ausfall, 228
ML-Systeme
deklarativ, 62
Ausfälle, 226
iterative Prozesse, 32 -35
Anforderungen
Anpassungsfähigkeit, 31
Wartbarkeit, 31
Zuverlässigkeit, 29
Skalierbarkeit, 30 -31
im Vergleich zu herkömmlicher Software, 22 -23
MLOPs, Entwurf von ML-Systemen und, 2-3
MNAR-Werte (fehlende nicht zufällige Werte), 124
Modellverzerrungen, KI-Ethik, 347 -348
Modellkalibrierung, 183 -184
Modellkarten, KI-Ethik, 351 -353
Modellkomprimierung, 206
Wissensdestillation, 208
Faktorisierung mit niedrigem Rang, 206 -208
Beschneidung, 208 -209
Quantisierung, 209 -211
Modellentwicklung, 34
Modellimplementierung, Fehlschläge und, 166
Modellparallelität, verteiltes Training und,
170 -172
Modellleistung, Unternehmensanalyse, 35
Überwachung, 250 , 263
(siehe auch Test in der Produktion)
Warnmeldungen und, 259
Dashboards und, 258
Protokolle und, 256 -257
Metriken und, 250
Genauigkeitsbezogene Metriken, 252
Merkmale, 253 -255
Vorhersagen, 252 -253
Rohdaten, 255
Mehrklassen-Klassifizierung, 37 , 38
mehrstufige Klassifizierung, 38
multimodale Daten, 53

N
n-Gramme, 120
NAS (neuronale Architektursuche), 174
natürliche Bezeichnungen, 91
Länge der Rückkopplungsschleife, 92

Empfehlungssysteme, 91
Verarbeitung natürlicher Sprache (NLP) (siehe NLP)
Neuronale Architektursuche (NAS), 174
Neuronale Netze, 150
positionale Einbettung, 133
Newsfeeds
Ranking von Beiträgen, 41
Nutzerbindung und, 41
NLP (natürliche Sprachverarbeitung), 114
Datenerweiterung und, 113
Merkmalstechnik, 122
Nicht-Wahrscheinlichkeitsstichproben, 83
Verzerrungen, 83
Norvig, Peter, 44
NoSQL, 63
Dokumentenmodell, 63
Graphenmodell, 65
Notizbücher, IDE und, 304
NSFW (not safe for work) Inhaltsfilterung, 41
NumPy, 56
O
Zielfunktionen, 40 -43
Beobachtbarkeit, 250 , 259-261
Offline-Bewertung von Modellen, 178
Grundlinien, 179
bestehende Lösungen, 181
menschlich, 180
zufällig, 180
einfache Heuristik, 180
Null-Regel, 180
OLAP (analytische Online-Verarbeitung), 69
OLTP-System (Online-Transaktionsverarbeitung),
69
On-Demand-Instanzen, 300
Vorhersage auf Anfrage, 198
Eine-Milliarde-Wort-Benchmark für Sprachmodellierung
Modellierung, 45
Online-Merkmale, 199
Online-Lernen, 268
Online-Vorhersage, 197 -201, 288
Umstellung von Batch-Vorhersage auf Online-Vorhersage, 201 -203
Streaming-Pipeline, 203 -205
Verletzungen der Betriebserwartung, 227
Operator-Fusion, Modelloptimierung und, 218
Orchestratoren
HashiCorp Nomad, 314
Kubernetes (K8s), 314
Ausreißer, Grenzfälle und, 232
Index | 363
Überabtastung
Überanpassung, 110
SMOTE, 110

P
Pandas, 56
Papiermühle, 305
Parallelisierung, Modelloptimierung und, 217
Parameterwerte im Zeitverlauf, 163
Pareto-Optimierung, 42
Parkett, 54 , 57
Binärdateien, 57
Muster
wechselnd, 8
komplex, 4
Perle, Judäa, 43
Leistungsmetriken, 162
Systemleistung, 163
Störung, 114 -116
Perturbationsmethode der Semi-Supervision, 99
Störungstests, 181 -182
positionelle Einbettung, 133 -135
fest, 135
Präzisionsmetriken, 107
Vorhersage, 6, 39
asynchron, 198
Batch-Vorhersage, 197 -201
Umstellung auf Online-Vorhersage, 201 -203
"Meist richtig", Nutzererfahrung, 332 -334
On-Demand-Vorhersage, 198
Online, 197 -201
Streaming-Pipeline, 203 -205
synchron, 198
Vorhersagen, Überwachung, 252 -253
Vorhersagekraft von Merkmalen, 140
Preisoptimierungsdienst, 73
Problemstellung, 35 -43
Verarbeitung
analytisch, 67
Stapelverarbeitung, 78 -79
ETL (Extrahieren, Transformieren, Laden), 70 -72
Stromverarbeitung, 78 -79
transaktional, 67
ACID und, 68
Produktionsumgebung, 192
Produktion, ML und, 12 -21
Projektziele, 26 -28
Projektrahmenplan, 34
Prototyping, Stapelvorhersage und, 201

Beschneidung, 208 -209
Öffentliche Cloud gegenüber privatem Rechenzentrum, 300-302
Q
Quantisierung, 209 -211
Abfragesprachen, 60
Quotenstichproben, 83
R
Zufällige Basislinien, 180
Echtzeit-Transport
Datenfluss und, 74-77
Datenströme und, 78
Angemessener Maßstab, 294
Rückruf-Metriken, 107
Empfehlungssysteme, Etiketten, 91
Regression
Klassenungleichgewicht und, 102
Aufgaben, 39
Regressionsmodelle, 36
relationale Datenbanken, 60
relationale Modelle, 59 -62
Daten-Normalisierung, 59
NoSQL, 63
Dokumentenmodell, 63
Graphenmodell, 65
Tabellen, 59
Zuverlässigkeit, 29
Wiederholung, 7
sich wiederholende Aufgaben, Planung, 311
Anforderungsgesteuerte Datenübergabe, 75
Neuabtastung, 109
dynamische Probenahme, 110
Überabtastung
Überanpassung und, 110
SMOTE, 110
Zwei-Phasen-Lernen, 110
Unterprobenahme, 109
Probenahme in Reservoiren, 86-87
Ressourcenverwaltung, 311
Ressourcenverwaltungsschicht, Infrastruktur, 295
REST (repräsentative Zustandsübertragung), 74
Fahrtenverwaltungsdienst, 73
ROC-Kurve (Receiver Operating Characteristics),
108
Rogati, Monica, 44
ROI (Return on Investment), Reifegrad der
Einführung, 28
Zeilenlöschung, 125
364 | Index

Zeilenmajor-Format, 54 -56
CSV (Komma-getrennte Werte), 54
NumPy, 56
RPC (Fernsteuerungsaufruf), 74

S
Stichproben, 82
Wichtigkeitsstichproben, 87
Nichtwahrscheinlichkeit, 83
Verzerrungen, 83
Vorratsstichproben, 86 -87
Einfache Zufallsstichproben, 84
geschichtete Stichproben, 84
gewichtete Stichproben, 85
Skalierbarkeit, 30 -31
Autoskalierung, 30
Skalierung, 7
Einsatz-Mythen, 196
Planer, 313 -314
Borg, 314
Slurm, 314
Schemata, Dokumentenmodell, 64
Scoping eines Projekts, 34
Selbstschulung, 98
Semi-Supervision, 98 -99
Stimmungsanalyse-Klassifikator, 120
Serialisierung, 193
Dienste
Datenfluss und, 73 -74
Treiberverwaltung, 73
Preisoptimierung, 73
Fahrtenmanagement, 73
SGD (stochastischer Gradientenabstieg), 169
Schatteneinsatz, 282
SHAP (SHapley Additive ExPlanungen, 142
einfache Heuristik, Offline-Bewertung, 180
einfache markenerhaltende Transformationen, 114
einfache Zufallsstichproben, 84
Simpsons Paradoxon, 186
Schiefe Verteilung, Merkmalsskalierung und, 127
scheibenbasierte Bewertung, 185 -188
Slicing
Fehleranalyse, 188
Heuristik-basiert, 188
Slice-Finder, 188
Slurm, 314
Smartphones
Datenquellen und, 52
ML (maschinelles Lernen) und, 9

reibungsloses Scheitern, Benutzererfahrung, 334
SMOTE (Synthetische Minderheiten-Überabtastung
Technik), 110
Schnorchel, 95
Schneeball-Stichproben, 83
weiche AutoML, 173 -174
Software-Systemfehler
Abstürze, 228
Abhängigkeiten, 227
Bereitstellung, 227
Hardware, 228
Spam-Filterung, 41
Aufteilung
Datenvervielfältigung, 139
Datenlecks und, 138
SQL, 60
SQL-Datenbanken, 61
SSD (Solid State Disk), 297
Stapeln, Ensembles, 161
Beteiligte, Forschungsprojekte, 13 -15
Moderne Modelle, 152
zustandsorientierte Ausbildung, 265 -268
automatisiert, 277 -278
zustandslose Umschulung, 265 - 268
manuell, 275
stochastischer Gradientenabstieg (SGD), 169
Speicher- und Berechnungsschicht, Infrastruktur, 295 ,
296 , 297
Datenverarbeitungsressourcen, 297
FLOPS (Gleitkommaoperationen pro Sek.
ond), 298
Private Rechenzentren, 300 -302
öffentliche Wolke, 300 -302
Einheiten, 297
Speicher-Engines, 67
geschichtete Stichproben, 84
Stromverarbeitung, 78-79
Streaming-Daten, Echtzeit-Transport, 78
Streaming-Merkmale, 199
Streaming-Pipeline, 203 -205
strukturierte Daten, 66 -67
Sutton, Richard, 44
Synchrone Vorhersage, 198
Synthetisches Minderheiten-Überabtastungsverfahren
(SMOTE), 110
Metriken zur Systemleistung, 163
System-generierte Daten, 50
Stichwortverzeichnis | 365
T
Tags, Modellspeicher, 323
Aufgaben
Klassifizierung, 36
binär, 37
hohe Kardinalität, 37
Mehrklassig, 37 , 38
mehrstufig, 38
Kennzeichnungen, 91
Regression, 36 , 39
Teams
funktionsübergreifende Zusammenarbeit, 335
Datenwissenschaftler, 336 -339
Produktionsmanagement, 336
Telemetrie, 260
Testen in der Produktion, 263 , 281
A/B-Tests, 283 -284
Banditen, 287 -291
Kanarienvogel-Freigabe, 285
Verschachtelungsexperimente, 285 -287
Schatteneinsatz und, 282
Textdaten, 57
Textdateigröße, 57
Theoretische Beschränkungen, Ausfälle und, 166
Daten Dritter, 52
Zeitkorrelierte Daten, Datenverluste und, 137
Ausbildung
automatische Umschulung, 275 -277
Verteilt, 168
Datenparallelität und, 168 -170
Modellparallelität und, 170 -172
zustandsabhängig, 265 -268
automatisiert, 277 -278
zustandslose Umschulung, 265 -268
manuell, 275
Trainingsdaten, 81
Ungleichgewicht der Klassen, 102
Methoden auf Algorithmus-Ebene, 110 -113
Herausforderungen, 103 -105
Bewertungsmetriken, 106 -108
Neuabtastung, 109 -110
Datenerweiterung, 113
Störung, 114 -116
einfache markenerhaltende Transformationen,
114
Datenverteilungen, 229
Datenlecks, 135
Beschriftung, 88
Handetiketten, 88 -90

Fehlen von Etiketten, 94 -102
natürliche Kennzeichnungen, 91 -94
Benutzer-Feedback, 93
n-Gramme, 121
verrauschte Proben, 116
Stichproben, 82
Wichtigkeitsstichproben, 87
Nichtwahrscheinlichkeit, 83 -84
Vorratsstichproben, 86 -87
einfache Zufallsstichproben, 84
geschichtete Stichproben, 84
gewichtetes Sampling, 85
Training des Modells, Iteration und, 32 -33
Datentechnik, 34
Projekt-Scoping, 34
transaktionale Verarbeitung, 67
ACID und, 68
Transfer-Lernen, 99 -101
Zwei-Phasen-Lernen, 110
U
Unterabtastung, 109
ungesehene Daten, 6
unstrukturierte Daten, 66 -67
Aktualisierungen, Einsatzmythen, 196
Anwendungsfälle, 9-12
Benutzererfahrung, 331
Konsistenz, 332
Vorhersagen, meist richtig, 332 -334
reibungsloses Scheitern, 334
Benutzer-Feedback, 93
Benutzereingabedaten, 50
V
vCPU (virtuelle CPU), 299
Vektorisierung, Modelloptimierung, 217
Versionierung, 163 -165
Code-Versionierung, 164
W
WASM (WebAssembly), 223
Schwache Überwachung, 95 -98
Schnorchel, 95
gewichtetes Sampling, 85
Worteinbettungen, 133
Workflow-Verwaltung, 314
Luftstrom, 315 -316
Argo, 316 -318
366 | Index

DAG (gerichteter azyklischer Graph), 312
Kubeflow, 318
Metaflow, 318
X
XGBoost, 142

Z
Null-Regel-Baselines, 180
Null-Schuss-Lernen, 100
Index | 367
Über den Autor
Chip Huyen (https://huyenchip.com) ist Mitbegründer und CEO von Claypot AI, das eine
der Infrastruktur für maschinelles Lernen in Echtzeit. Zuvor war sie bei NVIDIA,
Snorkel AI und Netflix, wo sie einige der weltweit größten Unternehmen bei der
bei der Entwicklung und Bereitstellung von Systemen für maschinelles Lernen.

Als Studentin in Stanford entwickelte und unterrichtete sie den Kurs TensorFlow for Deep Learning Research.
Learning Forschung. Derzeit unterrichtet sie CS 329S: Systeme für maschinelles Lernen
Design in Stanford. Dieses Buch basiert auf den Vorlesungsunterlagen des Kurses.

Sie ist auch die Autorin von vier vietnamesischen Bestsellern, darunter die Reihe Xách
ba lô lên và Đi (Quảng Văn 2012, 2013). Die Reihe gehörte zu den Top 10 der FAHASA
Readers Choice Books im Jahr 2014.

Chips Expertise liegt in der Schnittmenge von Softwareentwicklung und maschinellem Lernen.
LinkedIn zählte sie zu den 10 Top Voices in Software Development in 2019,
und Top Voices in Data Science & AI im Jahr 2020.

Kolophon
Das Tier auf dem Cover von Designing Machine Learning Systems ist ein Rothuhn
Rebhuhn (Alectoris rufa), auch bekannt als französisches Rebhuhn.

Dieser seit Jahrhunderten als Wildvogel gezüchtete, wirtschaftlich wichtige und weitgehend nicht wandernde
Mitglied der Fasanenfamilie ist im westlichen Kontinentaleuropa heimisch, obwohl
Populationen wurden jedoch auch anderswo eingeführt, unter anderem in England, Irland und Neuseeland.
Neuseeland.

Das relativ kleine, aber kräftig gebaute Rothuhn zeichnet sich durch eine kunstvolle Färbung und Federzeichnung aus.
mit hellbraunem bis grauem Gefieder auf dem Rücken, einem hellrosa Bauch, einer cremefarbenen Kehle
Bauch, einer cremefarbenen Kehle, einem leuchtend roten Schnabel und rötlichen oder schwarzen Streifen an den
Flanken.

Das Rothuhn ernährt sich hauptsächlich von Samen, Blättern, Gräsern und Wurzeln, aber auch von Insekten.
Rebhühner brüten jedes Jahr in trockenen Tieflandgebieten, z. B. auf Ackerland, und legen ihre Eier in
Bodennester. Obwohl sie weiterhin in großer Zahl gezüchtet werden, gelten diese Vögel heute
aufgrund des starken Rückgangs der Populationen, der zum Teil auf die Überjagung und das Verschwinden des Lebensraums zurückzuführen ist, als nahezu bedroht.
Überjagung und das Verschwinden von Lebensräumen zurückzuführen ist. Wie alle Tiere auf O'Reilly Cover sind sie
lebenswichtig für unsere Welt.

Die Umschlagillustration stammt von Karen Montgomery und basiert auf einem antiken Strichstich
aus The Riverside Natural History. Die Umschlagschriftarten sind Gilroy Semibold und Guard-
ian Sans. Die Textschrift ist Adobe Minion Pro, die Überschriftenschrift ist Adobe Myriad
Condensed, und die Codeschrift ist Ubuntu Mono von Dalton Maag.

Lernen Sie von Experten.

Werden Sie selbst einer.

Bücher | Live-Online-Kurse

Sofortige Antworten | Virtuelle Veranstaltungen

Videos | Interaktives Lernen

Beginnen Sie auf oreilly.com.

©2022 O'Reilly Media, Inc. O'Reilly ist eine eingetragene Marke von O'Reilly Media, Inc. | 175
Dies ist ein Offline-Tool, Ihre Daten bleiben lokal und werden nicht an einen Server gesendet!
Feedback & Fehlerberichte